,record_id,title,abstract,year,label_included,duplicate_record_id
0,1,Using Developer Information as a Factor for Fault Prediction,"We have been investigating different prediction models to identify which files of a large multi-release industrial software system are most likely to contain the largest numbers of faults in the next release. To make predictions we considered a number of different file characteristics and change information about the files, and have built fully- automatable models that do not require that the user have any statistical expertise. We now consider the effect of adding developer information as a prediction factor and assess the extent to which this affects the quality of the predictions.",2007,1,
1,2,Reliability analysis of protective relays in fault information processing system in China,"The reliability indices of protective relays are first put forward in this paper. A Markov probability model is then established to evaluate the reliability of relay protection. With the state space analytical method, all the steady state probabilities and state transition probabilities can be calculated utilizing the data stored in the fault information processing system. We can get an equation that represents the influence of routine test intervals on relay unavailability. Based on this, the optimum routine test interval for protective relays can be determined. This paper also proposes an efficient method of processing large amount of information by the fault information processing system and evaluating the reliability of protective relays with it, and the corresponding software package is also developed. The application of it to an actual power system in China proves the method to be correct and effective",2006,0,
2,3,Test effort optimization by prediction and ranking of fault-prone software modules,"Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using ID3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the NASA projects data set of PROMOSE repository.",2010,1,
3,4,A Rough Set Model for Software Defect Prediction,High assurance software requires extensive and expensive assessment. Many software organizations frequently do not allocate enough resources for software quality. We research the defect detectors focusing on the data sets of software defect prediction. A rough set model is presented to deal with the attributes of data sets of software defect prediction in this paper. Appling this model to the most famous public domain data set created by the NASA's metrics data program shows its splendid performance.,2008,1,
4,5,Using Faults-Slip-Through Metric as a Predictor of Fault-Proneness,"Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (Naive Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based techniques (genetic programming and artificial immune recognition systems) on FST data collected from two large industrial projects from the telecommunication domain. Results: Using area under the receiver operating characteristic (ROC) curve and the location of (PF, PD) pairs in the ROC space, the faults slip-through metric showed impressive results with the majority of the techniques for predicting fault-prone modules at both integration and system test levels. There were, however, no statistically significant differences between the performance of different techniques based on AUC, even though certain techniques were more consistent in the classification performance at the two test levels. Conclusions: We can conclude that the faults-slip-through metric is a potentially strong predictor of fault-proneness at integration and system test levels. The faults-slip-through measurements interact in ways that is conveniently accounted for by majority of the data mining techniques.",2010,1,
5,6,Reducing Features to Improve Bug Prediction,"Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.",2009,1,
6,7,Variance Analysis in Software Fault Prediction Models,"Software fault prediction models play an important role in software quality assurance. They identify software subsystems (modules,components, classes, or files) which are likely to contain faults. These subsystems, in turn, receive additional resources for verification and validation activities. Fault prediction models are binary classifiers typically developed using one of the supervised learning techniques from either a subset of the fault data from the current project or from a similar past project. In practice, it is critical that such models provide a reliable prediction performance on the data not used in training. Variance is an important reliability indicator of software fault prediction models. However, variance is often ignored or barely mentioned in many published studies. In this paper, through the analysis of twelve data sets from a public software engineering repository from the perspective of variance, we explore the following five questions regarding fault prediction models: (1) Do different types ofclassification performance measures exhibit different variance? (2) Does the size of the data set imply a more (or less) accurate prediction performance? (3) Does the size of training subset impact model's stability? (4) Do different classifiers consistently exhibit different performance in terms of model's variance? (5) Are there differences between variance from 1000 runs and 10 runs of 10-fold cross validation experiments? Our results indicate that variance is a very important factor in understanding fault prediction models and we recommend the best practice for reporting variance in empirical software engineering studies.",2009,1,
7,8,Mutual Fault-tolerant and Standby SCADA System Based on MAS for Multi-area Centralized Control Centers,The general policies to construct the mutual fault- tolerant and standby SCADA system based on multi-agent technology for multi- area centralized control centers were presented in the paper in order to raise the safety and operational reliability of the power grid without additional equipment investment. The economic efficiency and feasibility of the system construction based on the policies are analyzed. The architecture of MAS and the function design of the Agents are introduced in detail and the specific implementation scheme and the corresponding key technologies are elucidated. The data and application fault-tolerance of SCADA system is realized to guarantee the reliability and continuity of the power grid operation.,2006,0,
8,9,Fault Management Driven Design with Safety and Security Requirements,"This paper exemplifies principles of embedded system design that props safety and security using operational errors management in frame of a dedicated Computer-Based System architecture. After reviewing basic principles of Cyber-Physical Systems as a novel slant (or marker?) to modeling and design in this domain, attention is focused on a real-world solution of a safety and security critical embedded system application offering genuine demonstration of that approach. The contribution stresses those features that distinguish the real project from a demonstration case study.",2010,0,
9,10,Reduction of faults in software testing by fault domination,"Although mutation testing is one of the practical ways of enhancing test effectiveness in software testing, it could be sometimes infeasible in practical work for a large scale software so that the mutation testing becomes time-consuming and even in prohibited time. Therefore, the number of faults assumed to exist in the software under test should be reduced so as to be able to confine the time complexity of test within a reasonable period of time. This paper utilizes the concept of fault dominance and equivalence, which has long been employed in hardware testing, for revealing a novel way of reducing the number of faults assumed to hide in software systems. Once the number of faults assumed in software is decreased sharply, the effectiveness of mutation testing will be greatly enhanced and become a feasible way of software testing. Examples and experimental results are presented to illustrate the effectiveness and the helpfulness of the technology proposed in the paper.",2007,0,
10,11,Electrical Test Structures for the Characterisation of Optical Proximity Correction,"Simple electrical test structures have been designed that will allow the characterisation of corner serif forms of optical proximity correction. The structures measure the resistance of a short length of conducting track with a right angled corner. Varying amounts of OPC can be applied to the outer and inner corners of the feature and the effect on the resistance of the track measured. These structures have been simulated and the results are presented in this paper. In addition a preliminary test mask has been fabricated which has test structures suitable for on-mask electrical measurement. Measurement results from these structures are also presented. Furthermore structures have been characterised using an optical microscope, a dedicated optical mask metrology system, an AFM scanner and finally a FIB system. In the future the test mask will be used to print the structures using a step and scan lithography tool so that they can be measured on-wafer. Correlation of the mask and wafer results will provide a great deal of information about the effects of OPC at the CAD level and the impact on the final printed features.",2007,0,
11,12,A real-time fault diagnosis system for UPS based on FFT frequency analysis,"UPS provides emergency power when utility power is not available, so the reliability of UPS is more important than inverter drive systems. In this paper, a fault diagnosis system for UPS is proposed using FFT frequency analysis of output current of inverter side of UPS under linear and nonlinear load conditions. Software PLL for precise synchronization of one period sampling and double buffer memory for real time processing are proposed. Experimental results show the increase of even harmonics including dc offset in case of fault conditions such as increase of resistance and delay or misfiring of IGBT turn-on, and prove the possibility of UPS fault diagnosis system if the criteria for fault decision are well defined.",2010,0,
12,13,5B: emerging technologies - reliable and fault-tolerant wireless sensor networks,"Wireless sensor networks create invisible interconnections with the physical world for the measurement, monitoring, and management of data from multiple sensors and probes with little constraint on location. These networks provide distributed processing, data storage, wireless communication, and dedicated application software with high reliability, inherent redundancy, failure-tolerant security and easily encrypted privacy. They have enormous potential to transform our society and are subjects of intense current research and application development. Three enabling hardware technologies which constitute a network node are microprocessors, MEMS sensors, and low-power radios. Sensor networks represent the paradigm shift in computing where they anticipate our needs and sometimes act on our behalf. The objective of this presentation is to discuss the reliable and fault-tolerant wireless sensor networks, focusing on environmental, behavioral, and biomedical areas. Special focus will be on wearable monitors and body wireless sensor network. An example of physiological monitoring by body area network will be discussed.",2005,0,
13,14,Application method of wavelets in the fault diagnosis of motion system,"In motion system, many types of faults are related with the abnormity of torque signal. A method was presented which was based on the wavelets function. The compactly supported orthonormal wavelets were introduced. Under it, the fault could be detected, and the type of fault could be diagnosed as well. For using convenience, the flow chart of using this method was also offered. On X-Y motion control system, collision experiments were implemented for test the given method. Using the sampled torque signals, the practicability was verified from the clear diagnosis of different types of collisions.",2008,0,
14,15,Fault detection in Flexible Assembly Systems using Petri net,"A significant part of the activities in a manufacturing system involve assembly tasks. Nowadays, these tasks are object of automation due to the market increasing demand for quality, productivity and variety of the products. Consequently, the automation of assembly systems should consider flexibility to face product diversification, functionalities, delivery times, and volumes involved. However, these systems are vulnerable to faults due to the characteristic of their mechanism and the complex interaction among their control devices. In this context, the present work is focused on the modeling design of flexible assembly systems control, including the occurrence of faults. The proposed method structures a sequence of steps for the models construction of assembly processes and their fault detection, based on the theory of discrete events systems and Petri net. This work use in special, production flow schema/mark flow graph (PFS/MFG) technique to describe and model the flexible assembly systems control through a rational and systematic procedure, as well as, the processes data record based on quantitative techniques for fault detection. This approach is applied to a flexible assembly systems installed and in operation to compare the effectiveness of the developed procedure.",2008,0,
15,16,An Evaluation of Similarity Coefficients for Software Fault Localization,"Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important technique for the development of dependable software. In this paper we study different similarity coefficients that are applied in the context of a program spectral approach to software fault localization (single programming mistakes). The coefficients studied are taken from the systems diagnosis/automated debugging tools Pinpoint, Tarantula, and AMPLE, and from the molecular biology domain (the Ochiai coefficient). We evaluate these coefficients on the Siemens Suite of benchmark faults, and assess their effectiveness in terms of the position of the actual fault in the probability ranking of fault candidates produced by the diagnosis technique. Our experiments indicate that the Ochiai coefficient consistently outperforms the coefficients currently used by the tools mentioned. In terms of the amount of code that needs to be inspected, this coefficient improves 5% on average over the next best technique, and up to 30% in specific cases",2006,0,
16,17,Fault-Tolerance in Universal Middleware Bridge,Universal middleware bridge (UMB) provides seamless interoperation among heterogeneous home network middleware. There have been high demands for the UMB components (UMB core and adaptors) to have fault- tolerance capabilities. This paper presents a TMO structuring approach together with new implementation techniques for the fault-tolerant TMO-replica structuring scheme called PSTR. PSTR implementations of UMB components provide fault tolerance capabilities essential in realizing high reliability for the UMB facility.,2008,0,
17,18,Performance evaluation of a fault-tolerant irregular network,"In an attempt to improve the fault-tolerance of the Omega network, this paper examines the performance of the proposed Theta network (THN), and compares it with other networks having similar characteristics. The irregular nature of the network has the inherent advantage of improving the latency of the network. Analytical results exhibit the favorable performance of THN at low cost, making the reliability degrade gracefully with time, while maintaining full-access capability over a reasonably long time. We study methods for routing requests in the presence and absence of faulty components in THN, where 50% of the requests pass at the minimum path length of 2.",2002,0,
18,19,The effect of registration error on tracking distant augmented objects,"We conducted a user study of the effect of registration error on performance of tracking distant objects in augmented reality. Categorizing error by types that are often used as specifications, we hoped to derive some insight into the ability of users to tolerate noise, latency, and orientation error. We used measurements from actual systems to derive the parameter settings. We expected all three errors to influence userspsila ability to perform the task correctly and the precision with which they performed the task. We found that high latency had a negative impact on both performance and response time. While noise consistently interacted with the other variables, and orientation error increased user error, the differences between ldquohighrdquo and ldquolowrdquo amounts were smaller than we expected. Results of userspsila subjective rankings of these three categories of error were surprisingly mixed. Users believed noise was the most detrimental, though statistical analysis of performance refuted this belief. We interpret the results and draw insights for system design.",2008,0,
19,20,Improving Bug Assignment with Bug Tossing Graphs and Bug Similarities,"In open-source software development the bug report is usually assigned to a developer for bug fixing. A large number of bug reports are tossed (reassigned) to other developers, for example because the bugs have been assigned by mistake. The tossing events increase bug-fix time. In order to quickly identify the fixer to bug reports we present an approach based on the bug tossing history and textual similarities between bug reports. This proposed approach is evaluated on Eclipse and Mozilla. The results show that our approach can significantly improve the efficiency of bug assignment: the bug resolver is often identified with fewer tossing events.",2010,0,
20,21,Doppler estimation and correction for shallow underwater acoustic communications,"Reliable mobile underwater acoustic communication systems must compensate for strong, time-varying Doppler effects. Many Doppler correction techniques rely on a single bulk correction to compensate first-order effects. In many cases, residual higher-order effects must be tracked and corrected using other methods. The contributions of this paper are evaluations of (1) signal-to-noise ratio (SNR) performance from three Doppler estimation and correction methods and (2) communication performance of Doppler correction with static vs. adaptive equalizers. The evaluations use our publicly available shallow water experimental dataset, which consists of 360 packet transmission samples (each 0.5s long) from a five-channel receiver array.",2010,0,
21,22,Forward error protection for robust video streaming based on distributed video coding principles,"This paper proposes an error resilient coding scheme that employs distributed video coding tools. A bitstream, produced by any standard motion-compensated predictive codec (MPEG-x, H.26x), is sent over an error-prone channel. In addition, a Wyner-Ziv encoded auxiliary bitstream is sent as redundant information to serve as a forward error correction code. At the decoder side, error concealed reconstructed frames are used as side information by the Wyner-Ziv decoder, and the corrected frame is used as a reference by future frames, thus reducing drift. We explicitly target the problem of rate allocation at the encoder side, by estimating the channel induced distortion in the transform domain. Experimental results conducted over a simulated error-prone channel reveal that the proposed scheme has comparable or better performance than a scheme where forward error correction codes are used. Moreover the proposed solution shows good performance when compared to a scheme that uses the intra-macroblock refresh procedure.",2008,0,
22,23,Analysis on Interruption and Plane Layout of Shear Wall for Frame-Shear Wall Structure with Top Fault Shear Wall,"According to the force-deformation characteristics of frame-shear wall structure, the calculation and analysis model of the ""Style Box"" type layout of frame-shear wall structure is advanced with the basic and simple arrangement of frame-shear wall structure. The different plane and vertical layout with interrupting shear walls of the frame-shear wall structure with top fault shear walls is discussed primarily by using the method of plane layout and vertical interruptable position being considered at the same time. Based on the main parameters of the frame-shear wall structure with top fault shear walls in different ways, it puts forward the importance of the shear wall location of the plane layout to the overall performance of the frame-shear wall structure with top fault shear walls except the lateral stiffness of the shear wall corresponding to the shear wall interrupted ratio.",2009,0,
23,24,Behavioral modular description of fault tolerant distributed systems with AADL Behavioral Annex,"AADL is an architecture description language intended for model-based engineering of high-integrity distributed systems. The AADL Behavior Annex (AADL-BA) is an extension allowing the refinement of behavioral aspects described through an AADL architectural description. When implementing Distributed Real-time Embedded system (DRE), fault tolerance concerns are integrated by applying replication patterns. We considered a simplified design of the primary backup replication pattern as a running example to analyze the modeling capabilities of AADL and its annex. Our contribution lies in the identification of the drawbacks and benefits of this modeling language for accurate description of the synchronization mechanisms integrated in this example.",2010,0,
24,25,Reducing human error in simulation in General Motors,"We focus on the steps taken to minimize human error in simulation modeling in General Motors. While errors are costly and undesirable in any field, they are especially harmful in simulation which has been struggling to gain acceptance in the business world for a long time. The solution discussed can be summarized as ""enter the data once and use the best tool for the job"".",2003,0,
25,26,Analysis of pressure and Blanchard altitude errors computed using atmospheric data obtained from an F-18 aircraft flight,"Pressure altitude is commonly utilized as an altitude reference for an inertial navigation system (INS) to damp the error growth in the inherently unstable vertical channel. A precise altitude reference for use in the INS vertical channel can be obtained using the Blanchard algorithm, which computes altitude from atmospheric pressure, temperature, aircraft ground velocity, and wind velocity data. This paper computes both the pressure and Blanchard altitudes for an entire test flight of an F-18 aircraft from the atmospheric data measured during the flight. The flight repeats 4 cycles of a climb, level-off, dive, level-off trajectory. The altitude computed from GPS during flight is considered to be the truth altitude. The errors in the pressure and Blanchard altitudes are computed and compared. In addition both altitude errors are analyzed in order to determine the scale factor, bias offset, and time delay utilizing the least square error fit method. The Blanchard altitude is a much more precise altitude reference than pressure altitude during actual flight of an F-18 aircraft.",2002,0,
26,27,Residual error models for the SOLT and SOLR VNA calibration algorithms,"Uncertainty calculation of vector network analyzers (VNAs) using the SOLT or SOLR calibration algorithms is often performed using residual directivity, match and tracking. In the literature the uncertainty equations are often stated without a derivation from a proper model equation. In this paper we derive the model equations for both the SOLT and SOLR calibration, the two cases do not result in the same model equation. The results are also compared to the commonly used expressions for uncertainty in the EA guidelines for VNA evaluation. For one-port measurements our results confirm the expressions in the EA guide but for two-ports there are significant differences. The symbolically derived model equations are verified using numerical simulations.",2007,0,
27,28,A method for dead reckoning parameter correction in pedestrian navigation system,"This paper presents a method for correcting dead reckoning parameters, which are heading and step size, for a pedestrian navigation system. In this method, the compass bias error and the step size error can be estimated during the period that the Global Positioning System (GPS) signal is available. The errors are used for correcting those parameters to improve the accuracy of position determination using only the dead reckoning system when the GPS signal is not available. The results show that the parameters can be estimated with reasonable accuracy. Moreover, the method also helps to increase the positioning accuracy when the GPS signal is available.",2003,0,
28,29,Single-stage power factor correction converter with parallel power processing for wide line and load changes,"A new single-phase single-stage power factor correction converter with a simple auxiliary circuit is proposed. Using parallel power processing, this converter can be operated in wide line and load changes while limiting the link voltage below 400 V. Experimental results show that the measured power factor and efficiency are about 0.98 and 81%, respectively, at rated condition and the auxiliary circuit to reduce the link voltage is effective",2002,0,
29,30,Using variable-length error-correcting codes in MPEG-4 video,Reversible variable length (RVL) codes are used in MPEG-4 video coding to improve its error resilience. Algorithms used to design variable-length error-correcting (VLEC) codes are modified so as to construct efficient RVL codes with a smaller average length than those found in the literature. It is also shown that RVL codes are a special (weak) class of VLEC codes. Consequently more powerful VLEC codes can be used in the MPEG-4 codec and it is shown that performance gains of up to 20 dB in peak signal to noise ratio (PSNR) can be obtained using a soft-decision sequential decoder with relatively simple VLEC codes. This increase in performance is obtained at the expense of an order of magnitude increase in decoding complexity,2005,0,
30,31,Restoration of Directional Overcurrent Relay Coordination in Distributed Generation Systems Utilizing Fault Current Limiter,"A new approach is proposed to solve the directional overcurrent relay coordination problem, which arises from installing distributed generation (DG) in looped power delivery systems (PDS). This approach involves the implementation of a fault current limiter (FCL) to locally limit the DG fault current, and thus restore the original relay coordination. The proposed restoration approach is carried out without altering the original relay settings or disconnecting DGs from PDSs during fault. Therefore, it is applicable to both the current practice of disconnecting DGs from PDSs, and the emergent trend of keeping DGs in PDSs during fault. The process of selecting FCL impedance type (inductive or resistive) and its minimum value is illustrated. Three scenarios are discussed: no DG, the implementation of DG with FCL and without FCL. Various simulations are carried out for both single- and multi-DG existence, and different DG and fault locations. The obtained results are reported and discussed.",2008,0,
31,32,The Design Of Embedded Bus monitoring And Fault Diagnosis System Based On Protocol SAE J1939,"Embedded bus monitoring and fault diagnosis system, which was based on protocol SAE J1939 was designed in this paper. And this system took the 32-bit embedded one as a hardware platform, customized a WinCE6.0 operation system and used EVC as the tool to design the embedded application. The functions of CAN communication, protocol defamations etc were realized. Good human-computer interaction is developed and the system has already been applied on the bus.",2010,0,
32,33,A Fault Analysis and Classifier Framework for Reliability-Aware SRAM-Based FPGA Systems,"This paper presents a new framework for the analysis of SRAM-based FPGA systems with respect to their dependability properties against single, multiple and cumulative upsets errors. The aim is to offer an environment for performing fault classification and error propagation analyses for designed featuring fault detection or tolerance techniques against soft errors, where the focus is not only the overall achieved fault coverage, but an understanding of the fault/error relation inside the internal elements of the system. We propose a fault analyzer/classifier laying on top of a classical fault injection engine, used to monitor the evolution of the system after a fault as occurred, with respect to the applied reliability-oriented design technique. The paper introduces the framework and reports some experimental results of its application to a case study, to highlight the benefits of the proposed solution.",2009,0,
33,34,Stress wave analysis of turbine engine faults,"Stress Wave Analysis (SWAN) provides real-time measurement of friction and mechanical shock in operating machinery. This high frequency acoustic sensing technology filters out background levels of vibration and audible noise, and provides a graphic representation of machine health. By measuring shock and friction events, the SWAN technique is able to detect wear and damage at the earliest stages and is able to track the progression of a defect throughout the failure process. This is possible because as the damage progresses, the energy content of friction and shock events increases. This `stress wave energy' is then measured and tracked against normal machine operating conditions. This paper describes testing that was conducted on several types of aircraft and industrial gas turbine engines to demonstrate SWAN's ability to accurately detect a broad range of discrepant conditions and characterize the severity of damage",2000,0,
34,35,High-level vulnerability over space and time to insidious soft errors,"The integrity of computational results is being increasingly threatened by soft errors, especially for computations that are large-scale or performed under harsh conditions. Existing methods for soft error estimation do not clearly characterize the vulnerability associated with a particular result. 1) We propose a metric which captures the intrinsic vulnerability over space and time (VST) to soft errors that corrupt computational results. The method of VST estimation bridges the gap between the inherently low-level faults and the high-level computational failures that they eventually cause. 2) We define a model of an insidious soft error and try to clear up confusion around the concept of silent data corruption. 3) We present experimental results from three vulnerability studies involving floating-point addition, CORDIC, and FFT computations. The results show that traditional vulnerability metrics can be confounded by seemingly reliable but inefficient implementations which actually incur high vulnerability per computation. The VST method characterizes vulnerability accurately, provides a figure-of-merit for comparing alternative implementations of an algorithm, and in some cases uncovers pronounced and unexpected fluctuations in vulnerability.",2008,0,
35,36,Image Defect Recognition Based on Rough Set,"This paper applies rough set theory to recognition system for image defect, and designs a decision algorithm on rough set suitable for image defect recognition. Firstly, the image is made regionalization and sequential discrete set is proposed, the continuous attributes of image is discretized. Then the decision table model on discrete condition attributes and decision attributes is constructed. Further the condition attributes significance function and reduction algorithm is given. A novel approach for decision rule analysis and rough set recognition is proposed. Finally, this paper takes the example for fabric defect recognition to validate these algorithms. The result shows the rough set algorithm is effective for image defect recognition with less calculation and fast speed.",2009,0,
36,37,Using design patterns and constraints to automate the detection and correction of inter-class design defects,"Developing code free of defects is a major concern for the object oriented software community. The authors classify design defects as those within classes (intra-class), those among classes (inter-classes), and those of semantic nature (behavioral). Then, we introduce guidelines to automate the detection and correction of inter-class design defects. We assume that design patterns embody good architectural solutions and that a group of entities with organization similar, but not equal, to a design pattern represents an inter-class design defect. Thus, the transformation of such a group of entities, such that its organization complies exactly with a design pattern, corresponds to the correction of an inter-class design defect. We use a meta-model to describe design patterns and we exploit the descriptions to infer sets of detection and transformation rules. A constraint solver with explanations uses the descriptions and rules to recognize groups of entities with organizations similar to the described design patterns. A transformation engine modifies the source code to comply with the recognized distorted design patterns. We apply these guidelines on the Composite pattern using PTIDEJ, our prototype tool that integrates the complete guidelines",2001,0,
37,38,Error localization for robust video transmission,"The convergence of Internet, multimedia and mobile applications has led to an increased demand for efficient and reliable video data transmission over heterogeneous networks. Due to their coding efficiency, variable-length codes (VLC) are usually employed in the entropy coding stage of video compression standards. However, error propagation is a major problem associated with VLC. We propose the use of a class of self-synchronizing VLC (SSVLC) to achieve the dual goal of optimal coding efficiency and optimal error localization. Performance evaluation has confirmed that the use of SSVLC provides better performance than standard VLC techniques.",2002,0,
38,39,On the error-control coding techniques used in GSM/EDGE radio access networks,In this paper the error-control coding techniques used in GSM/EDGE Radio Access Network (GERAN) are considered. Application of coding schemes is restricted by the corresponding traffic channels (TCH). Knowing the general scheme is necessary for modeling the work of the complete error-control system. This knowledge can be useful for the implementation of educational software used for the investigation of the properties of different codecs and their characteristics in radio channels using various radio channel models.,2004,0,
39,40,A Multi-Agent Fault Detection System for Wind Turbine Defect Recognition and Diagnosis,This paper describes the use of a combination of anomaly detection and data-trending techniques encapsulated in a multi-agent framework for the development of a fault detection system for wind turbines. Its purpose is to provide early error or degradation detection and diagnosis for the internal mechanical components of the turbine with the aim of minimising overall maintenance costs for wind farm owners. The software is to be distributed and run partly on an embedded microprocessor mounted physically on the turbine and on a PC offsite. The software will corroborate events detected from the data sources on both platforms and provide information regarding incipient faults to the user through a convenient and easy to use interface.,2007,0,
40,41,Motion correction of PET images using realignment for intraframe movement,"A method is presented for the motion correction of PET images using realignment for intraframe movement. A newly introduced aspect of the method is that it corrects not only interframe but also intraframe movement, using currently used PET images (which are not corrected for intraframe movement) and motion tracking data. Although our method requires motion tracking data, it does not require very short time image acquisition nor list mode data acquisition. So, it is applicable to currently used PET images. In our method the following hypothesis is assumed. That is if no movement happens, there exists a linear model such that counts of each voxel per unit time follows the model. Model parameters may depend on the voxels. In a simple example, our method successfully corrected motion artifact and estimates the parameters accurately. It may give a simple and practical solution to the motion correction problems.",2003,0,
41,42,Study on Data Mining for Grounding Fault Line Selection in 6kV Ineffectively Grounded System of Coal Mine,"A great amount of fault wave has been recorded by the devices for detecting phase-to-ground faults in ineffectively grounded systems. However, a better method hasn't found for effectively taking advantage of these data to improve the result of fault line selection. Data mining techniques can be used for fault line selection in ineffectively grounded system to gain knowledge from the existing data and to improve the technique of fault line selection. This paper briefly describes the principles, methods and implementation of data mining techniques, classifies the fault samples of ineffectively grounded systems by using clustering analysis method, employs different fault line selection methods according to the types of faults, and consequently provides a set of criteria for modeling of typical ineffectively grounded systems and verifying the validity of real-time fault line selections. The validity of the methods has been convinced by the calculation using the data obtained from the real performance of a substation in coal mine. It has been shown to be promising to employ the data mining techniques in ineffectively grounded systems fault detection. This paper provides very good methods for resolving the difficulties with onsite tests, enhancing the techniques of fault line selection and establishing the fault detection management systems.",2010,0,
42,43,Fault-Tolerant Coverage Planning in Wireless Networks,"Typically wireless networks coverage is planned with static redundancy to compensate temporal variations in the environment. As a result, the service still is delivered but the network coverage could have entered a critical state, meaning that further changes in the environment may lead to service failure. Service failures have to be explicitly notified by the applications. Therefore, in this paper we propose a methodology for fault-tolerant coverage planning. The idea is detecting the critical state and removing it by on-line system reconfiguration, and restoration of the original static redundancy. Even in case of a failure the system automatically generates a new configuration to restore the service, leading to shorter repair times. We describe how this approach can be applied to wireless mesh networks, often used in industrial applications like manufacturing, automation and logistics. The evaluation results show that the underlying model used for error detection and system recovery is accurate enough to correctly identify the system state.",2008,0,
43,44,Detection of high impedance fault in distribution feeder using wavelet transform and artificial neural networks,"This work presents a novel analysis method that can simulate the potential effect of high impedance fault (HIF). The proposed method offers a new scheme for protecting the overhead distribution feeder. The wavelet transform (WT) method was successfully applied in many fields. The characteristics of scaling and translation of WT can be used to identify stable and transient signals. Discrete wavelet transforms (DWT) are initially used to extract distinctive features of the voltage and current signals, and are transformed into a series of detailed and approximated wavelet components. The coefficients of variation of the wavelet components are then calculated. This information is introduced into the training artificial neural networks (ANN) to determine an HIF from the operations of the switches. The simulated results clearly reveal that the proposed method can accurately identify the HIF in the distribution feeder.",2004,0,
44,45,A hierarchical framework for fault propagation analysis in complex systems,"In complex systems, there are few critical failure modes. Prognostic models are focused at predicting the evolution of those critical faults, assuming that other subsystems in the same system are performing according to their design specifications. In practice, however, all the subsystems are undergoing deterioration that might accelerate the time evolution of the critical fault mode. This paper aims at analyzing this aspect, i.e. interaction between different fault modes in various subsystems, of the failure prognostic problem. The application domain focuses on an aero propulsion system of the turbofan type. Creep in the high-pressure turbine blade is one of the most critical failure modes of aircraft engines. The effects of health deterioration of low-pressure compressor and high-pressure compressor on creep damage of high-pressure turbine blades are investigated and modeled.",2009,0,
45,46,Data Mining Using Rough Sets and Orthogonal Signal Correction-Orthogonal Partial Least Squares Analysis,"The paper put forward Data mining using rough sets and orthogonal signal correction-orthogonal partial least squares analysis (RS-OSC-OPLS/O2PLS). first, dimensionality reduction and de-noising with rough sets and orthogonal signal correction;second, Data mining using orthogonal partial least squares analysis. The method was proved to be feasible and effective after tested with 13 kinds of nationalities crowds data.",2010,0,
46,47,Estimation of Systematic Errors of MODIS Thermal Infrared Bands,"This letter reports a statistical method to estimate detector-dependent systematic error in Moderate Resolution Imaging Spectroradiometer (MODIS) thermal infrared (TIR) Bands 20-25 and 27-36. There exist scan-to-scan overlapped pixels in MODIS data. By analyzing a sufficiently large amount of those most overlapped pixels, the systematic error of each detector in the TIR bands can be estimated. The results show that the Aqua MODIS data are generally better than the Terra MODIS data in 160 MODIS TIR detectors. There are no detector-dependent systematic errors in Bands 31 and 32 for both Terra and Aqua MODIS data. The maximum detector errors are 3.00 K in Band 21 of Terra and -8.15 K in that of Aqua for brightness temperatures of more than 250 K",2006,0,
47,48,Comparing Web Services Performance and Recovery in the Presence of Faults,"Web-services are supported by a complex software infrastructure that must ensure high performance and availability to the client applications. Web services industry holds a well established platform for performance benchmarking (e.g., TPC-App and SPEC jAppServer2004 benchmarks). In addition, several studies have been published recently by main vendors focusing web services performance. However, as peak performance evaluation has been the main focus, the characterization of the impact of faults in such systems has been largely disregarded. This paper proposes an approach for the evaluation and comparison of performance and recovery time in web services infrastructures. This approach is based on fault injection and is illustrated through a concrete example of benchmarking three alternative software solutions for web services deployment.",2007,0,
48,49,Evidence-Based Analysis and Inferring Preconditions for Bug Detection,"An important part of software maintenance is fixing software errors and bugs. Static analysis based tools can tremendously help and ease software maintenance. In order to gain user acceptance, a static analysis tool for detecting bugs has to minimize the incidence of false alarms. A common cause of false alarms is the uncertainty over which inputs into a program are considered legal. In this paper we introduce evidence-based analysis to address this problem. Evidence-based analysis allows one to infer legal preconditions over inputs, without having users to explicitly specify those preconditions. We have found that the approach drastically improves the usability of such static analysis tools. In this paper we report our experience with the analysis in an industrial deployment.",2007,0,
49,50,"PSoC design in GM(1,1) error analysis and its application in temperature prediction","In the study of prediction filed, no matter what methods we used, the main purpose is to minimize the prediction error; however, the goals cannot be fulfilled completely. Even we choose GM(1,1) model, which in the newest soft computing method, we also need to minimize the prediction error. Hence, in this paper, we focus on the influence parameter alpha in GM(1,1) model in the first, then, analyze the characteristics of alpha step by step, and use numerical method to find the prediction error corresponding with alpha value. Second, after the mathematics model is presented, we use PSoC to design a GM(1,1) error analysis model, which based on the characteristic of GM(1,1) model. Also an example, which is temperature prediction case is given to assist us to implement our approach in the final section.",2008,0,
50,51,Calculation of transverse voltages of communication lines induced by the fault current of power system,"A double-line model is presented to calculate the transverse voltage of communication lines induced by the fault current of power lines. By dividing the communication line into several fictitious segments, a chain composed by the coupling P1-type circuit with distributed source is formed. The enhanced node voltage analysis (ENVA) is also developed in order to evaluate such a kind of model. The ENVA cuts the number of nodes down greatly because of treating the active and coupling impedance branches as a whole. In addition, the transverse voltages in time domain can be obtained easily from those calculated in frequency domain by means of fast Fourier transform. The numerical examples prove the validity and efficiency of the method by comparison with analytical results. The model is of significance to the design and the rights-of-way selection of power lines and communication lines.",2002,0,
51,52,General review of fault diagnostic in wind turbines,"Global wind electricity-generating capacity increased by 28.7 percent in 2008 to 120,798 Gigawatts. This represents a twelve-fold increase from a decade ago, when world wind-generating capacity stood at less than 5 GW [1]. With wind becoming a key part of the electrical mix in Denmark (20% with 3.1 GW), Spain (8% with 10 GW), and Germany (6% with 18.4 GW), wind turbine reliability is having a bigger effect on overall electrical grid system performance and reliability [1]. This shows the impact of faults and downtime on the reliability of wind turbine especially for offshore wind farms which although are some of the most environmentally friendly and efficient methods to generate electricity in the world. However, the maintenance costs are high because of their remote location. This can amount to as much as 25 to 30% of the total energy production [2]. The aim of this paper is to present an overview of fault detection in wind turbines, study and analyze the faults and their root-causes. The paper also explores different techniques used in early fault detection to form base information for future work to build a general fault diagnostic scheme for wind turbines.",2010,0,
52,53,Reconfigurable context-free grammar based data processing hardware with error recovery,"This paper presents an architecture for context-free grammar (CFG) based data processing hardware for re-configurable devices. Our system leverages on CFGs to tokenize and parse data streams into a sequence of words with corresponding semantics. Such a tokenizing and parsing engine is sufficient for processing grammatically correct input data. However, most pattern recognition applications must consider data sets that do not always conform to the predefined grammar. Therefore, we augment our system to detect and recover from grammatical errors while extracting useful information. Unlike the table look up method used in traditional CFG parsers, we map the structure of the grammar rules directly onto the field programmable gate array (FPGA). Since every part of the grammar is mapped onto independent logic, the resulting design is an efficient parallel data processing engine. To evaluate our design, we implement several XML parsers in an FPGA. Our XML parsers are able to process the full content of the packets up to 3.59 Gbps on Xilinx Virtex 4 devices",2006,0,
53,54,Autonomous Fault Recovery Technology for Achieving Fault-Tolerance in Video on Demand System,"With the advances of compression technology, storage devices and networks, video on demand (VoD) service is becoming popular. The system needs to provide continuous service and heterogeneous service levels for users. However, these requirements cannot be satisfied in conventional VoD system which is constructed on redundant content servers and centralized management. In this paper, autonomous VoD system is proposed to meet the requirements. The system is constructed on faded information field architecture. Under the proposed architecture, autonomous fault detection and fault recovery technologies are proposed to achieve fault-tolerance for continuous service. The effectiveness of the proposed technologies are proved through simulation. The results show that an average of 30% improvement in recovery time and users' video service can be recovered without stopping compared with conventional VoD system",2006,0,
54,55,Event-based fault detection of manufacturing cell: Data inconsistencies between academic assumptions and industry practice,"Some problems with event-based faults in manufacturing systems cannot be handled by existing fault detection solutions, including finding faults in event-based data for systems for which limited information is known. A new fault detection solution that finds faults in event-based data using model generation is presented here. This solution assumes that some information is known about the system from its design information and data structure. An example application of this solution is presented for a Ford machining cell that has been experiencing a gantry waiting problem. In the course of this example application, five inconsistencies were found between relatively common academic assumptions made by this fault detection solution (as well as others) and the actual cell's set-up and data. These inconsistencies and possible means of addressing them are discussed. Some of these means to resolve the inconsistencies have been implemented, and preliminary results in generating models using the fault detection solution are presented.",2010,0,
55,56,Error monitoring for optical metropolitan network services,"Service providers rely on performance monitoring capabilities not only to ensure integrity of their network but also to support service-level agreements with their customers. The depth of monitoring is directly tied to the technology and protocol used in the transport layer of the network. Next-generation services based on enterprise-centric, non-SONET/SDH protocols, such as Gigabit Ethernet and Fibre Channel, as well as managed protocol-independent wavelength transport, have created a number of challenges for service providers because of the differences in how error monitoring is performed. In this article we describe and compare protocol-dependent and protocol-independent error monitoring techniques that apply to these service offerings",2002,0,
56,57,A method of inverter circuit fault diagnosis based on BP neural network and D-S evidence theory,"With the study and analysis on intelligent fault diagnosis for inverting circuit, an improved diagnosis method combined BP neuron network and D-S evidence theory was proposed. Each measuring point was extracted by BP neural network to obtain the local diagnosis, which is adopted to design the belief function of D-S evidence theory. Multiple monitoring points' information is fused to receive the comprehensive global diagnosis result. The experimental results show that this method has the better feasibility and effectiveness on fault diagnosis in inverter's key components-inverting circuit.",2010,0,
57,58,On fault diagnosis tree and its control flow,"The coarse-grained organization of the existing fault diagnosis scheme can not realize the automatic and intelligent diagnosis. This paper provides a tree structure of fault diagnosis scheme named T04FDS, which integrates the fault classification relations and nesting relation of part and whole. By building the state transform system to represent the diagnosis process, and describing the control flow with operations of stack. Furthermore the thesis presents the physical realization of T04FDS fault diagnosis, and finally proves the feasibility of this method by giving an example of fault diagnosis scheme for computer wireless network card.",2009,0,
58,59,Master-Slave TMR Inspired Technique for Fault Tolerance of SRAM-Based FPGA,"In order to increase reliability and availability of Static-RAM based field programmable gate arrays (SRAM-based FPGAs), several methods of tolerating defects and permanent faults have been developed and applied. These methods are not well adapted for handling high fault rates for SRAM based FPGAs. In this paper, both single and double faults affecting configurable logic blocks (CLBs) are addressed. We have developed a new fault-tolerance technique that capitalizes on the partial reconfiguration capabilities of SRAM-based FPGA. The proposed fault-tolerance method is based on triple modular redundancy (TMR) combined with master-slave technique, and exploiting partial reconfiguration to tolerate permanent faults. Simulation results on reliability improvement corroborate the efficiency of the proposed method and prove that it compares favorably to previous methods.",2010,0,
59,60,Robust Speech Recognition Using a Cepstral Minimum-Mean-Square-Error-Motivated Noise Suppressor,"We present an efficient and effective nonlinear feature-domain noise suppression algorithm, motivated by the minimum-mean-square-error (MMSE) optimization criterion, for noise-robust speech recognition. Distinguishing from the log-MMSE spectral amplitude noise suppressor proposed by Ephraim and Malah (E&M), our new algorithm is aimed to minimize the error expressed explicitly for the Mel-frequency cepstra instead of discrete Fourier transform (DFT) spectra, and it operates on the Mel-frequency filter bank's output. As a consequence, the statistics used to estimate the suppression factor become vastly different from those used in the E&M log-MMSE suppressor. Our algorithm is significantly more efficient than the E&M's log-MMSE suppressor since the number of the channels in the Mel-frequency filter bank is much smaller (23 in our case) than the number of bins (256) in DFT. We have conducted extensive speech recognition experiments on the standard Aurora-3 task. The experimental results demonstrate a reduction of the recognition word error rate by 48% over the standard ICSLP02 baseline, 26% over the cepstral mean normalization baseline, and 13% over the popular E&M's log-MMSE noise suppressor. The experiments also show that our new algorithm performs slightly better than the ETSI advanced front end (AFE) on the well-matched and mid-mismatched settings, and has 8% and 10% fewer errors than our earlier SPLICE (stereo-based piecewise linear compensation for environments) system on these settings, respectively.",2008,0,
60,61,On-board fault-tolerant SAR processor for spaceborne imaging radar systems,"A real-time high-performance and fault-tolerant FPGA-based hardware architecture for the processing of synthetic aperture radar (SAR) images has been developed for advanced spaceborne radar imaging systems. In this paper, we present the integrated design approach, from top-level algorithm specifications, system architectures, design methodology, functional verification, performance validation, down to hardware design and implementation.",2005,0,
61,62,Fault-Tolerant BPEL Workflow Execution via Cloud-Aware Recovery Policies,"BPEL is the de facto standard for business process modeling in today's enterprises and is a promising candidate for the integration of business and scientific applications that run in Grid or Cloud environments. In these distributed infrastructures, the occurrence of faults is quite likely. Without sophisticated fault handling, workflows are frequently abandoned due to software or hardware failures, leading to a waste of CPU hours. The fault handling mechanisms provided by BPEL are well suited for handling faults of the business logic, but infrastructure-induced errors should be handled automatically to avoid over-complication of workflow design and keep concerns separated. This paper identifies classes of faults that can be resolved automatically by the infrastructure, and provides a policy-based approach to configure this automatic behavior without the need for adding explicit fault handling mechanisms to the BPEL process. The proposed approach provides automatic redundancy of services using a Cloud infrastructure to allow substitution of defective services. An implementation based on the ActiveBPEL engine and Amazon's Elastic Compute Cloud is presented.",2009,0,
62,63,Neural fault isolator for Wireless Sensor Networks,"Wireless sensor networks are emerging as an innovative technology that can help to improve business processes. In such environments malfunctions and break-down states must be efficiently diagnosed to reduce to a minimum the economic losses. In this paper we present a fault isolation approach based on neural networks, which utilizes only a minimum set of information such as the sensor value, node ID and timestamp as inputs. We believe that this information set could be provided by any WSN regardless of its specific implementation. This abstraction makes the fault isolator generically applicable in enterprise business systems. The neural fault isolator was evaluated in a trial with 36 nodes and has proved to be highly efficient in the isolation of failed components.",2008,0,
63,64,Test Compaction for Transition Faults under Transparent-Scan,"Transparent-scan was proposed as an approach to test generation and test compaction for scan circuits. Its effectiveness was demonstrated earlier in reducing the test application time for stuck-at faults. We show that similar advantages exist when considering transition faults. We first show that a test sequence under the transparent-scan approach can imitate the application of broadside tests for transition faults. Test compaction can proceed similar to stuck-at faults by omitting test vectors from the test sequence. A new approach for enhancing test compaction is also described, whereby additional broadside tests are embedded in the transparent-scan sequence without increasing its length or reducing its fault coverage",2006,0,
64,65,Constrained free form deformation based algorithm for geometric distortion correction of echo planar diffusion tensor images,"In order to differentiate between normal and abnormal variations in brain diffusion tensor images, it is necessary to develop medical atlases. Atlas creation requires removal of spatial distortions in individual subject diffusion weighted images. In this paper we suggest a new approach using non-linear warping based on optic flow to map both baseline and diffusion weighted echo planar images to the anatomically correct T2 weighted spin echo image. The method is readily implemented and does not require a pre-processing step of rigid alignment. A global histogram matching precedes the base line EP image correction. A Markov random field based classification algorithm was implemented to cluster T2 weighted images into four different tissue type classes. This information was then used to synthesize diffusion based image models used in the warping algorithm to correct the geometric distortions in the diffusion weighted EP images.",2004,0,
65,66,Detecting Single and Multiple Faults Using Intelligent DSP and Agents,"In this paper intelligent agents and DSP techniques are integrated to detect single and multiple faults in electrical circuits. Agents are used to model the AC electrical circuit. A DSP engine is embedded into the agents to analyse the signals, i.e. the energy transfer between the physical components. An AC to DC rectifier circuit is chosen as test-bed for the proposed solutions",2006,0,
66,67,Real-time model based sensor fault tolerant control system on a chip,"In this paper, we proposed a model based sensor fault tolerant control system embedded in a generic PIC microcontroller for use in a temperature control system. The model based fault tolerant control algorithm is embedded in the microcontroller for stand-alone real-time implementation. The algorithm consists of a PID controller element (for nominal control) and a fault compensating element. Results from simulations and real time implementation are shown to demonstrate the ease of real time implementation.",2009,0,
67,68,Efficiency enhancement of microstrip patch antenna with defected ground structure,"Defected ground structures (DGS) have been developed to improve characteristics of many microwave devices. Although the DGS has advantages in the area of the microwave filter design, microstrip antenna design for different applications such as cross polarization reduction and mutual coupling reduction etc., it can also be used for the antenna size reduction. The etching of a defect in the ground plane is a unique technique for the antenna size reduction. The DGS is easy to be an equivalent LC resonator circuit. The value of the inductance and capacitance depends on the area and size of the defect. By varying the various dimensions of the defect, the desired resonance frequency can be achieved. In this paper the effect of dumbbell shaped DGS, to the size reduction of a microstrip patch antenna is investigated. Then a cavity backed structure is used to increase the efficiency of the microstrip patch antenna, in which the electric walls are placed surrounding the patch. The simulation is carried out with IE3D full wave EM simulator.",2008,0,
68,69,Effects of Defects on the In-plane Dynamic Energy Absorption of Metal Honeycombs,"The in-plane dynamic energy absorption of metal honeycombs with defects consisting of missing cells are analyzed using explicit dynamic finite element method. Two types of structural defects (a single defect located in the center of the model and a double defect) are firstly introduced. Then the influence of the defects and the impact velocities on the energy absorption abilities of metal honeycombs is investigated. Researches show that single and isolated defects reduce the absorbed energy of cellular materials. The separation distance between two defects has little effect on the dynamic energy absorption, while the size of the single defect has great influence on it. These results will provide some useful guides for the safety evaluation and the dynamic energy absorption design of metal honeycombs.",2010,0,
69,70,Fault Diagnosis on Board for Analog to Digital Converters,"This paper describes a general purpose high reliable data acquisition system which allows A/D converter testing by histogram and two tone tests for the fault diagnosis on the same board. A reliability analysis has been carried out in order to optimize the project, the components choice and redundancy configuration. The software has been written in Matlab and LabVIEW, with an easy graphical user interface.",2007,0,
70,71,Safing and fault protection for the MESSENGER mission to Mercury,"The MErcury Surface, Space ENvironment, GEochemistry, and Ranging (MESSENGER) mission is a NASA Discovery-class, deep-space mission to orbit the planet Mercury. Its purpose is to map the planet surface using various scientific instruments and explore the interior of the planet using measurements from instruments such as a magnetometer and observation of planetary libration. This paper discusses the architecture and implementation of the methods by which faults in the MESSENGER spacecraft are detected and the effects of those faults mitigated. The responsibility of the redundant Fault Protection Processors (FPPs) is to detect faults and take autonomous corrective actions that will keep the spacecraft healthy and safe.",2002,0,
71,72,A new error concealment algorithm for H.264 video transmission,"In this paper, a new error concealment algorithm for the new coding standard H.264 is presented. The algorithm consists of a block size determination step to determine the size type of the lost block and a motion vector recovery step to find the lost motion vector from multiple reference frames. The main feature of this algorithm are as follows. In the block size determination step, we propose a criterion to determine the size type of the lost block from the current frame. In the motion vector recovery step, the optimal motion vector for the lost block chosen from multiple previous reference frames with the minimum value of the side match distortion. The proposed algorithm not only can determine the most correct mode for the lost block, but also can save much more computation time for motion vector recovery. Experimental results show that the proposed algorithm achieves 0.47 dB improvement over the conventional VM method.",2004,0,
72,73,An EMF activity tree based BPEL defect pattern testing method,"For testing BPEL defects efficiently, a novel BPEL defect pattern testing architecture based on the EMF activity tree technology is proposed. The EMF activity tree that is similar to abstract syntax tree is used to describe the BPEL service process structure. The mapping method from the DOM object tree of a BPEL file to the EMF activity tree and the recursive algorithm to generate an EMF activity tree are represented in detail. A typical EMF activity tree is shown and the visitor design pattern based traversal method is stated. The directions to enhance this technology are illustrated finally.",2010,0,
73,74,Fault tolerant generator systems for wind turbines,"The objective of this paper is to review the possibilities of applying fault tolerance in generator systems for wind turbines based on what has been presented in the literature. In order to make generator systems fault tolerant in a suitable way, it is necessary to gain insight into the probability of different failures, so that suitable measures can be taken. Therefore, a literature survey of reliability of wind turbines, electrical machines and power electronic converters is given. Five different ways of achieving fault tolerance identified in the literature are discussed together with their applicability for wind turbines: (1) converters with redundant semiconductors, (2) fault tolerant converter topologies, (3) fault tolerance by increasing the number of phases, (4) fault tolerance of switched reluctance machines, and (5) design for fault tolerance of PM machines and converters. Because converters fail more often than machines, it makes sense to use of fault tolerant converter topologies. Increasing the number of phases is a useful form of fault tolerance because it can be achieved without increasing the cost significantly.",2009,0,
74,75,Fault recovery in linear systems via intrinsic evolution,"We investigate fault recovery using reconfiguration for analog linear feedback control systems. We assume any faults occur only within the linear system and accessibility to its internal circuitry is impossible. Consequently, the only way to restore service - even degraded service - is by inserting a compensation network into the control loop. System failures are manifested by a change in the original bandwidth. The compensators are evolved intrinsically.",2004,0,
75,76,Distance estimation technique for single line-to-ground faults in a radial distribution system,A simple yet powerful algorithm to estimate the distance to a single line-to-ground fault on a distribution feeder is proposed. The algorithm is implemented in a power monitor instrument and the estimation of distance is made within the instrument itself. The algorithm is designed to work where the only available data to the instrument are a single point measurement taken at the substation and the positive and zero-sequence impedance of the primary feeder. The single point measurement consists of three-phase voltage and current waveforms. Network topology data are not available to the algorithm. The new technique accommodates computational power and data constraints while maintaining adequate accuracy of the measurements,2000,0,
76,77,Optimal Wavelet Design for Multicarrier Modulation with Time Synchronization Error,"Wavelet packet based multi-carrier modulation (WPMCM) is an efficient transmission technique which has the advantage of being a generic scheme whose characteristics can be customized to fulfill a design specification. However, WPMCM is sensitive and vulnerable to time synchronization errors because its symbols overlap. In this paper, we design new wavelets to alleviate WPMCM's vulnerability to timing errors. First, a filter design framework that facilitates the development of new wavelet bases is built. Then the expressions for errors due to time offset in WPMCM transmission are derived and stated as a convex optimization problem. Finally, an optimal filter that best handles these deleterious effects is designed by means of semi definite programming (SDP). Through computer simulations the performance advantages of the newly designed filter over standard wavelet filters are proven.",2009,0,
77,78,Optimum design of a class of fault tolerant isotropic Gough-Stewart platforms,"Optimal geometric design is of key importance to the performance of a manipulator. First, this paper extends the work in Y. Yi, et al., (2004) to generate a class of isotropic Gough-Stewart platforms (GSPs) with an odd number of struts. Then, it develops methods for finding a highly fault tolerant GSP from that class. Two optimization criteria are considered, isotropy and fault tolerance. To meet the mission critical needs imposed by laser weapons applications, nine-strut isotropic GSPs that retain kinematic stability despite the loss of any three struts are found. First, we develop methods for generating a five parameter class of isotropic nine-strut GSPs. Next, new measures of fault tolerance are introduced and used to optimize the free parameter space. The optimized design is much more fault tolerant than the GSP currently baselined for the airborne laser.",2004,0,
78,79,Designing Fault Tolerant Web Services Using BPEL,"The Web services technology provides an approach for developing distributed applications by using simple and well defined interfaces. Due to the flexibility of this architecture, it is possible to compose business processes integrating services from different domains. This paper presents an approach, which uses the specification of services orchestration, in order to create a fault tolerant model combining active and passive replication technique. This model support fault of crash. The characteristics and the results obtained by implementing this model are described along this paper.",2008,0,
79,80,Sub-cycle detection of incipient cable splice faults to prevent cable damage,"This paper presents an innovative method for subcycle detection of incipient cable failures caused by self-clearing faults occurring in cable splices due to insulation breakdown. Because of their short duration, conventional overcurrent protection will not detect these types of faults. The protection scheme described in this paper has been integrated into a universal relay platform. It is fast enough to operate for sub-cycle faults and has the logic to differentiate them from other types of faults. Imminent cable failure can be detected",2000,0,
80,81,"Software-Based Online Detection of Hardware Defects Mechanisms, Architectural Support, and Evaluation","As silicon process technology scales deeper into the nanometer regime, hardware defects are becoming more common. Such defects are bound to hinder the correct operation of future processor systems, unless new online techniques become available to detect and to tolerate them while preserving the integrity of software applications running on the system. This paper proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extension (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible: testing techniques can be modified/upgraded in the field to trade off performance with reliability without requiring any change to the hardware. We evaluated our technique on a commercial chip-multiprocessor based on Sun's Niagara and found that it can provide very high coverage, with 99.22% of all silicon defects detected. Moreover, our results show that the average performance overhead of software-based testing is only 5.5%. Based on a detailed RTL-level implementation of our technique, we find its area overhead to be quite modest, with only a 5.8% increase in total chip area.",2007,0,
81,82,Optimizing Joint Erasure- and Error-Correction Coding for Wireless Packet Transmissions,"To achieve reliable packet transmission over a wireless link without feedback, we propose a layered coding approach that uses error-correction coding within each packet and erasure-correction coding across the packets. This layered approach is also applicable to an end-to-end data transport over a network where a wireless link is the performance bottleneck. We investigate how to optimally combine the strengths of error- and erasure-correction coding to optimize the system performance with a given resource constraint, or to maximize the resource utilization efficiency subject to a prescribed performance. Our results determine the optimum tradeoff in splitting redundancy between error-correction coding and erasure-correction codes, which depends on the fading statistics and the average signal to noise ratio (SNR) of the wireless channel. For severe fading channels, such as Rayleigh fading channels, the tradeoff leans towards more redundancy on erasure-correction coding across packets, and less so on error-correction coding within each packet. For channels with better fading conditions, more redundancy can be spent on error-correction coding. The analysis has been extended to a limiting case with a large number of packets, and a scenario where only discrete rates are available via a finite number of transmission modes.",2008,0,
82,83,Modeling transformers with internal incipient faults,"Incipient fault detection in transformers can provide early warning of electrical failure and could prevent catastrophic losses. To develop transformer incipient fault detection technique, a transformer model to simulate internal incipient faults is required. This paper presents a methodology to model internal incipient winding faults in distribution transformers. These models were implemented by combining deteriorating insulation models with an internal short circuit fault model. The internal short circuit fault model was developed using finite element analysis. The deteriorating insulation model, including an aging model and an arcing model connected in parallel, was developed based on the physical behavior of aging insulation and the arcing phenomena occurring when the insulation was severely damaged. The characteristic of the incipient faults from the simulation were compared with those from some potential experimental incipient fault cases. The comparison showed the experimentally obtained characteristic's of terminal behavior of the faulted transformer were similar to the simulation results from the incipient fault models",2002,0,
83,84,An Efficient Technique for Error-Free Implementation of H.264 Using Algebraic Integer Encoding,"Video coding technology plays a key role in various multimedia applications. H.264 is the newest video coding standard and has achieved a significant improvement in coding efficiency. The 4*4 integer transform, as one of the key techniques in H.264 video compression standard, is very important for the whole performance of H.264 codec. In this paper we propose a novel algorithm for fast and error-free (infinite-precision) implementation of H.264 based on algebraic integer-encoding scheme. The proposed algorithm has regular structure. Simulation results show that this algorithm will result in reduction of computation complexity while enhancing the quality of obtained image simultaneously. Determining the quality of an image is an open problem that is highly dependent on the specific application that this image will be used for. We propose new measuring quantities for image quality.",2010,0,
84,85,Detection of Rotor Faults in Squirrel-Cage Induction Motors using Adjustable Speed Drives,"The need for detection of rotor faults at an earlier stage, so that maintenance can be planned ahead, has pushed the development of monitoring methods with increasing sensitivity and noise immunity. Addressing diagnostic techniques based on current signatures analysis (MCSA), the characteristic components introduced by specific faults in the current spectrum are investigated and a diagnosis procedure correlate the amplitudes of such components to the fault extent. In this paper, the impact of feedback control on asymmetric rotor cage induction machine behavior is analyzed. It is shown that the variables usually employed in diagnosis procedures assuming open-loop operation are no longer effective under closed-loop operation. Simulation results show that signals already present at the drive are suitable to effective diagnostic procedure. The utilization of the current regulator error signals and the influence of the regulators gains on their utilization in rotor failure detection are the aim of the present work. The use of a band-pass filter bank to detect the presence of sidebands is also proposed in the paper",2006,0,
85,86,FPGA Implementation of Wideband IQ Imbalance Correction in OFDM Receivers,"This paper describes the implementation of a digital compensation scheme, called CSAD, for correcting the effects of wideband gain and phase imbalances in dual-branch OFDM receivers. The proposed scheme is implemented on a Xilinx Virtex-4 field programmable gate array (FPGA). The flexible architecture of the implementation makes it readily adaptable for different broadband applications, such as DVB-T/H, WLAN, and WiMAX. The proposed correction scheme is resilient against multipath fading and frequency offset. When applied to DVB-T, it is shown that an 11-bit arithmetic precision is sufficient to achieve the required BER of 2x10<sup>-4</sup> at an SNR of 16.5 dB. Using this bit-precision, the implementation consumes 1686 Virtex-4 slices equivalent to about 42600 gates.",2008,0,
86,87,Error prediction for multi-classification,"This paper describes an error prediction mechanism for multiclassification systems. First, a multiclassification system is constructed by combining a suite of two-class classifiers. While training, each sub-classifier does not utilize all the training data and the remaining data are used for testing purpose. Thus, the classification system can predict its own performance after training. We have tested this mechanism on several well-known benchmark datasets. Experimental results are demonstrated for its effectiveness.",2005,0,
87,88,Automatic detection and correction of purple fringing using the gradient information and desaturation,"This paper proposes a method to automatically detect and correct purple fringing that is one of the color artifacts due to characteristics of charge coupled device sensors in a digital camera. The proposed method consists of two steps. In the first step, we detect purple fringed regions that satisfy specific properties: hue characteristics around highlight regions with large gradient magnitudes. In the second step, color correction of the purple fringed regions is made by desaturating the pixels in the detected regions. The proposed method is able to detect purple fringe artifacts more precisely than Kang's method. It can be used as a post processing in a digital camera.",2008,0,
88,89,SOA-Based Alarm Integration for Multi-Technology Network Fault Management,"In order for the service provider to offer better quality of telecom services to customers, one of the possible ways is to monitor and control all kinds of deployed network resources, which are used to support the operation of these services, and to proactively analyze and recover any trouble reported from the network resources. In this study, two system integration scenarios along with the associated interface specifications for multi-technology resource alarm notification and retrieval services have been described. An NGOSS-based development methodology was followed, and a number of useful commercials tools were introduced to facilitate the evolution and transformation of legacy BSS/OSS, so that these BSS/OSS are able to support loosely-coupled interoperability by using standards-based interfaces based on the service-oriented architecture. The functionalities of multi-technology network fault management were realized by means of JMS and Web Service techniques. The implementation described in this paper shows the feasibility of the proposed development methodology.",2008,0,
89,90,Fault Diagnosis of Bearings in Rotating Machinery Based on Vibration Power Signal Autocorrelation,"Since fault in a great number of bearings commences from a single point defect, research on this category of faults has shared a great deal in predictive diagnosis literature. Single point defects will cause certain characteristic fault frequencies to appear in machine vibration spectrum. In traditional methods, data extracted from frequency spectrum has been used to identify damaged bearing part. Because of impulsive nature of fault strikes, and complex modulations present in vibration signal, a simple spectrum analysis may result in erroneous conclusions. When a shaft rotates at constant speed, strikes due to a single point defect repeat at constant intervals. Each strike shows a high energy distribution around it. This paper considers the time intervals between successive impulses in auto-correlated vibration power signals. The most frequent interval between successive impulses determines the period of defective part. This period is related to fault frequency and therefore shows the defective part. A comparison of results extracted from the traditional and the proposed methods shows the efficiency improvement of the second method in respect of the first one",2006,0,
90,91,A Distributed Fault-Tolerant Algorithm for Event Detection Using Heterogeneous Wireless Sensor Networks,"Distributed event detection using wireless sensor networks has received growing interest in recent years. In such applications, a large number of inexpensive and unreliable sensor nodes are distributed in a geographical region to make firm and accurate local decisions about the presence or absence of specific events based on their sensor readings. However, sensor readings can be unreliable, due to either noise in the sensor readings or hardware failures in the devices, and may cause nodes to make erroneous local decisions. We present a general fault-tolerant event detection scheme that allows nodes to detect erroneous local decisions based on the local decisions reported by their neighbors. This detection scheme does not assume homogeneity of sensor nodes and can handle cases where nodes have different accuracy levels. We prove analytically that the derived fault-tolerant estimator is optimal under the maximum a posteriori (MAP) criterion. An equivalent weighted voting scheme is also derived. Further, we describe two new error models that take into account the neighbor distance and the geographical distributions of the two decision quorums. These models are particularly suitable for detection applications where the event under consideration is highly localized. Our fault-tolerant estimator is simulated using a network of 1024 nodes deployed randomly in a square region and assigned random probability of failures",2006,0,
91,92,Diagnosis of rotor faults in brushless DC (BLDC) motors operating under non-stationary conditions using windowed Fourier ridges,"There are several applications where the motor is operating in continuous non-stationary operating conditions. Actuators in the aerospace and transportation industries are examples of this kind of operation. Diagnostics of faults in such applications is, however, challenging. A novel method using windowed Fourier ridges is proposed in this paper for the detection of rotor faults in BLDC motors operating under continuous non-stationarity. Experimental results are presented to validate the concept and depict the ability of the proposed algorithm to track and identify rotor faults. The proposed algorithm is simple and can be implemented in real-time without much computational burden.",2005,0,
92,93,Supervision and fault management of process-tasks and terminology,"The supervision of technical processes is aimed at showing the present state, indicating undesired or unpermitted states, and taking appropriate actions to avoid damage or accidents. The deviations from normal process behavior result from faults and errors, which can be attributed to many causes. They may result in some shorter or longer time periods with malfunctions or failures if no counteractions are taken. One reason for supervision is to avoid these malfunctions or failures. In the following article the basic tasks of supervision are shortly described.",2007,0,
93,94,An industrial case study of implementing and validating defect classification for process improvement and quality management,"Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements",2005,0,
94,95,Evaluation of soft-bit error sequence generators at the output of the decoding process,"Soft decision decoding algorithms are widely used in modern wireless systems; convolutional and turbo codes are usually adopted as the inner scheme thanks to their capability to correct symbol errors at low SNR values. Such algorithms can achieve high coding gains using soft decoding, and modern digital hardware technology enables efficient and low cost practical implementations. We apply the experience gained in previous work, concerning the simulation of bit error processes (Costamagna, E. et al., Proc. IEEE, vol.90 p.842-59, 2002), to implement soft-bit generative models based on hidden Markov chains and chaotic attractors. Both the input and the output of the demodulation process of a GSM-GPRS and a 3GPP UMTS transceiver are observed, developing our earlier analysis (Costamagna et al., IEEE 58th VTC, 2003), and the quality of the soft-bit sequences generated for the input is evaluated comparing the sequences obtained at the output of the demodulator when simulated or target sequences are supplied at the input. Moreover, the deep significance of some statistical features exhibited by the sequences in order to describe their error burst behavior is briefly discussed.",2004,0,
95,96,Procedure call duplication: minimization of energy consumption with constrained error detection latency,"This paper presents a new software technique for detecting transient hardware errors. The objective is to guarantee data integrity in the presence of transient errors and to minimize energy consumption at the same time. Basically, we duplicate computations and compare their results to detect errors. There are three choices for duplicate computations: (1) duplicating every statement in the program and comparing their results, (2) re-executing procedures with duplicated procedure calls and comparing the results, (3) re-executing the whole program and comparing the final results. Our technique is the combination of (1) and (2): Given a program, our technique analyzes procedure call behavior of the program and determines which procedures should have duplicated statements (choice (1)) and which procedure calls should be duplicated (choice (2)) to minimize energy consumption while controlling error detection latency constraints. Then, our technique transforms the original program into the program that is able to detect errors with reduced energy consumption by re-executing the statements or procedures. In benchmark program simulation, we found that our technique saves over 25% of the required energy on average compared to previous techniques that do not take energy consumption into consideration",2001,0,
96,97,The complexity of adding failsafe fault-tolerance,"In this paper, we focus our attention on the problem of automating the addition of failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. And, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of adding failsafe fault-tolerance in distributed programs is NP-hard. Towards this end, we reduce the 3-SAT problem to the problem of adding failsafe fault-tolerance. We also identify a class of specifications, monotonic specifications and a class of programs, monotonic programs. Given a (positive) monotonic specification and a (negative) monotonic program, we show that failsafe fault-tolerance can be added in polynomial time. We note that the monotonicity restrictions are met for commonly encountered problems such as Byzantine agreement, distributed consensus, and atomic commitment. Finally, we argue that the restrictions on the specifications and programs are necessary to add failsafe fault-tolerance in polynomial time; we prove that if only one of these conditions is satisfied, the addition of failsafe fault-tolerance is still NP-hard.",2002,0,
97,98,Comparisons of error control techniques for wireless video multicasting,"This paper explores three different methods, employed separately and in combination, to improve the quality of video delivery on wireless local area networks. The approaches are: leader-driven multicast (LDM), monitoring MAC layer unicast (re)transmissions by other receivers; application-level forward error correction (FEC) using block erasure codes; negative feedback from selected receivers in the form of extra parity requests (EPR). The performance of these three methods is evaluated using both experiments on a mobile computing testbed and simulation. The results indicate that, while LDM is helpful in improving the raw packet reception rate, the combination of FEC and EPR is most effective in improving the frame delivery rate",2002,0,
98,99,Research of remote fault diagnostic system based on Grid,"So far, methods of fault diagnosis have been numerous. But as the complexity of modern equipment, the variability of fault, as well as trends in global manufacturing, it is difficult for any separate organization of independent to complete all the work of fault diagnosis. This article reviewed the equipment fault diagnostic technology in the course of development, in particular, remote fault diagnostic system based on Internet technology, and then on this basis, this paper proposed a new fault diagnostic method, that is grid-based remote fault diagnosis, which integrates the resources of related organization to work together. The architecture of this diagnostic system is proposed, and also the work flow of this system is described.",2009,0,
99,100,Finding Faults: Manual Testing vs. Random+ Testing vs. User Reports,"The usual way to compare testing strategies, whether theoretically or empirically, is to compare the number of faults they detect. To ascertain definitely that a testing strategy is better than another, this is a rather coarse criterion: shouldn't the nature of faults matter as well as their number? The empirical study reported here confirms this conjecture. An analysis of faults detected in Eiffel libraries through three different techniques-random tests, manual tests, and user incident reports-shows that each is good at uncovering significantly different kinds of faults. None of the techniques subsumes any of the others, but each brings distinct contributions.",2008,0,
100,101,Adaptive Fault Management of Parallel Applications for High-Performance Computing,"As the scale of high-performance computing (HPC) continues to grow, failure resilience of parallel applications becomes crucial. In this paper, we present FT-Pro, an adaptive fault management approach that combines proactive migration with reactive checkpointing. It aims to enable parallel applications to avoid anticipated failures via preventive migration and, in the case of unforeseeable failures, to minimize their impact through selective checkpointing. An adaptation manager is designed to make runtime decisions in response to failure prediction. Extensive experiments, by means of stochastic modeling and case studies with real applications, indicate that FT-Pro outperforms periodic checkpointing, in terms of reducing application completion times and improving resource utilization, by up to 43 percent.",2008,0,
101,102,Fault-accommodating thruster force allocation of an AUV considering thruster redundancy and saturation,"A new approach to the fault-accommodating allocation of thruster forces of an autonomous underwater vehicle (AUV) is investigated in this paper. This paper presents a framework that exploits the excess number of thrusters to accommodate thruster faults during operation. First, a redundancy resolution scheme is presented that considers the presence of an excess number of thrusters along with any thruster faults and determines the reference thruster forces to produce the desired motion. This framework is then extended to incorporate a dynamic state feedback technique to generate reference thruster forces that are within the saturation limit of each thruster. Results from both computer simulations and experiments are provided to demonstrate the viability of the proposed scheme",2002,0,
102,103,Designing Real-Time and Fault-Tolerant Middleware for Automotive Software,"Automotive software development poses a great deal of challenges to automotive manufacturers since an automobile is inherently distributed and subject to fault-tolerance and real-time requirements. Middleware is a software layer that can handle the intrinsic complexities of distributed systems and arises as an indispensable run-time platform for automotive systems. This paper explains the concept of middleware by enumerating its functions and categorizes middleware according to adopted communication models. It also extracts five essential requirements of automotive middleware and proposes a middleware design for automotive systems based on the message-oriented middleware (MOM) structure. The proposed middleware effectively addresses the derived requirements and includes many essential features such as real-time guarantee, fault-tolerance, and a global time base",2006,0,
103,104,Design of Energy-Efficient High-Speed Links via Forward Error Correction,"In this brief, we show that forward error correction (FEC) can reduce power in high-speed serial links. This is achieved by trading off the FEC coding gain with specifications on transmit swing, analog-to-digital converter (ADC) precision, jitter tolerance, receive amplification, and by enabling higher signal constellations. For a 20-in FR4 link carrying 10-Gb/s data, we demonstrate: 1) an 18-mW/Gb/s savings in the ADC; 2) a 1-mW/Gb/s reduction in transmit driver power; 3) up to 6?? improvement in transmit jitter tolerance; and 4) a 25- to 40-mV improvement in comparator offset tolerance with 3?? smaller swing.",2010,0,
104,105,Fault classification for distance protection,This paper presents an overview of power system fault classification methods and challenges. It also contains some ideas about structured testing.,2002,0,
105,106,On-line Fault Diagnosis Model of the Hydropower Units Based on MAS,"The paper introduced a novel on-line fault diagnosis system model of the hydropower units based on multi-agent system. In allusion to the classical MAS-based fault diagnosis model, it proposes a new function of information interactive between the mission-controlled subsystem and the task decomposition subsystem to increase the transmission rate of control signals and designs the status-monitoring subsystem to detect the abnormal signals directly from local to increase the fault diagnostic sensitivity. In the fault-diagnosis subsystem, a multi-agent interactive parallel structure is designed to meet the requirements of the high reliability and good real-time. A Java-based language so called as JAFMAS is used to build a multi-agent cooperation platform. Experimental results show the effectiveness and feasibility of the proposed method.",2009,0,
106,107,Non-differential protection of a generator's stator utilizing fault transients,"This paper presents a novel protection scheme for detecting faults on the stator of a generator unit which is directly connected to a distribution system. In the scheme, a multi-channel fault transient detection unit, using the outputs of the current transformers (CTs) at the output of the generator terminal, is employed to extract the fault-generated transient current signals. The detector unit is tuned to extract two bands of fault generated transient signals with different center frequencies. A spectral comparison technique is applied to firstly compute the spectral energies of the two band signals, and then the fault diagnosis determines whether it is an internal and external fault by comparing the ratio of the two signals with a predefined threshold. The scheme offers advantages of immunity to CT saturation, and is capable of detecting both low level and interturn faults. In addition, the protection scheme is also simple in application, and is cost-effective in that it only requires one set of CTs. Simulation studies show that the proposed technique can give correct responses for various fault conditions",2001,0,
107,108,Towards fault tolerance pervasive computing,"Pervasive computing exists in the user's environment, the technology is sustainable if it is invisible to the user and does not intrude the user's consciousness. This requires that functioning of the multitude of devices in the environment be oblivious to the user. Therefore, the system has to be resilient to various kinds of faults and should be able to function despite faults. In addition, pervasive computing provides a platform for context-aware computing that enables automatic configuration of a pervasive system based on the environment context. The aim of this article is to highlight the various challenges and issues that confront fault tolerance pervasive computing, discuss their implications, prevent some solutions to these problems, and describe how some of these solutions are implemented in our system.",2005,0,
108,109,Globally optimal uneven error-protected packetization of scalable code streams,"In this paper, we present a family of new algorithms for rate-fidelity optimal packetization of scalable source bit streams with uneven error protection. In the most general setting where no assumption is made on the probability function of packet loss or on the rate-fidelity function of the scalable code stream, one of our algorithms can find the globally optimal solution to the problem in O(N<sup>2</sup>L<sup>2</sup>) time, compared to a previously obtained O(N<sup>3</sup>L<sup>2</sup>) complexity, where N is the number of packets and L is the packet payload size. If the rate-fidelity function of the input is convex, the time complexity can be reduced to O(NL<sup>2</sup>) for a class of erasure channels, including channels for which the probability function of losing n packets is monotonically decreasing in n and independent erasure channels with packet erasure rate no larger than N/2(N + 1). Furthermore, our O(NL<sup>2</sup>) algorithm for the convex case can be modified to rind an approximation solution for the general case. All of our algorithms do away with the expediency of fractional bit allocation, a limitation of some existing algorithms.",2004,0,
109,110,A Fault Recovery Approach in Fault-Tolerant Processor,"A fault recovery scheme of a fault-tolerant processor for embedded systems is introduced in this paper. The microarchitecture of the fault-tolerant processor called RSED is modified from superscalar processor architecture. The fault-tolerant mechanism of RSED is implemented mainly using temporal redundancy technique. Fault recovery scheme is an important part of the fault-tolerant mechanism. In order to resolve the problem of possible single point of failures, a novel TMR approach is adopted to generate re-execution instruction address. Compared with similar works, the fault recovery scheme proposed can recover processor execution more reliably.",2009,0,
110,111,Reaching efficient fault-tolerance for cooperative applications,"Cooperative applications are widely used, e.g. as parallel calculations or distributed information processing systems. Whereby such applications meet the users demand and offer a performance improvement, the susceptibility to faults of any used computer node is raised. Often a single fault may cause a complete application failure. On the other hand, the redundancy in distributed systems can be utilized for fast fault detection and recovery. So, we followed an approach that is based an duplication of each application process to detect crashes and faulty functions of single computer nodes. We concentrate on two aspects of efficient fault-tolerance-fast fault detection and recovery without delaying the application progress significantly. The contribution of this work is first a new fault detecting protocol for duplicated processes. Secondly, we enhance a roll forward recovery scheme so that it is applicable to a set of cooperative processes in conformity to the protocol",2000,0,
111,112,Straight-Edge Extraction in Distorted Images Using Gradient Correction,"Many camera lenses, particularly low-cost or wide-angle lenses, can cause significant image distortion. This means that features extracted naively from such images will be incorrect. A traditional approach to dealing with this problem is to digitally rectify the image to correct the distortion, and then to apply computer vision processing to the corrected image. However, this is relatively expensive computationally, and can introduce additional interpolation errors. We propose instead to apply processing directly to the distorted image from the camera, modifying whatever algorithm is used to correct for the distortion during processing, without a separate rectification pass. In this paper we demonstrate the effectiveness of this approach using the particular classic problem of gradient-based extraction of straight edges. We propose a modification of the Burns line extractor that works on a distorted image by correcting the gradients on the fly using the chain rule, and correcting the pixel positions during the line-fitting stage. Experimental results on both real and synthetic images under varying distortion and noise show that our gradient-correction technique can obtain approximately a 50% reduction in computation time for straight-edge extraction, with a modest improvement in accuracy under most conditions.",2009,0,
112,113,Automatic Spelling Correction Rule Extraction and Application for Spoken-Style Korean Text,"Nowadays, spoken-style text is prevailing because lots of information are being written in spoken-style such as Short-Message-Service(SMS) messages. However, the spokenstyle text contains more spelling errors than the traditional written-style text. In this paper, we propose a rule-based spelling correction system which can automatically extract spelling correction rules from the correction corpus and apply extracted rules to spelling errors of input sentences. In order to preserve both high precision and high recall, we devise a candidate-elimination algorithm which determines appropriate context size of spelling correction rules based on rule accuracy. Experimental results showed that the proposed system can extract 42,537 spelling correction rules and apply the rules to correct spelling errors on the test corpus and thus, the rate of precision is increased from 31.08% to 79.04% on the basis of message unit.",2007,0,
113,114,Value-based scheduling of distributed fault-tolerant real-time systems with soft and hard timing constraints,We present an approach for scheduling of fault-tolerant embedded applications composed of soft and hard real-time processes running on distributed embedded systems. The hard processes are critical and must always complete on time. A soft process can complete after its deadline and its completion time is associated with a value function that characterizes its contribution to the quality-of-service of the application. We propose a quasi-static scheduling algorithm to generate a tree of fault-tolerant distributed schedules that maximize the application's quality value and guarantee hard deadlines.,2010,0,
114,115,Trend analysis techniques for incipient fault prediction,"This paper extends the application of the Laplace Test Statistic for trend analysis and prediction of incipient faults for power systems. The extensions proposed in this paper consider the situation where two parameters believed to contribute explicitly to the eventual failure are monitored. The developed extensions applied to actual incipient failure events provide promising results for prediction of the impending failure. It is demonstrated that by incorporating two parameters in the trend analysis, the robustness to outliers is increased and the flexibility is augmented by increasing the degrees of freedom in the generation of the alarm signal.",2009,0,
115,116,Notice of Retraction<BR>Comprehensive Evaluation of Certain Power Vehicle Fault Based on Rough Sets,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>With the advent of intelligence, people put forward higher request to the fault diagnosis of weapon. As the base of certain weapon system, the power vehicle can work well or not is directly relate to the tasks of the battle and training can complete smoothly or not. Firstly, the paper introduces the fundamental conception of Rough Sets and gives the method of ascertaining the evaluation value and the weights. Secondly, based on the operation characteristics of certain power vehicle, the paper analyses and creates an index system of the power vehicle, then RS is used to impersonally distribute weights. Finally, the comprehensive evaluation model based RS is given out, and the fault sequence and evaluating grade are educed. The computing results show this practical model is very significant to the weapon maintenance and offers a reference for the evaluation of civil equipment fault.",2009,0,
116,117,Fisher Discriminance of Fault Predict for Decision-Making Systems,"A new technology of fault prediction was presented based on the neural network and Fisher discriminance in statistics. First, many enough character of running situation of decision-making were extracted from the real-time observation data. Secondly, the FP software systems were designed and the algorithm of FP of decision-making systems was presented. Finally, a simply example indicated that the algorithm is effectively.",2009,0,
117,118,FAIL-MPI: How Fault-Tolerant Is Fault-Tolerant MPI?,"One of the topics of paramount importance in the development of cluster and grid middleware is the impact of faults since their occurrence in grid infrastructures and in large-scale distributed systems is common. MPI (message passing interface) is a popular abstraction for programming distributed and parallel applications. FAIL (FAult Injection Language) is an abstract language for fault occurrence description capable of expressing complex and realistic fault scenarios. In this paper, we investigate the possibility of using FAIL to inject faults in a fault-tolerant MPI implementation. Our middleware, FAIL-MPI, is used to carry quantitative and qualitative faults and stress testing",2006,0,
118,119,An Integrated Silicon Carbide (SiC) Based Single Phase Rectifier with Power Factor Correction,"Silicon carbide (SiC) based power devices exhibit superior properties such as very low switching losses, fast switching behavior, improved reliability and high temperature operation capabilities. These properties contribute toward the ability to increase switching frequency, decrease the size of passive components and switches, and reduce the need for cooling, thus making the devices an excellent candidate for AC/DC power supplies. In this paper a SiC based integrated single phase rectifier with power factor correction (PFC) is presented. The proposed topology has many advantages including fewer semiconductor components; the presence of AC side inductor resulting in reduced EMI interference, and higher performance. This approach takes advantage of the superior properties of SiC devices and the reduced number of devices in the proposed converter to achieve higher efficiency, smaller size and better performance at high temperature. A performance and efficiency evaluation of the rectifier is presented and the results are compared with benchmark Si solutions",2005,0,
119,120,Neutron Soft Errors in Xilinx FPGAs at Lawrence Berkeley National Laboratory,The Lawrence Berkeley National Laboratory cyclotron offers broad-spectrum neutrons for single event effects testing. We discuss results from this beamline for neutron soft upsets in Xilinx Virtex-4 and -5 field-programmable-gate-array (FPGA) devices.,2008,0,
120,121,TIP-OPC: a new topological invariant paradigm for pixel based optical proximity correction,"As the 193 nm lithography is likely to be used for 45 nm and even 32 nm processes, much more stringent requirement will be posed on optical proximity correction (OPC) technologies. Currently, there are two OPC approaches - the model-based OPC (MB-OPC) and the inverse lithography technology (ILT). MB-OPC generates masks which is less complex compared with ILT. But ILT produces much better results than MB-OPC in terms of contour fidelity because ILT is a pixel based method. Observing that MB-OPC preserves the mask shape topologies which leads to a lower mask complexity, we combine the strengths of both methods - the topology invariant property and the pixel based mask representation. To the best of our knowledge, it is the first time that this topological invariant pixel based OPC (TIP-OPC) paradigm is proposed, which fills the critical hole of the OPC landscape and potentially has many new applications. Our technical novelty includes the lithography friendly mask topological invariant operations, the efficient fast Fourier transform based cost function sensitivity computation and the TIP-OPC algorithm. The experimental results show that TIP-OPC can achieve much better post OPC contours compared with MB-OPC while maintaining the mask shape topologies.",2007,0,
121,122,A New Iterative Approach to the Corrective Security-Constrained Optimal Power Flow Problem,"This paper deals with techniques to solve the corrective security-constrained optimal power flow (CSCOPF) problem. To this end, we propose a new iterative approach that comprises four modules: a CSCOPF which considers only a subset of potentially binding contingencies among the postulated contingencies, a (steady-state) security analysis (SSSA), a contingency filtering (CF) technique, and an OPF variant to check post-contingency state feasibility when taking into account post-contingency corrective actions. We compare performances of our approach and its possible variants with classical CSCOPF approaches such as the direct approach and Benders decomposition (BD), on three systems of 60, 118, and 1203 buses.",2008,0,
122,123,Systems Reliability Analysis and Fault Diagnosis Based on Bayesian Networks,"This paper presents the application of Bayesian networks(BN) to the reliability analysis and fault diagnosis of systems. For systems, it is essential to do reliability analysis. Also it is necessary to do fault diagnosis when a system failed, but the better way is to do fault diagnosis before the system has failed. Bayesian networks have many special characteristics, one of them is that they are parallel structures, so the time of solving is much shorter than that of many common methods. Bayesian networks permit not only computing the reliability indices of a system but also presenting the effect of each component or some components on the system reliability to distinguish the unsubstantial part of the system, so we can know which part is the weakest one of the system. Two examples proved the validity and superiority of the method in the application of the reliability analysis and fault diagnosis of system.",2009,0,
123,124,Prediction Error Prioritizing Strategy for Fast Normalized Partial Distortion Motion Estimation Algorithm,"A prediction error prioritizing-based normalized partial distortion search algorithm for fast motion estimation is proposed in this letter. The distortion behavior of each pixel in a macroblock is first analyzed to point out the priority/order of sum of absolute difference calculation. Afterward, the normalized partial distortion search algorithm is applied for half-stop of the distortion calculation. In addition, a dynamic search range decision algorithm is adopted for automatically changing the size of the search range to further increase the motion estimation speed. The computational complexity can be reduced significantly through the proposed algorithm, though leaving a PSNR degradation that could be dismissed.",2010,0,
124,125,Fault-Tolerant Policy for Optical Network Based Distributed Computing System,"The optical network based distributed computing system has been thought as a promising technology to support large-scale data-intensive distributed applications. For such a system with so many heterogeneous resources and middlewares involved, faults seem to be inevitable. However, for those applications that need to be finished before the given deadline, a fault in the system will lead to the failure of the application. Therefore, fault-tolerant policy is necessary to improve the performance of the system when faults could happen. In this paper, we address to the fault-tolerant problem for the optical network based distributed computing system. We first propose an overlay approach which applies the existing fault-tolerant policies for distributed computing and optical network. Then we present a joint fault-tolerant policy which takes into account the fault tolerance for computing resource and network resource in the same time. We compare the performances of different polices by simulation. The simulation results show that the joint fault-tolerant policy achieves much better performances compared to overlay approaches.",2008,0,
125,126,Silicon Wafer Defect Extraction Based on Morphological Filter and Watershed Algorithm,"Defect extraction techniques are studied regarding the silicon wafer surface defect. We design a new filter based on multiple structuring elements and suggest an improved marker-based and region merging watershed. To begin with, the filter which generalized close-opening and open-closing filter based on the morphological filter with multiple structuring elements is introduced to eliminate the noise and simplify the image and morphological gradient image while preserving the details. And then in order to reduce the over-segmentation of the watershed algorithm, this paper suggests an improved marker-based and region merging method, region average gray value and edge strength criterion is used in merging operation and has a good effect on segmentation. Finally, the improved watershed algorithm is applied to the filtered gradient image to get the defect contours. The experiments show that this method can eliminate the noises and extract accurately location and closed region contours, which lays a good foundation for defect feature extraction and selection.",2008,0,
126,127,Diagnostic fault detection & intelligent reconfiguration of fuel delivery systems,"The reliable operation of an engines fuel delivery system is fundamental. A failure in the fuel system that impacts the ability to deliver fuel to the engine will have an immediate effect on system performance and safety. There are very few diagnostic systems that monitor the health of the fuel system and even fewer that can accommodate for detected faults. Current diagnostic techniques call for careful maintenance of fuel system components. These techniques tend to be backward thinking in that they are based on previous experience which is not always a good indicator for future systems. This paper describes a technique developed at the Penn State Applied Research Laboratories Condition Based Maintenance Department for fault detection and reconfiguration for fuel delivery system components. This technique has been applied to a diesel engine test rig. The test rig is fully instrumented with sensors including those for fuel pressure. Even though this technique is being applied on a diesel engine, the approach is fully compatible to any fuel delivery system",2005,0,
127,128,Evaluation of security and fault tolerance in mobile agents,"The reliable execution of a mobile agent is a very important design issue in building a mobile agent system and many fault-tolerant schemes have been proposed so far. Security is a major problem of mobile agent systems, especially when money transactions are concerned . Security for the partners involved is handled by encryption methods based on a public key authentication mechanism and by secret key encryption of the communication. In this paper, we examine qualitatively the security considerations and challenges in application development with the mobile code paradigm. We identify a simple but crucial security requirement for the general acceptance of the mobile code paradigm, and evaluate the current status of mobile code development in meeting this requirement. We find that the mobile agent approach is the most interesting and challenging branch of mobile code in the security context. Therefore, we built a simple agent- based information retrieval application, the Traveling Information Agent system, and discuss the security issues of the system in particulars.",2008,0,
128,129,"Comprehensive Analysis of Performance, Fault-Tolerance and Scalability in Grid Resource Management System","The management of the large scale heterogeneous resources is a critical issue in grid computing. The resource management system (RMS) is an essential component of grids. To ensure the QoS of the upper layer service, it raises high requirement for the performance, fault-tolerance and scalability of RMS. In this paper, we study three typical structures of RMS, including centralized, hierarchical and peer-to-peer structures, and make a comprehensive analysis of performance, fault tolerance and scalability. We put forward the performance, fault tolerance and scalability evaluation metrics of the RMS, and give the mathematical expressions and detailed calculation processes. Besides, we make further discussions on the interactions of the performance, fault-tolerance and scalability, and make a comparison of the RMSs with the three typical structures. We believe that the results of this work will help system architects make informed choices for building the RMS.",2009,0,
129,130,Recent improvements on the specification of transient-fault tolerant VHDL descriptions: a case-study for area overhead analysis,"We present a new approach to design reliable complex circuits with respect to transient faults in memory elements. These circuits are intended to be used in harmful environments like radiation. During the design flow, this methodology is also used to perform an early-estimation of the obtained reliability level. Usually, this reliability estimation step is performed in the laboratory, by means of radiation facilities (particle accelerators). By doing so, the early-estimated reliability level is used to balance the design process into a trade-off between maximum area overhead due to the insertion of redundancy and the minimum reliability required for a given application. This approach is being automated through the development of a CAD tool (FT-PRO). Finally, we present also a case-study of a simple microprocessor used to analyze the FT-PRO performance in terms of the area overhead required to implement the fault-tolerant circuit.",2000,0,
130,131,Error calculation techniques and their application to the Antenna Measurement Facility Comparison within the European Antenna Centre of Excellence,"This paper gives an overview of the ongoing activities under the Antenna Measurement activity of the Antenna Centre of Excellence (ACE) network within the EU 6th framework research program. In particular, in this work an attempt is made to establish a common uncertainty estimation criteria in spherical near field and far field antenna measurement systems. The results from this activity are important instruments to verify the measurements accuracies for antenna measurement ranges as well as to investigate and evaluate possible improvements in measurement set-ups and procedures. These results will be used in the facility comparison campaigns in order to calculate a reference pattern for each of the high accuracy reference antennas (VAST 12, SATIMO SH800 and SATIMO SH2000) measured during the last 4 years by different institutions in Europe and US.",2007,0,
131,132,GNSS pseudorange error density tracking using Dirichlet Process Mixture,"In satellite navigation system, classical localization algorithms assume that the observation noise is white-Gaussian. This assumption is not correct when the signal is reflected on the surrounding obstacles. That leads to a decrease of accuracy and of continuity of service. To enhance the localization performances, a better observation noise density can be use in an adapted filtering process. This article aims to show how the Dirich-let Process Mixture can be employed to track the observation density on-line. This sequential estimation solution is adapted when the noise is non-stationary. The approach will be tested under a simulation scenario with multiple propagation conditions. Then, this density modeling will be used in Rao-Blackwellised Particle Filter.",2010,0,
132,133,On handling dependent evidence and multiple faults in knowledge fusion for engine health management,"Diagnostic architectures that fuse outputs from multiple algorithms are described as knowledge fusion or evidence aggregation. Knowledge fusion using a statistical framework such as Dempster-Shafer (D-S) has been used in the context of engine health management. Fundamental assumptions made by this approach include the notion of independent evidence and single fault. In most real world systems, these assumptions are rarely satisfied. Relaxing the single fault assumption in D-S based knowledge fusion involves working with a hyper-power set of the frame of discernment. Computational complexity limits the practical use of such extension. In this paper, we introduce the notion of mutually exclusive diagnostic subsets. In our approach, elements of the frame of discernment are subsets of faults that cannot be mistaken for each other, rather than failure modes. These subsets are derived using a systematic analysis of connectivity and causal relationship between various components within the system. Specifically, we employ a special form of reachability analysis to derive such subsets. The theory of D-S can be extended to handle dependent evidence for simple and separable belief functions. However, in the real world the conclusions of diagnostic algorithms might not take the form of simple or separable belief functions. In this paper, we present a formal definition of algorithm dependency based on three metrics: the underlying technique an algorithm is using, the sensors it is using, and the feature of the sensor that the algorithm is using. With this formal definition, we partition evidence into highly dependent, weakly dependent and independent evidence. We present examples from a Honeywell auxiliary power unit to illustrate our modified D-S method of evidence aggregation",2006,0,
133,134,Transmission Line Fault Location Using Two-Terminal Data Without Time Synchronization,"This letter presents a new transmission line fault location method that uses current and voltage sinusoidal phasors at both ends, without necessity of data synchronization. The main difference among the classical Johns method resides in the fact that the proposed method is based on magnitude of fault point voltage and does not demand exact phase angles of the acquired signals. Simulated and real case results are presented, showing that the proposed algorithm is robust, accurate, and provides adequate performance. Practical applications confirm that the synchronization is not really necessary, making the method faster and easier to apply than classical methods in many real situations",2007,0,
134,135,Measurement-based frame error model for simulating outdoor Wi-Fi networks,"We present a measurement-based model of the frame error process on a Wi-Fi channel in rural environments. Measures are obtained in controlled conditions, and careful statistical analysis is performed on the data, providing information which the network simulation literature is lacking. Results indicate that most network simulators use a frame loss model that can miss important transmission impairments even at a short distance, particularly when considering antenna radiation pattern anisotropy and multi-rate switching.",2009,0,
135,136,A new design technique for optimum logic filter using matrix type-B error correcting coding,"A brief examination in digital communications gives that the receiver has to decide and distinguish between a number of discrete signals in background noise. For this case an optimum filter is designed and some techniques are developed as f.i. the MAP (Maximum a posteriori probability), the maximum likelihood - ML, the matched filter, the Kalman filter, etc. In this paper we introduce a new design technique, which we called the optimum logic filter (OLF), using sophisticated matrix type-B error correcting coding.",2005,0,
136,137,Incremental fault-tolerant design in an object-oriented setting,"With the increasing emphasis on dependability in complex, distributed systems, it is essential that system development can be done gradually and at different levels of detail. We propose an incremental treatment of faults as a refinement process on object-oriented system specifications. An intolerant system specification is a natural abstraction from which a fault-tolerant system can evolve. With each refinement step a fault and its treatment are introduced, so the fault-tolerance of the system increases during the design process. Different kinds of faults are identified and captured by separate refinement relations according to how the tolerant system relates to abstract properties of the intolerant one in terms of safety, and liveness. The specification language utilized is object-oriented and based upon first-order predicates on communication traces. Fault-tolerance refinement relations are formalized within this framework",2001,0,
137,138,Fault-tolerant control of PMSM drive unit,"Since the Fuel-Cell Vehicle's demonstration in the public transport, its fault diagnosis and fault tolerant control strategy become more and more important. This paper studies on the PMSM drive of FCV and presents a sensorless control algorithm in the fault mode based analytical redundancy. Simulation analysis and experiment verification are presented to compare the control algorithm using Expanded Kalman Filter (EKF) and Phase Locked Loop.",2009,0,
138,139,The Learning with Errors Problem (Invited Survey),"In this survey we describe the Learning with Errors (LWE) problem, discuss its properties, its hardness, and its cryptographic applications.",2010,0,
139,140,Group communication protocols under errors,"Group communication protocols constitute a basic building block for highly dependable distributed applications. Designing and correctly implementing a group communication system (GCS) is a difficult task. While many theoretical algorithms have been formalized and proved for correctness, only few research projects have experimentally assessed the dependability of GCS implementations under complex error scenarios. This paper describes a thorough error-injection experimental campaign conducted on Ensemble, a popular GCS. By employing synthetic benchmark applications, we stress selected components of the GCS $the group membership service, the FIFO-ordered reliable multicast - under various error models, including errors in the memory (text and heap segments) and in the network messages. The data show that about 5-6% of the failures are due to an error escaping Ensemble's error-containment mechanism and manifesting as a fail silence violation. This constitutes an impediment to achieving high dependability, the natural objective of GCSs. Our results are derived for a particular system (Ensemble), and more investigation involving other GCSs is required to generalize the conclusions. Nevertheless, through an accurate analysis of the failure causes and the error propagation patterns, this paper offers insights into the design and the implementation of robust GCSs.",2003,0,
140,141,Extended fault modeling used in the space shuttle PRA,"A probabilistic risk assessment (PRA) has been completed for the space shuttle with NASA sponsorship and involvement. This current space shuttle PRA is an advancement over past PRAs conducted for the space shuttle in the technical approaches utilized and in the direct involvement of the NASA centers and prime contractors. One of the technical advancements is the extended fault modeling techniques used. A significant portion of the data collected by NASA for the space shuttle consists of faults, which are not yet failures but have the potential of becoming failures if not corrected. This fault data consists of leaks, cracks, material anomalies, and debonding faults. Detailed, quantitative fault models were developed for the space shuttle PRA which involved assessing the severity of the fault, detection effectiveness, recurrence control effectiveness, and mission-initiation potential. Each of these attributes was transformed into a quantitative weight to provide a systematic estimate of the probability of the fault becoming a failure in a mission. Using the methodology developed, mission failure probabilities were estimated from collected fault data. The methodology is an application of counter-factual theory and defect modeling which produces consistent estimates of failure rates from fault rates. Software was developed to analyze all the relevant fault data collected for given types of faults in given systems. The software allowed the PRA to be linked to NASA's fault databases. This also allows the PRA to be updated as new fault data is collected. This fault modeling and its implementation with FRAS was an important part of the space shuttle PRA.",2004,0,
141,142,DMTracker: finding bugs in large-scale parallel programs by detecting anomaly in data movements,"While software reliability in large-scale systems becomes increasingly important, debugging in large-scale parallel systems remains a daunting task. This paper proposes an innovative technique to find hard-to-detect software bugs that can cause severe problems such as data corruptions and deadlocks in parallel programs automatically via detecting their abnormal behaviors in data movements. Based on the observation that data movements in parallel programs typically follow certain patterns, our idea is to extract data movement (DM)-based invariants at program runtime and check the violations of these invariants. These violations indicate potential bugs such as data races and memory corruption bugs that manifest themselves in data movements. We have built a tool, called DMTracker, based on the above idea: automatically extract DM-based invariants and detect the violations of them. Our experiments with two real-world bug cases in MVAPICH/MVAPICH2, a popular MPI library, have shown that DMTracker can effectively detect them and report abnormal data movements to help programmers quickly diagnose the root causes of bugs. In addition, DMTracker incurs very low runtime overhead, from 0.9% to 6.0%, in our experiments with High Performance Linpack (HPL) and NAS Parallel Benchmarks (NPB), which indicates that DMTracker can be deployed in production runs.",2007,0,
142,143,Lightweight Fault-Tolerance for Peer-to-Peer Middleware,"We address the problem of providing transparent, lightweight, fault-tolerance mechanisms for generic peer-to-peer middleware systems. The main idea is to use the peer-to-peer overlay to provide for fault-tolerance rather than support it higher up in the middleware architecture, e.g. in the form of services. To evaluate our approach we have implemented a fault-tolerant middleware prototype that uses a hierarchical peer-to-peer overlay in which the leaf peers connect to sensors that provide data streams. Clients connect to the root of the overlay and request streams that are routed upwards through intermediate peers in the overlay up to the client. We report encouraging preliminary results for latency, jitter and resource consumption for both the non-faulty and faulty cases.",2010,0,
143,144,Automated Support for Propagating Bug Fixes,"We present empirical results indicating that when programmers fix bugs, they often fail to propagate the fixes to all of the locations in a code base where they are applicable, thereby leaving instances of the bugs in the code. We propose a practical approach to help programmers to propagate many bug fixes completely. This entails first extracting a programming rule from a bug fix, in the form of a graph minor of an enhanced procedure dependence graph. Our approach assists the programmer in specifying rules by automatically matching simple rule templates; the programmer may also edit rules or compose them from scratch. A graph matching algorithm for detecting rule violations is then used to locate the places in the code base where the bug fix is applicable. Our approach does not require that rules occur repeatedly in the code base. We present empirical results indicating that the approach nevertheless exhibits good precision.",2008,0,
144,145,Bandwidth effect on distance error modeling for indoor geolocation,"In this paper we introduce a model for the distance error measured from the estimated time of arrival (TOA) of the direct path (DP) between the transmitter and the receiver in a typical multipath indoor environment. We use the results of a calibrated Ray tracing software in a sample office environment. First we divide the whole floor plan into LOS and Obstructed LOS (OLOS), and then we model the distance error in each environment considering the variation of bandwidth of the system. We show that the behavior of the distance error in LOS environment can be modeled as Gaussian, while behavior of the OLOS is a mixture of Gaussian and exponential distribution. We also related the statistics of the distributions to the bandwidth of the system.",2003,0,
145,146,Impact of solid-state fault current limiters on protection equipment in transmission and distribution systems,"Solid-state fault current limiters (SSFCLs) offer a number of benefits when incorporated within transmission and distribution systems. SSFCLs can limit the magnitude of a fault current seen by a system using different methods, such as inserting a large impedance in the current path or controlling the voltage applied to the fault. However, these two methods can introduce a few problems when SSFCLs are used in a system along with other protection equipment such as protective relays and sensors. An experiment was designed and implemented to evaluate the behavior of the protective relays in a mimic distribution system with a SSFCL. This paper introduces the details of the experiment and the result shows that the distorted current and voltage waveforms resulting from the action of the SSFCL disturb the protective equipment.",2010,0,
146,147,Considering Fault Correction Lag in Software Reliability Modeling,"The fault correction process is very important in software testing, and it has been considered into some software reliability growth models (SRGMs). In these models, the time-delay functions are often used to describe the dependency of the fault detection and correction processes. In this paper, a more direct variable ""correction lag"", which is defined as the difference between the detected and corrected fault numbers, is addressed to characterize the dependency of the two processes. We investigate the correction lag and find that it appears Bell-shaped. Therefore, we adopt the Gamma function to describe the correction lag. Based on this function, a new SRGM which includes the fault correction process is proposed. And the experimental results show that the new model gives better fit and prediction than other models.",2008,0,
147,148,Effects of Defects on the Thermal and Optical Performance of High-Brightness Light-Emitting Diodes,"Defects in terms of voids, cracks, and delaminations are often generated in light-emitting diodes (LEDs) devices and modules. During various manufacturing processes, accelerated testing, inappropriate handling, and field applications, defects are most frequently induced in the early stage of process development. One loading is due to the nonuniform loads caused by temperature, moisture, and their gradients. In this research, defects in various cases are modeled by a nonlinear finite-element method (FEM) to investigate the existence of interfaces, interfacial open and contacts in terms of thermal contact resistance, stress force nonlinearity, and optical discontinuity, in order to analyze their effects on the LED's thermal and optical performance. The simulation results show that voids and delaminations in the die attachment would enhance the thermal resistance greatly and decrease the LED's light extraction efficiency, depending on the defects' sizes and locations generated in packaging.",2009,0,
148,149,The minimum worst case error of fuzzy approximators,"The approximation capability of fuzzy systems is an important topic of research when the systems are regarded as input-output maps. By using the notion of information-based complexity (IBC), we derive the minimum worst case error of a fuzzy approximator, which is independent of the detailed construction of the fuzzy rule bases",2001,0,
149,150,Evaluation of risk in canal irrigation systems due to non-maintenance using fuzzy fault tree approach,"The safety and performance of many existing irrigation systems could be improved by doing the preventive maintenance activities. Modeling canal irrigation systems in terms of condition and performance that can be directly correlated with particular canal system maintenance activities. There are two categories of scheduled maintenance activity in irrigation maintenance systems: maintenance may be targeted towards restoring deliveries (""restorative maintenance""), or towards reducing the risk of failures (""preventative maintenance""). This paper covers the latter kind of maintenance scheduling by 'risk analysis'. The purpose of this risk analysis is to forecast the impact of preventative maintenance on deliveries from the main channel systems. After gaining the experience from the preliminary risk analysis through questionnaire based survey and failure history of past record, a 'fuzzy fault tree (FFT)' method is developed for the rapid risk assessment and it is necessitated for the irrigation system manager/engineer. 'Risk analysis' of irrigation systems from the point of view of maintenance is studied and applied to the Tirunelveli Channel Systems located in India. The effectiveness is calculated for each preventative maintenance tasks and it is ranked according to the effectiveness/cost ratio.",2003,0,
150,151,Waveform matching approach for fault diagnosis of a high-voltage transmission line employing harmony search algorithm,"An accurate and effective technology for fault diagnosis of a high-voltage transmission line plays an important role in supporting rapid system restoration. The fault diagnosis of a high-voltage transmission line involves three major tasks, namely fault-type identification, fault location and fault time estimation. The diagnosis problem is formulated as an optimisation problem in this work: the variables involved in the fault diagnosis problem, such as the fault location, and the unknown variables such as ground resistance, are taken into account as optimisation variables; the sum of the discrepancy of the approximation components of the actual and expected waveforms is taken as the optimisation objective. Then, according to the characteristics of the formulated optimisation problem, the harmony search, an effective heuristic optimisation algorithm developed in recent years, is employed to solve this problem. Test results for a sample power system have shown that the developed fault diagnosis model and method are correct and efficient.",2010,0,
151,152,Fault tolerance in autonomic computing environment,"Since the characteristic of current information systems is the dynamic change of their configurations and scales with non-stop provision of their services, the system management should inevitably rely on autonomic computing. Since fault tolerance is one of the important system management issues, it should also be incorporated in an autonomic computing environment. This paper argues what should be taken into consideration and what approach could be available to realize the fault tolerance in such environments.",2002,0,
152,153,Fault Location Using Sparse IED Recordings,"Basic goal of power system is to continuously provide electrical energy to users. Like with any other system, failures in power system can occur. In those situations it is critical that remedial actions are applied as soon as possible. To apply correct remedial actions it is very important that accurate fault condition and location are detected. In this paper, different fault location algorithms followed with description of intelligent techniques used for implementation of corresponding algorithms are presented. New approach for fault location using sparse measurements is examined. According to available data, it decides between different algorithms and selects an optimal one. New approach is developed by utilizing different data structures in order to efficiently implement algorithm decision engine, which is presented in paper.",2007,0,
153,154,A Bio-network Based Fault-Tolerant Architecture for Supervisory System,"Supervisory systems show increasing importance in many plants to ensure the secure and stable production. And the fault tolerance ability is a key problem. Concerned with the problem of low automation level and high operation cost, a layered Bio-Network is studied inspired by biology system, and Bio-Entity as its functional unit is analyzed. In the bottom layer, the communication infrastructure is built on Web Service to hide the difference among underlying heterogeneous systems. In the Bio-System layer, each Bio-Entity is delegated by an agent to gain the functions. Then, in the application layer, various services are encapsulated upon Bio-Entity. The application interface makes the integration with other systems more convenient. Based on Bio-Network, a novel framework of supervisory system for typical waterworks is proposed. The fault tolerant functions are analyzed in the context of Bio-Network. The practical application proves that Bio-Network based system configuration is improved and the cost is reduced.",2008,0,
154,155,Error Analysis of the Complex Kronecker Canonical Form,"In some interesting applications in control and system theory, i.e. in engineering, in ecology (Leslie population model), in financial/actuarial (Leontief multi input - multi output) science, linear descriptor (singular) differential/difference equations with time-invariant coefficients and (non-) consistent initial conditions have been extensively used. The solution properties of those systems are based on the Kronecker canonical form, which is an important component of the Matrix Pencil Theory. In this paper, we present some preliminary results for the error analysis of the complex Kronecker canonical form based on the Euclidean norm. Finally, under some weak assumptions an interesting new necessary condition is also derived.",2010,0,
155,156,Joint Fault-Tolerant Design of the Chinese Space Robotic Arm,"In this paper, joint reliability design for the Chinese space robotic arm has been discussed. Redundant controller unit, redundant can bus communication unit and latch-up power protection unit have been outlined. The fault tree of the joint has been built. Moreover, the new algorithm of auto-adjust thresholding value has been presented for fault detection, and the fault-tolerant strategies of joint have been proposed. Experimental results demonstrate the effectiveness of the joint fault-tolerant design",2006,0,
156,157,Fault tolerance in scalable agent support systems: integrating DARX in the AgentScape framework,"Open multi-agent systems need to cope with the characteristics of the Internet, e.g., dynamic availability of computational resources, latency, and diversity of services. Large-scale multi-agent systems employed on wide-area distributed systems are susceptible to both hardware and software failures. This paper describes AgentScape, a multi-agent system support environment, DARX, a framework for providing fault tolerance in large scale agent systems, and a design for the integration of the two.",2003,0,
157,158,An empirical validation of the relationship between the magnitude of relative error and project size,"Cost estimates are important deliverables of a software project. Consequently, a number of cost prediction models have been proposed and evaluated. The common evaluation criteria have been MMRE, MdMRE and PRED(k). MRE is the basic metric in these evaluation criteria. The implicit rationale of using a relative error measure like MRE, rather than an absolute one, is presumably to have a measure that is independent of project size. We investigate if this implicit claim holds true for several data sets: Albrecht, Kemerer, Finnish, DMR and Accenture-ERP. The results suggest that MRE is not independent of project size. Rather, MRE is larger for small projects than for large projects. A practical consequence is that a project manager predicting a small project may falsely believe in a too low MRE. Vice versa when predicting a large project. For researchers, it is important to know that MMRE is not an appropriate measure of the expected MRE of small and large projects. We recommend therefore that the data set be partitioned into two or more subsamples and that MMRE is reported per subsample. In the long term, we should consider using other evaluation criteria.",2002,0,
158,159,Exact symbol-error probability analysis for orthogonal space-time block codes: two- and higher dimensional constellations cases,"Exact expressions are obtained for the symbol-error probability of orthogonal space-time block codes at the output of the coherent maximum-likelihood decoder in the general case of arbitrary input signal constellation and code. Such expressions are derived for the cases of both deterministic (fixed) and random Rayleigh/Ricean fading channels, and both the two- and higher dimensional constellations.",2004,0,
159,160,Which concurrent error detection scheme to choose ?,"Concurrent error detection (CED) techniques (based on hardware duplication, parity codes, etc.) are widely used to enhance system dependability. All CED techniques introduce some form of redundancy. Redundant systems we subject to common-mode failures (CMFs). While most of the studies of CED techniques focus on area overhead, few analyze the CMF vulnerability of these techniques. In this paper, we present simulation results to quantitatively compare various CED schemes based on their area overhead and the protection (data integrity) they provide against multiple failures and CMFs. Our results indicate that, for the simulated combinational logic circuits, although diverse duplex systems (with two different implementations of the same logic function) sometimes have marginally higher area overhead, they provide significant protection against multiple failures and CMFs compared to other CED techniques like parity prediction",2000,0,
160,161,Optimal scheduling of imprecise computation tasks in the presence of multiple faults,"With the advance of applications such as multimedia, image/speech processing and real-time AI, real-time computing models allowing to express the timeliness versus precision trade-off are becoming increasingly popular. In the imprecise computation model, a task is divided into a mandatory part and an optional part. The mandatory part should be completed by the deadline even under worst-case scenario; however, the optional part refines the output of a mandatory part within the limits of the available computing capacity. A non-decreasing reward function is associated with the execution of each optional part. Since the mandatory parts have hard deadlines, provisions should be taken against faults which may occur during execution. An FT-Optimal framework allows the computation of a schedule that simultaneously maximizes the total reward and tolerates transient faults of mandatory parts. We extend the framework to a set of tasks with multiple deadlines, multiple recovery blocks and precedence constraints among them. To this aim, we first obtain the exact characterization of imprecise computation schedules which can tolerate up to k faults, without missing any deadlines of mandatory parts. Then, we show how to generate FT-Optimal schedules in an efficient way. Our solution works for both linear and general concave reward functions",2000,0,
161,162,Initial Evaluation of the Fracture Behavior of Piezoelectric Single Crystals Due to Artificial Surface Defects,"This study is part of a new research program to develop fundamental understanding of the fracture and fatigue behavior of piezoelectric single crystals through the combination of computational and experimental approaches. In this work we present 1) experimental results on the creation of artificial surface defects in piezoelectric single crystals using a focused ion beam (FIB) system and 2) initial observations on the crystal's fracture behavior under an electrical field. The major advantage of using a FIB is that one can control the size, shape, and orientation of artificial defects precisely, allowing realistic surface defects, e.g., half-penny-shaped, 100 mum long, <1 mum wide, and 50 mum deep. We have demonstrated that multiple artificial defects with varying inclination angles relative to the specimen's crystallographic orientation can be machined in a few hours. In this paper, we report the experimental details of the FIB milling, typical defect shape, and initial results on the effects of high electric field on the fracture behavior of single crystals.",2006,0,
162,163,A parallel and fault tolerant file system based on NFS servers,"One important piece of system software for clusters is the parallel file system. All current parallel file systems and parallel I/O libraries for clusters do not use standard servers, thus it is very difficult to use these systems in heterogeneous environments. However why use proprietary or special-purpose servers on the server end of a parallel file system when you have most of the necessary functionality in NFS servers already? This paper describes the fault tolerance implemented in Expand (Expandable Parallel File System), a parallel file system based on NFS servers. Expand allows the transparent use of multiple NFS servers as a single file system, providing a single name space. The different NFS servers are combined to create a distributed partition where files are stripped. Expand requires no changes to the NFS server and uses RPC operations to provide parallel access to the same file. Expand is also independent of the clients, because all operations are implemented using RPC and NFS protocol. Using this system, we can join heterogeneous servers (Linux, Solaris, Windows 2000, etc.) to provide a parallel and distributed partition. Fault tolerance is achieved using RAID techniques applied to parallel files. The paper describes the design of Expand and the evaluation of a prototype of Expand, using the MPI-IO interface. This evaluation has been made in Linux clusters and compares Expand with PVFS.",2003,0,
163,164,Detection and correction of limit cycle oscillations in second -order recursive digital filter,"In this paper the effects of limit cycle oscillations in recursive second order digital filter is studied and the remedies for curing the problems of limit cycle oscillations are described. Limit cycle deletion using state space representation for second order system implemented with finite word-length register, is depicted. Necessary and sufficient condition to prevent limit cycle oscillation has also been described.",2005,0,
164,165,A Flexible Macroblock Scheme for Unequal Error Protection,"This paper proposes an enhanced error protection scheme using flexible macroblock ordering in H.264/AVC. The algorithm uses a two-phase system. In the first phase, the importance of every macroblock is calculated based on its influence on the current frame and future frames. In the second phase, the macroblocks with the highest impact factor are grouped together in a separate slice group using the flexible macroblock ordering feature of H.264/AVC. By using an unequal error protection scheme, the slice group containing the most important macroblocks can be better protected than the other slice group. The proposed algorithm offers better concealment opportunities than the algorithms which are predefined for flexible macroblock ordering in H.264/AVC.",2006,0,
165,166,Automated defect to fault translation for ASIC standard cell libraries,"Popular generic fault models, which exhibit limited realism for different IC technologies, have been widely misused due to their simplicity and cost-effective implementation. This paper introduces a system for deriving accurate, technology specific fault models that are based on analog defect simulation. The technique is formally defined and a systematic approach is developed. It is supported by a new software tool that provides a push-button solution for the previously tedious task of obtaining accurate ASIC cell defect to fault mappings. Furthermore, upon completion of the cell defect analysis, the tool automatically generates VITAL compliant, defect-injectable, VHDL cell models",2001,0,
166,167,A comparison of neural networks and model-based methods applied for fault diagnosis of electro-hydraulic control systems,The paper aims to investigate two advanced methods used in fault diagnosis of electro-hydraulic (EH) control systems. The theoretical background of the neural network method and model-based approach are presented and the implementation of these methods is summarised with procedures in easy steps to follow for application. The pros and cons of these methods are also analysed based on fault detection capability. It is concluded that a combination of the neural network method and the model-based approach will be beneficial.,2002,0,
167,168,A Predictive Fault Tolerance Agent based on Ubiquitous Computing for A Home Study System,"DOORAE (Distance Object Oriented Collaboration Environment) is a framework for supporting development on multimedia collaborative environment. It provides functions well capable of developing multimedia distance education system for students as well as teachers. It includes session management, access control, concurrency control and handling late comers. There are two approaches to software architecture on which applications for multimedia distance education environment in situation-aware middleware are based. This paper proposes a new model of fault tolerance agent based on situation-aware ubiquitous computing for a multimedia home study system which is based on CARV.",2007,0,
168,169,Model of stator inter-turn short circuit fault in doubly-fed induction generators for wind turbine,"The doubly fed induction generator (DFIG) is an important component of wind turbine systems. It is necessary to identify incipient faults quickly. This paper proposes a complete simulation model of DFIG in wind turbine about inter-turn short circuit fault at stator windings, which is based on multi-circuit theory. A detail analysis about simulation results is presented, especially about short circuit current. By analysis, the apparent 150 Hz, 450 Hz and current phase angle difference are taken as fault features and the fault phase also can be detected by phase angle difference. Both simulated results and experimental results of emulated inter-turn short circuit fault by paralleling a resistance with phase A are carried out. They verify the preceding analysis results. Moreover, their coincidence certificates this model is good and simulation results of inter-turn short circuit fault are correct.",2004,0,
169,170,Self-healing strategies for component integration faults,"Software systems increasingly integrate Off-The-Shelf (OTS) components. However, due to the lack of knowledge about the reused OTS components, this integration is fragile and can cause in the field a lot of failures that result in dramatic consequences for users and service providers, e.g. loss of data, functionalities, money and reputation. As a consequence, dynamic and automatic fixing of integration problems in systems that include OTS components can be extremely beneficial to increase their reliability and mitigate these risks. In this paper, we present a technique for enhancing component-based systems with capabilities to self-heal common integration faults by using a predetermined set of healing strategies. The set of faults that can be healed has been determined from the analysis of the most frequent integration bugs experienced by users according to data in bug repositories available on Internet. An implementation based on AOP techniques shows the viability of this technique to heal faults in real case studies.",2008,0,
170,171,Recursive Evaluation of Fault Tolerance Mechanisms for SLA Management,"Service level agreements (SLAs) have been introduced into the grid in order to build a basis for its commercial uptake. The challenge for Grid providers in agreeing and operating SLA-bound jobs is to ensure their fulfillment even in the case of failures. Hence, fault-tolerance mechanisms are an essential means of the provider's SLA management. The high utilization of commercial operated clusters leads to scenarios in which typically a job migration effects other jobs scheduled. The effects result from the unavailability of enough free resources which would be needed to catch all resource outages. Consequently before initiating a migration, its effects for other jobs have to be compared and the initiation of fault- tolerance (FT-) mechanisms have to be evaluated recursively. This paper presents a measurement for the benefit of initiating a FT-mechanism, the recursive evaluation, and termination condition. Performing such an impact evaluation of an initiated chain of FT-mechanisms is often more profitable than performing a single FT-mechanism and accordingly this is important for the Grid commercialization.",2008,0,
171,172,Corrective control strategies in case of infeasible operating situations,"A new method that deals with power systems infeasible operating situations is proposed in this paper. In case these situations occur, appropriate corrective actions must be efficiently obtained and quickly implemented. In order to accomplish this, it is necessary (a) to quantify the systems unsolvability degree (UD), and (b) to determine a corrective control strategy to pull the system back into the feasible operation region. UD is determined through the smallest distance between the infeasible (unstable) operating point and the feasibility boundary in parameter (load) space. In this paper the control strategies can be obtained by two methods, namely the proportionality method (PM) and the nonlinear programming based method (NLPM). Capacitor banks, tap changing transformers and load shedding are the usual controls available. Simulations have been carried out, for small to large systems, under contingency and heavy load situations, in order to show the efficiency of the proposed method. It can be a very useful tool in operation planning studies, particularly in voltage stability analysis.",2001,0,
172,173,Fault detection using phenomenological models,"There exist many different established approaches to detect system faults. This paper discusses the various system models and the associated fault detection techniques. Specifically, phenomenological models are presented in detail. Fault detection using principal components analysis and the cluster and classify method is illustrated with real operational data from an electrically powered vehicle.",2003,0,
173,174,Using Search Methods for Selecting and Combining Software Sensors to Improve Fault Detection in Autonomic Systems,"Fault-detection approaches in autonomic systems typically rely on runtime software sensors to compute metrics for CPU utilization, memory usage, network throughput, and so on. One detection approach uses data collected by the runtime sensors to construct a convex-hull geometric object whose interior represents the normal execution of the monitored application. The approach detects faults by classifying the current application state as being either inside or outside of the convex hull. However, due to the computational complexity of creating a convex hull in multi-dimensional space, the convex-hull approach is limited to a few metrics. Therefore, not all sensors can be used to detect faults and so some must be dropped or combined with others. This paper compares the effectiveness of genetic-programming, genetic-algorithm, and random-search approaches in solving the problem of selecting sensors and combining them into metrics. These techniques are used to find 8 metrics that are derived from a set of 21 available sensors. The metrics are used to detect faults during the execution of a Java-based HTTP web server. The results of the search techniques are compared to two hand-crafted solutions specified by experts.",2010,0,
174,175,Experimental study on the impact of endoscope distortion correction on computer-assisted celiac disease diagnosis,"The impact of applying barrel distortion correction to endoscopic imagery in the context of automated celiac disease diagnosis is experimentally investigated. For a large set of feature extraction techniques, it is found that contrasting to intuition, no improvement but even significant result degradation of classification accuracy can be observed. For techniques relying on geometrical properties of the image material (shape), moderate improvements of classification accuracy can be achieved. Reasons for this somewhat unexpected results are discussed and ways how to exploit potential distortion correction benefits are sketched.",2010,0,
175,176,A comparison of phase space reconstruction and spectral coherence approaches for diagnostics of bar and end-ring connector breakage faults in polyphase induction motors using current waveforms,"Two signal (waveform) analysis approaches are investigated in this paper for motor drive fault identification-one linear and the other nonlinear. Twenty-one different motor-drive operating conditions including healthy, 1 through 10 broken bars, and 1 through 10 broken end-ring connectors are investigated. Highly accurate numerical simulations of current waveforms for the various operating conditions are generated using the time stepping coupled finite element-state space method for a 208-V, 60-Hz, 2-pole, 1.2-hp, squirrel cage 3-phase induction motor. The linear signal analysis method is based on spectral coherence, whereas the nonlinear signal analysis method is based on stochastic models of reconstructed phase spaces. Conclusions resulting from the comparisons of these two methods are drawn.",2002,0,
176,177,Ontology-based fault diagnosis for industrial control applications,"Traditional fault detection systems in industrial control applications are just able to report occurring faults. Fault diagnosis systems are more desirable for plant operators, as such systems are capable to reduce the number of occurring alarms by elimination of consecutive alarms and prioritization of critical alarms. The disadvantage of those systems is that they have to be implemented anew for every control application, as the system dependencies vary from application to application. Ontology-based fault diagnosis systems do not have this disadvantage. Only the ontology has to be created for the new system, which greatly reduces time and effort for new systems, as old ontologies can be reused for the new system.",2010,0,
177,178,Ground control for the geometric correction of PAN imagery from Indian remote sensing (IRS) satellites,"Efficacy of a hand-held global positioning system (GPS) receiver in stand-alone mode of GPS measurements was investigated by taking measurements on different dates. These measurements were compared to those observed by DGPS and total station for the validation of hand-held GPS receiver accuracy. Ground control point (GCP) coordinates were derived from 1:25,000 and 1:50,000 scale topographic maps, and by using two different types of GPS receivers- a stand-alone hand-held and dual frequency DGPS receivers. GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using affine mapping function. GCPs derived from maps yielded root mean squares (RMS) error from 15 to 35 m respectively. However, GCPs derived by DGPS or stand-alone mode hand-held GPS receiver gave RMS error in the range of 3 to 6 meter, which is very close to spatial resolution of PAN sensor imagery (5.8 m). The mean values of GCP coordinates observed with the help of hand-held GPS receiver in stand-alone mode might prove a cost effective solution for the determination of GCP coordinates in the geometric correction of current high-resolution imagery from IRS satellites.",2003,0,
178,179,Synchronization Probabilities using Conventional and MVDR Beam Forming with DOA Errors,"In this paper, code synchronization probabilities of the direct sequence spread spectrum (DS/SS) system are investigated when the receiver utilizes an adaptive antenna array. Performance studies of three beam forming algorithms in the presence of direction-of-arrival (DOA) errors are presented. The investigated algorithms are the conventional, the minimum variance distortionless response (MVDR), and the MVDR+ADL where the MVDR algorithm is enhanced against DOA error via the adaptive diagonal loading (ADL). The paper includes a large number of analytical and simulation results where the effects of DOA errors are investigated. It can be concluded that the MVDR beam former is more sensitive to DOA errors than the conventional beam former, especially, at large DOA errors and high signal-to-noise-ratio (SNR) values. However, this sensitivity can be reduced notably by using the ADL.",2007,0,
179,180,Fault Tree Reuse Across Multiple Reasoning Paradigms,"The development of a diagnostic model can be a very time-consuming and manually intensive process. One must first analyze the Test Program Set (TPS) to determine the fault tree and then integrate with that any additional knowledge that can be obtained from external data sources (such as test results, maintenance actions from the various maintenance levels, run-time failure information, etc.). The diagnostic models defined in the IEEE Std. 1232-2002 (AI-ESTATE) each define a different method that can be used for a diagnostic reasoner. It has been determined that each of these models utilize the information found in the basic TPS fault tree. As the fault tree represents hard won engineering knowledge that is expensive to reproduce, it is desirable to share the fault tree representations across multiple reasoner models. This paper will layout how each model type in the AI-ESTATE standard utilizes the fault tree to perform diagnostics and how, through the use of the XML representation of the AI-ESTATE fault tree model, that basic fault tree can be shared between reasoner models. It would also be desirable to find a way to gain that fault tree knowledge without having to manually reproduce it. As such, this paper will also describe how that information can at least be semi-automatically extracted from TPS design artifacts.",2006,0,
180,181,Position location error analysis by AOA and TDOA using a common channel model for CDMA cellular environments,"AOA and TDOA are known to be promising methods and are developed separately. Combination or cooperation of the two methods has not been possible due to lack of the applicable channel models. The COST-207 model is utilized to provide the angular information for the AOA method. The angular characteristic of the channel can be obtained from the given temporal channel model, hence fusing of the approaches is now possible. Different properties of the methods are revealed to verify the proposition in literature. As a future research area, the fusing of the two different methods for better reliability and accuracy is mentioned",2000,0,
181,182,"Mutual coupling in microstrip antenna array: evaluation, reduction, correction or compensation","Mutual coupling between the antenna elements in a microstrip antenna array is a potential source of performance degradation, particularly in a highly congested environment. The degradation includes impedance mismatching, increased side-lobe level, deviation of the radiation pattern from the desired one, and decrease of gain due to the excitation of a surface wave. To deal with these problems, the first thing is to evaluate the mutual coupling and to select the element with low mutual coupling. Then, it is still desired to reduce the mutual coupling further by taking some measures. Finally, in certain critical cases, it is necessary to involve the mutual coupling effects accurately through numerical analysis, such as ultra low side lobe arrays and adaptive ing arrays. All these issues are discussed and some numerical examples are given. Due to limited space, the paper focuses mainly on the work done in our laboratory.",2005,0,
182,183,Geometric and shading correction for images of printed materials using boundary,"A novel technique that uses boundary interpolation to correct geometric distortion and shading artifacts present in images of printed materials is presented. Unlike existing techniques, our algorithm can simultaneously correct a variety of geometric distortions, including skew, fold distortion, binder curl, and combinations of these. In addition, the same interpolation framework can be used to estimate the intrinsic illumination component of the distorted image to correct shading artifacts. We detail our algorithm for geometric and shading correction and demonstrate its usefulness on real-world and synthetic data.",2006,0,
183,184,A novel co-evolutionary approach to automatic software bug fixing,"Many tasks in software engineering are very expensive, and that has led the investigation to how to automate them. In particular, software testing can take up to half of the resources of the development of new software. Although there has been a lot of work on automating the testing phase, fixing a bug after its presence has been discovered is still a duty of the programmers. In this paper we propose an evolutionary approach to automate the task of fixing bugs. This novel evolutionary approach is based on co-evolution, in which programs and test cases co-evolve, influencing each other with the aim of fixing the bugs of the programs. This competitive co-evolution is similar to what happens in nature for predators and prey. The user needs only to provide a buggy program and a formal specification of it. No other information is required. Hence, the approach may work for any implementable software. We show some preliminary experiments in which bugs in an implementation of a sorting algorithm are automatically fixed.",2008,0,
184,185,An NN-based atmospheric correction algorithm for Landsat/TM thermal infrared data,"Land surface temperature (LST) is a key variable for studies of global or regional land surface processes, energy and water cycle, and thus, has important applications in various areas. Atmospheric correction is a major issue in LST retrieval using remote sensing data because the presence of the atmosphere always influences the radiation from the ground to the space sensor. Atmospheric correction of thermal infrared (TIR) data for land surface temperature retrieval is to estimate the three atmospheric parameters: transmittance, path radiance and the downward radiance. Typically the atmospheric parameters are obtained using atmospheric profiles combined with a radiative transfer model (RTM). But this approach is time-consuming and expensive, which is impractical for high-speed (near-realtime) operational atmospheric correction. An artificial neural network (NN) based atmospheric correction model for Landsat/TM thermal infrared data is proposed. The multi-layer feed-forward neural network (MFNN) is selected, in which the atmospheric profiles (temperature, humidity and pressure), elevation and scan angle are the input variables, and the atmospheric parameters are the output variables. The MFNN is combined with the radiative transfer simulation, using MODTRAN 4.0 and the latest global assimilated data. Finally, the transmittance and path radiance derived by the MFNN-based algorithm is compared with MODTRAN4.0 results. The RMSE for both parameters are 0.0031 and 0.035 Wm<sup>-2</sup>sr<sup>-1</sup>m<sup>-1</sup>, respectively. The results indicate that the proposed approach can be a practical method for Landsat/TM thermal data in both accuracy and efficiency.",2010,0,
185,186,Fault tree analysis of a fire hazard of a power distribution cabinet with Petri Nets,Motivation of this study is to verify system safety analysis of HAVELSAN Peace Eagle Program developed hardware items for Ground Support Systems. A preliminary hazard analysis for each of the hardware developed items are performed and safety hazard analysis models are constructed with risk assessment of hazards based on their probability of occurrences for future operational and maintenance activities. An example for this kind of analysis the system safety fault tree analysis model of a Ground Support Segment Mission Simulator subsystem Power Distribution Adapter Cabinet design with hazardous risk assessments criteria according to the military standard specifications. Same analysis approach then modeled with Petri Nets that has extensions from fault tree analysis approach and enables the modeler to represent the probability of occurrences in the system design phase. Same model can be built in the specification phase which creates the potential for early validation of the system design behavior.,2010,0,
186,187,Three-Dimensional Pareto-Optimal Design of Inductive Superconducting Fault Current Limiters,"The inductive-type superconducting fault current limiters (LSFCLs) mainly consist of a primary copper coil, a secondary complete or partial superconductor cylinder, and a closed or open magnetic iron core. Satisfactory performance of such device significantly depends on optimal selection of its employed materials and construction dimensions, as well as its electrical, thermal, and magnetic parameters. Therefore, it is very important to identify a comprehensive model describing the LSFCL behavior in a power system prior to its fabrication. When a fault occurs, the dynamic model should essentially characterize the overall phenomena to compare the simulation results by varying LSFCL parameters to maximize the merits of a fault current limiter while minimizing its drawbacks during the normal state. The principle object of this paper is to achieve a feasible and full penetrative approach in 3-D alignments, i.e., a Pareto-optimal design of LSFCLs by means of multicriteria decision-making techniques after defining the LSFCL model in a power system CAD/electromagnetic transients including dc environment.",2010,0,
187,188,Stochastic fault tree analysis with self-loop basic events,"This paper presents an analytical approach for performing fault tree analysis (FTA) with stochastic self-loop events. The proposed approach uses the flow-graph concept, and moment generating function (MGF) to develop a new stochastic FTA model for computing the probability, mean time to occurrence, and standard deviation time to occurrence of the top event. The application of the method is demonstrated by solving one example.",2005,0,
188,189,Coverage gain estimation for multi-burst forward error correction in DVB-H networks,"An approach for increasing the reception robustness of mobile broadcast streaming services has been developed for mobile broadcast systems based on time-slicing, such as DVB-H, employing multi-burst FEC at link or application layer. Multiple bursts will be encoded jointly in order to overcome burst errors caused by signal level variations. The approach shows high potential which can be characterized by a link margin gain due to reduced CNR requirements to cope with fast fading and shadowing. Nevertheless, the achieved gain depends on several system parameters (encoding period and coding rate), the physical environment (correlation of shadowing and multi-path fading) and on the mobility of the users (velocity and trajectory). The paper deals with the coverage estimation and network gain due to multi-burst FEC for vehicular users in a realistic urban scenario. Since the user behavior has to be considered the gain cannot be directly included into the link budget. Thus, a methodology has been developed in order to estimate the coverage of multi-burst FEC services based on dynamic system-level simulations. Results are shown by means of simulations in realistic scenarios and field measurements in urban environments.",2009,0,
189,190,Dynamic strength scaling for delay fault propagation in nanometer technologies,"This paper proposes an algorithm for the detection of resistive delay faults in deep submicron technology using dynamic strength scaling, which is applicable for 45 nm and below. The approach uses an advanced coding system to build logical functions that are sensitive to strength and able to detect even the slightest voltage changes in the circuit. Such changes are caused by interconnection resistive behavior and result in timing-related defects.",2009,0,
190,191,LEON3 ViP: A Virtual Platform with Fault Injection Capabilities,"In addition to functional simulation for validation of hardware/software designs, there are additional robustness requirements that need advanced simulation techniques and tools to analyze the system behavior in the presence of faults. In this paper, we present the design of a fault injection framework for LEON3, a 32bit SPARC CPU based system used by the European Space Agency, described at Transaction Level using System C. First of all an extension of a previous XML formalization of basic binary faults, like memory and CPU registers corruption, is done in order to support TLM2.0transaction's parameters corruptions. Next a novel Dynamic Binary Instrumentation (DBI) technique for C++ binaries is used to insert fault injection wrappers in SystemC transaction path. For binary faults in model components the use of TLM2.0 transport_dbg is proposed. This way each component with fault injection capabilities exposes a standard interface to allow internal component inspection and modification.",2010,0,
191,192,Research on Transformer Fault Diagnosis Expert System Based on DGA Database,"This paper analyzes and designs the transformer fault diagnosis system based on dissolved gas analysis (DGA) database in which DGA data is managed by the Oracle database. The fault diagnosis module includes the single analyzing item and the Integrated analyzing item, such as, improvement three-ratio method, grey relational entropy, fuzzy clustering, artificial neural networks, and so on. They reduce the insufficiency in diagnosis method which is used now. The system realizes each function of the modules by using the lamination method, it is able to diagnose problems existing in oil chromatogram analysis data of transformer, and the accuracy of the system is also testified by practical example.",2009,0,
192,193,Modular fault recovery in timed discrete-event systems: application to a manufacturing cell,"This paper extends the previous results of the authors on fault recovery to timed discrete-event systems (TDES), and discusses the application of the proposed methodology to a manufacturing cell. It is assumed that the plant can be modelled as a TDES, the faults are permanent, and that a diagnosis system is available that detects and isolates faults with a bounded delay (expressed in clock ticks). Thus, the combination of the plant and the diagnosis system, as the system to be controlled, has three modes: normal, transient and recovery. Initially, the plant is in the normal mode. Once a fault occurs, the system enters the transient mode. After the fault is detected and isolated by the diagnosis system, the system enters the recovery mode. This framework does not depend on the diagnosis technique used, as long as lower and upper bounds for diagnosis delay are available. A modular switching supervisory scheme is proposed to satisfy the system specifications. The design consists of a normal-transient supervisor, and multiple recovery supervisors each for recovery from a particular failure mode. The issue of the nonblocking property of the system under supervision, and also supervisor admissibility (controllability), in particular coerciveness, are studied. The proposed approach is applied to a manufacturing cell consisting of two machines and two conveyors. A modular switching supervisor is designed to ensure the specifications in the normal mode are met. In cases of failure, the supervisor sends appropriate recovery commands so that the cell can complete its production cycle",2005,0,
193,194,Managing fault-induced delayed voltage recovery in Metro Atlanta with the Barrow County SVC,"Georgia Transmission Corporation (GTC) commissioned the Barrow County Static Var Compensator (SVC) with a continuous rating of 0 to +260 Mvar in June of 2008. This paper presents the northern Metro Atlanta Georgia area transmission system, the requirements for voltage and var support, the dynamic performance study used to verify performance of the SVC, and provides an overview of the SVC design and control strategy, including the SVC's response to an actual power system disturbance. The Barrow County SVC is connected to the 230 kV bus at the Winder Primary Substation to effectively manage the exposure to fault-induced delayed voltage recovery (FIDVR), where the system voltage remains low (<80%) for several seconds following a disturbance and potentially leading to voltage collapse. The SVC configuration includes two thyristor-switched capacitors for rapid insertion of reactive power following a disturbance to decrease voltage recovery time and control the system's dynamic performance.",2009,0,
194,195,Investigation of fault tolerant of direct torque control in induction motor drive,"AC drives based on direct torque control (DTC) of induction machines are known for their high dynamic performances, obtained with very simple control schemes. So many studies have been performed with ASIC or FPGA DTC implementation. In this paper, we investigate for the tolerance of such drive to sensors defects, when the control algorithm is to be implemented in an FPGA. So, authors specially focus on the influence of the FPGA implementation design on the DTC fault tolerance. Simulations are carried out with system generator (SG) toolbox working in the MATLAB/SIMULINK environment. Results are presented and discussed to evaluate the DTC operating under considered faults.",2004,0,
195,196,Operational Fault Detection in cellular wireless base-stations,"The goal of this work is to improve availability of operational base-stations in a wireless mobile network through non-intrusive fault detection methods. Since revenue is generated only when actual customer calls are processed, we develop a scheme to minimize revenue loss by monitoring real-time mobile user call processing activity. The mobile user call load profile experienced by a base-station displays a highly non-stationary temporal behavior with time-of-day, day-of-the-week and time-of-year variations. In addition, the geographic location also impacts the traffic profile, making each base-station have its own unique traffic patterns. A hierarchical base-station fault monitoring and detection scheme has been implemented in an IS-95 CDMA Cellular network that can detect faults at - base station level, sector level, carrier level, and channel level. A statistical hypothesis test framework, based on a combination of parametric, semi-parametric and non-parametric test statistics are defined for determining faults. The fault or alarm thresholds are determined by learning expected deviations during a training phase. Additionally, fault thresholds have to adapt to spatial and temporal mobile traffic patterns that slowly changes with seasonal traffic drifts over time and increasing penetration of mobile user density. Feedback mechanisms are provided for threshold adaptation and self-management, which includes automatic recovery actions and software reconfiguration. We call this method, Operational Fault Detection (OFD). We describe the operation of a few select features from a large family of OFD features in Base Stations; summarize the algorithms, their performance and comment on future work.",2006,0,
196,197,Fault Tolerance for Manufacturing Components,"This article proposes a multiagent system for industrial production elements that transfers the concept of fault tolerance to the manufacturing levels of the organisation, acting automatically under open protocols when there is degradation or failure of any of the components, ensuring that normal operation is resumed within a delimited time. The main characteristics of this system are the drastic reduction in recovery times, the support for the significant heterogeneity existing in these scenarios and their high level of automation, while practically dispensing with the intervention of system administrators.",2006,0,
197,198,A study of student strategies for the corrective maintenance of concurrent software,"Graduates of computer science degree programs are increasingly being asked to maintain large, multi-threaded software systems; however, the maintenance of such systems is typically not well-covered by software engineering texts or curricula. We conducted a think-aloud study with 15 students in a graduate-level computer science class to discover the strategies that students apply, and to what effect, in performing corrective maintenance on concurrent software. We collected think-aloud and action protocols, and annotated the protocols for a number of behavioral attributes and maintenance strategies. We divided the protocols into groups based on the success of the participant in both diagnosing and correcting the failure. We evaluated these groups for statistically significant differences in these attributes and strategies. In this paper, we report a number of interesting observations that came from this study. All participants performed diagnostic executions of the program to aid program comprehension; however, the participants that used this as their predominant strategy for diagnosing the fault were all unsuccessful. Among the participants that successfully diagnosed the fault and displayed high confidence in their diagnosis, we found two commonalities. They all recognized that the fault involved the violation of a concurrent-programming idiom. And, they all constructed detailed behavioral models (similar to UML sequence diagrams) of execution scenarios. We present detailed analyses to explain the attributes that correlated with success or lack of success. Based on these analyses, we make recommendations for improving software engineering curriculums by better training students how to apply these strategies effectively.",2008,0,
198,199,The feasibility study on the combined equipment between micro-SMES and inductive/electronic type fault current limiter,"The concept of the combined equipment between micro-SMES and inductive/electronic type FCL is proposed in this paper. Having the multifunction for a superconducting device, the new equipment can serve as the protective component for a dual power system. The specification of a testing model was determined and the transient performance was analyzed by Matlab software. The results show that the combined equipment is realizable for a dual power system application, where it has the major function of limiting fault current (FCL function) and the minor function of maintaining power fluctuation (SMES function).",2003,0,
199,200,Effective Static Analysis to Find Concurrency Bugs in Java,"Multithreading and concurrency are core features of the Java language. However, writing a correct concurrent program is notoriously difficult and error prone. Therefore, developing effective techniques to find concurrency bugs is very important. Existing static analysis techniques for finding concurrency bugs either sacrifice precision for performance, leading to many false positives, or require sophisticated analysis that incur significant overhead. In this paper, we present a precise and efficient static concurrency bugs detector building upon the Eclipse JDT and the open source WALA toolkit (which provides advanced static analysis capabilities). Our detector uses different implementation strategies to consider different types of concurrency bugs. We either utilize JDT to syntactically examine source code, or leverage WALA to perform interprocedural data flow analysis. We describe a variety of novel heuristics and enhancements to existing analysis techniques which make our detector more practical, in terms of accuracy and performance. We also present an effective approach to create inter-procedural data flow analysis using WALA for complex analysis. Finally we justify our claims by presenting the results of applying our detector to a range of real-world applications and comparing our detector with other tools.",2010,0,
200,201,A case study of evaluation technique for soft error tolerance on SRAM-based FPGAs,"SRAM-based field programmable gate arrays (FPGAs) are vulnerable to a single event upset (SEU), which is induced by radiation effect. Therefore, the dependable design techniques become important, and the accurate dependability analysis method is required to demonstrate their robustness. Most of present analysis techniques are performed by using full reconfiguration to emulate the soft error. However, it takes long time to analyze the dependability because it requires many times of reconfiguration to complete the soft error injection. In the present paper, we construct the soft error estimation system to analyze the reliability and to reduce the estimation time. Moreover, we apply Monte Carlo simulation to our approach, and identify trade-off between accuracy of error rate and estimation time. As a result of our experimentation for 8-bit full-adder and multiplier, we can show the dependability of the implemented system. Also, the constructed system can reduce the estimation time. According to the result, when performing about 50% circuit Monte Carlo simulation, the error rate is within 20%.",2010,0,
201,202,Effects of finite weight resolution and calibration errors on the performance of adaptive array antennas,"Adaptive antennas are now used to increase the spectral efficiency in mobile telecommunication systems. A model of the received carrier-to-interference plus noise ratio (CINR) in the adaptive antenna beamformer output is derived, assuming that the weighting units are implemented in hardware, The finite resolution of weights and calibration is shown to reduce the CINR. When hardware weights are used, the phase or amplitude step size in the weights can be so large that it affects the maximum achievable CINR. It is shown how these errors makes the interfering signals leak through the beamformer and we show how the output CINR is dependent on power of the input signals. The derived model is extended to include the limited dynamic range of the receivers, by using a simulation model. The theoretical and simulated results are compared with measurements on an adaptive array antenna testbed receiver, designed for the GSM-1800 system. The theoretical model was used to find the performance limiting part in the testbed as the 1 dB resolution in the weight magnitude. Furthermore, the derived models are used in illustrative examples and can be used for system designers to balance the phase and magnitude resolution and the calibration requirements of future adaptive array antennas",2001,0,
202,203,Fault Tolerant Methods for Intermitted Failures in Virtual Large Scale Disks,"Recently, the demand of low cost large scale storages increases. We developed VLSD (Virtual Large Scale Disks) toolkit for constructing virtual disk based distributed storages, which aggregate free spaces of individual disks. However, current implementation of VLSD can mask only stop failure but cannot mask other kinds of failures such as intermitted failure. In this paper, we introduce two classes to VLSD in order to increase the intermitted fault tolerance. One is Retry Disk which retries to read/write at failures, another is VotedRAID1 which masks failures by majority voting. In this paper, we describe these classes in detail and evaluate their fault tolerance.",2010,0,
203,204,Advances in scatter correction for 3D PET/CT,"We report on several significant improvements to the implementation of image-based scatter correction for 3D PET and PET/CT. Among these advances are: a new algorithm to scale the estimated scatter sinogram to the measured data, thereby largely compensating for external scatter; the ability to handle CT image truncation during this scaling; the option to iterate the scatter calculation for improved accuracy; the use of ordered subset estimation maximization (OSEM) reconstruction for the estimated emission images from which the scatter contributions are simulated; reporting of data quality parameters such as scatter and randoms fractions, and noise equivalent count rate (NECR), for each patient bed position; and extensive quality control output. Scatter correction (2 iterations, OSEM) typically requires 15-45 sec per bed. Very good agreement between the estimated scatter and measured emission data for several typical clinical scans is reported for CPS Pico-3D and HiRez LSO PET/CT systems. The data characteristics extracted during scatter correction are useful for patient specific count rate modeling and scan optimization",2004,0,
204,205,Time and frequency domain analyses based expert system for impulse fault diagnosis in transformers,"The presence of insulation failure in the transformer winding is detected using the voltage and current oscillograms recorded during the impulse test. Fault diagnosis in transformers has several parameters such as the severity of fault, the kind of fault and the location of the fault. Detection of major faults involving a large section of the coils have never been a big issue and several visual and computational methods have already been proposed by several researchers. The present paper describes an expert system based on re-confirmative method for the diagnosis of minor insulation failures involving small number of turns in transformers during impulse tests. The proposed expert system imitates the performance of an experienced testing personnel. To identify and locate a fault, an inference engine is developed to perform deductive reasoning based on the rules in the knowledge base and different statistical techniques. The expert system includes both the time-domain and frequency-domain analyses for fault diagnosis. The basic aim of the expert system is to provide a non-expert with the necessary information and interaction in order to make fault diagnosis in a friendly windowed environment. The rules for fault diagnosis have been so designed that these are valid for the range of power transformers used in practice up to a voltage level of 33 kV. The fault diagnosis algorithm has been tested using experimental results obtained for a 3 MVA transformer and simulation results obtained for 5 and 7 MVA transformers",2002,0,
205,206,Fault Detection Using the k-Nearest Neighbor Rule for Semiconductor Manufacturing Processes,"It has been recognized that effective fault detection techniques can help semiconductor manufacturers reduce scrap, increase equipment uptime, and reduce the usage of test wafers. Traditional univariate statistical process control charts have long been used for fault detection. Recently, multivariate statistical fault detection methods such as principal component analysis (PCA)-based methods have drawn increasing interest in the semiconductor manufacturing industry. However, the unique characteristics of the semiconductor processes, such as nonlinearity in most batch processes, multimodal batch trajectories due to product mix, and process steps with variable durations, have posed some difficulties to the PCA-based methods. To explicitly account for these unique characteristics, a fault detection method using the k-nearest neighbor rule (FD-kNN) is developed in this paper. Because in fault detection faults are usually not identified and characterized beforehand, in this paper the traditional kNN algorithm is adapted such that only normal operation data is needed. Because the developed method makes use of the kNN rule, which is a nonlinear classifier, it naturally handles possible nonlinearity in the data. Also, because the FD-kNN method makes decisions based on small local neighborhoods of similar batches, it is well suited for multimodal cases. Another feature of the proposed FD-kNN method, which is essential for online fault detection, is that the data preprocessing is performed automatically without human intervention. These capabilities of the developed FD-kNN method are demonstrated by simulated illustrative examples as well as an industrial example.",2007,0,
206,207,ANN based detection of electrical faults in generator-transformer units,"In the paper a model of decision system based on ANN, will be shown. As protected object the generator-transformer unit has been taking into consideration. The range of detected faults are initially narrows to faults of electromagnetic character (three-phase, two-phase, two-phase to earth and one-phase faults) within the generator - unit transformer - high voltage transmission line configuration.",2004,0,
207,208,Research on remote intelligent fault-diagnosis of CNC lathe based on bayesian networks,"Considering the development of smart machine tools and Internet-based manufacturing and in order to manage the manufacturing process more efficiently, a unit of remote intelligent fault-diagnosis based on Bayesian Networks (BN) was designed and software based on internet was realized as well as the case study concerning CNC lathe. It is a compensation of machine tool's self-detection whose major job is to find the fault of hardware and programming. The case study proved the reliability and advantages of the intelligent model based on BN.",2010,0,
208,209,Soft error optimization of standard cell circuits based on gate sizing and multi-objective genetic algorithm,"A radiation harden technique based on gate sizing and multi-objective genetic algorithm (MOGA) is developed to optimize the soft error tolerance of standard cell circuits. Soft error rate (SER), chip area and longest path delay are selected as the optimization goals and fast fitness evaluation algorithms for the three goals are developed and embedded into the MOGA. All the three goals are optimized simultaneously by optimally sizing the gates in the circuit, which is a complex NP-Complete problem and resolved by MOGA through exploring the global design space of the circuit. Syntax analysis technique is also employed to make the proposed framework can optimize not only pure combinational logic circuit but also the combinational parts of sequential logic circuit. Optimizing experiments carried out on ISCAS'85 and ISCAS'89 standard benchmark circuits show that the proposed optimization algorithm can decrease the SER 74.25% with very limited delay overhead (0.28%). Furthermore, the algorithm can also reduce the area for most of the circuit under test by average 5.23%. The proposed technique is proved to be better than other works in delay and area overhead and suitable to direct the design of soft error tolerance integrated circuits in high reliability realms.",2009,0,
209,210,Nonlinear Systems Fault Diagnosis with Differential Elimination,"The differential elimination algorithm is used to eliminate the non-observed variables of the nonlinear systems. By incorporating the algebraic observability and diagnosability concepts and using numerical differentiation algorithms, another approach to the certain classes of nonlinear systems fault diagnosis problem is presented.",2009,0,
210,211,Fault Tolerant Actuation for Steer-by-Wire Applications,"This paper introduces a R&D project concerned with the development of a fault-tolerant actuation system for steer-by-wire applications. The essential safety and reliability requirements for automotive vehicles are assessed. General redundancy schemes and current practices are examined. The paper then focuses on the use of actuators based on permanent magnetic brushless dc motors, and analyses internal fault-tolerant potentials of the actuator technology with possible control schemes evaluated. Finally key innovations that may provide practical and affordable solutions are discussed.",2007,0,
211,212,A procedure to correct the error in the structure function based thermal measuring methods,In this paper a methodology is presented to correct the systematic error of structure function based thermal material parameter measuring methods. This error stems from the fact that it is practically impossible to avoid parallel heat-flow paths in case of forced one-dimensional heat conduction. With the presented method we show how to subtract the effect of the parallel heat-flow paths from the measured structure function. With this correction methodology the systematic error of structure function based thermal material parameter measuring methods can be practically eliminated. Application examples demonstrate the accuracy increase obtained with the use of the method.,2004,0,
212,213,Backward-compatible robust error protection of JPEG XR compressed video,"The new JPEG XR image encoding standard offers a great compression rate while maintaining a good visual quality. Nonetheless, it has low error robustness, making it unusable in case of unreliable transmission over error prone channels, e.g., wireless channels. An improvement to the standard was developed, which can correct transmission errors, both bit or packet losses, and which is fully compatible with legacy decoders. Data interleaving and channel coding can offer a good protection against transmission errors; different levels of protection can be adopted, in order to trade-off between error protection capabilities and decompressed image quality.",2010,0,
213,214,Application of fuzzy neuro for generator stator earth fault detection,"In this paper the use of a fuzzy neural net for stator earth fault detection is presented. A generator model is simulated using EMTDC software. Earth faults are simulated between 0.1% to 100% distance points from the generator neutral. The combination of both EMTDC simulation and neural network presented in this paper introduces a new, complementary method that performs better in instances where the interpretation of traditional methods is somewhat dubious.",2004,0,
214,215,Research and Realization of Digital Circuit Fault Probe Location Process,"This paper presents three core files relating to circuit fault diagnosis which is generated by LASAR (logic automated stimulus response), i.e. fault dictionary, node truth table and pin connection table, analyses the content of fault dictionary, pin connection table and node truth table, finds the necessary information for fault location, summarizes the procedure of circuit test and fault location. Finally the digital circuit diagnosis system which can locate the fault on the pin of components is designed. With the help of probe, fault location of component pins can be accurately pinpointed.",2008,0,
215,216,New results for fault detection of untimed continuous Petri nets,"In this paper we study fault diagnosis of systems modeled by untimed continuous Petri nets. In particular, we generalize our previous works in this framework where we solved this problem only for special classes of continuous Petri nets, namely state machines and backward conflict free nets. We show that the price to pay for this generalization is that only three diagnosis states can be defined, rather than four. However, this is not a significant restriction because it is in accordance with all the literature on finite state automata.",2009,0,
216,217,Application of Particle Swarm Optimization and RBF Neural Network in Fault Diagnosis of Analogue Circuits,"BP neural network has the shortcoming of over-fitting, local optimal solution, which affects the practicability of BP neural network. RBF neural network is a feedforward neural network, which has the global optimal closing ability. However, the parameters in RBF neural network need determination. Particle swarm optimization is presented to choose the parameters of RBF neural network. The particle swarm optimization-RBF neural network method has high classification performance, and is applied to fault diagnosis of analogue circuits. Finally, the result of fault diagnosis cases shows that the particle swarm optimization - RBF neural network method has higher classification than BP neural network.",2009,0,
217,218,Development of a Testbench for Validation of DMT and DT2 Fault-Tolerant Architectures on SOI PowerPC7448,The purpose of TAFT fault tolerance studies conducted at CNES is to prepare the space community for the significant evolution linked to the usage of COTS components for developing spacecraft supercomputers. CNES has patented the DMT and DT2 fault-tolerant architectures with 'light' features. The development of a DMT/DT2 testbench based on a PowerPC7448 microprocessor from e2v is presented in this paper.,2008,0,
218,219,Compact Power Divider using Defected Ground Structure for Wireless Applications,"Use of different types of defected ground structures (DCS) has been reported in this paper to design compact power dividers in microstrip medium. Unit cell's (of DGS) equivalent circuit has been used to evaluate the performance of power divider. Based on this approach, compact two-way equal power dividers have been designed in GSM (900 MHz) band. Results show a size reduction of 35% and 32% for the power dividers using T shaped DGS and split ring DGS over the conventional power divider.",2008,0,
219,220,Utilisation of motion similarity in Colour-plus-Depth 3D video for improved error resiliency,"Robust 3D stereoscopic video transmission over error-prone networks has been a challenging task. Sustainability of the perceived 3D video quality is essential in case of channel losses. Colour-plus-Depth format on the other hand, has been popular for representing the stereoscopic video, due to its flexibility, low encoding cost compared to left-right stereoscopic video and backwards compatibility. Traditionally, the similarities existing between the colour and the depth map videos are not exploited during 3D video coding. In other words, both components are encoded separately. The similarities include the similarity in motion, image gradients and segments. In this work, we propose to exploit the similarity in the motion characteristics of the colour and the depth map videos by computing only a set of motion vectors and duplicating it for the sake of error resiliency. As the previous research has shown that the stereoscopic video quality is primarily affected by the colour texture quality, especially the motion vectors are computed for the colour video component and the corresponding vectors are used to encode the depth maps. Since the colour motion vectors are protected by duplication, the results have shown that both the colour video quality and the overall stereoscopic video quality are maintained in error-prone conditions at the expense of slight loss in depth map video coding performance. Furthermore, total encoding time is reduced by not calculating the motion vectors for depth map.",2010,0,
220,221,Embryonics+immunotronics: a bio-inspired approach to fault tolerance,"Fault tolerance has always been a standard feature of electronic systems intended for long-term missions. However, the high complexity of modern systems makes the incorporation of fault tolerance a difficult task. Novel approaches to fault tolerance can be achieved by drawing inspiration from nature. Biological organisms possess characteristics such as healing and learning that can be applied to the design of fault-tolerant systems. This paper extends the work on bio-inspired fault-tolerant systems at the University of York. It is proposed that by combining embryonic arrays with an immune inspired network, it is possible to achieve systems with higher reliability",2000,0,
221,222,"Video image based attenuation correction for PETbox, a preclinical PET tomograph","PETBox is a new simplified bench top PET scanner dedicated for pre-clinical imaging of mice. It has only two facing detector heads in a static gantry. Using iterative methods, limited-angle reconstruction of 3D images is possible. The geometry of the PETBox is such that very oblique emission angles are detected traversing significant lengths of tissue, making attenuation correction necessary. To that effect, we have developed a method by which two orthogonal optical views are combined to create a 3-dimensional estimate of the subject. This estimate is used to produce attenuation correction data that significantly improve the quantitative accuracy of the reconstructed images. In this paper, we present the method and evaluate its accuracy.",2009,0,
222,223,Analysis and design of SEPIC converter in boundary conduction mode for universal-line power factor correction applications,"In this paper, a SEPIC converter operated in boundary conduction mode for power factor correction applications with arbitrary output voltage is proposed, analyzed and designed. By developing an equivalent circuit model for the coupled inductor structure, a SEPIC converter with or without coupled inductors (and ripple current steering) can be analyzed and designed in a unified framework. Power factor correction under boundary conduction operation mode can be achieved conveniently using a simple commercially available control IC. Experimental results are provided to validate the circuit design",2001,0,
223,224,Categorization of minimum error forecasting zones using a geostatistic wind model,"In this paper a geostatistic wind direction model is applied to trace a wind speed map, based on data from official measurement weather stations distributed within the region of Andalucia-Spain. Each station's performance is assessed by comparing real measurements to those resulting from the linear interpolation of the rest. Once an error is associated to the station, the error is drawn in a map, in which minimum error zones can be delimited. Frequency and wind speed in each direction are the magnitudes of interest to get a first categorization of wind resources associated to the region. The interest of the method relies in the possibility of forecasting everywhere within the region with an error inside the tolerable margins.",2009,0,
224,225,A novel feature extraction and optimisation method for neural network-based fault classification in TCSC-compensated lines,"The suitability of fault classifiers introduced hitherto to operate correctly under a real TCSC transmission system remains a challenge since the computations are determined based on a number of postulations. This paper describes an alternative approach to fault classification in TCSC tines using artificial neural networks (ANNs). Special emphasis is placed on illustrating a combined wavelet transform and selforganising map (SOM) methodology to extract, validate and optimise the key characteristics of the fault transient phenomena in a TCSC line such that the input features to the ANNs are near optimal. As a result, it is shown that the fault classification proposed provides the ability to accurately classify the fault type, obviating the need for any predefined assumptions. Extensive simulation studies have been made to verify that the proposed method is both powerful and appropriate for fault classification.",2002,0,
225,226,Thermoreflectance imaging of defects in thin-film solar cells,We have identified and characterized various defects in thin-film a-Si and CIGS solar cells with sub-micron spatial resolution using thermoreflectance imaging. A megapixel silicon-based CCD was used to obtain noncontact thermal images simultaneously with visible electroluminescence (EL) images. EL can be indicative of pre-breakdown sites due to trap assisted tunneling and stress induced leakage currents. Physical defects appear at reverse bias voltages of 8 V in a-Si samples. Linear and nonlinear shunt defects are investigated as well as electroluminescent breakdown regions at reverse biases as low as 4.5 V. Pre-breakdown sites with electroluminescence are investigated.,2010,0,
226,227,Induction Motor-Drive Systems with Fault Tolerant Inverter-Motor Capabilities,"A low-cost fault tolerant drive topology for low- speed applications such as ""self-healing/limp-home"" needs for vehicles and propulsion systems, with capabilities for mitigating transistor open-circuit switch and short-circuit switch faults is presented in this paper. The present fault tolerant topology requires only minimum hardware modifications to the conventional off-the-shelf six-switch three-phase drive, with only the addition of electronic components such as triacs/SCRs and fast-acting fuses. In addition, the present approach offers the potential of mitigating not only transistor switch faults but also drive related faults such as rectifier diode short-circuit fault or dc link capacitor fault. In this new approach, some of the drawbacks associated with the known fault mitigation techniques such as the need for accessibility to a motor neutral, overrating the motor to withstand higher fundamental rms current magnitudes above its rated rms level, the need for larger size dc link capacitors, or higher dc bus voltage, are overcome here using the present approach. Given in this paper is a complete set of simulation results that demonstrate the soundness and effectiveness of the present topology.",2007,0,
227,228,On the relationships of faults for Boolean specification based testing,"Various methods of generating test cases based on Boolean specifications have previously been proposed. These methods are fault-based in the sense that test cases are aimed at detecting particular types of faults. Empirical results suggest that these methods are good at detecting particular types of faults. However, there is no information on the ability of these test cases in detecting other types of faults. The paper summarizes the relationships of faults in a Boolean expression in the form of a hierarchy. A test case that detects the faults at the lower level of the hierarchy will always detect the faults at the upper level of the hierarchy. The hierarchy helps us to better understand the relationships of faults in a Boolean expression, and hence to select fault-detecting test cases in a more systematic and efficient manner",2001,0,
228,229,Error sources in in-plane silicon tuning-fork MEMS gyroscopes,"This paper analyzes the error sources defining tactical-grade performance in silicon, in-plane tuning-fork gyroscopes such as the Honeywell-Draper units being delivered for military applications. These analyses have not yet appeared in the literature. These units incorporate crystalline silicon anodically bonded to a glass substrate. After general descriptions of the tuning-fork gyroscope, ordering modal frequencies, fundamental dynamics, force, and fluid coupling, which dictate the need for vacuum packaging, mechanical quadrature, and electrical coupling are analyzed. Alternative strategies for handling these engineering issues are discussed by introducing the Systron Donner/BEI quartz rate sensor, a successful commercial product, and the Analog Device (ADXRS), which is designed for automotive applications.",2006,0,
229,230,Flexible Error Concealment for H.264 Based on Directional Interpolation,"The losses of packets cannot he avoided if real-time video is transported over error prone environments. To conceal missing parts of video pictures, the spatial and temporal correlation feature of natural video sequences is used. However, in some cases - for instance in case of a scene change - there is no temporal correlation available and thus spatial error concealment has to be used. This article proposes flexible spatial error concealment based on directional interpolation method that performs well also if only two neighboring boundaries are used as common for H.264 spatially predicted frames. The proposed method was implemented and tested in a H.264 codec together with other error concealment methods to evaluate their performance",2005,0,
230,231,Nonlinear observers with approximately linear error dynamics: the multivariable case,"Exact error linearization uses nonlinear input-output injection to design observers with linear error dynamics in certain coordinates. This approach can only be applied nongenerically. We propose an observer for a wider class of multivariable systems which uniformly minimizes the nonlinear part of the system that cannot be canceled by nonlinear input-output injection. Our approach is numerical, constructive, and provides locally exponentially stable error dynamics. An example compares our design with a high-gain method",2001,0,
231,232,Time-Varying Network Fault Model for the Design of Dependable Networked Embedded Systems,"Dependability is becoming a key design aspect of today networked embedded systems (NES's) due to their increasing application to safety-critical tasks. Dependability evaluation must be based on modelling and simulation of faulty application behaviors, which must be related to faulty NES behaviors under actual defects. However, NES's behave differently from traditional embedded systems when testing activities are performed on them. In particular, issues arise on the definition of correct behavior, on the best point to observe it, and on the temporal properties of the faults to be injected. The paper describes these issues, discusses some possible solutions and presents a new time-varying network-based fault model to represent failures in a more abstract and efficient way. Finally, the fault model has been used to support the design of a network-based control application where packet losses, end-to-end delay and signal distortion must be carefully controlled.",2009,0,
232,233,Pilot signal-based real-time measurement and correction of phase errors caused by microwave cable flexing in planar near-field tests,Millimeter and submillimeter wave receivers in scanning planar near-field test systems are commonly based on harmonic mixing and thus require at least one flexible microwave cable to be connected to them. The phase errors originated in these cables get multiplied and added to the phase of the final detected signal. A complete submillimeter setup with on-the-fly measurement of phase errors is presented. The novel phase error correction system is based on the use of a pilot signal to measure the phase errors caused by cable flexing. The measured phase error surface in the quiet-zone region of a 310 GHz compact antenna test range (CATR) based on a hologram is shown as an application example. The maximum measured phase error due to the cable within a 8090 cm<sup>2</sup> scan area was 38.,2003,0,
233,234,Research about Software Fault Injection Technology Based on Distributed System,"Firstly, the paper made a contrast between the current domestic and international research condition, and introduced the basic concept of fault injection and distributed system. secondly, it discussed the classification and requirements of fault injection. There are mainly three types of distributed fault, namely the memory fault, CPU fault and correspondence fault. Besides, it discussed the method of distributed software fault injection about DOCTOR and illustrated the comprehensive structure and its respective parts of DOCTOR in detail. Thirdly, it reached a conclusion about the fault model of distributed system of fault injection and its realization method.",2010,0,
234,235,Fault detection and location of open-circuited switch faults in matrix converter drive systems,"Matrix converter based electric vehicles can be effectively applied to military vehicles due to weight and volume reduction as well as high temperature operation with no dc-bus capacitors fragile in a harsh environment. For successful applications for military vehicle areas, satisfactory reliability issues have to be incorporated into the matrix converter drives. This paper proposes a fault diagnostic technique for detecting and locating open-circuited faults in switching components of matrix converter drive systems. In this paper, the fault-mode behaviors of the matrix converter are, in detail, explored under the open-circuited switch fault conditions. Based on the investigated knowledge of the converter behaviors, the proposed scheme enables the matrix converter drive to detect and exactly identify power switches in which open-circuited faults have occurred. The proposed fault diagnostic algorithm is based on monitoring nine voltage errors assigned to nine bi-directional switches of the matrix converter. The voltage error signals are constructed with simple comparison of measured input and output voltages. In case that any of bi-directional switches are associated with open-circuited switch faults, the dedicated voltage error signals rise over a certain threshold value, which can be possible to detect a fault occurrence and locate the faulty switch. Since the developed diagnostic method requires no construction of reference output voltages from the pulsewidth modulation (PWM) reference signals, it can be implemented with simple and robust features. Verification results are presented to demonstrate the feasibility of the proposed technique.",2009,0,
235,236,Provisioning fault-tolerant scheduled lightpath demands in WDM mesh networks,"In this paper, we consider the problem of routing and wavelength assignment (RWA) of fault-tolerant scheduled lightpath demands (FSLDs) in all optical wavelength division multiplexing (WDM) networks under single component failure. In scheduled traffic demands, besides the source, destination, and the number of lightpath demands between a node-pair, their set-up and tear-down times are known, in this paper, we develop integer linear programming (ILP) formulations for dedicated and shared scheduled end-to-end protection schemes under single link/node failure for scheduled traffic demand with two different objective functions: 1) minimize the total capacity required for a given traffic demand while providing 100% protection for all connections; and 2) given a certain capacity, maximize the number of demands accepted while providing 100% protection for accepted connections. The ILP solutions schedule both the primary and end-to-end protection routes and assign wavelengths for the duration of the traffic demands. As the time disjointness that could exist among fault-tolerant scheduled lightpath demands is captured in our formulations, it reduces the amount of global resources required. The numerical results obtained from CPLEX indicate that dedicated scheduled (with set-up and tear-down times) protection provides significant savings (up to 33 %) in capacity utilization over dedicated conventional (without set-up and tear-down times) end-to-end protection scheme; shared scheduled protection provides considerable savings (up to 21 %) in capacity utilization over shared conventional end-to-end protection schemes. Also the numerical results indicate that shared scheduled protection achieves the best performance followed by dedicated scheduled protection scheme, and shared conventional end-to-end protection in terms of the number of requests accepted, for a given network capacity.",2004,0,
236,237,Induced error-correcting code for 2 bit-per-cell multi-level DRAM,"Traditionally, memories employ SEC-DED (Single Error Correcting and Double Error Detecting) Error Correcting Codes (ECC). While such codes have been considered for MLDRAM (Multi-Level Dynamic Random Access Memory), their use is inefficient, due to likely double-bit errors in a single cell. For this reason we propose an induced ECC architecture that uses ECC in such a way that no common error corrupts two bits. Induced ECC allows significant increase in reliability of the MLDRAM",2001,0,
237,238,Design of Timing Error Detectors for Orthogonal Space-Time Block Codes,"We present a method for the design of low complexity timing error detectors in orthogonal space-time block coding (OSTBC) receivers. A general expression for the S-curve of timing error detectors is derived. Based on this result, we obtain sufficient conditions for a difference of threshold crossings timing estimate that is robust to channel fading. A number of timing error detectors for 3- and 4-transmit antenna codes are presented. The performance is evaluated by examining their tracking capabilities within a timing loop of an OSTBC receiver. Symbol-error-rate results are presented showing negligible loss due to timing synchronization. In addition, we study the performance as a function of the timing drift and show that the receiver is able to track up to the normalized timing drift bandwidth of 0.001",2006,0,
238,239,Multi-Agent Fault Diagnosis in Manufacturing Systems Using Soft Computing,"The expeditious and accurate diagnosis of faults in manufacturing systems is essential in order to avoid expensive downtime. Many artificial intelligence approaches to automated fault diagnosis use techniques that are too computationally complex to achieve a diagnosis in real-time or are too inflexible for dynamic systems. Other approaches use either structural or symptom-based reasoning. Functional approaches are unable to provide real-time response due to their computational complexity, whereas, symptom-based approaches are only able to handle situations specifically coded in rules. Current hybrid approaches that combine the two methods are too structured in their approach to switching between the reasoning methods and, therefore fail to provide the flexible, rapid response of humans experts. This paper presents a robust, extensible approach to fault diagnosis that allows unstructured switching between reasoning methods using multiple fuzzy intelligent agents that examine the problem domain from a variety of perspectives.",2007,0,
239,240,Spatial error concealment algorithm based on improved SUSAN operator,"In the transmission of real-time video compressed streams, error concealment method is to restore the damaged or lost data packets. This paper improves the existing spatial error concealment algorithm based on SUSAN detection operator. On the one hand, as recovering the error, the detection pixels were reduced by considering the relationship of nearby pixels. On the other hand, more associated pixels were fully considered. The experimental results show that the proposed algorithm enhances the Peak Signal to Noise Ratio in the case of reducing 4%-8% computational complexity, and is more beneficial to real-time application.",2010,0,
240,241,"Impact of Channel Errors on Decentralized Detection Performance of Wireless Sensor Networks: A Study of Binary Modulations, Rayleigh-Fading and Nonfading Channels, and Fusion-Combiners","We provide new results on the performance of wireless sensor networks in which a number of identical sensor nodes transmit their binary decisions, regarding a binary hypothesis, to a fusion center (FC) by means of a modulation scheme. Each link between a sensor and the fusion center is modeled independent and identically distibuted (i.i.d.) either as slow Rayleigh-fading or as nonfading. The FC employs a counting rule (CR) or another combining scheme to make a final decision. Main results obtained are the following: 1) in slow fading, a) the correctness of using an average bit error rate of a link, averaged with respect to the fading distribution, for assessing the performance of a CR and b) with proper choice of threshold, on/off keying (OOK), in addition to energy saving, exhibits asymptotic (large number of sensors) performance comparable to that of FSK; and 2) for a large number of sensors, a) for slow fading and a counting rule, given a minimum sensor-to-fusion link SNR, we determine a minimum sensor decision quality, in order to achieve zero asymptotic errors and b) for Rayleigh-fading and nonfading channels and PSK (FSK) modulation, using a large deviation theory, we derive asymptotic error exponents of counting rule, maximal ratio (square law), and equal gain combiners.",2008,0,
241,242,A signature-based approach for diagnosis of dynamic faults in SRAMs,"This paper focuses on diagnosis of dynamic faults in SRAMs. The current techniques for fault diagnosis are mainly based on the signature method. Here, we introduce an extension of the signature scheme by taking in account additional information related to the addressing order during March test execution. A first advantage of the proposed approach is its capability to distinguish between static and dynamic faults. Another main feature is the correct identification of the location of the failure in a given memory component: the core-cell array, write drivers, sense amplifiers, address decoders and pre- charge circuits. Moreover, since this approach does not modify the March test, there is no increase of test complexity, conversely to other existing diagnosis techniques.",2008,0,
242,243,Defect tolerance for gracefully-degradable microfluidics-based biochips,"Defect tolerance is an important design consideration for microfluidics-based biochips that are used for safety-critical applications. We propose a defect tolerance methodology based on graceful degradation and dynamic reconfiguration. We first introduce tile-based biochip architecture, which is scalable for large-scale bioassays. A clustered defect model is used to evaluate the graceful degradation method for tile-based biochips. The proposed schemes ensure that the bioassays mapped to a droplet-based microfluidic array during design can be executed on a defective biochip through operation rescheduling and/or resource rebinding. Real-life biochemical procedures, namely polymerase chain reaction (PCR) and multiplexed in-vitro diagnostics on human physiological fluids, are used to evaluate the proposed defect tolerance schemes.",2005,0,
243,244,"Decoding of the (24, 12, 8) extended golay code up to four errors","A new decoder is proposed to decode the (24, 12, 8) binary extended Golay code up to four errors. It consists of the conventional hard decoder for correcting up to three errors, the detection algorithm for four errors and the soft decoding for four errors. For a weight-4 error in a received 24-bit word, Method 1 or 2 is developed to determine all six possible error patterns. The emblematic probability value of each error pattern is then defined as the product of four individual bit-error probabilities corresponding to the locations of the four errors. The most likely one among these six error patterns is obtained by choosing the maximum of the emblematic probability values of all possible error patterns. Finally, simulation results of this decoder in additive white Gaussian noise show that at least 93% and 99% of weight-4 error patterns that occur are corrected if the two E<sub>b</sub>/N<sub>0</sub> ratios are greater than 2 and 5 dB, respectively. Consequently, the proposed method can achieve a better percentage of successful decoding for four errors at variable signal-to-noise ratios than Lu et al.'s algorithm in software. However, the speed of the method is slower than Lu et al.'s algorithm.",2009,0,
244,245,Joint Generalized Antenna Combination and Symbol Detection Based on Minimum Bit Error Rate: A Particle Swarm Optimization Approach,"In order to reduce hardware cost and achieve superior performance in multi-input multi-output (MIMO) systems, this paper proposes a novel scheme for joint antenna combination and symbol detection. More specifically, the new approach simultaneously determines the transformation weighting for antenna combination to lower the RF chains called for and to design the minimum bit error rate (MBER) detector to effectively mitigate the impairment due to interference. The joint decision statistic, however, is highly nonlinear and the particle swarm optimization (PSO) algorithm is employed to reduce the computational overhead. Conducted simulation results show that the new approach yields satisfactory performance with reduced computational overhead compared with pervious works.",2008,0,
245,246,Finite Element Analysis of Switched Reluctance Motor under Dynamic Eccentricity Fault,"This paper describes the results of a two-dimensional finite element analysis carried out on an 8/6 switched reluctance motor for studying the effects of dynamic eccentricity on the static characteristics of the motor. Flux contours, flux-linkage profiles and mutual fluxes are obtained for both healthy and faulty motor. Besides, Static torque profiles of phases are obtained for different degrees of eccentricity and it is shown that at low current; the effect of eccentricity is considerable compared to that of the rated current case. Finally, Fourier analysis of the torque profiles is performed to make their difference visible.",2006,0,
246,247,Effective congestion and error control for scalable video coding extension of the H.264/AVC,"We present an effective congestion and error control mechanism for scalable video coding (SVC) extension of the H.264/AVC video dissemination over Internet. The congestion control is used to determine the appropriate number of SVC video layers based on bandwidth inference congestion (BIC) control protocol for layered multicast scenarios and the error control is achieved by unequal forward error correction (FEC) layered protection using block erasure coding. Through the real Internet streaming experiments, we demonstrate the effectiveness of the proposed layered SVC delivery, in terms of subscription layer, average packet loss rate and PSNRs, under several layered-definition scalabilities.",2008,0,
247,248,Routability estimation of FPGA-based fault injection,"In the past years various approaches to hardware-based fault injection using FPGA-based hardware have been presented. Some approaches insert additional functions at the fault location (any location in the circuit, e.g. I/Os of components or their interconnection nets), while others utilize the reconfigurability of FPGAs. A common feature of each of these methods is the execution of hardware-based fault simulation using the stuck-at fault model at gate level. The expansion of a circuit by insertion of additional functions at the fault location constitutes an overhead of FPGA resources. An optimized mapping of the circuit into an FPGA and a routable placement in the FPGA is difficult to achieve due to the generation of additional functions at the fault locations. Therefore, an optimized assignment of the fault locations to the FPGA-resources (configurable logic blocks, look-up tables, I/O blocks, etc.) precedes and thereby guarantees the mapping and routability of very large circuits in an acceptable runtime. In this paper an approach to node assignment is introduced, which achieves a reduction in FPGA overhead as well as routability of the expanded circuit in a minimal runtime.",2003,0,
248,249,A Research on I.C. Engine Misfire Fault Diagnosis Based on Rough Sets Theory and Neural Network,"A method for diagnosis of misfire fault in internal combustion engine based on exhaust density of HC, CO2, O2 and the engines work parameters are presented in this paper. Rough sets theory is used to simplify attribute parameter reflecting exhaust emission and conditions of internal combustion engine and in which unnecessary properties are eliminated. The engines work parameters, exhaust emission with misfire fault and without fault are tested by the experimentation of CA6100 engine. A diagnosis model which describing the relationship between the misfire degree and the internal combustion engines exhaust emission and work parameters is established based on rough sets theory and RBF neural network. The model reduces the sample size, optimizes the neural network, increase the diagnosis correctness. The model is also trained by test data and MATLAB software. The model has been used to diagnosis internal combustion engine misfire fault, the result illustrates that this diagnosis model is suitable. This system can reduce input node number and overcome some shortcomings, such as neural network scale is too large and the rate of classification is slow.",2010,0,
249,250,High-Intensity Radiated Field fault-injection experiment for a fault-tolerant distributed communication system,"Safety-critical distributed flight control systems require robustness in the presence of faults. In general, these systems consist of a number of input/output (I/O) and computation nodes interacting through a fault-tolerant data communication system. The communication system transfers sensor data and control commands and can handle most faults under typical operating conditions. However, the performance of the closed-loop system can be adversely affected as a result of operating in harsh environments. In particular, High-Intensity Radiated Field (HIRF) environments have the potential to cause random fault manifestations in individual avionic components and to generate simultaneous system-wide communication faults that overwhelm existing fault management mechanisms. This paper presents the design of an experiment conducted at the NASA Langley Research Center's HIRF Laboratory to statistically characterize the faults that a HIRF environment can trigger on a single node of a distributed flight control system.",2010,0,
250,251,On-line fault diagnosis in a Petri Net framework,"The paper addresses the fault detection problem for discrete event systems modeled by Petri nets (PN). Assuming that the PN structure and initial marking are known, faults are modeled by unobservable transitions. The paper recalls a previously proposed diagnoser that works online and employs an algorithm based on the definition and solution of some integer linear programming problems to decide whether the system behavior is normal or exhibits some possible faults. To reduce the on-line computational effort, we prove some results showing that if the unobservable subnet enjoys suitable properties, the algorithm solution may be obtained with low computational complexity. We characterize the properties that the PN modeling the system fault behavior has to fulfill and suitably modify the proposed diagnoser.",2009,0,
251,252,The prediction of fault currents in a large multiwinding reactor transformer,"The fault currents occurring in power transformers are determined by the leakage reactances within the windings. Where the transformers are used in a phase-shifting mode, there is additional coupling between phases which influences the fault current. This work describes the modelling of the self and mutual inductances in a 90 MVA autotransformer with a tertiary winding, on the assumption that the airgap formed by the transformer window dictates the reluctance of the leakage flux paths. Recordings made during a short-circuit between two phases of the tertiary winding show a remarkably close comparison with the predicted waveforms.",2003,0,
252,253,How Long Will It Take to Fix This Bug?,"Predicting the time and effort for a software problem has long been a difficult task. We present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. Our technique leverages existing issue tracking systems: given a new issue report, we use the Lucene framework to search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach using effort data from the JBoss project. Given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating naive predictions by a factor of four.",2007,0,
253,254,Data reduction and clustering techniques for fault detection and diagnosis in automotives,"In this paper, we propose a data-driven method to detect anomalies in operating Parameter Identifiers (PIDs) and in the absence of any anomaly, classify faults in automotive systems by analyzing PIDs collected from the freeze frame data. We first categorize the operating parameter data using automotive domain knowledge. The dataset thus obtained is then analyzed using Principal Component Analysis (PCA) and Independent Component Analysis (ICA) for finding coherence among the PIDs. Then we use clustering algorithms based on both linear distance and information theoretic measures to assign coherent PIDs to the same class or cluster. A comparative analysis of the behavior of PIDs belonging to the same cluster can now be made for detecting anomaly in PIDs. Since a system fault is characterized by the values by of all PIDs across all the clusters, we use the joint probability distribution of the independent components of all PIDs to characterize the fault and find the divergence between the joint distributions of training and test data to classify faults. The proposed method can analyze available parameter data, categorize PIDs into informative or non-informative category, and detect fault condition from the clusters. We demonstrate the algorithm by way of an application to operating parameter data collected during faults in catalytic converters of vehicles.",2010,0,
254,255,Effect of atmospheric correction for different land use on Landsat 7 ETM+ satellite imagery,"Various changes in the atmosphere of the earth and different illuminations resulting from rough terrain change the spectral reflection values of satellite images. Studies making use of real reflection values belonging to the object will provide more accurate data. The atmospheric correction process to be applied in this study is used to prevent the negative effects resulting from atmosphere and different illuminations in order to represent the reflections from the ground on the image in the best way possible. Using atmospheric correction, differentiations in reflection values sensed by different sensors or platforms resulting from atmosphere and some technical problems will be prevented. In this study, the aim is to determine the changes in the spectral reflection values concerning land use following the atmospheric correction to be applied on Landsat image data. For this reason, atmospheric correction was applied on Landsat image data. The relations of each band with each other before and after the correction were determined. The changes between spectral reflection values of all bands before and after correction regarding three different land uses as forest, agricultural area and residential area were examined visually and statistically.",2009,0,
255,256,An Efficient Fault Tolerance Scheme for Preventing Single Event Disruptions in Reconfigurable Architectures,"Reconfigurable architectures are becoming increasingly popular with space related design engineers as they are inherently flexible to meet multiple requirements and offer significant performance and cost savings for critical applications. As the microelectronics industry has advanced, integrated circuit (IC) design and reconfigurable architectures (FPGAs, reconfigurable SoC and etc) have experienced dramatic increase in density and speed. These advancements have serious implications for the reconfigurable architectures when used in space environment where IC is subject to total ionization dose (TID) and single event effects as well. Due to transient nature of single event upsets (SEUs), these are most difficult to avoid in space-borne reconfigurable architectures. We present a unique SEU fault tolerance technique based upon double redundancy with comparison to overcome the overheads associated with the conventional schemes",2006,0,
256,257,Anshan: Wireless Sensor Networks for Equipment Fault Diagnosis in the Process Industry,"Wireless sensor networks provide an opportunity to enhance the current equipment diagnosis systems in the process industry, which have been based so far on wired networks. In this paper, we use our experience in the Anshan Iron and Steel Factory, China, as an example to present the issues from the real field of process industry, and our solutions. The challenges are three fold: First, very high reliability is required; second, energy consumption is constrained; and third, the environment is very challenging and constrained. To address these issues, it is necessary to put systematic efforts on network topology and node placement, network protocols, embedded software, and hardware. In this paper, we propose two technologies i.e. design for reliability and energy efficiency (DRE), and design for reconfiguration (DRC). Using these techniques we developed Anshan, a wireless sensor network for monitoring the temperature of rollers in a continuously annealing line and detecting equipment failures. Project Anshan includes 406 sensor nodes and has been running for four months continuously.",2008,0,
257,258,Heterogeneous Error Protection of H.264/AVC Video Using Hierarchical 16-QAM,"Heterogeneous error protection (HEP) of H.264/AVC coded video is investigated using hierarchical quadrature amplitude modulation (HQAM), which takes into consideration the non- uniformly distributed importance of intracoded frame (I-frame) and predictive coded frame (P-frame) as well as the sensitivity of the coded bitsream against transmission errors. The HQAM constellation are used to give different degrees of error protection of the most important information of the video content. The performance of the transmission system is evaluated under additive Gaussion Noise (AWGN). The simulation results indicate that the strategy produces a high quality of the reconstructed video data compared with uniform protection.",2009,0,
258,259,A design of the novel coupled line bandpass filter using defected ground structure,"In this paper, a novel coupled line bandpass filter with a DGS (Defected Ground Structure) is proposed to realize a compact size with low insertion loss characteristic. The proposed bandpass filter can provide an attenuation pole due to the resonance characteristic of the DGS. The equivalent circuit parameters for the DGS are extracted by using an EM simulation process and the circuit analysis method. The design method for the proposed 3-pole bandpass filter is derived based on coupled line filter theory and the derived equivalent circuit of the DGS. The experimental results show an excellent agreement with theoretical simulation results.",2000,0,
259,260,The use of characteristic features of wireless cellular networks for transmission of GNSS assistance and correction data,"Precise Global Navigation Satellite System (GNSS) positioning using Real Time Kinematics (RTK) correction data is currently utilized in many fields of surveying, mapping and precision agriculture. In the near future, sub decimeter precision data usage is expected to extend to autonomous vehicles navigation and public safety areas. To satisfy this increasing demand of precision positioning correction bandwidth, new techniques and protocols in assistance and correction data transmission are needed. This paper reviews one such possible technique involving sending correction dataset via public wireless cellular networks. The data will be transmitted through a hybrid system integrating correction data broadcasted in the wireless cellular network control plane with AGNSS assistance data and correction metadata in the user plane. Through this system, the bandwidth intensive, low refresh rate data of GNSS system ephemeris, reference station and satellite identification is omitted from the main data stream. Instead, a constant bit rate (CBR) stream for correction data is used and bandwidth is conserved. The results show that the proposed system can achieve scalability required for widespread usage of sub decimeter level positioning data from GNSS.",2010,0,
260,261,An on-line monitoring and multi-layer fault diagnosis system of electrical equipment based on geographic information system,"Automated mapping/facilities management/geographic information system (AM/FM/GIS), which provides a powerful way to process graphic and non-graphic information, can construct a spatial database system with topological structure and analysis function by combining diversified information in power system with geographic position-related graphic information. Based on the AM/FM/GIS and on-line monitoring system, an integrated system is put forward which can implement state monitoring, multi-layer fault diagnosis and assess the faults. By using this integrated system, latent fault and defect can be eliminated, loss due to power cut is reduced and the reliability of running power system is improved. Application indicates it is economical, pragmatic and has excellent performance.",2005,0,
261,262,Coseismic fault rupture detection and slip measurement by ASAR precise correlation using coherence maximization: application to a north-south blind fault in the vicinity of Bam (Iran),"Using the phase differences between satellite radar images recorded before and after an earthquake, interferometry allows mapping the projection along the line of sight (LOS) of the ground displacement. Acquisitions along multiple LOS theoretically allow deriving the complete deformation vector; however, due to the orbit inclination of current radar satellites, precision is poor in the north-south direction. Moreover, large deformation gradients (e.g., fault ruptures) prevent phase identification and unwrapping and cannot be measured directly by interferometry. Subpixel correlation techniques using the amplitude of the radar images allow measuring such gradients, both in slant-range and in azimuth. In this letter, we use a correlation technique based on the maximization of coherence for a radar pair in interferometric conditions, using the complex nature of the data. In the case of highly coherent areas, this technique allows estimating the relative distortion between images. Applied to ASAR images acquired before and after the December 26, 2003 Bam earthquake (Iran), we show that the near-field information retrieved by this technique is useful to constrain geophysical models. In particular, we confirm that the major gradients of ground displacement do not occur across the known fault scarp but approximately 3 km west of it, and we also estimate directly the amplitude of right lateral slip, while retrieving this value from interferometry requires passing through the use of a model for the earthquake fault and slip.",2006,0,
262,263,Reducing cost and tolerating defects in page-based intelligent memory,"Active Pages is a page-based model of intelligent memory specifically designed to support virtualized hardware resources. Previous work has shown substantial performance benefits from off loading data-intensive tasks to a memory system that implements Active Pages. With a simple VLIW processor embedded near each page on DRAM, Active Page memory systems achieve up to 1000X speedups over conventional memory systems. In this study, we examine Active Page memories that share, or multiplex, embedded VLIW processors across multiple physical Active Pages. We explore the trade-off between individual page-processor performance and page-level multiplexing. We find that hardware costs of computational logic can be reduced from 31% of DRAM chip area to 12%, through multiplexing, without significant loss in performance. Furthermore, manufacturing defects that disable up to 50% of the page processors can be tolerated through efficient resource allocation and associative multiplexing",2000,0,
263,264,A defect-to-yield correlation study for marginally printing reticle defects in the manufacture of a 16Mb flash memory device,"This paper presents a defect-to-yield correlation for marginally printing defects in a gate and a contact 4X DUV reticle by describing their respective impact on the lithography manufacturing process window of a 16Mb flash memory device. The study includes site-dependent sort yield signature analysis within the exposure field, followed by electrical bitmap and wafer strip back for the lower yielding defective sites. These defects are verified using both reticle inspection techniques and review of printed resist test wafers. Focus/exposure process windows for defect-free feature and defective feature are measured using both in-line SEM CD data and defect printability simulation software. These process window models are then compared against wafer sort yield data for correlation. A method for characterizing the lithography manufacturing process window is proposed which is robust to both marginally printing reticle defects and sources of process variability outside the lithography module",2000,0,
264,265,PEDS: A Parallel Error Detection Scheme for TCAM Devices,"Ternary content-addressable memory (TCAM) devices are increasingly used for performing high-speed packet classification. A TCAM consists of an associative memory that compares a search key in parallel against all entries. TCAMs may suffer from error events that cause ternary cells to change their value to any symbol in the ternary alphabet ""0"",""1"",""*"". Due to their parallel access feature, standard error detection schemes are not directly applicable to TCAMs; an additional difficulty is posed by the special semantic of the ""*"" symbol. This paper introduces PEDS, a novel parallel error detection scheme that locates the erroneous entries in a TCAM device. PEDS is based on applying an error-detection code to each TCAM entry, and utilizing the parallel capabilities of the TCAM, by simultaneously checking the correctness of multiple TCAM entries. A key feature of PEDS is that the number of TCAM lookup operations required to locate all errors depends on the number of symbols per entry rather than the (orders-of-magnitude larger) number of TCAM entries. For large TCAM devices, a specific instance of PEDS requires only 200 lookups for 100-symbol entries, while a naive approach may need hundreds of thousands lookups. PEDS allows flexible and dynamic selection of trade-off points between robustness, space complexity, and number of lookups.",2009,0,
265,266,Noise identification and fault diagnosis for the new products of the automobile gearbox,"A noise identification and fault diagnosis system for the new products of the automobile gearbox is introduced. The framework of the developed software is described, which includes function modules as data acquisition, feature extracting, time frequency transform, order analysis, learning and training, and so on. The prototype system has been partially put in practice in a certain automobile gear-box manufacture company.",2009,0,
266,267,Soft Defects: Challenge and Chance for Failure Analysis,"Failure analysis on advanced logic and mixed signal ICs more and more has to deal with so called 'soft defects'. In this paper, an analysis flow especially for parameter dependent scan fails is presented. For the two major localization techniques, namely soft defect localization (SDL) and internal signal measurement enhanced activation and localization procedures using test systems are proposed.",2007,0,
267,268,Layout to Logic Defect Analysis for Hierarchical Test Generation,"As shown by previous studies, shorts between the interconnect wires should be considered as the predominant cause of failures in CMOS circuits. Fault models and tools for targeting these defects, such as the bridging fault test pattern generators have been available for a long time. However, this paper proposes a new hierarchical approach based on critical area extraction for identifying the possible shorted pairs of nets on the basis of the chip layout information, combined with logic-level test pattern generation for bridging faults. Experiments on real design layouts will show that only a fraction of all the possible pairs of nets have non-zero shorting probabilities. Furthermore, it will also be proven at the logic-level that nearly all such bridging faults can be tested by a simple and robust one-pattern logic test. The methods proposed in this paper are supported by a design flow implementing existing commercial and academic CAD software.",2007,0,
268,269,A Fault Propagation Approach for Highly Distributed Service Compositions,"Today, the techniques for realizing service compositions (e.g. WS-BPEL) have become mature. Nevertheless, when it comes to execution faults within service compositions, many problems are still unsolved. Especially the propagation and global handling of errors in service compositions yet remains an open issue. In this paper, we describe some preliminary results of our ongoing work in the field of fault propagation and exception handling in service compositions. We provide some service classification criteria and show how they relate to service composition fault handling. Further, we present a fault propagation approach for service compositions.",2008,0,
269,270,Using design based binning to improve defect excursion control for 45nm production,"For advanced device (45 nm and below), we proposed a novel method to monitor systematic and random excursion. By integrating design information and defect inspection results into automated software (DBB), we can identify design/process marginality sites with defect inspection tool. In this study, we applied supervised binning function (DBC) and defect criticality index (DCI) to identify systematic and random excursion problems on 45 nm SRAM wafers. With established SPC charts, we will be able to detect future excursion problem in manufacturing line early.",2007,0,
270,271,Comparison of Outlier Detection Methods in Fault-proneness Models,"In this paper, we experimentally evaluated the effect of outlier detection methods to improve the prediction performance of fault-proneness models. Detected outliers were removed from a fit dataset before building a model. In the experiment, we compared three outlier detection methods (Mahalanobis outlier analysis (MOA), local outlier factor method (LOFM) and rule based modeling (RBM)) each applied to three well-known fault-proneness models (linear discriminant analysis (LDA), logistic regression analysis (LRA) and classification tree (CT)). As a result, MOA and RBM improved Fl-values of all models (0.04 at minimum, 0.17 at maximum and 0.10 at mean) while improvements by LOFM were relatively small (-0.01 at minimum, 0.04 at maximum and 0.01 at mean).",2007,0,
271,272,Algorithm-based fault tolerance for spaceborne computing: basis and implementations,"We describe and test the mathematical background for using checksum methods to validate results returned by a numerical subroutine operating in a fault-prone environment that causes unpredictable errors in data. We can treat subroutines whose results satisfy a necessary condition of a linear form; the checksum tests compliance with this necessary condition. These checksum schemes are called algorithm-based fault tolerance (ABFT). We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent in finite-precision numerical calculations. Two series of tests are described. The first tests the general effectiveness of the linear ABFT schemes we propose, and the second verifies the correct behavior of our parallel implementation of them. We find that under simulated fault conditions, it is possible to choose a fault detection scheme that for average case matrices can detect 99% of faults with no false alarms, and that for a worst-case matrix population can detect 80% of faults with no false alarms",2000,0,
272,273,Bug busters,"One way to deal with bugs is to avoid them entirely. The approach would be wasteful because we'd be underutilizing the many automated tools and techniques that can catch bugs for us. Most tools for eliminating bugs work by tightening the specifications of what we build. At the program code level, tighter specifications affect the operations allowed on various data types, our program's behavior, and our code's style. Furthermore, we can use many different approaches to verify that our code is on track: the programming language, its compiler, specialized tools, libraries, and embedded tests are our most obvious friends. We can delegate bug busting to code. Many libraries come with hooks or specialized builds that can catch questionable argument values, resource leaks, and wrong ordering of function calls. Bugs many be a fact of life, but they're not inevitable. We have some powerful tools to find them before they mess with our programs, and the good news is that these tools get better every year.",2006,0,
273,274,Design and validation of portable communication infrastructure for fault-tolerant cluster middleware,"We describe the communication infrastructure (CI) for our fault-tolerant cluster middleware, which is optimized for two classes of communication: for the applications and for the cluster management middleware. This CI was designed for portability and for efficient operation on top of modern user-level message passing mechanisms. We present a functional fault model for the CI and show how platform-specific faults map to this fault model. Based on this fault model, we have developed a fault injection scheme that is integrated with the CI and is thus portable across different communication technologies. We have used fault injection to validate and evaluate the implementation of the CI itself as well as the cluster management middleware in the presence of communication faults.",2002,0,
274,275,Effect of rotor position error on commutation in sensorless BLDC motor drives,"In this paper, two kinds of commutation modes of the brushless DC(BLDC) motor drives, the delaying commutation and the leading commutation, are discussed in detail. The current of the unexcited phase is calculated under an ideal operation condition, and the condition of circulating current occurring is analyzed. The result with the compensated commutation is provided. The theoretical analysis is confirmed by the experiment results.",2005,0,
275,276,A Fault-Tolerant Active Pixel Sensor to Correct In-Field Hot-Pixel Defects,"Solid-state image sensors develop in-field defects in all common environments. Experiments have demonstrated the growth of significant quantities of hot-pixel defects that degrade the dynamic range of an image sensor and potentially limit low-light imaging. Existing software- only techniques for suppressing hot-pixels are inadequate because these defective pixels saturate at relatively low illumination levels. The redundant fault-tolerant active pixel sensor design is suggested to isolate point-like hot-pixel defects. Emulated hot-pixels have been induced in hardware implementations of this pixel architecture and measurements of pixel response indicate that it generates an accurate output signal throughout the sensor's entire dynamic range, even when standard pixels would be otherwise saturated by the hot defect. A correction algorithm repairs the final image by building a simple look-up table of illumination- response of a working pixel. In emulated hot-pixels, the true illumination value can be recovered with an error of plusmn5% under typical conditions.",2007,0,
276,277,Automatic error recovery in targetless logic emulation,"Targetless logic emulation refers to a verification system in which there are no external hardware targets interfacing with the emulator. In such systems input stimuli to the DUT come either from a user provided vector file or a HDL testbench running on a software simulator and the DUT runs on hardware based logic emulator. Many users use such targetless environment for automated long running verification tests consisting of huge sets of input stimuli, consequently an automatic recovery method is of significant interest in such systems. The automatic error recovery method shall be able to complete the emulation session gracefully skipping error points and subsequently report various errors and mismatch conditions for user debug. The paper presents a novel methodology and verification infrastructure based on periodic checkpointing, which provides a robust way of error condition detection, subsequent restoration of last saved system state and resume emulation run by skipping offending operations. It does not require any special hardware extension and provides a fully customizable checkpoint frequency selection scheme. It is seen to add only a minimal overhead on overall hardware emulation speed.",2009,0,
277,278,Weather radar equation correction for frequency agile and phased array radars,"This paper presents the derivation of a correction to the Probert-Jones weather radar equation for use with advanced frequency agile, phased array radars. It is shown that two additional terms are required to account for frequency hopping and electronic beam pointing. The corrected weather radar equation provides a basis for accurate and efficient computation of a reflectivity estimate from the weather signal data samples. Lastly, an understanding of calibration requirements for these advanced weather radars is shown to follow naturally from the theoretical framework.",2007,0,
278,279,Waveform analysis of the bridge type SFCL during load changing and fault time,"DC reactor type superconducting fault current limiter (SFCL) has drawn the interest of some researchers in developing such device and more research work is being carried out in order to make it practically feasible. We have pointed out one issue that is not properly examined yet on such a device during load changing time. As we know, it is very difficult to introduce DC bias voltage to the reactor coil of the bridge type SFCL and some researchers are developing such device without using DC bias current. In such a case, the voltage drop occurs at the load terminal during the load increasing time caused by the DC reactor's inductance. By using the Electro-Magnetic Transients in DC systems which is the simulator of electric networks (EMTDC) software we carried out analysis of first few half cycles of the voltage and current waveforms after the load is increased. We also performed the same analysis for fault conditions. The peak value of the waveforms is considered in calculating the voltage drop at load terminal during the load changing time. The analysis can be used in selecting an appropriate inductance value for designing such SFCL.",2003,0,
279,280,DuoTracker: Tool Support for Software Defect Data Collection and Analysis,"In today software industry defect tracking tools either help to improve an organizationAs software development process or an individualAs software development process. No defect tracking tool currently exists that help both processes. In this paper we present DuoTracker, a tool that makes possible to track and analyze software defects for organizational and individual software process decision making. To accomplish this, DuoTracker has capabilities to classify defects in a manner that makes analysis at both organizational and individual software processes meaningful. The benefit of this approach is that software engineers are able to see how their personal software process improvement impacts their organization and vice versa. This paper shows why software engineers need to keep track of their program defects, how this is currently done, and how DuoTracker offers a new way of keeping track of software errors. Furthermore, DuoTracker is compared to other tracking tools that enable software developers to record program defects that occur during their individual software processes.",2006,0,
280,281,Investigation of the effects of transmission faults upon a renewable energy generating plant,"In recent years the number of renewable energy generators connected to Ireland's electricity grid has steadily increased. The Republic of Ireland is now expected to source 13.2% of the electricity it consumes from renewables by 2010, which represents a significant challenge to the electricity system operators and planners. This paper describes the modelling and simulation of a small hybrid wind/hydro generating plant connected to the distribution network. The effects upon the plant of transmission network faults and continuous voltage unbalance are investigated.",2005,0,
281,282,Detection of defects in wood slabs by using a microwave imaging technique,"In this paper, an experimental set up based on interrogating microwaves is used to obtain images of the cross section of dielectric cylinders. In particular, a microwave tomographic configuration is used to inspect wood slabs in order to search for defects and voids. The measured data (samples of the scattered electric field) are inverted by using an efficient reconstruction technique, which is able to handle the ill-posedness of the inverse scattering problem. The developed experimental apparatus is validated in this paper by means of several numerical simulations. Preliminary experimental results are also reported.",2007,0,
282,283,Influence of the Transmission Channel Parameters on Error Rates and Picture Quality in DVB Baseband Transmission,"The paper deals with the component analysis of DVB (digital video broadcasting) transmission model in baseband and its source, channel and link coding. The transmission channel model is based on the digital filter design and it can be designed with the variable transmission parameters (e.g. cut-off frequency) and linear distortions with additive noise and reflected signal. Results of achieved BER (bit error rate) and SER (symbol error rate) and corresponding PQE (picture quality evaluation) analysis are presented, including the evaluation of subjective picture quality influence on normalized cut-off frequency of the channel",2006,0,
283,284,Rotor position sensor fault detection Isolation and Reconfiguration of a Doubly Fed Induction Machine control,"In this paper, a Doubly Fed Induction Machine (DFIM) operating in motor mode and supplied by two Voltage Source Inverters (VSI), in stator and rotor sides, is presented. The aim is to analyze the position sensor fault effects on a Direct Torque Control (DTC) of the DFIM. This justifies the necessity of a reconfiguration control when a position sensor fault appears in order to avoid an interruption in system operations. In the other hand, this study emphasizes the close dependency between system performance and the output accuracy of the rotor position sensor. Moreover, simulation results point out the operation system deterioration in case of position sensor fault, which leads in most cases to its shut down in contrast to industrial expectations. This work presents a control reconfiguration for a DFIM speed drive when a position sensor fault occurs, in order to ensure system service continuity. For this purpose, SABER simulation results illustrate the system behavior before and after a position sensor fault. System performance preservation is carried out after control reconfiguration. The proposed solution is relevant especially due to its simplicity.",2009,0,
284,285,Computation and analysis of output error probability for C17 benchmark circuit using bayesian networks error modeling,"The reliability of digital circuits is in question since the new scaled transistor technologies continue to emerge. The major factor deteriorating the circuit performance is the random and dynamic nature of errors encountered during its operation. Output-error probability is the direct measure of circuit's reliability. Bayesian networks error modeling is the approach used to compute error probability of digital circuits. In our paper, we have used this technique to compute and analyze the output error probability of LGSynth's C17 benchmark circuit. The simulations are based on MATLAB and show important relationships among output-error probability, execution time and number of priors involved in the analysis.",2010,0,
285,286,Raising network fault management intelligence,"Most large network management centers have relatively low skilled personnel as their first level operations staff. Many organizations attempt to cope with this situation by restricting the set of problems these people have to deal with to those which are well understood and documented. Several software packages exist which can correlate and filter incoming events from the network and present a select subset to the operator. Unfortunately, programming these fault management applications requires considerable expertise and effort. Often, once the initial development is done, the implementation remains static, while the network itself is dynamic. This paper proposes a methodology for documenting known faults and responses, programming fault correlation engines, continuously examining real behavior, and feeding the result back into the programming process. This results in a continuous improvement in fault management intelligence, with corresponding improvement in network availability and thus value of the network to the organization",2000,0,
286,287,Analysis of Timing Error Aperture Jitter on the Performance of Sigma Delta ADC for Software Radio Mobile Receivers,"Jitter is the limiting effect for high speed analog-to digital converters with high resolution and wide digitization bandwidth, which are required in receivers in order to support high data rates. The rapid development of digital wireless system has led to a need of high resolution and high speed analog to digital converter. The proper selection of data converters, both analog to digital converters and digital to analog converters (DACs) is one of the most challenging steps in designing software radio. The performance of a data converter is dependent upon the accuracy and stability of the clock supplied to the circuits. When data converter employ a high sampling rate, clocking issues become magnified and significant distortion can be result. This paper describes the effect of aperture jitter on the performance of sigma delta ADC and present analytical evaluation of the performance and mean error power spectrum due to aperture jitter application has favored the use of oversampling delta sigma ADC (analog-to-digital converters) due to their better speed-accuracy tradeoff. Delta-sigma modulator is one of the key building blocks, which can be implemented using DT (discrete-time) and CT (continuous-time) techniques. Compared to their DT counterparts, CT delta-sigma modulators have recently attracted more and more attentions due to their advantages in terms of high speed, low power, low noise and intrinsic anti-aliasing capability. In this paper, we concentrate on the discrete implementation. Section 2 presents an aperture jitter effect in SDM in terms of SNR. In the last few years different authors derived formulas to quantify the SNR limiting effect of jitter in ADCs. While Walden used a worst case approach, Kobayashi presented an exact formula which allows calculating the SNR in the presence of an aperture jitter.",2009,0,
287,288,"The Impact of Tower Shadow, Yaw Error, and Wind Shears on Power Quality in a WindDiesel System","To study the impact of aerodynamic aspects of a wind turbine (WT) (i.e., tower shadow, wind shears, yaw error, and turbulence) on the power quality of a wind-diesel system, all electrical, mechanical, and aerodynamic aspects of the WT must be studied. Moreover, the contribution of the diesel generator system and its controllers should be considered. This paper describes how the aerodynamic and mechanical aspects of a WT can be simulated using TurbSim, AeroDyn, and FAST where the electrical parts of WT, diesel generator, its controllers, and electrical loads are modeled by Simulink blocks. Simulation results obtained from the model are used to observe the power and voltage variations at the WT generator terminals under different operating conditions. Furthermore, the effects of tower shadow, wind shears, yaw error, and turbulence on the power quality in a stand-alone wind-diesel system utilizing a fixed-speed WT are studied.",2009,0,
288,289,Based on Compact Type of Wavelet Neural Network Tolerance Analog Circuit Fault Diagnosis,"Based on the classical wavelet neural network, this paper put forward a sort of improved multiple-input multiple-output compact type of wavelet neural network, adopted adaptive learning rate and additional momentum BP algorithm to carry out training, studied its tolerance analog circuit fault diagnosis applications. Simulation results displayed that the compact type of wavelet neural network learning is fast, it can be effective diagnosed and located to tolerance analog circuit fault.",2009,0,
289,290,Lab VIEW based implementation of remedial action for DC arcing faults in a spacecraft,"In this paper remedial action for DC arcing faults in spacecraft has been designed and implemented using Lab VIEW. The Lab VIEW is an innovative graphical programming system designed to facilitate computer controlled data acquisition and analysis. DC arcing faults in spacecraft has been designed and implemented using the experimental data obtained at NASA Glenn research center. It is important to keep the continuity of the power supply and at the same time increase the reliability of spacecraft energy power system. In the frequency domain, fast Fourier transformation (FFT) is used for the feature extraction of the fault signal and odd harmonics frequency components of the phase currents are analyzed.",2003,0,
290,291,A Fault Tolerant Optimization Algorithm based on Evolutionary Computation,"In this paper we describe how an evolutionary algorithm is capable of running on a distributed environment with volatile resources. When executing algorithms in a desktop computing or resource harvesting context, resources can be reclaimed by their owners without warning, which may produce data loss and process to fail. The interest of the algorithm presented in the paper is that although it doesn't keep processes from failing, or data from being lost, it does improve the quality of results because of its design, not employing any special task control, checkpoint/restart or resource redundancies. By means of a series of experiments, we test the performance of the algorithm by studying the number of process failing and the quality of solutions when compared with the classic flavor of the evolutionary algorithm. The new algorithm, which shows its advantages, therefore improve dependability of distributed system",2006,0,
291,292,Efficient Error Correcting Codes for On-Chip DRAM Applications for Space Missions,"New systematic single error correcting codes-based circuits are introduced for random access memories, with ultimate minimal encoding/decoding complexity, low power and high performance. These new, codes-based circuits can be used in combinational circuits and in on-chip random access memories of reconfigurable architectures with high performance and ultimate minimum decoding/encoding complexity. Due to the overhead of parity check bits associated with the error-correcting-codes, there has always been a demand for an efficient and compact code for small memories in terms of data width. The proposed codes give improved performance even for small memories over the other codes. Area and power comparisons have been performed to benchmark the performance index of our codes. The code-centric circuits offer significant advantages over existing error correcting codes-based circuits in the literature in terms of lower size, power and cost which make them suitable for wider range of applications such as those targeting space. The paper describes the new efficient code and associated circuits for its implementation",2005,0,
292,293,Fault Diagnosis for Engine Based on EMD and Wavelet Packet BP Neural Network,"To solve the problem of fault diagnosis for engine, due to the complexity of the equipments and the particularity of the operating environments, generally speaking, there is no one-to-one correspondence between the characteristic parameters and status, so, the methods of diagnosis are very complicated. A novel fault diagnosis method based on empirical mode decomposition (EMD) and wavelet packet BP neural network is proposed in this paper. Firstly, the given signal is analyzed by wavelet packet to remove the noise; Then the de-noised data is decomposed into a number of IMFs by EMD and extract their frequency eigenvectors, then using these eigenvectors as the training samples of the BP network, training the BP network to identify the faults. Finally, the simulation experiments shows that the proposed method for fault diagnosis of engine is effective and the de-nosing process using wavelet packet transform is essential.",2009,0,
293,294,Errors estimation and minimization for the 5-axis milling machine,"This paper presents the tool path optimization algorithms to compute and estimate the non-linear inverse kinematics errors of the 5-axis milling machine. The approach is based on a global approximation of the required surface by a virtual surface constructed from the tool trajectories. Errors are computed from the difference between the required surface and the virtual surface and displayed numerically and graphically through the virtual machine simulator. The simulator is based on 3D representation and employing the inverse kinematics approach to derive the corresponding rotational and translation movement of the mechanism. The simulator makes it possible to estimate the errors of a 3D tool-path based on a prescribed set of the cutter location (CL) points as well as a set of the cutter contact (CC) points with tool inclination angle. Errors, particularly near the vicinity of the large milling errors, are minimized using a discrete algorithm based on a shortest path strategy. Furthermore, the simulator can be used to simulate the milling process, verify the final cut of the actual tool-path before testing with the real machine. Thus, it reduces the cost of iterative trial and errors.",2002,0,
294,295,Soft error assessments for servers,"In order to assess the soft error rate (SER) of a server, it is important to not only quantify the soft error contribution of the individual semiconductor components, but also to account for derating and for SER mitigation like hardening and shielding. Derating describes the fact that not every soft error has an impact. A large number of soft errors vanish based on electrical, logical or timing considerations. They have no impact. Additionally, a server can, to a large degree, be protected from the impact of soft errors by implementing error detection and correction means. In these cases the impact of the soft error is limited to the extra compute time needed for the correction. Summing up the SER contributions from transistors and circuits results in the so-called raw soft error rate, a rate which describes just the bottom layer of the system stack. Powerful protection mechanisms at higher layers can reduce that rate by several orders of magnitude. Awareness of this vertical interaction across the different layers in the system stack leads to servers optimized for robustness.",2010,0,
295,296,On the Use of Dynamic Binary Instrumentation to Perform Faults Injection in Transaction Level Models,"Transaction Level Modelling (TLM) has been widely accepted as systems modelling framework focused in system components communication. This approach allows efficient accurate estimation and rapid design space exploration. Besides of the functional simulation for validation of a hardware/software designs, there are additional reliability requirements that need advanced simulation techniques to analyze the system behaviour in the presence of faults. Several traditional VHDL fault injection mechanisms like mutants or saboteurs have been adapted to SystemC model descriptions. The main drawback of these approaches is the necessity of source code modification to carry out the fault injection campaigns. In this paper, we propose the use of Dynamic Binary Instrumentation (DBI) to perform fault injection in SystemC TLM models. DBI is a technique to intercept software routine calls allowing argument and return value corruption and data structures modification at runtime. This technique needs neither source code modifications nor recompilation of models in order to generate module mutants or in order to insert saboteurs in the signal communication path.",2009,0,
296,297,Online Fault Diagnosis of Discrete Event Systems. A Petri Net-Based Approach,"This paper is concerned with an online model-based fault diagnosis of discrete event systems. The model of the system is built using the interpreted Petri nets (IPN) formalism. The model includes the normal system states as well as all possible faulty states. Moreover, it assumes the general case when events and states are partially observed. One of the contributions of this work is a bottom-up modeling methodology. It describes the behavior of system elements using the required states variables and assigning a range to each state variable. Then, each state variable is represented by an IPN model, herein named module. Afterwards, using two composition operators over all the modules, a monolithic model for the whole system is derived. It is a very general modeling methodology that avoids tuning phases and the state combinatory found in finite state automata (FSA) approaches. Another contribution is a definition of diagnosability for IPN models built with the above methodology and a structural characterization of this property; polynomial algorithms for checking diagnosability of IPN are proposed, avoiding the reachability analysis of other approaches. The last contribution is a scheme for online diagnosis; it is based on the IPN model of the system and an efficient algorithm to detect and locate the faulty state. Note to Practitioners-The results proposed in this paper allow: 1) building discrete event system models in which faults may arise; 2) testing the diagnosability of the model; and 3) implementing an online diagnoser. The modeling methodology helps to conceive in a natural way the model from the description of the system's components leading to modules that are easily interconnected. The diagnosability test is stated as a linear programming problem which can be straightforward programmed. Finally, the algorithm for online diagnosis leads to an efficient procedure that monitors the system's outputs and handles the normal behavior model. This provides an oppo- rtune detection and location of faults occurring within the system",2007,0,
297,298,Impact of correlation errors on the optimum Kalman filter gain identification in a single sensor environment,"The impact of errors in the innovation correlation functions evaluation, related to the suboptimal filter, on the identification of the optimum steady state Kalman filter gains are investigated. This issue arises in all real time applications, where the correlations must be calculated from experimental data. An identification algorithm proposed in the literature, with formal proof of convergence, is revisited and summarized. Based on this algorithm, equations describing this impact are developed. Simulation results are presented and discussed. As contribution, experimental results of the identification algorithm, applied to estimate the states of a position servo systems, are presented.",2004,0,
298,299,A Fault Tolerant Control strategy for an unmanned aerial vehicle based on a Sequential Quadratic Programming algorithm,"In this paper a fault tolerant control strategy for the nonlinear model of an unmanned aerial vehicle (UAV) equipped with numerous redundant controls is proposed. Asymmetric actuator failures are considered and, in order to accommodate them, a sequential quadratic programming (SQP) algorithm which takes into account nonlinearities, aerodynamic and gyroscopic couplings, state and control limitations is implemented. This algorithm computes new trims such that around the new operating point, the faulty linearized model remains nearby from the fault free model. For the faulty linearized models, linear state feedback controllers based on an eigenstructure assignment method are designed to obtain soft transients during accommodation. Real time implementation of the SQP algorithm is also discussed.",2008,0,
299,300,"A Framework to Evaluate the Trade-Off among AVF, Performance and Area of Soft Error Tolerant Microprocessors","Because of the increasing susceptibility of the integrated circuits to soft errors, many techniques have been proposed in all the design levels to reduce the AVF (architecturally vulnerable factor) of microprocessors with extra performance and area overheads. These overheads have a negative impact on the reliability. Conventional reliability evaluation frameworks do not take both performance and area overheads into account. A new metric, mMWTF (modified mean work to failure), is proposed in this paper to capture the trade-off among AVF, performance and area. A quantitative approach to evaluate mMWTF is also presented, in which fault injection is used to estimate the AVF. To modify the conventional fault injection methods which inject only SEU (single event upset), a new method is proposed to injects both SEU and MBU (multi bits upset), the latter of which happens more frequently with the shrinking feature size. Because of the new metric and the new fault injection method, the framework presented in this paper is more accurate than conventional ones. As a case study, two control flow checking techniques are proposed and evaluated in this paper. The evaluation results demonstrate that the techniques with better balance among AVF, performance and area can better improve the reliability of microprocessors.",2008,0,
300,301,Falcon: fault localization in concurrent programs,"Concurrency fault are difficult to find because they usually occur under specific thread interleavings. Fault-detection tools in this area find data-access patterns among thread interleavings, but they report benign patterns as well as actual faulty patterns. Traditional fault-localization techniques have been successful in identifying faults in sequential, deterministic programs, but they cannot detect faulty data-access patterns among threads. This paper presents a new dynamic fault-localization technique that can pinpoint faulty data-access patterns in multi-threaded concurrent programs. The technique monitors memory-access sequences among threads, detects data-access patterns associated with a program's pass/fail results, and reports dataaccess patterns with suspiciousness scores. The paper also presents the description of a prototype implementation of the technique in Java, and the results of an empirical study we performed with the prototype on several Java benchmarks. The empirical study shows that the technique can effectively and efficiently localize the faults for our subjects.",2010,0,
301,302,Enhanced error concealment with mode selection,"Delay sensitive video transmission over error prone networks can suffer from packet erasures when channel conditions are not favorable. Use of error concealment (EC) at the video decoder is necessary in such cases to prevent error induced artefacts making the affected video frames visibly intolerable. This paper proposes an EC method that incorporates enhanced temporal and spatial concealment elements, the use of which is controlled by a mode selection (MS) algorithm well matched to the characteristics of the temporal concealment approach. The performance of the individual enhancements and of the MS algorithm are compared with the respective features of the method employed in the H.264 joint model (JM) decoder and with other state of the art methods. The overall performance of the proposed method is shown to offer significant gains (up to 9 dB) compared to that of the JM decoder for a wide range of natural and animation image sequences without any considerable increase in complexity",2006,0,
302,303,A Fault-Location Method for Application With Current Differential Relays of Three-Terminal Lines,This paper presents a new method for locating faults on three-terminal power lines. Estimation of a distance to fault and indication of a faulted section is performed using three-phase current from all three terminals and additionally three-phase voltage from the terminal at which a fault locator is installed. Such a set of synchronized measurements has been taken into consideration with the aim of developing a fault-location algorithm for applications with current differential relays of three-terminal lines. The delivered fault-location algorithm consists of three subroutines designated for locating faults within particular line sections and a procedure for indicating the faulted line section. Testing and evaluation of the algorithm has been performed with fault data obtained from versatile Alternate Transients Program-Electromagnetic Transients Program simulations. The sample results of the evaluation are reported and discussed.,2007,0,
303,304,Bad Words: Finding Faults in Spirit's Syslogs,"Accurate fault detection is a key element of resilient computing. Syslogs provide key information regarding faults, and are found on nearly all computing systems. Discovering new fault types requires expert human effort, however, as no previous algorithm has been shown to localize faults in time and space with an operationally acceptable false positive rate. We present experiments on three weeks of syslogs from Sandia's 512-node ""Spirit"" Linux cluster, showing one algorithm that localizes 50% of faults with 75% precision, corresponding to an excellent false positive rate of 0.05%. The salient characteristics of this algorithm are (1) calculation of nodewise information entropy, and (2) encoding of word position. The key observation is that similar computers correctly executing similar work should produce similar logs.",2008,0,
304,305,Efficient Test Pattern Compression Method Using Hard Fault Preferring,"This paper describes new compression method that is used for test pattern compaction and compression in algorithm called COMPAS, which utilizes a test data compression method based on pattern overlapping. This algorithm reorders and compresses deterministic test patterns previously generated in an ATPG by overlapping them. Independency of COMPAS on used ATPG is discussed and verified. New method improves compression ratio by preprocessing input data to determine the degree of random test resistance for each fault. This information allows the algorithm to reorder test patterns more efficiently and results to 10% compression ratio improvement in average. Compressed data sequence is well suited for decompression by the scan chains in the embedded tester cores.",2008,0,
305,306,Detection of small defects by THz-waves for non-destructive testing in dielectric layered structures,"In this study, the small defects detection in dielectric layered structures by THz waves for nondestructive testing. Finite element method were used for modelling of the structures.",2010,0,
306,307,"On line sensor fault detection, isolation and accommodation in tactical aerospace vehicle","This paper presents on line sensor fault detection, isolation (FDI) and the associated fault tolerant control (FTC) algorithm for a tactical aerospace vehicle. A study on the analytical redundancy and a sensor fault detection scheme (FDI ) into a flight control system has been performed for a tactical aerospace vehicle using the longitudinal model. There are various methods available in the academic literature to apply FDI and FTC schemes to control systems and some have already been applied to real applications. Among these, observer-based approaches have arisen as one of the most widespread. The basic ideas behind observer-based FDI schemes are the generation of residuals, and the use of an optimal threshold function to differentiate faults from disturbances. Generally, the residuals, also known as diagnostic signals, are generated from estimates of the system's measurements obtained by a Luenberger observer or a Kalman filter. The threshold function is then used to 'detect' the fault by separating the residuals from false faults and disturbances. The change in residual signal is used to detect and isolate the fault and corresponding fault tolerant control action is taken to arrest the failure of the aerospace vehicle. A closed-loop simulation with nonlinear 6-degree of freedom (6-DoF) model shows that the above FDI and FTC scheme will be able to reduce the probability of mission failure due to the fault in one of the sensors.",2004,0,
307,308,Induction machines performance evaluator 'torque speed estimation and rotor fault diagnostic',"This paper proposes a new DSP based tool for evaluating the performance of induction motors based on the data extracted from the stator current. In the proposed algorithm, a pattern recognition technique according to Bayes minimum error classifier is developed to detect incipient rotor faults such as broken rotor bars and static eccentricity in induction motors. Also, part of the algorithm is based on the acceleration method presented in the IEEE Std. 112. It helps to calculate the motor's torque using two line currents and voltages. The use of linear and quadratic time-frequency representations is investigated as a viable solution to the task at hand. Speed information is vital in this approach, so an algorithm to track the speed related saliency induced harmonics from the machine's line current spectrogram is presented. Capturing the harmonics gives the rotor speed that can also be used to extract the feature vector for diagnostic. The implementation of the algorithm on TMS320C6000 family of DSP chips is currently underway. The complete algorithm is then be used to obtain the induction motor's performance curves. This is a complete stand-alone panel mounted induction motor diagnostic tool currently being developed in their lab. This package will be used in conjunction with a drive system (inverter) for online performance monitoring and preventing unwanted shutdown of the induction motor. The difficulties encountered, including a limited dynamic range and the presence of cross terms, are addressed and the suggested solution is provided. Experimental results corroborating the proposed algorithm are presented, and a discussion of the advantages and disadvantages of such an approach is touched upon",2002,0,
308,309,Correction of MR k-space data corrupted by spike noise,"Magnetic resonance images are reconstructed from digitized raw data, which are collected in the spatial-frequency domain (also called k-space). Occasionally, single or multiple data points in the k-space data are corrupted by spike noise, causing striation artifacts in images. Thresholding methods for detecting corrupted data points can fail because of small alterations, especially for data points in the low spatial frequency area where the k-space variation is large. Restoration of corrupted data points using interpolations of neighboring pixels can give incorrect results. The authors propose a Fourier transform method for detecting and restoring corrupted data points using a window filter derived from the striation-artifact structure in an image or an intermediate domain. The method provides an analytical solution for the alteration at each corrupted data point. It can effectively restore corrupted k-space data, removing striation artifacts in images, provided that the following 3 conditions are satisfied. First, a region of known signal distribution (for example, air background) is visible in either the image or the intermediate domain so that it can be selected using a window filter. Second, multiple spikes are separated by the full-width at half-maximum of the point spread function for the window filter. Third, the magnitude of a spike is larger than the minimum detectable value determined by the window filter and the standard deviation of k-space random noise.",2000,0,
309,310,Initial Experiences with a New FPGA Based Traveling Wave Fault Recorder Installed on a MV Distribution Network,"This paper presents the initial results obtained from a newly developed FPGA based traveling wave fault recorder installed on a medium voltage (MV) distribution line. The recorder is capable of recording six input signals, simultaneously sampling at 40 mega samples per second (MSPS) and at 14 bit resolution. It uses high bandwidth 17 MHz Rogowski coils connected to the secondary of a current transformer inside the substation to acquire the high frequency traveling wave components. Initial results during the testing phase show that recorder is capable of recording high fidelity signals relating to switching events. It has also highlighted that the distribution line is subject to many other transient phenomena in addition to faults and switching events which must be taken into consideration when choosing a suitable triggering mechanism.",2008,0,
310,311,Evaluation of the Low Frame Error Rate Performance of LDPC Codes Using Importance Sampling,"We present an importance sampling method for the evaluation of the low frame error rate (FER) performance of LDPC codes under iterative decoding. It relies on a combinatorial characterization of absorbing sets, which are the dominant cause of decoder failure in the low FER region. The biased density in the importance sampling scheme is a mean-shifted version of the original Gaussian density, which is suitably centered between a codeword and a dominant absorbing set. This choice of biased density yields an unbiased estimator for the FER with a variance lower by several orders of magnitude than the standard Monte Carlo estimator. Using this importance sampling scheme in software, we obtain good agreement with the experimental results obtained from a fast hardware emulator of the decoder.",2007,0,
311,312,A New Method for Earth Fault Line Detection Based on Two-Dimensional Wavelet Transform in Distribution Automation,"A novel method based on two-dimensional wavelet transform to detect single-phase faults in distribution systems is proposed in this paper. After structuring analytic signals of zero sequence current, the two-dimensional wavelet transform is applied. Thus the analysis of combined signal of amplitude and phase is realized. Compared with the use of single amplitude or single phase, combined signal carries more details of transient signal. Theoretical analysis and MATLAB based simulation show that the presented method can exactly and effectively choose the faulty line in single phase-to-ground fault",2005,0,
312,313,An FFT-based method to evaluate and compensate gain and offset errors of interleaved ADC systems,"Interleaved analog-digital converter (ADC) systems can be used to increase the sampling rate for a given ADC implementation technique. In theory, the maximum sampling rate that can be achieved is limited only by the bandwidth and the practical limits related to the power and space of integrated circuits. In this paper, a solution to increase the sampling rate of a digitizing system based on interleaved ADCs is presented. An error analysis, which takes into consideration offset and gain errors of the different ADC channels, is performed in order to quantify the effect of such errors in the system's performance. A software method based on the fast Fourier transform is presented for offset and gain error compensation of interleaved ADC associations. Numerical simulations and experimental results are used to validate the theory and the proposed compensation algorithm.",2004,0,
313,314,Fault Diagnosis of Generator Based on D-S Evidence Theory,"It is difficult to identify the fault type with the signal gathered from the sensors. In this paper, a new fusion algorithm based on the Dempster-Shafer theory of evidence and neural networks is brought forward. This method combines the advantages of D-S evidence theory and the BP neural network. Neural networks are used to pretreated the data gathered from the embedded sensors in the monitoring system of hydropower plant. Compared with the approaches that only adopt D-S evidence theory or neural networks, the accuracy of diagnostic results is obviously improved, and the signals analysis proved this conclusion. This method has been applied in the monitoring system of JiLin FengMan hydropower plant successfully.",2008,0,
314,315,Improved error bounds for the erasure/list scheme: the binary and spherical cases,We derive improved bounds on the error and erasure rate for spherical codes and for binary linear codes under Forney's erasure/list decoding scheme and prove some related results.,2004,0,
315,316,Faulted phase selection on double circuit transmission line using wavelet transform and neural network,"Modern numerical relays often incorporate the logic for combined single and three-phase auto-reclosing scheme; single phase to earth faults initiate single-phase tripping and reclosure, and all the other faults initiate three-phase tripping and reclosure. Accurate faulted phase selection is required for such a scheme. This paper presents a novel scheme for detection and classification of faults on double circuit transmission line. The proposed approach uses combination of wavelet transform and neural network, to solve the problem. While wavelet transform is a powerful mathematical tool which can be employed as a fast and very effective means of analyzing power system transient signals, artificial neural network has a ability to classify non-linear relationship between measured signals by identifying different patterns of the associated signals. The proposed algorithm consists of time-frequency analysis of fault generated transients using wavelet transform, followed by pattern recognition using artificial neural network to identify the faulted phase. MATLAB/Simulink software is used to generate fault signals and verify the correctness of the algorithm. The adaptive discrimination scheme is tested by simulating different types of fault and varying fault resistance, fault location and fault inception time, on a given power system model. The simulation results show that the proposed phase selector scheme is able to identify faulted phase on the double circuit transmission line rapidly and correctly.",2009,0,
316,317,Algorithm-Based Fault Tolerance for Fail-Stop Failures,"Fail-stop failures in distributed environments are often tolerated by checkpointing or message logging. In this paper, we show that fail-stop process failures in ScaLAPACK matrix-matrix multiplication kennel can be tolerated without checkpointing or message logging. It has been proved in previous algorithm-based fault tolerance that, for matrix-matrix multiplication, the checksum relationship in the input checksum matrices is preserved at the end of the computation no mater which algorithm is chosen. From this checksum relationship in the final computation results, processor miscalculations can be detected, located, and corrected at the end of the computation. However, whether this checksum relationship can be maintained in the middle of the computation or not remains open. In this paper, we first demonstrate that, for many matrix matrix multiplication algorithms, the checksum relationship in the input checksum matrices is not maintained in the middle of the computation. We then prove that, however, for the outer product version algorithm, the checksum relationship in the input checksum matrices can be maintained in the middle of the computation. Based on this checksum relationship maintained in the middle of the computation, we demonstrate that fail-stop process failures (which are often tolerated by checkpointing or message logging) in ScaLAPACK matrix-matrix multiplication can be tolerated without checkpointing or message logging.",2008,0,
317,318,Maintaining Consistency between Loosely Coupled Services in the Presence of Timing Constraints and Validation Errors,"Loose coupling is often cited as a defining characteristic of service-oriented architectures. Interactions between services take place via messages in an asynchronous environment where communication and processing delays can be unpredictable; further, interacting parties are not required to be on-line at the same time. Despite loose coupling, many service interactions have timing and validation constraints. For example, business interactions that take place using RosettaNet partner interface processes (PIPs) such as request price and availability, request purchase order, notify of invoice, etc. have to meet several timing and message validation constraints. A failure to deliver a valid message within its time constraint could cause mutually conflicting views of an interaction. For example, one party can regard it as timely whilst the other party regards it as untimely, leading to application level inconsistencies. The paper describes how business interactions, such as PIPs can be wrapped by simple handshake synchronisation protocols to provide bilateral consistency, thereby simplifying the task of coordinating peer-to-peer business processes",2006,0,
318,319,Compact Microstrip Quasi-Elliptic Bandpass Filter Using Open-Loop Dumbbell Shaped Defected Ground Structure,"A novel square open-loop dumbbell-shaped defected ground structure (DGS) unit is proposed. This unit provides a quasi-elliptic bandpass characteristic and the two transmission zeros near the passband edges can be controlled by the dimensions of DGS. Two quasi-elliptic bandpass filters using one and two DGS units; centered at 1.5 GHz were designed and implemented. Both the simulation and experimental results show that the DGS filter response is in good accordance with ideal quasi-elliptic model. The prototype filter with two DGS units yields higher order quasi-elliptic filtering and reports the measured 0.72 dB as insertion loss, 34 dB as matching, 51.8 % fractional bandwidth and about 20 dB stopband attenuation up to 10 GHz",2006,0,
319,320,Analyzing the soft error resilience of linear solvers on multicore multiprocessors,"As chip transistor densities continue to increase, soft errors (bit flips) are becoming a significant concern in networked multiprocessors with multicore nodes. Large cache structures in multicore processors are especially susceptible to soft errors as they occupy a significant portion of the chip area. In this paper, we consider the impacts of soft errors in caches on the resilience and energy efficiency of sparse linear solvers. In particular, we focus on two widely used sparse iterative solvers, namely Conjugate Gradient (CG) and Generalized Minimum Residuals (GMRES). We propose two adaptive schemes, (i) a Write Eviction Hybrid ECC (WEH-ECC) scheme for the L1 cache and (ii) a Prefetcher Based Adaptive ECC (PBA-ECC) scheme for the L2 cache, and evaluate the energy and reliability trade-offs they bring in the context of GMRES and CG solvers. Our evaluations indicate that WEH-ECC reduces the CG and GMRES soft error vulnerability by a factor of 18 to 220 in L1 cache, relative to an unprotected L1 cache, and energy consumption by 16%, relative to a cache with strong protection. The PBA-ECC scheme reduces the CG and GMRES soft error vulnerability by a factor of 9 A 10<sup>3</sup> to 8.6 A 10<sup>9</sup>, relative to an unprotected L2 cache, and reduces the energy consumption by 8.5%, relative to a cache with strong ECC protection. Our energy overheads over unprotected L1 and L2 caches are 5% and 14% respectively.",2010,0,
320,321,Application-driven co-design of fault-tolerant industrial systems,"This paper presents a novel methodology for the HW/SW co-design of fault tolerant embedded systems that pursues the mitigation of radiation-induced upset events (which are a class of Single Event Effects - SEEs) on critical industrial applications. The proposal combines the flexibility and low cost of Software Implemented Hardware Fault Tolerance (SIHFT) techniques with the high reliability of selective hardware replication. The co-design flow is supported by a hardening platform that comprises an automatic software hardening environment and a hardware tool able to emulate Single Event Upsets (SEUs). As a case study, we selected a soft-micro (PicoBlaze) widely used in FPGA-based industrial systems, and a fault tolerant version of the matrix multiplication algorithm was developed. Using the proposed methodology, the design was guided by the requirements of the application, leading us to explore several trade-offs among reliability, performance and cost.",2010,0,
321,322,Defect-Tolerant Design and Optimization of a Digital Microfluidic Biochip for Protein Crystallization,"Protein crystallization is a commonly used technique for protein analysis and subsequent drug design. It predicts the 3-D arrangement of the constituent amino acids, which in turn indicates the specific biological function of a protein. Protein crystallization experiments are typically carried out in well-plates in the laboratory. As a result, these experiments are slow, expensive, and error-prone due to the need for repeated human intervention. Recently, droplet-based AdigitalA microfluidics have been used for executing protein assays on a chip. Protein samples in the form of nanoliter-volume droplets are manipulated using the principle of electrowetting-on-dielectric. We present the design of a multi-well-plate microfluidic biochip for protein crystallization; this biochip can transfer protein samples, prepare candidate solutions, and carry out crystallization automatically. To reduce the manufacturing cost of such devices, we present an efficient algorithm to generate a pin-assignment plan for the proposed design. The resulting biochip enables control of a large number of on-chip electrodes using only a small number of pins. Based on the pin-constrained chip design, we present an efficient shuttle-passenger-like droplet manipulation method and test procedure to achieve high-throughput and defect-tolerant well loading.",2010,0,
322,323,Influence of the AC system faults on HVDC system and recommendations for improvement,"The interaction between AC and DC systems in a long distance bulk power transmission system is very complicated. In this paper, taking some cases in China Southern Power Grid as examples, the main functions of the DC control system operating during the AC faults is discussed. During the AC faults, the protection and monitoring system for DC converter and the protection system for converter transformers and auxiliary transformers may work incorrectly, reasons for these cases are analyzed and recommendations are given to solve the defects. Experience from these cases will help us to improve the ability of operation and maintenance to insure the safety and provide useful references for the design of HVDC and the coordination of AC/DC system in China.",2009,0,
323,324,Dense error correction via l1-minimization,"We study the problem of recovering a non-negative sparse signal x isin Ropf<sup>n</sup> from highly corrupted linear measurements y = Ax+e isin Ropf<sup>m</sup>, where e is an unknown (and unbounded) error. Motivated by an observation from computer vision, we prove that for highly correlated dictionaries A, any non-negative, sufficiently sparse signal x can be recovered by solving an lscr<sup>1</sup>-minimization problem: min ||x||<sub>1</sub> + ||e||<sub>1</sub> subject to y = Ax + e. If the fraction rho of errors is bounded away from one and the support of x grows sublinearly in the dimension m of the observation, for large m, the above lscr<sup>1</sup>-minimization recovers all sparse signals x from almost all sign-and-support patterns of e. This suggests that accurate and efficient recovery of sparse signals is possible even with nearly 100% of the observations corrupted.",2009,0,
324,325,Efficient stimuli generators for detection of path delay faults,"This paper presents a way to construct accumulator based test vector generators intended for efficient detection of path delay faults. Experiments conducted using our path delay fault simulator, GFault, shows that our proposed generator can give as much as 30times reduction in test time for circuits in the ISCAS85 benchmark suite compared to an accumulator based pseudo random generator",2005,0,
325,326,A fault analysis and design consideration of pulsed-power supply for high-power laser,"According to the requirements of driving flashlamps, the design of a pulsed-power supply (PPS), based on capacitors as energy storage elements, is presented. Special consideration is given to some possible faults such as capacitor internal short-circuit, bus bar breakdown to ground, flashlamp sudden short or break (open circuit), and closing switch restrike in the preionization branch. These faults were analyzed in detail, and both fault current and voltage waveforms are shown through circuit simulation. Based on the analysis and computation undertaken, the pulsed-power system design and protection requirements are proposed. The preliminary experiments undertaken after circuit simulation demonstrated that the design of the PPS met the project requirements.",2003,0,
326,327,A fault tolerant control architecture for automated highway systems,A hierarchical controller for dealing with faults and adverse environmental conditions on an automated highway system is proposed. The controller extends a previous control hierarchy designed to work under normal conditions of operation. The faults are classified according to the capabilities remaining on the vehicle or roadside after the fault has occurred. Information about these capabilities is used by supervisors in each of the layers of the hierarchy to select appropriate fault handling strategies. We outline the strategies needed by the supervisors and give examples of their detailed operation,2000,0,
327,328,Analysis of interconnect crosstalk defect coverage of test sets,"This paper addresses the problem of evaluating the effectiveness of test sets to detect crosstalk defects in interconnects of deep sub-micron circuits. The fast and accurate estimation technique will enable: (a) evaluation of different existing tests, like functional, scan, logic BIST, and delay tests, for effective testing of crosstalk defects in interconnects, and (b) development of crosstalk tests if the existing tests are not sufficient, thereby minimizing the cost of interconnect testing. Based on a covering relationship we establish between transition tests in detecting crosstalk defects, we develop an abstract crosstalk fault model for circuit interconnects. Based on this fault model, and the covering relationship, we develop a fast and efficient method to estimate the fault coverage of any general test set. We also develop a simulation-based technique to calculate the probability of occurrence of the defects corresponding to each fault, which enables the fault coverage analysis technique to produce accurate estimates of the actual crosstalk defect coverage of a given test set. The crosstalk test and fault properties, as well as the accuracy of the proposed crosstalk coverage analysis techniques, have been validated through extensive simulation experiments. The experiments also demonstrate that the proposed crosstalk techniques are orders of magnitude faster than the alternative method of SPICE-level simulation. Finally, we demonstrate the practical applicability of the proposed fault coverage analysis technique by using it to evaluate the crosstalk fault coverage of logic BIST tests for the buses in a DSP core",2000,0,
328,329,Atmospheric correction of AMSR-E brightness temperatures for dry snow cover mapping,"Differences between the brightness temperatures (spectral gradient) collected by the Advanced Microwave Scanning Radiometer for EOS (AMSR-E) at 18.7 and 36.5 GHz are used to map the snow-covered area (SCA) over a region including the western U.S. The brightness temperatures are corrected to take into account for atmospheric effects by means of a simplified radiative transfer equation whose parameters are stratified using rawinsonde data collected from a few stations. The surface emissivity is estimated from the model, and the brightness temperatures at the surface are computed as the product of the surface temperature and the computed emissivity. The SCA derived from microwave data is compared with that obtained from the Moderate Resolution Imaging Spectroradiometer for both cases of corrected and noncorrected brightness temperatures. The improvement to the SCA retrievals based on the corrected brightness temperatures shows an average value around 7%",2006,0,
329,330,A low-tech solution to avoid the severe impact of transient errors on the IP interconnect,"There are many sources of failure within a system-on-chip (SoC), so it is important to look beyond the processor core at other components that affect the reliable operation of the SoC, such as the fabric included in every one that connects the IP together. We use ARM's AMBA 3 AXI bus matrix to demonstrate that the impact of errors on the IP interconnect can be severe: possibly causing deadlock or memory corruption. We consider the detection of 1-bit transient faults without changing the IP that connects to the bus matrix or the AMBA 3 standard and without adding extra latency while keeping the performance and area overhead low. We explore what can be done under these constraints and propose a combination of techniques for a low-tech solution to detect these rare events.",2009,0,
330,331,The study of analog circuit fault diagnosis method based on circuit transform function,"This is the paper use Laplace Transfer to compute the analog circuit transform function, and compute the fault transform functions with different kinds of fault to generate the fault diagnosis table. The table is used to complete fault diagnosis. At last the software Multisim is used to simulate an analog circuit to do the fault diagnosis, and this method is verified usefully.",2010,0,
331,332,Efficient Fault-Tolerant Backbone Construction in Tmote Sky Sensor Networks,"In this study, we have investigated the effectiveness of building AFault-Tolerant BackboneA for data dissemination in Tmote Sky sensor networks. Tmote Sky sensors provide programmable and adjustable output power for data transmission. Users can control adequate transmission power for each sensor. Based on our measurements of Tmote Sky, there is a steadily transmitted distance for every power level. For certain power level, successfully-transmitted ratio was approximately 100 percent when the distance between sender and receiver was less than the steadily-transmitted distance. In accordance with the character on Tmote Sky, the ideas of fault-tolerant backbone has been made for constructing a fault-tolerant and stable system for Tmote Sky. The fault-tolerant backbone protocol builds up a connected backbone, in which nodes are endowed with a sleep/awake schedule. Practical experimental results reveal the fast fault recovery and high successfully-transmitted ratio can be fulfilled in the realistic system. The following goals in the implementation have been reached, including self-configurable fault-tolerant groups, automatic backbone construction, automatic failure recovery, and route repair.",2009,0,
332,333,Anomaly detection: A robust approach to detection of unanticipated faults,"This paper introduces a methodology to detect as early as possible with specified degree of confidence and prescribed false alarm rate an anomaly or novelty (incipient failure) associated with critical components/subsystems of an engineered system that is configured to monitor continuously its health status. Innovative features of the enabling technologies include a Bayesian estimation framework, called particle filtering, that employs features or condition indicators derived from sensor data in combination with simple models of the systempsilas degrading state to detect a deviation or discrepancy between a baseline (no-fault) distribution and its current counterpart. The scheme provides the probability of abnormal condition and the probability of false alarm. The presence of an anomaly is confirmed for a given confidence level. The efficacy of the proposed anomaly detection architecture is illustrated with test data acquired from components typically found on aircraft and monitored via a test rig appropriately instrumented.",2008,0,
333,334,Research of Remote Fault Diagnosis System Based on Multi-Agent,"A Multi-agent based Remote fault Diagnosis system is an important system for high speed and automation which can not only monitor the status of the remote device, but serve for the remote device. Remote Fault diagnosis system are vital aspects in automation process, in this sense, remote diagnosis systems should support decision-making tools, the enterprise thinking and flexibility. In this paper a kind of Remote Diagnosis System based on multi-agent is presented. This model is based on a generic framework using multi-agent systems. Specifically, this paper analyses the architecture of Remote Fault Diagnosis System and the collaboration mechanism between Agents. The method brought forward in the paper was generally applicable to a general fault diagnosis.",2010,0,
334,335,Fault diagnosis for a delta-sigma converter by a neural network,The diagnosis of faults in a first order -converter is described. The circuit behaviour of fault-free circuits and circuits containing single faults were simulated and characterized by the output bitstream patterns. The latter were compared with that of the ideal fault-free circuit. A Simplified fuzzy ARTMAP was trained with metrics derived from the bitstreams and their assigned class. A diagnostic accuracy of 93% was achieved using just two of the metrics. The technique might be useful for the diagnosis of other circuits.,2004,0,
335,336,The Impact of Link Error Modeling on the Quality of Streamed Video in Wireless Networks,The influence of channel error characteristics on higher layer protocols or methods which are considering or even exploiting the error statistics is significant especially in wireless networks where fading and interference effects result in error pattern correlation properties (error bursts). In this work we are analysing the impact of the channel properties directly on the quality of streamed video. We are focusing on the quality of transmitted H.264/AVC video streaming over UMTS DCH (Dedicated Channel) and compare the quality of the streamed video simulated over measured link error traces (the measurements performed in a live UMTS network) to simulations with a memoryless channel and to models with enhanced error characteristics. The results show that appropriate modeling of the link layer error characteristics is very important but it can also be concluded that the error correlation properties of the link- or the network-layer model do not have an impact on the quality of the video stream as long as the resulting IP packet error probability remains unchanged.,2006,0,
336,337,A portable gait analysis and correction system using a simple event detection method,"Microcontrollers are widely used in the area of portable control systems, though they are only beginning to be used for portable, unobtrusive Functional Electrical Stimulation (FES) systems. This paper describes the initial prototyping of such a portable system. This has the intended use of detecting time variant gait anomalies in patients with hemiplegia, and correcting for them. The system is described in two parts. Firstly, the portable hardware implementing two independent communicating microcontrollers for low powered parallel processing and secondly the simplified low power software. Both are designed specifically for long term, stable use and also to communicate with PC based visual software for testing and evaluation. The system operates by using bend sensors to defect the angles of the hip, knee and ankle of both legs. It computes an error signal with which to produce a stimulation wave cycle, that is synchronised and timed for the new gait cycle from that in which the error was observed. This system uses a PID controller to correct for the instability inherent with such a large time delay between observation and correction.",2002,0,
337,338,Reaction to errors in robot systems,"The paper analyzes the problem of error (failure) detection and handling in robot programming. First an overview of the subject is provided and later error detection and handling in MRROC++ are described. To facilitate system reaction to the detected failures, the errors are classified and certain suggestions are made as to how to handle those classes of errors.",2002,0,
338,339,Final Prediction Error of Autoregressive Model as a New Feature in the Analysis of Heart Rate Variability,"The aim of this study is to offer a new heart rate variability (HRV) index that increases the accuracy in the discrimination of patients with congestive heart failure (CHF) from the control group. For this purpose, final prediction errors (FPE), which shows the quality of the conformity of autoregressive (AR) model, are calculated for model degrees from 1 to 100. Although the optimal AR model order and FPE values are widely used in the literature, they have not been used as possible HRV indices. In this study, we used FPE as an HRV feature for discriminating the patients with CHF from normal subjects and made a comparison with the other common HRV indices. As a result, we showed that FPE of AR model is a possible significant HRV feature.",2007,0,
339,340,One-Dimensional Variational Retrieval of the Wet Tropospheric Correction for Altimetry in Coastal Regions,"The altimeter range is corrected for tropospheric humidity by means of microwave radiometer measurements (Envisat/MWR, Jason-1/JMR, Jason-2/AMR). Over an open ocean, the altimeter/radiometer combination is satisfactory. However, in coastal areas, radiometer measurements are contaminated by the surrounding land surfaces, and the humidity retrieval method is not appropriate anymore. In this paper, a variational assimilation technique is proposed to retrieve the wet tropospheric correction near coasts. The method is first developed on simulations using the data from a meteorological model. A performance assessment is performed, as well as a comparison with a standard algorithm. The method is then applied on actual measurements, thus evaluating its feasibility.",2010,0,
340,341,Compact Test Generation for Small-Delay Defects Using Testable-Path Information,"Testing for small-delay defects requires fault-effect propagation along the longest testable paths. However, the selection of the longest testable paths requires high CPU time and leads to large pattern counts. Dynamic test compaction for small-delay defects has remained largely unexplored thus far. We propose a path-selection scheme to accelerate ATPG based on stored testable critical-path information. A new dynamic test-compaction technique based on structural analysis is also introduced. Simulation results are presented for a set of ISCAS'89 benchmark circuits.",2009,0,
341,342,Use of fault tree analysis for evaluation of system-reliability improvements in design phase,"Traditional failure mode and effects analysis is applied as a bottom-up analytical technique to identify component failure modes and their causes and effects on the system performance, estimate their likelihood, severity and criticality or priority for mitigation. Failure modes and their causes, other than those associated with hardware, primarily electronic, remained poorly addressed or not addressed at all. Likelihood of occurrence was determined on the basis of component failure rates or by applying engineering judgement in their estimation. Resultant prioritization is consequently difficult so that only the apparent safety-related or highly critical issues were addressed. When thoroughly done, traditional FMEA or FMECA were too involved to be used as a effective tool for reliability improvement of the product design. Fault tree analysis applied to the product as a top down in view of its functionality, failure definition, architecture and stress and operational profiles provides a methodical way of following products functional flow down to the low level assemblies, components, failure modes and respective causes and their combination. Flexibility of modeling of various functional conditions and interaction such as enabling events, events with specific priority of occurrence, etc., using FTA, provides for accurate representation of their functionality interdependence. In addition to being capable of accounting for mixed reliability attributes (failure rates mixed with failure probabilities), fault trees are easy to construct and change for quick tradeoffs as roll up of unreliability values is automatic for instant evaluation of the final quantitative reliability results. Failure mode analysis using fault tree technique that is described in this paper allows for real, in-depth engineering evaluation of each individual cause of a failure mode regarding software and hardware components, their functions, stresses, operability and interactions",2000,0,
342,343,Local magnetic error estimation using action and phase jump analysis of orbit data,"It's been shown in previous conferences that action and phase jump analysis is a promising method to measure normal quadrupole components, skew quadrupole components and even normal sextupole components. In this paper, the action and phase jump analysis is evaluated using new RHIC data.",2007,0,
343,344,Mining Frequent Patterns from Software Defect Repositories for Black-Box Testing,"Software defects are usually detected by inspection, black-box testing or white-box testing. Current software defect mining work focuses on mining frequent patterns without distinguishing these different kinds of defects, and mining with respect to defect type can only give limited guidance on software development due to overly broad classification of defect type. In this paper, we present four kinds of frequent patterns from defects detected by black-box testing (called black-box defect) based on a kind of detailed classification named ODC-BD (Orthogonal Defect Classification for Blackbox Defect). The frequent patterns include the top 10 conditions (data or operation) which most easily result in defects or severe defects, the top 10 defect phenomena which most frequently occur and have a great impact on users, association rules between function modules and defect types. We aim to help project managers, black-box testers and developers improve the efficiency of software defect detection and analysis using these frequent patterns. Our study is based on 5023 defect reports from 56 large industrial projects and 2 open source projects.",2010,0,
344,345,Low error rate LDPC decoders,"Low-density parity-check (LDPC) codes have been demonstrated to perform very close to the Shannon limit when decoded iteratively. However challenges persist in building practical high-throughput decoders due to the existence of error floors at low error rate levels. We apply high-throughput hardware emulation to capture errors and error-inducing noise realizations, which allow for in-depth analysis. This method enables the design of LDPC decoders that operate without error floors down to very low bit error rate (BER) levels. Such emulation-aided studies facilitate complex systems designs.",2009,0,
345,346,Fault-tolerant and energy-efficient permutation routing protocol for wireless networks,"A wireless network (WN) is a distributed system where each node is a small hand-held commodity device called a station. Wireless sensor networks have received increasing interest in recent years due to their usage in monitoring and data collection in a wide variety of environments like remote geographic locations, industrial plants, toxic locations or even office buildings. Two of the most important issues related to a WN are their energy constraints and their potential for developing faults. A station is usually powered by a battery which cannot be recharged while on a mission. Hence, any protocol run by a WN should be energy-efficient. Moreover, it is possible that all stations deployed as part of a WN may not work perfectly. Hence, any protocol designed for a WN should work well even when some of the stations are faulty. We design a protocol which is both energy-efficient and fault-tolerant for permutation routing in a WN.",2003,0,
346,347,A novel approach to architecture of radar fault diagnosis system based on mobile agents,"In order to improve radar fault diagnosis system, a new architecture of fault diagnosis system based on mobile agents is proposed. The architecture is based on an embedded network built-in radar system. It utilizes all kinds of mobile fault diagnostic agents in embedded network to detect shortcomings of distributed subsystems in radar. In the architecture, all MFDAs can migrate in embedded network, and can be centralized a personal computer so as to be updated and retrained conveniently for different batches of radar systems. In this paper, three kinds of start-up modes of fault diagnosis are illustrated, two kinds of multi-agent cooperation diagnostic frameworks are introduced, and a kind of structure of MFDA is addressed.",2010,0,
347,348,A Flexible and efficient bit error rate simulation method for high-speed differential link analysis using time-domain interpolation and superposition,"In this paper, a flexible and efficient time-domain method for calculating the bit error rate of high-speed differential links is presented. The method applies interpolation and superposition to the step response of a channel to construct the jittery data or/and clock waveforms at the receiver. With the statistics of the actual reference-crossing points extracted from the constructed receiver waveforms, the bathtub curves can be derived and extrapolated to get the eye margin at the given bit error rate. A software has been developed and applied for high-speed differential link design using the method. Good correlation has been achieved between the simulated results using this method and the measurement data with a bit error rate tester.",2008,0,
348,349,Do Crosscutting Concerns Cause Defects?,"There is a growing consensus that crosscutting concerns harm code quality. An example of a crosscutting concern is a functional requirement whose implementation is distributed across multiple software modules. We asked the question, ""How much does the amount that a concern is crosscutting affect the number of defects in a program?"" We conducted three extensive case studies to help answer this question. All three studies revealed a moderate to strong statistically significant correlation between the degree of scattering and the number of defects. This paper describes the experimental framework we developed to conduct the studies, the metrics we adopted and developed to measure the degree of scattering, the studies we performed, the efforts we undertook to remove experimental and other biases, and the results we obtained. In the process, we have formulated a theory that explains why increased scattering might lead to increased defects.",2008,0,
349,350,Impact of Transmission Network Reinforcement on Improvement of Power System Voltage Stability and Solving the Dynamic Delayed Voltage Recovery and Motor Stalling Problem After System Faults in the Saudi Electricity in the Western Region,"The Saudi Electricity company power system in the Western Region of the Kingdom (SEC-WR) is unique in its load pattern, growth trends and type, generation recourses and network configuration. The power system of the SEC-WR faced and is facing a high load growth. The high load increase gives rise to a very high loading of the transmission system elements, mainly power transformers and cables. The Western Region load is mainly composed of air conditioner (AC) during high load season. In case of faults this nature of load induces delayed voltage recovery following fault clearing on the transmission system. The sustained low voltage following transmission line faults could cause customer interruptions and may be equipment damage. The integrity of the transmission system may also be affected. The transient stability of the system may be affected. This may also influence the stability of the generating units in the system. The existing dynamic model of SEC-WR System has been described. The response of the model to the actual faults is compared with actual records obtained from the dynamic system monitor (DSM) installed in several locations in the SEC-WR System. The solution of the delayed voltage recovery problem after system faults may be achieved by reinforcement of the system, adding static VAr compensators (SVC) to provide the dynamic reactive power support to the system, reducing the fault clearing time and by under voltage load shedding. This paper analyzes and discusses the first alternative, the system reinforcement",2006,0,
350,351,Higher-order corrections to the pi criterion for the periodic operation of chemical reactors,"The present work develops a method to determine higher-order corrections to the pi criterion, derived from basic results of Center Manifold theory. The proposed method is based on solving the Center Manifold PDE via power series. The advantage of the proposed approach is the improvement of the accuracy of the pi criterion in predicting performance under larger amplitudes. The proposed method is applied to a continuous stirred tank reactor, where the yield of the desired product must be maximized.",2009,0,
351,352,Practical Criteria for the Separability of Eddy-Current Testing Signals on Multiple Defects,"Practical quantities have been introduced by the authors to characterize the interaction among multiple defects located in a specimen under eddy-current testing (ECT). If these quantities indicate that the interaction between the pairs of defects is negligible, the signal of an ECT probe for multiple defects can be calculated as the superposition of those signals, which are obtained for each defect as if it would be a single one. Conversely, if the criteria holds, the measured ECT signal can be decomposed into signals associated with the individual defects, which essentially simplifies the solution of the inverse problem of defect reconstruction. As an application of the criteria, the design of a novel barcoding system is presented, which is developed for marking metallic parts by laser treating, and where the applicability of linear signal processing methods for the reading out of the barcode is a requirement.",2008,0,
352,353,On-Line Reconfigurable XGFT Network-on-Chip Designed for Improving the Fault-Tolerance and Manufacturability of the MPSoC Chips,"Large System-on-Chip (SoC) circuits will contain an increasing number of processors which will communicate with each other across Networks-on-Chip (NOC). The faulty processors could be replaced with faultless ones, whereas only a single defect in the NOC can make the whole chip unusable. Therefore, the fault-tolerance of the NOC is a crucial component of the fault-tolerance and manufacturability of the SoCs. This paper presents a fault-tolerant extended generalized fat tree (XGFT) NOC developed for future multi-processor SoCs (MPSoC). Its fault-tolerance is improved with a new version of fault-diagnosis-and-repair (FDAR) system, which makes it possible to diagnose and repair the NOC on-line. It detects such static, dynamic and transient faults which block packets or produce bit errors, and reconfigures the faulty switches to operate correctly. Processors can also use it for reconfiguring the faulty switch nodes after the faults are located with other test methods. Simulation and synthesis results show that slightly defected XGFTs are able to achieve good performance after they are repaired with the FDAR while the costs of the FDAR remain tolerable",2006,0,
353,354,Attitude correction algorithm using GPS measurements for flight vehicles,For flight systems with an on-board seeker the attitude error is the major factor to determine the seeker pointing error at the time of object acquisition. To achieve a desired mission it must be minimized. The proposed algorithm corrects the attitude error in the guidance computer during flight by taking its position and velocity measurements from GPS or radar. This is possible since navigator's position and velocity states are correlated with attitude state. Computer simulation is shown to prove the proposed algorithm.,2002,0,
354,355,Hardware/software optimization of error detection implementation for real-time embedded systems,"This paper presents an approach to system-level optimization of error detection implementation in the context of fault-tolerant real-time distributed embedded systems used for safety-critical applications. An application is modeled as a set of processes communicating by messages. Processes are mapped on computation nodes connected to the communication infrastructure. To provide resiliency against transient faults, efficient error detection and recovery techniques have to be employed. Our main focus in this paper is on the efficient implementation of the error detection mechanisms. We have developed techniques to optimize the hardware/software implementation of error detection, in order to minimize the global worst-case schedule length, while meeting the imposed hardware cost constraints and tolerating multiple transient faults. We present two design optimization algorithms which are able to find feasible solutions given a limited amount of resources: the first one assumes that, when implemented in hardware, error detection is deployed on static reconfigurable FPGAs, while the second one considers partial dynamic reconfiguration capabilities of the FPGAs.",2010,0,
355,356,Development of Defect Classification Algorithm for POSCO Rolling Strip Surface Inspection System,"Surface inspection system (SIS) is an integrated hardware-software system which automatically inspects the surface of the steel strip. It is equipped with several cameras and illumination over and under the steel strip roll and automatically detects and classifies defects on the surface. The performance of the inspection algorithm plays an important role in not only quality assurance of the rolled steel product, but also improvement of the strip production process control. Current implementation of POSCO SIS has good ability to detect defects, however, classification performance is not satisfactory. In this paper, we introduce POSCO SIS and suggest a new defect classification algorithm which is based on support vector machine technique. The suggested classification algorithm shows good classification ability and generalization performance",2006,0,
356,357,Design of a fault-tolerant coarse-grained,"This paper considers the possibility of implementing low-cost hardware techniques which would allow to tolerate temporary faults in the datapaths of coarse-grained reconfigurable architectures (CGRAs). Our goal was to use less hardware overhead than commonly used duplication or triplication methods. The proposed technique relies on concurrent error detection by using residue code modulo 3 and re-execution of the last operation, once an error is detected. We have chosen the DART architecture as a vehicle to study the efficiency of this approach to protect its datapaths. Simulation results have confirmed hardware savings of the proposed approach over duplication.",2010,0,
357,358,"HGRID: Fault Tolerant, Log2N Resource Management for Grids","Grid resource discovery service is currently a very important focus of research. We propose a scheme that presents essential characteristics for efficient, self-configuring and fault-tolerant resource discovery and is able to handle dynamic attributes, such as memory capacity. Our approach consists of an overlay network with a hypercube topology connecting the grid nodes and a scalable, fault-tolerant, self-configuring search algorithm. By design, the algorithm improves the probability of reaching all working nodes in the system even in the presence of failures (inaccessible, crashed or heavy loaded nodes). We analyze the static resilience of the presented approach, that is to say, how well the algorithm is able to find resources without having to update the routing tables. The results show that the presented approach has a high static resilience.",2009,0,
358,359,Adaptive Error-Resilience Transcoding and Fairness Grouping for Video Multicast Over Wireless Networks,"In this paper, we present a two-pass intra-refresh transcoder for on-the-fly enhancing error resilience of a compressed video in a three-tier streaming system. Furthermore, we consider the problem of multicasting a video to multiple clients with diverse channel conditions. We propose a MINMAX loss rate estimation scheme to determine a single intra- refresh rate for all the clients in a multicast group. For the scenario that a quality variation constraint is imposed on the users, we also propose a grouping method to partition a multicast group of heterogeneous users into a minimal number of sub-groups to minimize the channel bandwidth consumption while meeting the quality variation constraint and achieving fairness among all sub-groups. Experimental results show that the proposed method can effectively mitigate the error propagation due to packet loss as well as achieve fairness not only among all sub-groups and also clients in a multicast group.",2007,0,
359,360,Error Control for IPTV over xDSL Networks,"We discuss the necessity of error control for supporting IPTV over imperfect access networks. In particular, we consider typical DSL environments, and examine the physical-layer impairments and error-mitigation techniques. For these networks, we evaluate the performance of two different application-layer Forward Error Correction (FEC) methods. An overview of hybrid error-control methods and recent developments in standardization is also presented.",2008,0,
360,361,A new approach of halftoning based on error diffusion with rough set filtering,"The rough set filtering makes use of the concepts of indiscernibility relations and approximation spaces to define an equivalence class of neighboring pixels in a processing mask, then utilize the statistical mean of the equivalence classes to replace the gray levels of the central pixel in a processing mask. The error diffusion makes use of the correction factor composed of the weighted errors for these pixels prior to addition of the pixel to be processed to diffuse error over the neighboring pixels in a continuous tone image. Both of a system and an algorithm of implementation of halftoning on error diffusion with rough sets are introduced in the paper",2000,0,
361,362,Outage performance of mrt with unequal-power co-channel interference and channel estimation error,In this letter we investigate the outage performance of maximal ratio transmission (MRT) with unequal-power co-channel interference (CCI) and channel estimation error. The exact expression for the outage probability is presented. Our results are applicable to the MRT systems with arbitrary numbers of transmit and receive antennas.,2007,0,
362,363,Optimal placement of sensor in gearbox fault diagnosis based on VPSO,"The optimization layout of the acceleration sensor and application of particle swarm optimization (PSO) algorithm to solve the fitness problems of such optimization are discussed in this paper. Based on the gearbox finite element modeling and the result of modal analysis, use the particle swarm optimization with adaptive velocity (VPSO) algorithm, and take the two kinds of fitness function as evaluation goal, has realized the optimization and positioning of gearbox sensor layout, analyzed optimization result.",2010,0,
363,364,Study of SINS/GPS/DVL integrated navigation system's fault tolerance,This paper put forward several fault tolerant arithmetic combining engineering applications on the AUV (autonomous underwater vehicle). The arithmetic is based on traditional centralized Kalman Filter and can improve SINS/GPS/DVL integrated navigation system's precision and capability of fault tolerant. The simulation and the vehicle tests validate the arithmetic.,2005,0,
364,365,New method for current and voltage measuring offset correction in an induction motor sensorless drive,This paper presents a new algorithm for electromagnetic torque and flux estimation in a sensorless drive when uncompensated dc offset of current and/or voltage sensors are present. The novel feature of the offset error correction algorithm is an attempt not to eliminate the consequence of problem but to identify its source. The algorithm uses the first harmonic of estimated torque and dc value of estimated stator flux to identify the source and value of the current and/or voltage offset error. Identified values can be used for offset cancelation which improves estimation process.,2010,0,
365,366,The Digital Circuit Fault Diagnosis Interface Design and Realization Based on VXI,"This paper discusses in detail the development process of general interface adapter in the digital circuit fault diagnosis system based on VXI. After introducing VXI bus, the paper gives overall description of fault diagnosis system, presents a method of solving the problem about load matching and interface matching, and realizes the function of identification to read and write on the memory of interface circuit and to control chip selection. The method of identity installation in interface circuit to be selected is to give an ID number and add a memory to interface circuit to ensure the accuracy and effectiveness. The paper also describes the method of self diagnosis in the interface circuit that is the key to whole fault diagnosis system.",2008,0,
366,367,The DVB television signal transmission simulation using the forward error correction codes,The contribution deals with the simulation of the digital video signal transmission through the baseband transmission channel model. The simulation model that covers selected phenomena of DVB (digital video broadcasting) system signal processing is presented. The digital video signal is represented with the digital data of one noncompressed video frame that is channel encoded and protected against errors with the forward error correction (FEC) codes. The transmission channel model has influence on transmitted digital data and its distortion and the pertubative signals affect on the data decoding. The developed interactive simulation software (Matlab application) features are outlined too and the conclusion presents efficiency of the used FEC codes.,2003,0,
367,368,Residual Generators for Fault Diagnosis Using Computation Sequences With Mixed Causality Applied to Automotive Systems,"An essential step in the design of a model-based diagnosis system is to find a set of residual generators fulfilling stated fault detection and isolation requirements. To be able to find a good set, it is desirable that the method used for residual generation gives as many candidate residual generators as possible, given a model. This paper presents a novel residual generation method that enables simultaneous use of integral and derivative causality, i.e., mixed causality, and also handles equation sets corresponding to algebraic and differential loops in a systematic manner. The method relies on a formal framework for computing unknown variables according to a computation sequence. In this framework, mixed causality is utilized, and the analytical properties of the equations in the model, as well as the available tools for algebraic equation solving, are taken into account. The proposed method is applied to two models of automotive systems, a Scania diesel engine, and a hydraulic braking system. Significantly more residual generators are found with the proposed method in comparison with methods using solely integral or derivative causality.",2010,0,
368,369,Online drift correction in wireless sensor networks using spatio-temporal modeling,"Wireless sensor networks are deployed for the purpose of sensing and monitoring an area of interest. Sensors in the sensor network can suffer from both random and systematic bias problems. Even when the sensors are properly calibrated at the time of their deployment, they develop drift in their readings leading to erroneous inferences being made by the network. The drift in this context is defined as a slow, unidirectional, long-term change in the sensor measurements. In this paper we present a novel algorithm for detecting and correcting sensors drifts by utilising the spatio-temporal correlation between neigbouring sensors. Based on the assumption that neighbouring sensors have correlated measurements and that the instantiation of drift in a sensor is uncorrelated with other sensors, each sensor runs a support vector regression algorithm on its neigbourspsila corrected readings to obtain a predicted value for its measurements. It then uses this predicted data to self-assess its measurement and detect and correct its drift using a Kalman filter. The algorithm is run recursively and is totally decentralized. We demonstrate using real data obtained from the Intel Berkeley Laboratory that our algorithm successfully suppresses drifts developed in sensors and thereby prolongs the effective lifetime of the network.",2008,0,
369,370,Applying fault-tolerant solutions of circulant graphs to meshes and hypercubes,"Many important architectures such as rings, meshes and hypercubes can be modeled as circulant graphs. As a result, circulant graphs have received a lot of attention, and a new method was developed for designing fault-tolerant solutions for them. We review this method in this paper, and examine its applications to the design of fault-tolerant solutions for meshes and hypercubes. Our results indicate that these solutions are efficient.",2005,0,
370,371,"Vietnamese spelling detection and correction using Bi-gram, Minimum Edit Distance, SoundEx algorithms with some additional heuristics","The spelling checking problem is considered to contain two main phases: the detecting phase and the correcting phase. In this paper, we present a new approach for Vietnamese spelling checking based on Vietnamese characteristics for each phase. Our research approach includes the use of a syllable Bi-gram in combination with parts of speech (POS) to find out suspected syllables. In the correcting phase, we based on minimum edit distance, SoundEx algorithms and some heuristics to build a weight function for assessing suggestion candidates. The training corpus and the test set were collected from e-newspapers.",2008,0,
371,372,A fault-tolerant control architecture for induction motor drives in automotive applications,"This paper describes a fault-tolerant control system for a high-performance induction motor drive that propels an electrical vehicle (EV) or hybrid electric vehicle (HEV). In the proposed control scheme, the developed system takes into account the controller transition smoothness in the event of sensor failure. Moreover, due to the EV or HEV requirements for sensorless operations, a practical sensorless control scheme is developed and used within the proposed fault-tolerant control system. This requires the presence of an adaptive flux observer. The speed estimator is based on the approximation of the magnetic characteristic slope of the induction motor to the mutual inductance value. Simulation results, in terms of speed and torque responses, show the effectiveness of the proposed approach.",2004,0,
372,373,Fault classification and fault distance location of double circuit transmission lines for phase to phase faults using only one terminal data,"An accurate fault classification algorithm for double end fed parallel transmission lines based on application of artificial neural networks is presented in this paper. The proposed method uses the voltage and current available at only the local end of line. This method is virtually independent of the effects of remote end infeed and is insensitive to the variation of fault inception angle and fault location. The Simulation results show that phase-to-phase faults can be correctly detected, classified and located within one cycle after the inception of fault. Large number of faults simulations using MATLAB<sup>A</sup>7.01 have demonstrated the accuracy and effectiveness of the proposed algorithm. The proposed scheme allows the protection engineers to increase the reach setting i.e. greater portion of line length can be protected as compared to conventional techniques. The technique neither requires a communication link to retrieve the remote end data nor zero sequence current compensation for healthy phases.",2009,0,
373,374,Denoising fluorescence endoscopy - A motion compensated temporal recursive video filter with an optimal minimum mean square error parameterization,"Fluorescence endoscopy is an emerging technique for the detection of bladder cancer. A marker substance is brought into the patient's bladder which accumulates at cancer tissue. If a suitable narrow band light source is used for illumination, a red fluorescence of the marker substance is observable. Because of the low fluorescence photon count and because of the narrow band light source, only a small amount of light is detected by the camera's CCD sensor. This, in turn, leads to strong noise in the recorded video sequence. To overcome this problem, we apply a temporal recursive filter to the video sequence. The derivation of a filter function is presented, which leads to an optimal filter in the minimum mean square error sense. The algorithm is implemented as plug-in for the real-time capable clinical demonstrator platform RealTimeFrame and it is capable to process color videos with a resolution of 768times576 pixels at 50 frames per second.",2009,0,
374,375,Compact multilayer coupled stripline LTCC filter with defected ground structure,"A novel multilayer coupled stripline resonator structure is introduced to realize miniature broadband band-pass filter using low temperature co-fired ceramic (LTCC) process with defected ground structure (DGS). Wide bandwidth and good selectivity are obtained by exploiting four resonators and the filter exhibits a high rejection in stopband by adopting the tapered DGS. Moreover, an inductance feed back between the output and input is introduced to produce transmission zeros. Filter with size of <sub>0</sub> /12  <sub>0</sub> /12  h (<sub>0</sub> is the wavelength at the midband frequency; h is the substrate height) is designed, fabricated and measured. The measured responses agree well with simulation results.",2009,0,
375,376,Dynamic Fault Handling Mechanisms for Service-Oriented Applications,"Dynamic fault handling is a new approach for dealing with fault management in service-oriented applications. Fault handlers, termination handlers and compensation handlers are installed at execution time instead of being statically defined. In this paper we present this programming style and our implementation of dynamic fault handling in JOLIE, providing finally a nontrivial example of its usage.",2008,0,
376,377,Towards Software Quality Economics for Defect-Detection Techniques,"There are various ways to evaluate defect-detection techniques. However, for a comprehensive evaluation the only possibility is to reduce all influencing factors to costs. There are already some models and metrics for the cost of quality that can be used in that context. The existing metrics for the effectiveness and efficiency of defect-detection techniques and experiences with them are combined with cost metrics to allow a more fine-grained estimation of costs and a comprehensive evaluation of defect-detection techniques. The current model is most suitable for directly comparing concrete applications of different techniques",2005,0,
377,378,Fault tolerance based on the publish-subscribe paradigm for the BonjourGrid middleware,"How to federate the machines of all Boinc, Condor and XtremWeb projects? If you believe in volunteer computing and want to share more than one project then BonjourGrid may help. In previous works, we proposed a novel approach, called BonjourGrid, to orchestrate multiple instances of Institutional Desktop Grid middleware. It is our way to remove the risk of bottleneck and failure, and to guarantee the continuity of services in a distributed manner. Indeed, BonjourGrid can create a specific environment for each user based on a given computing system of his choice such as XtremWeb, Condor or Boinc. This work investigates, first, the procedure to deploy Boinc and Condor on top of BonjourGrid and, second, proposes a fault tolerant approach based on passive replication and virtualization to tolerate the crash of coordinators. The novelty resides here in an integrated environment based on Bonjour (publication-subscription mecanism) for both the coordination protocol and for the fault tolerance issues. In particular, it is not so frequent to our knowledge to describe and to implement a fault tolerant protocol according to the pub-sub paradigm. Experiments, conducted on the Grid'5000 testbed, illustrate a comparative study between Boinc (respectively Condor) on top of BonjourGrid and a centralized system using Boinc (respectively Condor) and second prove the robustness of the fault tolerant mechanism.",2010,0,
378,379,An approach to detecting domain errors using formal specification-based testing,"Domain testing, a technique for testing software or portions of software dominated by numerical processing, is intended to detect domain errors that usually arise from incorrect implementations of desired domains. This paper describes our recent work aiming to provide support for revealing domain errors using formal specifications. In our approach, formal specifications serve as a means for domain modeling. We describe a strong domain testing strategy that guide testers to select a set of test points so that the potential domain errors can be effectively detected, and apply our approach in two case studies for test cases generation.",2004,0,
379,380,A Robust Error Detection Mechanism for H.264/AVC Coded Video Sequences Based on Support Vector Machines,"Current trends in wireless communications provide fast and location-independent access to multimedia services. Due to its high compression efficiency, H.264/AVC is expected to become the dominant underlying technology in the delivery of future wireless video applications. The error resilient mechanisms adopted by this standard alleviate the problem of spatio-temporal propagation of visual artifacts caused by transmission errors by dropping and concealing all macroblocks (MBs) contained within corrupted segments, including uncorrupted MBs. Concealing these uncorrupted MBs generally causes a reduction in quality of the reconstructed video sequence.",2008,0,
380,381,Track Down HW Function Faults Using Real SW Invariants,System level functional verification by running real software stack on FPGA prototype is essential for achieving a high quality design. But it is hard to find the exact source of hardware function faults while running large closed source system software fails. This paper proposes the idea of tracking down faults through real system software control flow invariants with current trace output hardware support. It captures qualified control flow invariant trace in reference execution and test trace; and tracks down faults through comparing offline invariant trace and test trace. The approach can deal with both deterministic and nondeterministic execution. We implemented the proof of concept in full system simulator Bochs. Our experimentation with the real closed source MS Windows XP suggests that the approach is effective in tracking down hardware function faults.,2009,0,
381,382,Design of a fault-tolerant satellite cluster link establishment protocol,"The design of a protocol for satellite cluster link establishment and management that accounts for link corruption, node failures, and node re-establishment is presented in this paper. This protocol will need to manage the traffic flow between nodes in the satellite cluster, adjust routing tables due to node motion, allow for sub-networks in the cluster, and similar activities. This protocol development is in its initial stages. Preliminary results with eight nodes demonstrate its operations and potential problems that may arise when significant numbers of channel errors are present",2005,0,
382,383,A novel approach to calculate the severity and priority of bugs in software projects,"Discovering and fixing software bugs is a difficult maintenance task, and a considerable amount of effort is devoted by software developers on this issue. In the world of software one cannot get rid of the bugs, fixes, patches etc. each of them have a severity and priority associated to it. There is not yet any formal relation between these components as both of these either depends on the developer and tester or on customer and project manger to be decided on. On one hand, the priority of a component depends on the cost and the efforts associated with it. While on the other, the severity depends on the efforts required to accomplish a particular task. This research paper proposes a formula that can draw a relationship among severity and priority.",2010,0,
383,384,Reducing the soft-error rate of a high-performance microprocessor,"Single-bit upsets from transient faults have emerged as a key challenge in microprocessor design. Soft errors will be an increasing burden for microprocessor designers as the number of on-chip transistors continues to grow exponentially. Unlike traditional approaches, which focus on detecting and recovering from faults, this article introduces techniques to reduce the probability that a fault will cause a declared error. The first approach reduces the time instructions sit in vulnerable storage structures. The second avoids declaring errors on benign faults. Applying these techniques to a microprocessor instruction queue significantly reduces its error rate with only minor performance degradation",2004,0,
384,385,The Orion GN&C data-driven flight software architecture for automated sequencing and fault recovery,"The Orion Crew Exploration Vehicle (CEV) is being designed to include capabilities that allow significantly more automation than either the Space Shuttle or the International Space Station (ISS). In particular, the vehicle flight software has requirements to accommodate increasingly automated missions throughout all phases of flight. This paper presents the Guidance, Navigation & Control (GN&C) flight software architecture designed to provide evolvable automation capability that sequences through software modes and configurations. This software architecture is required to maintain flexibility to address the maturation of operational concepts over time, permit ground and crew operators to gain trust in the system, and provide capabilities for human override of the automation in `off-nominal' situations. To allow for mission flexibility, reconfigurability and reduce the recertification expense over the life of the program, a data-driven approach is used to load the mission event plan as well as the flight software artifacts associated with the GN&C subsystem. The flight software schema for automated mission sequencing is presented with a concept of operations for interactions with ground and crew members. This data is managed through a prototype database of GN&C level sequencing data, which tracks mission specific parameters to aid in the scheduling of GN&C activities. A prototype architecture for fault detection, isolation and recovery interactions with the automation software is presented as part of the upcoming design maturation to respond with appropriate GN&C and vehicle-level actions in `off-nominal' scenarios.",2010,0,
385,386,An experimental study of security vulnerabilities caused by errors,"The paper presents an experimental study which shows that, for the Intel x86 architecture, single-bit control flow errors in the authentication sections of targeted applications can result in significant security vulnerabilities. The experiment targets two well-known Internet server applications: FTP and SSH (secure shell), injecting single-bit control flow errors into user authentication sections of the applications. The injected sections constitute approximately 2-8% of the text segment of the target applications. The results show that out of all activated errors: (a) 1-2% comprised system security (create a permanent window of vulnerability); (b) 43-62% resulted in crash failures (about 8.5% of these errors create a transient window of vulnerability); and (c) 7-12% resulted in fail silence violations. A key reason for the measured security vulnerabilities is that, in the x86 architecture, conditional branch instructions are a minimum of one Hamming distance apart. The design and evaluation of a new encoding scheme that reduces or eliminates this problem is presented.",2001,0,
386,387,Defect Tolerance Based on Coding and Series Replication in Transistor-Logic Demultiplexer Circuits,"We present a family of defect tolerant transistor-logic demultiplexer circuits that can defend against both stuck-ON (short defect) and stuck-OFF (open defect) transistors. Short defects are handled by having two or more transistors in series in the circuit, controlled by the same signal. Open defects are handled by having two or more parallel branches in the circuit, controlled by the same signals, or more efficiently, by using a transistor-replication method based on coding theory. These circuits are evaluated, in comparison with an unprotected demultiplexer circuit, by: 1) modeling each circuit's ability to tolerate defects and 2) calculating the cost of the defect tolerance as each circuit's redundancy factor R, which is the relative number of transistors required by the circuit. The defect-tolerance model takes the form of a function giving the failure probability of the entire demultiplexer circuit as a function of the defect probabilities of its component transistors, for both defect types. With the advent of defect tolerance as a new design goal for the circuit designer, this new form of performance analysis has become necessary.",2007,0,
387,388,Adaptive Correction of Errors from Segmented Digital Ink Texts in Chinese Based on Context,"Digital ink texts in Chinese can neither be converted into users' desired layouts nor be recognized until their characters, lines, and paragraphs are correctly extracted. There are many errors in automatically segmented digital ink texts in Chinese because they are free forms and mixed with other languages, as well as their Chinese characters have small gaps and complex structures. Paragraphs, lines, and characters (recognizable language symbols) in digital ink may be wrongly extracted. An adaptive approach based on context is proposed to correct wrongly extracted these objects. Each extracted object is first adaptively visualized by color and shape labels according to relations between it and its neighbors. Users use simple gestures naturally and easily to merge and split wrongly extracted objects. Contexts are constructed from users' gestures and objects invoked by them, where users' intensions are identified. We have conducted experiments using real-life segmented digital ink texts in Chinese and compared the proposed approach with others. Experimental results demonstrate that the proposed approach is feasible, flexible, effective, and robust.",2010,0,
388,389,Memory Yield Improvement through Multiple Test Sequences and Application-Aware Fault Models,"In this paper, we propose a way to improve the yield of memory products by selecting the appropriate test strategy for memory Built- in Self-Test (BIST). We argue that by testing the memory through a sequence of test algorithms which differ in their fault coverage, it is possible to bin the memory into multiple yield bins and increase the yield and product revenue. Further, the test strategy must take into consideration the usage model of the memory. Thus, a number of video and audio buffers are used in sequential access mode, but are overtested using conventional memory test algorithms which model a large number of defects which do not impact the operation of the buffers. We propose a binning strategy where memory test algorithms are applied in different order of strictness such that bins have a specific defect / fault grade. Depending on the applications some of these bins need not be discarded but sold at a lower price as the functionality would never catch the fault due to its usage of memory. We introduce the notion of a test map for the on-chip memories in a SoC and provide results of yield simulation on two specific test strategies called ""Most Strict First"" and ""Least Strict First"". Our simulations indicate that significant improvements in yield are possible through the adoption of the proposed technique. We show that the BIST controller area and run-time overheads also reduce when information about the usage model of the memory, such as sequential access, is exploited.",2008,0,
389,390,Correcting asr outputs: Specific solutions to specific errors in French,"Automatic speech recognition (ASR) systems are used in a large number of applications, in spite of the inevitable recognition errors. In this study we propose a pragmatic approach to automatically repair ASR outputs by taking into account linguistic and acoustic information, using formal rules or stochastic methods. The proposed strategy consists in developing a specific correction solution for each specific kind of errors. In this paper, we apply this strategy on two case studies specific to French language. We show that it is possible, on automatic transcriptions of French broadcast news, to decrease the error rate of a specific error by 11.4% in one of two the case studies, and 86.4% in the other one. These results are encouraging and show the interest of developing more specific solutions to cover a wider set of errors in a future work.",2008,0,
390,391,The Design of Fault Diagnosis Expert System about Temperature Adjustment System Based on CLIPS,"The fault diagnosis of a certain launching unit's temperature controller is researched with the object-oriented programming method based on expert system theory, the fault-diagnosis expert-system software is designed and developed with VC++ 2008 and CLIPS. The structure of the system is firstly analyzed; The representation of knowledge, the design of database and the production rule are discussed then; lastly the diagnosis flow is studied up and a demonstration of the fault diagnosis is given.",2009,0,
391,392,A highly selective super-wide bandpass filter by cascading HMSIW with asymmetric defected ground structure,"The half mode substrate integrated waveguide (HMSIW) possesses the highpass characteristic of SIW but the size is nearly half reduced. A recently proposed asymmetric defected ground structure (ADGS), composed of two square headed slots connected with a rectangular slot transversely under a microstrip line, exhibits quasi-elliptic-function band-reject characteristics around 3 GHz with high selectivity. Based on the circuit model, the structure of the ADGS is modified to perform well at about 16 GHz. By combining the HMSIW and the modified ADGS, a super-wide bandpass filter operating at about 8-16 GHz with high selectivity at both upper and lower band is proposed. Both simulated and measured results have been presented to demonstrate the validity of the proposed wideband filter.",2010,0,
392,393,A real-time fault tolerant intra-body network,"This paper designs an intra-body network (IBN) of nodes, consisting of small sensors and processing elements (SPEs) placed at different locations within the body and a personal digital assistant placed externally but in close proximity to the body. The sensors measure specific physiological attributes such as electrophysiological and biochemical changes in the myocardium (action potentials of cells), glucose level, blood viscosity etc. and forward them to the processing element. Communication protocols for configuration and data access protocols are proposed. The privacy of the IBN data, fault tolerance and real-time data acquisition are addressed.",2002,0,
393,394,Evaluating speech recognition in the context of a spoken dialogue system: critical error rate,"Evaluating a speech recognition system is a key issue towards understanding its deficiencies and focusing potential improvements on useful aspects. When a system is designed for a given application, it is particularly relevant to have an evaluation procedure that reflects the role of the system in this application. Evaluating continuous speech recognition through word error rate is not completely appropriate when the speech recognizer is used as spoken dialogue system input. Some errors are particularly harmful, when they concern content words for example, while some others do not have any impact on the following comprehension step. The attempt is not to evaluate natural language understanding but to propose a more appropriate evaluation of speech recognition, by making use of semantic information to define the notion of critical errors.",2001,0,
394,395,Detecting VLIW Hard Errors Cost-Effectively through a Software-Based Approach,"Research indicates that as technology scales, hard errors such as wear-out errors are increasingly becoming a critical challenge for microprocessor design. While hard errors in memory structures can be efficiently detected by error correction code, detecting hard errors for functional units cost-effectively is a challenging problem. In this paper, we propose to exploit the idle cycles of the under-utilized VLIW functional units to run test instructions for detecting wear-out errors without increasing the hardware cost or significantly impacting performance. We also explore the design space of this software-based approach to balance the error detection latency and the performance for VLIW architectures. Our experimental results indicate that such a software-based approach can effectively detect hard errors with minimum impact on performance for VLIW processors, which is particularly useful for reliable embedded applications with cost constraints.",2007,0,
395,396,Evaluating the effectiveness of a software fault-tolerance technique on RISC- and CISC-based architectures,"This paper deals with a method able to provide a microprocessor-based system with safety capabilities by modifying the source code of the executed application, only. The method exploits a set of transformations which can automatically be applied, thus greatly reducing the cost of designing a safe system, and increasing the confidence in its correctness. Fault Injection experiments have been performed on a sample application using two different systems based on CISC and RISC processors. Results demonstrate that the method effectiveness is rather independent of the adopted platform",2000,0,
396,397,Experimental validation of fault injection analyses by the FLIPPER tool,The paper discusses the experimental validation of fault injection analyses accomplished with the FLIPPER tool. Validation has been accomplished through accelerated proton testing of a benchmark design provided by the European Space Agency.,2009,0,
397,398,A primary exploration of three-dimensional echocardiographic intra-cardiac virtual reality visualization of atrial septal defect: in vitro validation,"To evaluate the diagnostic value of three-dimensional echocardiography (3-DE) in congenital heart disease such as atrial septal defect (ASD) by virtual reality (VR), ten ASDs with different size and shape were created in ten fresh explained porcine hearts. HP SONOS 5500 imaging system was employed for 3-DE reconstructed and visualized by virtual reality computing techniques. The results showed that all ASDs were successfully reconstructed. The site, geometry were well appraised in its true form. The area, maximum and minimum diameter of ASD were measured on 3D reconstruction and compared with independently measured anatomic date. Good correlation was obtained (r>0.95, P<0.01). In conclusion, VR open an exciting opportunity in the field of diagnosis of 3-DE in congenital heart disease",2005,0,
398,399,A Fault-Tolerant Active Pixel Sensor for Mitigating Hot Pixel Defects,"Hot pixel defects are unavoidable in many solid-state image sensors. Affected pixels accumulate dark signal over the course of an exposure, grossly diminishing dynamic range and often rendering measurements unusable. Experiments suggest the mechanisms causing hot pixels are highly localized and the defect will be confined to a single pixel. A redundant, fault-tolerant active pixel sensor architecture that has previously been applied to other defect types is investigated for the suppression of hot pixels. A recovery scheme using minimal computational power is also described.",2007,0,
399,400,Testing for interconnect crosstalk defects using on-chip embedded processor cores,"Crosstalk effects degrade the integrity of signals traveling on long interconnects and must be addressed during production testing. External testing for crosstalk is expensive due to the need for high-speed testers. Built-in self-test, while eliminating the need for a high-speed tester, may lead to excessive test overhead as well as overly aggressive testing. To address this problem, we propose a new software-based self-test methodology for system-on-chip (SoC) devices based on embedded processors. It enables an on-chip embedded processor core to test for crosstalk in system-level interconnects by executing a self-test program in the normal operational mode of the SoC. We have demonstrated the feasibility of this method by applying it to test the interconnects of a processor-memory system. The defect coverage was evaluated using a system-level crosstalk defect simulation method.",2001,0,
400,401,Adaptive Fuzzy Prediction of Low-Cost Inertial-Based Positioning Errors,"Kalman filter (KF) is the most commonly used estimation technique for integrating signals from short-term high performance systems, like inertial navigation systems (INSs), with reference systems exhibiting long-term stability, like the global positioning system (GPS). However, KF only works well under appropriately predefined linear dynamic error models and input data that fit this model. The latter condition is rather difficult to be fulfilled by a low-cost inertial measurement unit (IMU) utilizing microelectromechanical system (MEMS) sensors due to the significance of their long- and short-term errors that are mixed with the motion dynamics. As a result, if the reference GPS signals are absent or the Kalman filter is working for a long time in prediction mode, the corresponding state estimate will quickly drift with time causing a dramatic degradation in the overall accuracy of the integrated system. An auxiliary fuzzy-based model for predicting the KF positioning error states during GPS signal outages is presented in this paper. The initial parameters of this model is developed through an offline fuzzy orthogonal-least-squares (OLS) training while the adaptive neuro-fuzzy inference system (ANFIS) is implemented for online adaptation of these initial parameters. Performance of the proposed model has been experimentally verified using low-cost inertial data collected in a land vehicle navigation test and by simulating a number of GPS signal outages. The test results indicate that the proposed fuzzy-based model can efficiently provide corrections to the standalone IMU predicted navigation states particularly position.",2007,0,
401,402,Fault tolerant PVFS2 based on data replication,"Aggregating the capacity and bandwidth of the commodity disks in the nodes of a cluster provides cost effective and high performance storage systems. Nevertheless, this strategy could be a feasible approach only if the mean time to failure of disks and nodes is faced. The number of failures increases with the nodes and it is especially important in parallel file systems, like PVFS, because having a file striped over server disks increases the probability of failures. This work proposes a strategy to include data replication in the second version of PVFS in order to provide fault tolerance. We also analyze the performance of the implementation of this approach.",2010,0,
