,record_id,title,abstract,year,label_included,duplicate_record_id
0,1,Using Developer Information as a Factor for Fault Prediction,"We have been investigating different prediction models to identify which files of a large multi-release industrial software system are most likely to contain the largest numbers of faults in the next release. To make predictions we considered a number of different file characteristics and change information about the files, and have built fully- automatable models that do not require that the user have any statistical expertise. We now consider the effect of adding developer information as a prediction factor and assess the extent to which this affects the quality of the predictions.",2007,1,
1,2,Reliability analysis of protective relays in fault information processing system in China,"The reliability indices of protective relays are first put forward in this paper. A Markov probability model is then established to evaluate the reliability of relay protection. With the state space analytical method, all the steady state probabilities and state transition probabilities can be calculated utilizing the data stored in the fault information processing system. We can get an equation that represents the influence of routine test intervals on relay unavailability. Based on this, the optimum routine test interval for protective relays can be determined. This paper also proposes an efficient method of processing large amount of information by the fault information processing system and evaluating the reliability of protective relays with it, and the corresponding software package is also developed. The application of it to an actual power system in China proves the method to be correct and effective",2006,0,
2,3,Test effort optimization by prediction and ranking of fault-prone software modules,"Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using ID3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the NASA projects data set of PROMOSE repository.",2010,1,
3,4,A Rough Set Model for Software Defect Prediction,High assurance software requires extensive and expensive assessment. Many software organizations frequently do not allocate enough resources for software quality. We research the defect detectors focusing on the data sets of software defect prediction. A rough set model is presented to deal with the attributes of data sets of software defect prediction in this paper. Appling this model to the most famous public domain data set created by the NASA's metrics data program shows its splendid performance.,2008,1,
4,5,Using Faults-Slip-Through Metric as a Predictor of Fault-Proneness,"Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (Naive Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based techniques (genetic programming and artificial immune recognition systems) on FST data collected from two large industrial projects from the telecommunication domain. Results: Using area under the receiver operating characteristic (ROC) curve and the location of (PF, PD) pairs in the ROC space, the faults slip-through metric showed impressive results with the majority of the techniques for predicting fault-prone modules at both integration and system test levels. There were, however, no statistically significant differences between the performance of different techniques based on AUC, even though certain techniques were more consistent in the classification performance at the two test levels. Conclusions: We can conclude that the faults-slip-through metric is a potentially strong predictor of fault-proneness at integration and system test levels. The faults-slip-through measurements interact in ways that is conveniently accounted for by majority of the data mining techniques.",2010,1,
5,6,Reducing Features to Improve Bug Prediction,"Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.",2009,1,
6,7,Variance Analysis in Software Fault Prediction Models,"Software fault prediction models play an important role in software quality assurance. They identify software subsystems (modules,components, classes, or files) which are likely to contain faults. These subsystems, in turn, receive additional resources for verification and validation activities. Fault prediction models are binary classifiers typically developed using one of the supervised learning techniques from either a subset of the fault data from the current project or from a similar past project. In practice, it is critical that such models provide a reliable prediction performance on the data not used in training. Variance is an important reliability indicator of software fault prediction models. However, variance is often ignored or barely mentioned in many published studies. In this paper, through the analysis of twelve data sets from a public software engineering repository from the perspective of variance, we explore the following five questions regarding fault prediction models: (1) Do different types ofclassification performance measures exhibit different variance? (2) Does the size of the data set imply a more (or less) accurate prediction performance? (3) Does the size of training subset impact model's stability? (4) Do different classifiers consistently exhibit different performance in terms of model's variance? (5) Are there differences between variance from 1000 runs and 10 runs of 10-fold cross validation experiments? Our results indicate that variance is a very important factor in understanding fault prediction models and we recommend the best practice for reporting variance in empirical software engineering studies.",2009,1,
7,8,Evaluating Defect Prediction Models for a Large Evolving Software System,"A plethora of defect prediction models has been proposed and empirically evaluated, often using standard classification performance measures. In this paper, we explore defect prediction models for a large, multi-release software system from the telecommunications domain. A history of roughly 3 years is analyzed to extract process and static code metrics that are used to build several defect prediction models with random forests. The performance of the resulting models is comparable to previously published work. Furthermore, we develop a new evaluation measure based on the comparison to an optimal model.",2009,1,
8,9,Application of neural network for predicting software development faults using object-oriented design metrics,"In this paper, we present the application of neural network for predicting software development faults including object-oriented faults. Object-oriented metrics can be used in quality estimation. In practice, quality estimation means either estimating reliability or maintainability. In the context of object-oriented metrics work, reliability is typically measured as the number of defects. Object-oriented design metrics are used as the independent variables and the number of faults is used as dependent variable in our study. Software metrics used include those concerning inheritance measures, complexity measures, coupling measures and object memory allocation measures. We also test the goodness of fit of neural network model by comparing the prediction result for software faults with multiple regression model. Our study is conducted on three industrial real-time systems that contain a number of natural faults that has been reported for three years (Mei-Huei Tang et al., 1999).",2002,1,
9,10,Using the Conceptual Cohesion of Classes for Fault Prediction in Object-Oriented Systems,"High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.",2008,1,
10,11,Software Fault Prediction using Language Processing,"Accurate prediction of faulty modules reduces the cost of software development and evolution. Two case studies with a language-processing based fault prediction measure are presented. The measure, refereed to as a QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgements of software quality. The two case studies consider the measure's application to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and QALP score. Results, while complex, show that little correlation exists in the first case study, while statistically significant correlations exists in the second. In this second study the QALP score is helpful in predicting faults in modules (files) with its usefulness growing as module size increases.",2007,1,
11,12,Visualization of test information to assist fault localization,"One of the most expensive and time-consuming components of the debugging process is locating the errors or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. The paper presents a technique that uses visualization to assist with these tasks. The technique uses color to visually map the participation of each program statement in the outcome of the execution of the program with a test suite, consisting of both passed and failed test cases. Based on this visual mapping, a user can inspect the statements in the program, identify statements involved in failures, and locate potentially faulty statements. The paper also describes a prototype tool that implements our technique along with a set of empirical studies that use the tool for evaluation of the technique. The empirical studies show that, for the subject we studied, the technique can be effective in helping a user locate faults in a program.",2002,1,
12,13,Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization,"Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization (ADMPSO) based on the PSO classification technique. ADMPSO can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.",2010,1,
13,14,Use of relative code churn measures to predict system defect density,"Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models, we show that while absolute measures of code chum are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.",2005,1,
14,15,Predicting defects in SAP Java code: An experience report,"Which components of a large software system are the most defect-prone? In a study on a large SAP Java system, we evaluated and compared a number of defect predictors, based on code features such as complexity metrics, static error detectors, change frequency, or component imports, thus replicating a number of earlier case studies in an industrial context. We found the overall predictive power to be lower than expected; still, the resulting regression models successfully predicted 50-60% of the 20% most defect-prone components.",2009,1,
15,16,Empirical assessment of machine learning based software defect prediction techniques,"The wide-variety of real-time software systems, including telecontrol/telepresence systems, robotic systems, and mission planning systems, can entail dynamic code synthesis based on runtime mission-specific requirements and operating conditions. This necessitates the need for dynamic dependability assessment to ensure that these systems perform as specified and not fail in catastrophic ways. One approach in achieving this is to dynamically assess the modules in the synthesized code using software defect prediction techniques. Statistical models; such as stepwise multi-linear regression models and multivariate models, and machine learning approaches, such as artificial neural networks, instance-based reasoning, Bayesian-belief networks, decision trees, and rule inductions, have been investigated for predicting software quality. However, there is still no consensus about the best predictor model for software defects. In this paper; we evaluate different predictor models on four different real-time software defect data sets. The results show that a combination of IR and instance-based learning along with the consistency-based subset evaluation technique provides a relatively better consistency in accuracy prediction compared to other models. The results also show that ""size"" and ""complexity"" metrics are not sufficient for accurately predicting real-time software defects.",2005,1,
16,17,Cost Curve Evaluation of Fault Prediction Models,"Prediction of fault prone software components is one of the most researched problems in software engineering. Many statistical techniques have been proposed but there is no consensus on the methodology to select the ""best model"" for the specific project. In this paper, we introduce and discuss the merits of cost curve analysis of fault prediction models. Cost curves allow software quality engineers to introduce project-specific cost of module misclassification into model evaluation. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Through the analysis of sixteen projects from public repositories, we observe that software quality does not necessarily benefit from the prediction of fault prone components. The inclusion of misclassification cost in model evaluation may indicate that even the ""best"" models achieve performance no better than trivial classification. Our results support a recommendation to adopt cost curves as one of the standard methods for software quality model performance evaluation.",2008,1,
17,18,A practical method for the software fault-prediction,"In the paper, a novel machine learning method, SimBoost, is proposed to handle the software fault-prediction problem when highly skewed datasets are used. Although the method, proved by empirical results, can make the datasets much more balanced, the accuracy of the prediction is still not satisfactory. Therefore, a fuzzy-based representation of the software module fault state has been presented instead of the original faulty/non-faulty one. Several experiments were conducted using datasets from NASA Metrics Data Program. The discussion of the results of experiments is provided.",2007,1,
18,19,An investigation of the relationships between lines of code and defects,"It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.",2009,1,
19,20,Predicting the location and number of faults in large software systems,"Advance knowledge of which files in the next release of a large software system are most likely to contain the largest numbers of faults can be a very valuable asset. To accomplish this, a negative binomial regression model has been developed and used to predict the expected number of faults in each file of the next release of a system. The predictions are based on the code of the file in the current release, and fault and modification history of the file from previous releases. The model has been applied to two large industrial systems, one with a history of 17 consecutive quarterly releases over 4 years, and the other with nine releases over 2 years. The predictions were quite accurate: for each release of the two systems, the 20 percent of the files with the highest predicted number of faults contained between 71 percent and 92 percent of the faults that were actually detected, with the overall average being 83 percent. The same model was also used to predict which files of the first system were likely to have the highest fault densities (faults per KLOC). In this case, the 20 percent of the files with the highest predicted fault densities contained an average of 62 percent of the system's detected faults. However, the identified files contained a much smaller percentage of the code mass than the files selected to maximize the numbers of faults. The model was also used to make predictions from a much smaller input set that only contained fault data from integration testing and later. The prediction was again very accurate, identifying files that contained from 71 percent to 93 percent of the faults, with the average being 84 percent. Finally, a highly simplified version of the predictor selected files containing, on average, 73 percent and 74 percent of the faults for the two systems.",2005,1,
20,21,Predictive data mining model for software bug estimation using average weighted similarity,"Software bug estimation is a very essential activity for effective and proper software project planning. All the software bug related data are kept in software bug repositories. Software bug (defect) repositories contains lot of useful information related to the development of a project. Data mining techniques can be applied on these repositories to discover useful interesting patterns. In this paper a prediction data mining technique is proposed to predict the software bug estimation from a software bug repository. A two step prediction model is proposed In the first step bug for which estimation is required, its summary and description is matched against the summary and description of bugs available in bug repositories. A weighted similarity model is suggested to match the summary and description for a pair of software bugs. In the second step the fix duration of all the similar bugs are calculated and stored and its average is calculated, which indicates the predicted estimation of a bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.",2010,1,
21,22,Mutual Fault-tolerant and Standby SCADA System Based on MAS for Multi-area Centralized Control Centers,The general policies to construct the mutual fault- tolerant and standby SCADA system based on multi-agent technology for multi- area centralized control centers were presented in the paper in order to raise the safety and operational reliability of the power grid without additional equipment investment. The economic efficiency and feasibility of the system construction based on the policies are analyzed. The architecture of MAS and the function design of the Agents are introduced in detail and the specific implementation scheme and the corresponding key technologies are elucidated. The data and application fault-tolerance of SCADA system is realized to guarantee the reliability and continuity of the power grid operation.,2006,0,
22,23,Fault Management Driven Design with Safety and Security Requirements,"This paper exemplifies principles of embedded system design that props safety and security using operational errors management in frame of a dedicated Computer-Based System architecture. After reviewing basic principles of Cyber-Physical Systems as a novel slant (or marker?) to modeling and design in this domain, attention is focused on a real-world solution of a safety and security critical embedded system application offering genuine demonstration of that approach. The contribution stresses those features that distinguish the real project from a demonstration case study.",2010,0,
23,24,Reduction of faults in software testing by fault domination,"Although mutation testing is one of the practical ways of enhancing test effectiveness in software testing, it could be sometimes infeasible in practical work for a large scale software so that the mutation testing becomes time-consuming and even in prohibited time. Therefore, the number of faults assumed to exist in the software under test should be reduced so as to be able to confine the time complexity of test within a reasonable period of time. This paper utilizes the concept of fault dominance and equivalence, which has long been employed in hardware testing, for revealing a novel way of reducing the number of faults assumed to hide in software systems. Once the number of faults assumed in software is decreased sharply, the effectiveness of mutation testing will be greatly enhanced and become a feasible way of software testing. Examples and experimental results are presented to illustrate the effectiveness and the helpfulness of the technology proposed in the paper.",2007,0,
24,25,Electrical Test Structures for the Characterisation of Optical Proximity Correction,"Simple electrical test structures have been designed that will allow the characterisation of corner serif forms of optical proximity correction. The structures measure the resistance of a short length of conducting track with a right angled corner. Varying amounts of OPC can be applied to the outer and inner corners of the feature and the effect on the resistance of the track measured. These structures have been simulated and the results are presented in this paper. In addition a preliminary test mask has been fabricated which has test structures suitable for on-mask electrical measurement. Measurement results from these structures are also presented. Furthermore structures have been characterised using an optical microscope, a dedicated optical mask metrology system, an AFM scanner and finally a FIB system. In the future the test mask will be used to print the structures using a step and scan lithography tool so that they can be measured on-wafer. Correlation of the mask and wafer results will provide a great deal of information about the effects of OPC at the CAD level and the impact on the final printed features.",2007,0,
25,26,A real-time fault diagnosis system for UPS based on FFT frequency analysis,"UPS provides emergency power when utility power is not available, so the reliability of UPS is more important than inverter drive systems. In this paper, a fault diagnosis system for UPS is proposed using FFT frequency analysis of output current of inverter side of UPS under linear and nonlinear load conditions. Software PLL for precise synchronization of one period sampling and double buffer memory for real time processing are proposed. Experimental results show the increase of even harmonics including dc offset in case of fault conditions such as increase of resistance and delay or misfiring of IGBT turn-on, and prove the possibility of UPS fault diagnosis system if the criteria for fault decision are well defined.",2010,0,
26,27,5B: emerging technologies - reliable and fault-tolerant wireless sensor networks,"Wireless sensor networks create invisible interconnections with the physical world for the measurement, monitoring, and management of data from multiple sensors and probes with little constraint on location. These networks provide distributed processing, data storage, wireless communication, and dedicated application software with high reliability, inherent redundancy, failure-tolerant security and easily encrypted privacy. They have enormous potential to transform our society and are subjects of intense current research and application development. Three enabling hardware technologies which constitute a network node are microprocessors, MEMS sensors, and low-power radios. Sensor networks represent the paradigm shift in computing where they anticipate our needs and sometimes act on our behalf. The objective of this presentation is to discuss the reliable and fault-tolerant wireless sensor networks, focusing on environmental, behavioral, and biomedical areas. Special focus will be on wearable monitors and body wireless sensor network. An example of physiological monitoring by body area network will be discussed.",2005,0,
27,28,Application method of wavelets in the fault diagnosis of motion system,"In motion system, many types of faults are related with the abnormity of torque signal. A method was presented which was based on the wavelets function. The compactly supported orthonormal wavelets were introduced. Under it, the fault could be detected, and the type of fault could be diagnosed as well. For using convenience, the flow chart of using this method was also offered. On X-Y motion control system, collision experiments were implemented for test the given method. Using the sampled torque signals, the practicability was verified from the clear diagnosis of different types of collisions.",2008,0,
28,29,Fault detection in Flexible Assembly Systems using Petri net,"A significant part of the activities in a manufacturing system involve assembly tasks. Nowadays, these tasks are object of automation due to the market increasing demand for quality, productivity and variety of the products. Consequently, the automation of assembly systems should consider flexibility to face product diversification, functionalities, delivery times, and volumes involved. However, these systems are vulnerable to faults due to the characteristic of their mechanism and the complex interaction among their control devices. In this context, the present work is focused on the modeling design of flexible assembly systems control, including the occurrence of faults. The proposed method structures a sequence of steps for the models construction of assembly processes and their fault detection, based on the theory of discrete events systems and Petri net. This work use in special, production flow schema/mark flow graph (PFS/MFG) technique to describe and model the flexible assembly systems control through a rational and systematic procedure, as well as, the processes data record based on quantitative techniques for fault detection. This approach is applied to a flexible assembly systems installed and in operation to compare the effectiveness of the developed procedure.",2008,0,
29,30,An Evaluation of Similarity Coefficients for Software Fault Localization,"Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important technique for the development of dependable software. In this paper we study different similarity coefficients that are applied in the context of a program spectral approach to software fault localization (single programming mistakes). The coefficients studied are taken from the systems diagnosis/automated debugging tools Pinpoint, Tarantula, and AMPLE, and from the molecular biology domain (the Ochiai coefficient). We evaluate these coefficients on the Siemens Suite of benchmark faults, and assess their effectiveness in terms of the position of the actual fault in the probability ranking of fault candidates produced by the diagnosis technique. Our experiments indicate that the Ochiai coefficient consistently outperforms the coefficients currently used by the tools mentioned. In terms of the amount of code that needs to be inspected, this coefficient improves 5% on average over the next best technique, and up to 30% in specific cases",2006,0,
30,31,Fault-Tolerance in Universal Middleware Bridge,Universal middleware bridge (UMB) provides seamless interoperation among heterogeneous home network middleware. There have been high demands for the UMB components (UMB core and adaptors) to have fault- tolerance capabilities. This paper presents a TMO structuring approach together with new implementation techniques for the fault-tolerant TMO-replica structuring scheme called PSTR. PSTR implementations of UMB components provide fault tolerance capabilities essential in realizing high reliability for the UMB facility.,2008,0,
31,32,Performance evaluation of a fault-tolerant irregular network,"In an attempt to improve the fault-tolerance of the Omega network, this paper examines the performance of the proposed Theta network (THN), and compares it with other networks having similar characteristics. The irregular nature of the network has the inherent advantage of improving the latency of the network. Analytical results exhibit the favorable performance of THN at low cost, making the reliability degrade gracefully with time, while maintaining full-access capability over a reasonably long time. We study methods for routing requests in the presence and absence of faulty components in THN, where 50% of the requests pass at the minimum path length of 2.",2002,0,
32,33,The effect of registration error on tracking distant augmented objects,"We conducted a user study of the effect of registration error on performance of tracking distant objects in augmented reality. Categorizing error by types that are often used as specifications, we hoped to derive some insight into the ability of users to tolerate noise, latency, and orientation error. We used measurements from actual systems to derive the parameter settings. We expected all three errors to influence userspsila ability to perform the task correctly and the precision with which they performed the task. We found that high latency had a negative impact on both performance and response time. While noise consistently interacted with the other variables, and orientation error increased user error, the differences between ldquohighrdquo and ldquolowrdquo amounts were smaller than we expected. Results of userspsila subjective rankings of these three categories of error were surprisingly mixed. Users believed noise was the most detrimental, though statistical analysis of performance refuted this belief. We interpret the results and draw insights for system design.",2008,0,
33,34,Improving Bug Assignment with Bug Tossing Graphs and Bug Similarities,"In open-source software development the bug report is usually assigned to a developer for bug fixing. A large number of bug reports are tossed (reassigned) to other developers, for example because the bugs have been assigned by mistake. The tossing events increase bug-fix time. In order to quickly identify the fixer to bug reports we present an approach based on the bug tossing history and textual similarities between bug reports. This proposed approach is evaluated on Eclipse and Mozilla. The results show that our approach can significantly improve the efficiency of bug assignment: the bug resolver is often identified with fewer tossing events.",2010,0,
34,35,Doppler estimation and correction for shallow underwater acoustic communications,"Reliable mobile underwater acoustic communication systems must compensate for strong, time-varying Doppler effects. Many Doppler correction techniques rely on a single bulk correction to compensate first-order effects. In many cases, residual higher-order effects must be tracked and corrected using other methods. The contributions of this paper are evaluations of (1) signal-to-noise ratio (SNR) performance from three Doppler estimation and correction methods and (2) communication performance of Doppler correction with static vs. adaptive equalizers. The evaluations use our publicly available shallow water experimental dataset, which consists of 360 packet transmission samples (each 0.5s long) from a five-channel receiver array.",2010,0,
35,36,Forward error protection for robust video streaming based on distributed video coding principles,"This paper proposes an error resilient coding scheme that employs distributed video coding tools. A bitstream, produced by any standard motion-compensated predictive codec (MPEG-x, H.26x), is sent over an error-prone channel. In addition, a Wyner-Ziv encoded auxiliary bitstream is sent as redundant information to serve as a forward error correction code. At the decoder side, error concealed reconstructed frames are used as side information by the Wyner-Ziv decoder, and the corrected frame is used as a reference by future frames, thus reducing drift. We explicitly target the problem of rate allocation at the encoder side, by estimating the channel induced distortion in the transform domain. Experimental results conducted over a simulated error-prone channel reveal that the proposed scheme has comparable or better performance than a scheme where forward error correction codes are used. Moreover the proposed solution shows good performance when compared to a scheme that uses the intra-macroblock refresh procedure.",2008,0,
36,37,Analysis on Interruption and Plane Layout of Shear Wall for Frame-Shear Wall Structure with Top Fault Shear Wall,"According to the force-deformation characteristics of frame-shear wall structure, the calculation and analysis model of the ""Style Box"" type layout of frame-shear wall structure is advanced with the basic and simple arrangement of frame-shear wall structure. The different plane and vertical layout with interrupting shear walls of the frame-shear wall structure with top fault shear walls is discussed primarily by using the method of plane layout and vertical interruptable position being considered at the same time. Based on the main parameters of the frame-shear wall structure with top fault shear walls in different ways, it puts forward the importance of the shear wall location of the plane layout to the overall performance of the frame-shear wall structure with top fault shear walls except the lateral stiffness of the shear wall corresponding to the shear wall interrupted ratio.",2009,0,
37,38,Behavioral modular description of fault tolerant distributed systems with AADL Behavioral Annex,"AADL is an architecture description language intended for model-based engineering of high-integrity distributed systems. The AADL Behavior Annex (AADL-BA) is an extension allowing the refinement of behavioral aspects described through an AADL architectural description. When implementing Distributed Real-time Embedded system (DRE), fault tolerance concerns are integrated by applying replication patterns. We considered a simplified design of the primary backup replication pattern as a running example to analyze the modeling capabilities of AADL and its annex. Our contribution lies in the identification of the drawbacks and benefits of this modeling language for accurate description of the synchronization mechanisms integrated in this example.",2010,0,
38,39,Reducing human error in simulation in General Motors,"We focus on the steps taken to minimize human error in simulation modeling in General Motors. While errors are costly and undesirable in any field, they are especially harmful in simulation which has been struggling to gain acceptance in the business world for a long time. The solution discussed can be summarized as ""enter the data once and use the best tool for the job"".",2003,0,
39,40,Analysis of pressure and Blanchard altitude errors computed using atmospheric data obtained from an F-18 aircraft flight,"Pressure altitude is commonly utilized as an altitude reference for an inertial navigation system (INS) to damp the error growth in the inherently unstable vertical channel. A precise altitude reference for use in the INS vertical channel can be obtained using the Blanchard algorithm, which computes altitude from atmospheric pressure, temperature, aircraft ground velocity, and wind velocity data. This paper computes both the pressure and Blanchard altitudes for an entire test flight of an F-18 aircraft from the atmospheric data measured during the flight. The flight repeats 4 cycles of a climb, level-off, dive, level-off trajectory. The altitude computed from GPS during flight is considered to be the truth altitude. The errors in the pressure and Blanchard altitudes are computed and compared. In addition both altitude errors are analyzed in order to determine the scale factor, bias offset, and time delay utilizing the least square error fit method. The Blanchard altitude is a much more precise altitude reference than pressure altitude during actual flight of an F-18 aircraft.",2002,0,
40,41,Residual error models for the SOLT and SOLR VNA calibration algorithms,"Uncertainty calculation of vector network analyzers (VNAs) using the SOLT or SOLR calibration algorithms is often performed using residual directivity, match and tracking. In the literature the uncertainty equations are often stated without a derivation from a proper model equation. In this paper we derive the model equations for both the SOLT and SOLR calibration, the two cases do not result in the same model equation. The results are also compared to the commonly used expressions for uncertainty in the EA guidelines for VNA evaluation. For one-port measurements our results confirm the expressions in the EA guide but for two-ports there are significant differences. The symbolically derived model equations are verified using numerical simulations.",2007,0,
41,42,A method for dead reckoning parameter correction in pedestrian navigation system,"This paper presents a method for correcting dead reckoning parameters, which are heading and step size, for a pedestrian navigation system. In this method, the compass bias error and the step size error can be estimated during the period that the Global Positioning System (GPS) signal is available. The errors are used for correcting those parameters to improve the accuracy of position determination using only the dead reckoning system when the GPS signal is not available. The results show that the parameters can be estimated with reasonable accuracy. Moreover, the method also helps to increase the positioning accuracy when the GPS signal is available.",2003,0,
42,43,Single-stage power factor correction converter with parallel power processing for wide line and load changes,"A new single-phase single-stage power factor correction converter with a simple auxiliary circuit is proposed. Using parallel power processing, this converter can be operated in wide line and load changes while limiting the link voltage below 400 V. Experimental results show that the measured power factor and efficiency are about 0.98 and 81%, respectively, at rated condition and the auxiliary circuit to reduce the link voltage is effective",2002,0,
43,44,Using variable-length error-correcting codes in MPEG-4 video,Reversible variable length (RVL) codes are used in MPEG-4 video coding to improve its error resilience. Algorithms used to design variable-length error-correcting (VLEC) codes are modified so as to construct efficient RVL codes with a smaller average length than those found in the literature. It is also shown that RVL codes are a special (weak) class of VLEC codes. Consequently more powerful VLEC codes can be used in the MPEG-4 codec and it is shown that performance gains of up to 20 dB in peak signal to noise ratio (PSNR) can be obtained using a soft-decision sequential decoder with relatively simple VLEC codes. This increase in performance is obtained at the expense of an order of magnitude increase in decoding complexity,2005,0,
44,45,Restoration of Directional Overcurrent Relay Coordination in Distributed Generation Systems Utilizing Fault Current Limiter,"A new approach is proposed to solve the directional overcurrent relay coordination problem, which arises from installing distributed generation (DG) in looped power delivery systems (PDS). This approach involves the implementation of a fault current limiter (FCL) to locally limit the DG fault current, and thus restore the original relay coordination. The proposed restoration approach is carried out without altering the original relay settings or disconnecting DGs from PDSs during fault. Therefore, it is applicable to both the current practice of disconnecting DGs from PDSs, and the emergent trend of keeping DGs in PDSs during fault. The process of selecting FCL impedance type (inductive or resistive) and its minimum value is illustrated. Three scenarios are discussed: no DG, the implementation of DG with FCL and without FCL. Various simulations are carried out for both single- and multi-DG existence, and different DG and fault locations. The obtained results are reported and discussed.",2008,0,
45,46,The Design Of Embedded Bus monitoring And Fault Diagnosis System Based On Protocol SAE J1939,"Embedded bus monitoring and fault diagnosis system, which was based on protocol SAE J1939 was designed in this paper. And this system took the 32-bit embedded one as a hardware platform, customized a WinCE6.0 operation system and used EVC as the tool to design the embedded application. The functions of CAN communication, protocol defamations etc were realized. Good human-computer interaction is developed and the system has already been applied on the bus.",2010,0,
46,47,A Fault Analysis and Classifier Framework for Reliability-Aware SRAM-Based FPGA Systems,"This paper presents a new framework for the analysis of SRAM-based FPGA systems with respect to their dependability properties against single, multiple and cumulative upsets errors. The aim is to offer an environment for performing fault classification and error propagation analyses for designed featuring fault detection or tolerance techniques against soft errors, where the focus is not only the overall achieved fault coverage, but an understanding of the fault/error relation inside the internal elements of the system. We propose a fault analyzer/classifier laying on top of a classical fault injection engine, used to monitor the evolution of the system after a fault as occurred, with respect to the applied reliability-oriented design technique. The paper introduces the framework and reports some experimental results of its application to a case study, to highlight the benefits of the proposed solution.",2009,0,
47,48,Stress wave analysis of turbine engine faults,"Stress Wave Analysis (SWAN) provides real-time measurement of friction and mechanical shock in operating machinery. This high frequency acoustic sensing technology filters out background levels of vibration and audible noise, and provides a graphic representation of machine health. By measuring shock and friction events, the SWAN technique is able to detect wear and damage at the earliest stages and is able to track the progression of a defect throughout the failure process. This is possible because as the damage progresses, the energy content of friction and shock events increases. This `stress wave energy' is then measured and tracked against normal machine operating conditions. This paper describes testing that was conducted on several types of aircraft and industrial gas turbine engines to demonstrate SWAN's ability to accurately detect a broad range of discrepant conditions and characterize the severity of damage",2000,0,
48,49,High-level vulnerability over space and time to insidious soft errors,"The integrity of computational results is being increasingly threatened by soft errors, especially for computations that are large-scale or performed under harsh conditions. Existing methods for soft error estimation do not clearly characterize the vulnerability associated with a particular result. 1) We propose a metric which captures the intrinsic vulnerability over space and time (VST) to soft errors that corrupt computational results. The method of VST estimation bridges the gap between the inherently low-level faults and the high-level computational failures that they eventually cause. 2) We define a model of an insidious soft error and try to clear up confusion around the concept of silent data corruption. 3) We present experimental results from three vulnerability studies involving floating-point addition, CORDIC, and FFT computations. The results show that traditional vulnerability metrics can be confounded by seemingly reliable but inefficient implementations which actually incur high vulnerability per computation. The VST method characterizes vulnerability accurately, provides a figure-of-merit for comparing alternative implementations of an algorithm, and in some cases uncovers pronounced and unexpected fluctuations in vulnerability.",2008,0,
49,50,Image Defect Recognition Based on Rough Set,"This paper applies rough set theory to recognition system for image defect, and designs a decision algorithm on rough set suitable for image defect recognition. Firstly, the image is made regionalization and sequential discrete set is proposed, the continuous attributes of image is discretized. Then the decision table model on discrete condition attributes and decision attributes is constructed. Further the condition attributes significance function and reduction algorithm is given. A novel approach for decision rule analysis and rough set recognition is proposed. Finally, this paper takes the example for fabric defect recognition to validate these algorithms. The result shows the rough set algorithm is effective for image defect recognition with less calculation and fast speed.",2009,0,
50,51,Using design patterns and constraints to automate the detection and correction of inter-class design defects,"Developing code free of defects is a major concern for the object oriented software community. The authors classify design defects as those within classes (intra-class), those among classes (inter-classes), and those of semantic nature (behavioral). Then, we introduce guidelines to automate the detection and correction of inter-class design defects. We assume that design patterns embody good architectural solutions and that a group of entities with organization similar, but not equal, to a design pattern represents an inter-class design defect. Thus, the transformation of such a group of entities, such that its organization complies exactly with a design pattern, corresponds to the correction of an inter-class design defect. We use a meta-model to describe design patterns and we exploit the descriptions to infer sets of detection and transformation rules. A constraint solver with explanations uses the descriptions and rules to recognize groups of entities with organizations similar to the described design patterns. A transformation engine modifies the source code to comply with the recognized distorted design patterns. We apply these guidelines on the Composite pattern using PTIDEJ, our prototype tool that integrates the complete guidelines",2001,0,
51,52,Error localization for robust video transmission,"The convergence of Internet, multimedia and mobile applications has led to an increased demand for efficient and reliable video data transmission over heterogeneous networks. Due to their coding efficiency, variable-length codes (VLC) are usually employed in the entropy coding stage of video compression standards. However, error propagation is a major problem associated with VLC. We propose the use of a class of self-synchronizing VLC (SSVLC) to achieve the dual goal of optimal coding efficiency and optimal error localization. Performance evaluation has confirmed that the use of SSVLC provides better performance than standard VLC techniques.",2002,0,
52,53,On the error-control coding techniques used in GSM/EDGE radio access networks,In this paper the error-control coding techniques used in GSM/EDGE Radio Access Network (GERAN) are considered. Application of coding schemes is restricted by the corresponding traffic channels (TCH). Knowing the general scheme is necessary for modeling the work of the complete error-control system. This knowledge can be useful for the implementation of educational software used for the investigation of the properties of different codecs and their characteristics in radio channels using various radio channel models.,2004,0,
53,54,A Multi-Agent Fault Detection System for Wind Turbine Defect Recognition and Diagnosis,This paper describes the use of a combination of anomaly detection and data-trending techniques encapsulated in a multi-agent framework for the development of a fault detection system for wind turbines. Its purpose is to provide early error or degradation detection and diagnosis for the internal mechanical components of the turbine with the aim of minimising overall maintenance costs for wind farm owners. The software is to be distributed and run partly on an embedded microprocessor mounted physically on the turbine and on a PC offsite. The software will corroborate events detected from the data sources on both platforms and provide information regarding incipient faults to the user through a convenient and easy to use interface.,2007,0,
54,55,Motion correction of PET images using realignment for intraframe movement,"A method is presented for the motion correction of PET images using realignment for intraframe movement. A newly introduced aspect of the method is that it corrects not only interframe but also intraframe movement, using currently used PET images (which are not corrected for intraframe movement) and motion tracking data. Although our method requires motion tracking data, it does not require very short time image acquisition nor list mode data acquisition. So, it is applicable to currently used PET images. In our method the following hypothesis is assumed. That is if no movement happens, there exists a linear model such that counts of each voxel per unit time follows the model. Model parameters may depend on the voxels. In a simple example, our method successfully corrected motion artifact and estimates the parameters accurately. It may give a simple and practical solution to the motion correction problems.",2003,0,
55,56,Study on Data Mining for Grounding Fault Line Selection in 6kV Ineffectively Grounded System of Coal Mine,"A great amount of fault wave has been recorded by the devices for detecting phase-to-ground faults in ineffectively grounded systems. However, a better method hasn't found for effectively taking advantage of these data to improve the result of fault line selection. Data mining techniques can be used for fault line selection in ineffectively grounded system to gain knowledge from the existing data and to improve the technique of fault line selection. This paper briefly describes the principles, methods and implementation of data mining techniques, classifies the fault samples of ineffectively grounded systems by using clustering analysis method, employs different fault line selection methods according to the types of faults, and consequently provides a set of criteria for modeling of typical ineffectively grounded systems and verifying the validity of real-time fault line selections. The validity of the methods has been convinced by the calculation using the data obtained from the real performance of a substation in coal mine. It has been shown to be promising to employ the data mining techniques in ineffectively grounded systems fault detection. This paper provides very good methods for resolving the difficulties with onsite tests, enhancing the techniques of fault line selection and establishing the fault detection management systems.",2010,0,
56,57,Fault-Tolerant Coverage Planning in Wireless Networks,"Typically wireless networks coverage is planned with static redundancy to compensate temporal variations in the environment. As a result, the service still is delivered but the network coverage could have entered a critical state, meaning that further changes in the environment may lead to service failure. Service failures have to be explicitly notified by the applications. Therefore, in this paper we propose a methodology for fault-tolerant coverage planning. The idea is detecting the critical state and removing it by on-line system reconfiguration, and restoration of the original static redundancy. Even in case of a failure the system automatically generates a new configuration to restore the service, leading to shorter repair times. We describe how this approach can be applied to wireless mesh networks, often used in industrial applications like manufacturing, automation and logistics. The evaluation results show that the underlying model used for error detection and system recovery is accurate enough to correctly identify the system state.",2008,0,
57,58,Detection of high impedance fault in distribution feeder using wavelet transform and artificial neural networks,"This work presents a novel analysis method that can simulate the potential effect of high impedance fault (HIF). The proposed method offers a new scheme for protecting the overhead distribution feeder. The wavelet transform (WT) method was successfully applied in many fields. The characteristics of scaling and translation of WT can be used to identify stable and transient signals. Discrete wavelet transforms (DWT) are initially used to extract distinctive features of the voltage and current signals, and are transformed into a series of detailed and approximated wavelet components. The coefficients of variation of the wavelet components are then calculated. This information is introduced into the training artificial neural networks (ANN) to determine an HIF from the operations of the switches. The simulated results clearly reveal that the proposed method can accurately identify the HIF in the distribution feeder.",2004,0,
58,59,A hierarchical framework for fault propagation analysis in complex systems,"In complex systems, there are few critical failure modes. Prognostic models are focused at predicting the evolution of those critical faults, assuming that other subsystems in the same system are performing according to their design specifications. In practice, however, all the subsystems are undergoing deterioration that might accelerate the time evolution of the critical fault mode. This paper aims at analyzing this aspect, i.e. interaction between different fault modes in various subsystems, of the failure prognostic problem. The application domain focuses on an aero propulsion system of the turbofan type. Creep in the high-pressure turbine blade is one of the most critical failure modes of aircraft engines. The effects of health deterioration of low-pressure compressor and high-pressure compressor on creep damage of high-pressure turbine blades are investigated and modeled.",2009,0,
59,60,Data Mining Using Rough Sets and Orthogonal Signal Correction-Orthogonal Partial Least Squares Analysis,"The paper put forward Data mining using rough sets and orthogonal signal correction-orthogonal partial least squares analysis (RS-OSC-OPLS/O2PLS). first, dimensionality reduction and de-noising with rough sets and orthogonal signal correction;second, Data mining using orthogonal partial least squares analysis. The method was proved to be feasible and effective after tested with 13 kinds of nationalities crowds data.",2010,0,
60,61,Estimation of Systematic Errors of MODIS Thermal Infrared Bands,"This letter reports a statistical method to estimate detector-dependent systematic error in Moderate Resolution Imaging Spectroradiometer (MODIS) thermal infrared (TIR) Bands 20-25 and 27-36. There exist scan-to-scan overlapped pixels in MODIS data. By analyzing a sufficiently large amount of those most overlapped pixels, the systematic error of each detector in the TIR bands can be estimated. The results show that the Aqua MODIS data are generally better than the Terra MODIS data in 160 MODIS TIR detectors. There are no detector-dependent systematic errors in Bands 31 and 32 for both Terra and Aqua MODIS data. The maximum detector errors are 3.00 K in Band 21 of Terra and -8.15 K in that of Aqua for brightness temperatures of more than 250 K",2006,0,
61,62,Comparing Web Services Performance and Recovery in the Presence of Faults,"Web-services are supported by a complex software infrastructure that must ensure high performance and availability to the client applications. Web services industry holds a well established platform for performance benchmarking (e.g., TPC-App and SPEC jAppServer2004 benchmarks). In addition, several studies have been published recently by main vendors focusing web services performance. However, as peak performance evaluation has been the main focus, the characterization of the impact of faults in such systems has been largely disregarded. This paper proposes an approach for the evaluation and comparison of performance and recovery time in web services infrastructures. This approach is based on fault injection and is illustrated through a concrete example of benchmarking three alternative software solutions for web services deployment.",2007,0,
62,63,Evidence-Based Analysis and Inferring Preconditions for Bug Detection,"An important part of software maintenance is fixing software errors and bugs. Static analysis based tools can tremendously help and ease software maintenance. In order to gain user acceptance, a static analysis tool for detecting bugs has to minimize the incidence of false alarms. A common cause of false alarms is the uncertainty over which inputs into a program are considered legal. In this paper we introduce evidence-based analysis to address this problem. Evidence-based analysis allows one to infer legal preconditions over inputs, without having users to explicitly specify those preconditions. We have found that the approach drastically improves the usability of such static analysis tools. In this paper we report our experience with the analysis in an industrial deployment.",2007,0,
63,64,"PSoC design in GM(1,1) error analysis and its application in temperature prediction","In the study of prediction filed, no matter what methods we used, the main purpose is to minimize the prediction error; however, the goals cannot be fulfilled completely. Even we choose GM(1,1) model, which in the newest soft computing method, we also need to minimize the prediction error. Hence, in this paper, we focus on the influence parameter alpha in GM(1,1) model in the first, then, analyze the characteristics of alpha step by step, and use numerical method to find the prediction error corresponding with alpha value. Second, after the mathematics model is presented, we use PSoC to design a GM(1,1) error analysis model, which based on the characteristic of GM(1,1) model. Also an example, which is temperature prediction case is given to assist us to implement our approach in the final section.",2008,0,
64,65,Calculation of transverse voltages of communication lines induced by the fault current of power system,"A double-line model is presented to calculate the transverse voltage of communication lines induced by the fault current of power lines. By dividing the communication line into several fictitious segments, a chain composed by the coupling P1-type circuit with distributed source is formed. The enhanced node voltage analysis (ENVA) is also developed in order to evaluate such a kind of model. The ENVA cuts the number of nodes down greatly because of treating the active and coupling impedance branches as a whole. In addition, the transverse voltages in time domain can be obtained easily from those calculated in frequency domain by means of fast Fourier transform. The numerical examples prove the validity and efficiency of the method by comparison with analytical results. The model is of significance to the design and the rights-of-way selection of power lines and communication lines.",2002,0,
65,66,General review of fault diagnostic in wind turbines,"Global wind electricity-generating capacity increased by 28.7 percent in 2008 to 120,798 Gigawatts. This represents a twelve-fold increase from a decade ago, when world wind-generating capacity stood at less than 5 GW [1]. With wind becoming a key part of the electrical mix in Denmark (20% with 3.1 GW), Spain (8% with 10 GW), and Germany (6% with 18.4 GW), wind turbine reliability is having a bigger effect on overall electrical grid system performance and reliability [1]. This shows the impact of faults and downtime on the reliability of wind turbine especially for offshore wind farms which although are some of the most environmentally friendly and efficient methods to generate electricity in the world. However, the maintenance costs are high because of their remote location. This can amount to as much as 25 to 30% of the total energy production [2]. The aim of this paper is to present an overview of fault detection in wind turbines, study and analyze the faults and their root-causes. The paper also explores different techniques used in early fault detection to form base information for future work to build a general fault diagnostic scheme for wind turbines.",2010,0,
66,67,Reconfigurable context-free grammar based data processing hardware with error recovery,"This paper presents an architecture for context-free grammar (CFG) based data processing hardware for re-configurable devices. Our system leverages on CFGs to tokenize and parse data streams into a sequence of words with corresponding semantics. Such a tokenizing and parsing engine is sufficient for processing grammatically correct input data. However, most pattern recognition applications must consider data sets that do not always conform to the predefined grammar. Therefore, we augment our system to detect and recover from grammatical errors while extracting useful information. Unlike the table look up method used in traditional CFG parsers, we map the structure of the grammar rules directly onto the field programmable gate array (FPGA). Since every part of the grammar is mapped onto independent logic, the resulting design is an efficient parallel data processing engine. To evaluate our design, we implement several XML parsers in an FPGA. Our XML parsers are able to process the full content of the packets up to 3.59 Gbps on Xilinx Virtex 4 devices",2006,0,
67,68,Autonomous Fault Recovery Technology for Achieving Fault-Tolerance in Video on Demand System,"With the advances of compression technology, storage devices and networks, video on demand (VoD) service is becoming popular. The system needs to provide continuous service and heterogeneous service levels for users. However, these requirements cannot be satisfied in conventional VoD system which is constructed on redundant content servers and centralized management. In this paper, autonomous VoD system is proposed to meet the requirements. The system is constructed on faded information field architecture. Under the proposed architecture, autonomous fault detection and fault recovery technologies are proposed to achieve fault-tolerance for continuous service. The effectiveness of the proposed technologies are proved through simulation. The results show that an average of 30% improvement in recovery time and users' video service can be recovered without stopping compared with conventional VoD system",2006,0,
68,69,Event-based fault detection of manufacturing cell: Data inconsistencies between academic assumptions and industry practice,"Some problems with event-based faults in manufacturing systems cannot be handled by existing fault detection solutions, including finding faults in event-based data for systems for which limited information is known. A new fault detection solution that finds faults in event-based data using model generation is presented here. This solution assumes that some information is known about the system from its design information and data structure. An example application of this solution is presented for a Ford machining cell that has been experiencing a gantry waiting problem. In the course of this example application, five inconsistencies were found between relatively common academic assumptions made by this fault detection solution (as well as others) and the actual cell's set-up and data. These inconsistencies and possible means of addressing them are discussed. Some of these means to resolve the inconsistencies have been implemented, and preliminary results in generating models using the fault detection solution are presented.",2010,0,
69,70,Error monitoring for optical metropolitan network services,"Service providers rely on performance monitoring capabilities not only to ensure integrity of their network but also to support service-level agreements with their customers. The depth of monitoring is directly tied to the technology and protocol used in the transport layer of the network. Next-generation services based on enterprise-centric, non-SONET/SDH protocols, such as Gigabit Ethernet and Fibre Channel, as well as managed protocol-independent wavelength transport, have created a number of challenges for service providers because of the differences in how error monitoring is performed. In this article we describe and compare protocol-dependent and protocol-independent error monitoring techniques that apply to these service offerings",2002,0,
70,71,A method of inverter circuit fault diagnosis based on BP neural network and D-S evidence theory,"With the study and analysis on intelligent fault diagnosis for inverting circuit, an improved diagnosis method combined BP neuron network and D-S evidence theory was proposed. Each measuring point was extracted by BP neural network to obtain the local diagnosis, which is adopted to design the belief function of D-S evidence theory. Multiple monitoring points' information is fused to receive the comprehensive global diagnosis result. The experimental results show that this method has the better feasibility and effectiveness on fault diagnosis in inverter's key components-inverting circuit.",2010,0,
71,72,On fault diagnosis tree and its control flow,"The coarse-grained organization of the existing fault diagnosis scheme can not realize the automatic and intelligent diagnosis. This paper provides a tree structure of fault diagnosis scheme named T04FDS, which integrates the fault classification relations and nesting relation of part and whole. By building the state transform system to represent the diagnosis process, and describing the control flow with operations of stack. Furthermore the thesis presents the physical realization of T04FDS fault diagnosis, and finally proves the feasibility of this method by giving an example of fault diagnosis scheme for computer wireless network card.",2009,0,
72,73,Master-Slave TMR Inspired Technique for Fault Tolerance of SRAM-Based FPGA,"In order to increase reliability and availability of Static-RAM based field programmable gate arrays (SRAM-based FPGAs), several methods of tolerating defects and permanent faults have been developed and applied. These methods are not well adapted for handling high fault rates for SRAM based FPGAs. In this paper, both single and double faults affecting configurable logic blocks (CLBs) are addressed. We have developed a new fault-tolerance technique that capitalizes on the partial reconfiguration capabilities of SRAM-based FPGA. The proposed fault-tolerance method is based on triple modular redundancy (TMR) combined with master-slave technique, and exploiting partial reconfiguration to tolerate permanent faults. Simulation results on reliability improvement corroborate the efficiency of the proposed method and prove that it compares favorably to previous methods.",2010,0,
73,74,Robust Speech Recognition Using a Cepstral Minimum-Mean-Square-Error-Motivated Noise Suppressor,"We present an efficient and effective nonlinear feature-domain noise suppression algorithm, motivated by the minimum-mean-square-error (MMSE) optimization criterion, for noise-robust speech recognition. Distinguishing from the log-MMSE spectral amplitude noise suppressor proposed by Ephraim and Malah (E&M), our new algorithm is aimed to minimize the error expressed explicitly for the Mel-frequency cepstra instead of discrete Fourier transform (DFT) spectra, and it operates on the Mel-frequency filter bank's output. As a consequence, the statistics used to estimate the suppression factor become vastly different from those used in the E&M log-MMSE suppressor. Our algorithm is significantly more efficient than the E&M's log-MMSE suppressor since the number of the channels in the Mel-frequency filter bank is much smaller (23 in our case) than the number of bins (256) in DFT. We have conducted extensive speech recognition experiments on the standard Aurora-3 task. The experimental results demonstrate a reduction of the recognition word error rate by 48% over the standard ICSLP02 baseline, 26% over the cepstral mean normalization baseline, and 13% over the popular E&M's log-MMSE noise suppressor. The experiments also show that our new algorithm performs slightly better than the ETSI advanced front end (AFE) on the well-matched and mid-mismatched settings, and has 8% and 10% fewer errors than our earlier SPLICE (stereo-based piecewise linear compensation for environments) system on these settings, respectively.",2008,0,
74,75,On-board fault-tolerant SAR processor for spaceborne imaging radar systems,"A real-time high-performance and fault-tolerant FPGA-based hardware architecture for the processing of synthetic aperture radar (SAR) images has been developed for advanced spaceborne radar imaging systems. In this paper, we present the integrated design approach, from top-level algorithm specifications, system architectures, design methodology, functional verification, performance validation, down to hardware design and implementation.",2005,0,
75,76,Fault-Tolerant BPEL Workflow Execution via Cloud-Aware Recovery Policies,"BPEL is the de facto standard for business process modeling in today's enterprises and is a promising candidate for the integration of business and scientific applications that run in Grid or Cloud environments. In these distributed infrastructures, the occurrence of faults is quite likely. Without sophisticated fault handling, workflows are frequently abandoned due to software or hardware failures, leading to a waste of CPU hours. The fault handling mechanisms provided by BPEL are well suited for handling faults of the business logic, but infrastructure-induced errors should be handled automatically to avoid over-complication of workflow design and keep concerns separated. This paper identifies classes of faults that can be resolved automatically by the infrastructure, and provides a policy-based approach to configure this automatic behavior without the need for adding explicit fault handling mechanisms to the BPEL process. The proposed approach provides automatic redundancy of services using a Cloud infrastructure to allow substitution of defective services. An implementation based on the ActiveBPEL engine and Amazon's Elastic Compute Cloud is presented.",2009,0,
76,77,Neural fault isolator for Wireless Sensor Networks,"Wireless sensor networks are emerging as an innovative technology that can help to improve business processes. In such environments malfunctions and break-down states must be efficiently diagnosed to reduce to a minimum the economic losses. In this paper we present a fault isolation approach based on neural networks, which utilizes only a minimum set of information such as the sensor value, node ID and timestamp as inputs. We believe that this information set could be provided by any WSN regardless of its specific implementation. This abstraction makes the fault isolator generically applicable in enterprise business systems. The neural fault isolator was evaluated in a trial with 36 nodes and has proved to be highly efficient in the isolation of failed components.",2008,0,
77,78,Test Compaction for Transition Faults under Transparent-Scan,"Transparent-scan was proposed as an approach to test generation and test compaction for scan circuits. Its effectiveness was demonstrated earlier in reducing the test application time for stuck-at faults. We show that similar advantages exist when considering transition faults. We first show that a test sequence under the transparent-scan approach can imitate the application of broadside tests for transition faults. Test compaction can proceed similar to stuck-at faults by omitting test vectors from the test sequence. A new approach for enhancing test compaction is also described, whereby additional broadside tests are embedded in the transparent-scan sequence without increasing its length or reducing its fault coverage",2006,0,
78,79,Constrained free form deformation based algorithm for geometric distortion correction of echo planar diffusion tensor images,"In order to differentiate between normal and abnormal variations in brain diffusion tensor images, it is necessary to develop medical atlases. Atlas creation requires removal of spatial distortions in individual subject diffusion weighted images. In this paper we suggest a new approach using non-linear warping based on optic flow to map both baseline and diffusion weighted echo planar images to the anatomically correct T2 weighted spin echo image. The method is readily implemented and does not require a pre-processing step of rigid alignment. A global histogram matching precedes the base line EP image correction. A Markov random field based classification algorithm was implemented to cluster T2 weighted images into four different tissue type classes. This information was then used to synthesize diffusion based image models used in the warping algorithm to correct the geometric distortions in the diffusion weighted EP images.",2004,0,
79,80,Detecting Single and Multiple Faults Using Intelligent DSP and Agents,"In this paper intelligent agents and DSP techniques are integrated to detect single and multiple faults in electrical circuits. Agents are used to model the AC electrical circuit. A DSP engine is embedded into the agents to analyse the signals, i.e. the energy transfer between the physical components. An AC to DC rectifier circuit is chosen as test-bed for the proposed solutions",2006,0,
80,81,Real-time model based sensor fault tolerant control system on a chip,"In this paper, we proposed a model based sensor fault tolerant control system embedded in a generic PIC microcontroller for use in a temperature control system. The model based fault tolerant control algorithm is embedded in the microcontroller for stand-alone real-time implementation. The algorithm consists of a PID controller element (for nominal control) and a fault compensating element. Results from simulations and real time implementation are shown to demonstrate the ease of real time implementation.",2009,0,
81,82,Efficiency enhancement of microstrip patch antenna with defected ground structure,"Defected ground structures (DGS) have been developed to improve characteristics of many microwave devices. Although the DGS has advantages in the area of the microwave filter design, microstrip antenna design for different applications such as cross polarization reduction and mutual coupling reduction etc., it can also be used for the antenna size reduction. The etching of a defect in the ground plane is a unique technique for the antenna size reduction. The DGS is easy to be an equivalent LC resonator circuit. The value of the inductance and capacitance depends on the area and size of the defect. By varying the various dimensions of the defect, the desired resonance frequency can be achieved. In this paper the effect of dumbbell shaped DGS, to the size reduction of a microstrip patch antenna is investigated. Then a cavity backed structure is used to increase the efficiency of the microstrip patch antenna, in which the electric walls are placed surrounding the patch. The simulation is carried out with IE3D full wave EM simulator.",2008,0,
82,83,Effects of Defects on the In-plane Dynamic Energy Absorption of Metal Honeycombs,"The in-plane dynamic energy absorption of metal honeycombs with defects consisting of missing cells are analyzed using explicit dynamic finite element method. Two types of structural defects (a single defect located in the center of the model and a double defect) are firstly introduced. Then the influence of the defects and the impact velocities on the energy absorption abilities of metal honeycombs is investigated. Researches show that single and isolated defects reduce the absorbed energy of cellular materials. The separation distance between two defects has little effect on the dynamic energy absorption, while the size of the single defect has great influence on it. These results will provide some useful guides for the safety evaluation and the dynamic energy absorption design of metal honeycombs.",2010,0,
83,84,Fault Diagnosis on Board for Analog to Digital Converters,"This paper describes a general purpose high reliable data acquisition system which allows A/D converter testing by histogram and two tone tests for the fault diagnosis on the same board. A reliability analysis has been carried out in order to optimize the project, the components choice and redundancy configuration. The software has been written in Matlab and LabVIEW, with an easy graphical user interface.",2007,0,
84,85,Safing and fault protection for the MESSENGER mission to Mercury,"The MErcury Surface, Space ENvironment, GEochemistry, and Ranging (MESSENGER) mission is a NASA Discovery-class, deep-space mission to orbit the planet Mercury. Its purpose is to map the planet surface using various scientific instruments and explore the interior of the planet using measurements from instruments such as a magnetometer and observation of planetary libration. This paper discusses the architecture and implementation of the methods by which faults in the MESSENGER spacecraft are detected and the effects of those faults mitigated. The responsibility of the redundant Fault Protection Processors (FPPs) is to detect faults and take autonomous corrective actions that will keep the spacecraft healthy and safe.",2002,0,
85,86,A new error concealment algorithm for H.264 video transmission,"In this paper, a new error concealment algorithm for the new coding standard H.264 is presented. The algorithm consists of a block size determination step to determine the size type of the lost block and a motion vector recovery step to find the lost motion vector from multiple reference frames. The main feature of this algorithm are as follows. In the block size determination step, we propose a criterion to determine the size type of the lost block from the current frame. In the motion vector recovery step, the optimal motion vector for the lost block chosen from multiple previous reference frames with the minimum value of the side match distortion. The proposed algorithm not only can determine the most correct mode for the lost block, but also can save much more computation time for motion vector recovery. Experimental results show that the proposed algorithm achieves 0.47 dB improvement over the conventional VM method.",2004,0,
86,87,An EMF activity tree based BPEL defect pattern testing method,"For testing BPEL defects efficiently, a novel BPEL defect pattern testing architecture based on the EMF activity tree technology is proposed. The EMF activity tree that is similar to abstract syntax tree is used to describe the BPEL service process structure. The mapping method from the DOM object tree of a BPEL file to the EMF activity tree and the recursive algorithm to generate an EMF activity tree are represented in detail. A typical EMF activity tree is shown and the visitor design pattern based traversal method is stated. The directions to enhance this technology are illustrated finally.",2010,0,
87,88,Fault tolerant generator systems for wind turbines,"The objective of this paper is to review the possibilities of applying fault tolerance in generator systems for wind turbines based on what has been presented in the literature. In order to make generator systems fault tolerant in a suitable way, it is necessary to gain insight into the probability of different failures, so that suitable measures can be taken. Therefore, a literature survey of reliability of wind turbines, electrical machines and power electronic converters is given. Five different ways of achieving fault tolerance identified in the literature are discussed together with their applicability for wind turbines: (1) converters with redundant semiconductors, (2) fault tolerant converter topologies, (3) fault tolerance by increasing the number of phases, (4) fault tolerance of switched reluctance machines, and (5) design for fault tolerance of PM machines and converters. Because converters fail more often than machines, it makes sense to use of fault tolerant converter topologies. Increasing the number of phases is a useful form of fault tolerance because it can be achieved without increasing the cost significantly.",2009,0,
88,89,Fault recovery in linear systems via intrinsic evolution,"We investigate fault recovery using reconfiguration for analog linear feedback control systems. We assume any faults occur only within the linear system and accessibility to its internal circuitry is impossible. Consequently, the only way to restore service - even degraded service - is by inserting a compensation network into the control loop. System failures are manifested by a change in the original bandwidth. The compensators are evolved intrinsically.",2004,0,
89,90,Distance estimation technique for single line-to-ground faults in a radial distribution system,A simple yet powerful algorithm to estimate the distance to a single line-to-ground fault on a distribution feeder is proposed. The algorithm is implemented in a power monitor instrument and the estimation of distance is made within the instrument itself. The algorithm is designed to work where the only available data to the instrument are a single point measurement taken at the substation and the positive and zero-sequence impedance of the primary feeder. The single point measurement consists of three-phase voltage and current waveforms. Network topology data are not available to the algorithm. The new technique accommodates computational power and data constraints while maintaining adequate accuracy of the measurements,2000,0,
90,91,Optimal Wavelet Design for Multicarrier Modulation with Time Synchronization Error,"Wavelet packet based multi-carrier modulation (WPMCM) is an efficient transmission technique which has the advantage of being a generic scheme whose characteristics can be customized to fulfill a design specification. However, WPMCM is sensitive and vulnerable to time synchronization errors because its symbols overlap. In this paper, we design new wavelets to alleviate WPMCM's vulnerability to timing errors. First, a filter design framework that facilitates the development of new wavelet bases is built. Then the expressions for errors due to time offset in WPMCM transmission are derived and stated as a convex optimization problem. Finally, an optimal filter that best handles these deleterious effects is designed by means of semi definite programming (SDP). Through computer simulations the performance advantages of the newly designed filter over standard wavelet filters are proven.",2009,0,
91,92,Optimum design of a class of fault tolerant isotropic Gough-Stewart platforms,"Optimal geometric design is of key importance to the performance of a manipulator. First, this paper extends the work in Y. Yi, et al., (2004) to generate a class of isotropic Gough-Stewart platforms (GSPs) with an odd number of struts. Then, it develops methods for finding a highly fault tolerant GSP from that class. Two optimization criteria are considered, isotropy and fault tolerance. To meet the mission critical needs imposed by laser weapons applications, nine-strut isotropic GSPs that retain kinematic stability despite the loss of any three struts are found. First, we develop methods for generating a five parameter class of isotropic nine-strut GSPs. Next, new measures of fault tolerance are introduced and used to optimize the free parameter space. The optimized design is much more fault tolerant than the GSP currently baselined for the airborne laser.",2004,0,
92,93,Designing Fault Tolerant Web Services Using BPEL,"The Web services technology provides an approach for developing distributed applications by using simple and well defined interfaces. Due to the flexibility of this architecture, it is possible to compose business processes integrating services from different domains. This paper presents an approach, which uses the specification of services orchestration, in order to create a fault tolerant model combining active and passive replication technique. This model support fault of crash. The characteristics and the results obtained by implementing this model are described along this paper.",2008,0,
93,94,Sub-cycle detection of incipient cable splice faults to prevent cable damage,"This paper presents an innovative method for subcycle detection of incipient cable failures caused by self-clearing faults occurring in cable splices due to insulation breakdown. Because of their short duration, conventional overcurrent protection will not detect these types of faults. The protection scheme described in this paper has been integrated into a universal relay platform. It is fast enough to operate for sub-cycle faults and has the logic to differentiate them from other types of faults. Imminent cable failure can be detected",2000,0,
94,95,"Software-Based Online Detection of Hardware Defects Mechanisms, Architectural Support, and Evaluation","As silicon process technology scales deeper into the nanometer regime, hardware defects are becoming more common. Such defects are bound to hinder the correct operation of future processor systems, unless new online techniques become available to detect and to tolerate them while preserving the integrity of software applications running on the system. This paper proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extension (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible: testing techniques can be modified/upgraded in the field to trade off performance with reliability without requiring any change to the hardware. We evaluated our technique on a commercial chip-multiprocessor based on Sun's Niagara and found that it can provide very high coverage, with 99.22% of all silicon defects detected. Moreover, our results show that the average performance overhead of software-based testing is only 5.5%. Based on a detailed RTL-level implementation of our technique, we find its area overhead to be quite modest, with only a 5.8% increase in total chip area.",2007,0,
95,96,Optimizing Joint Erasure- and Error-Correction Coding for Wireless Packet Transmissions,"To achieve reliable packet transmission over a wireless link without feedback, we propose a layered coding approach that uses error-correction coding within each packet and erasure-correction coding across the packets. This layered approach is also applicable to an end-to-end data transport over a network where a wireless link is the performance bottleneck. We investigate how to optimally combine the strengths of error- and erasure-correction coding to optimize the system performance with a given resource constraint, or to maximize the resource utilization efficiency subject to a prescribed performance. Our results determine the optimum tradeoff in splitting redundancy between error-correction coding and erasure-correction codes, which depends on the fading statistics and the average signal to noise ratio (SNR) of the wireless channel. For severe fading channels, such as Rayleigh fading channels, the tradeoff leans towards more redundancy on erasure-correction coding across packets, and less so on error-correction coding within each packet. For channels with better fading conditions, more redundancy can be spent on error-correction coding. The analysis has been extended to a limiting case with a large number of packets, and a scenario where only discrete rates are available via a finite number of transmission modes.",2008,0,
96,97,Modeling transformers with internal incipient faults,"Incipient fault detection in transformers can provide early warning of electrical failure and could prevent catastrophic losses. To develop transformer incipient fault detection technique, a transformer model to simulate internal incipient faults is required. This paper presents a methodology to model internal incipient winding faults in distribution transformers. These models were implemented by combining deteriorating insulation models with an internal short circuit fault model. The internal short circuit fault model was developed using finite element analysis. The deteriorating insulation model, including an aging model and an arcing model connected in parallel, was developed based on the physical behavior of aging insulation and the arcing phenomena occurring when the insulation was severely damaged. The characteristic of the incipient faults from the simulation were compared with those from some potential experimental incipient fault cases. The comparison showed the experimentally obtained characteristic's of terminal behavior of the faulted transformer were similar to the simulation results from the incipient fault models",2002,0,
97,98,An Efficient Technique for Error-Free Implementation of H.264 Using Algebraic Integer Encoding,"Video coding technology plays a key role in various multimedia applications. H.264 is the newest video coding standard and has achieved a significant improvement in coding efficiency. The 4*4 integer transform, as one of the key techniques in H.264 video compression standard, is very important for the whole performance of H.264 codec. In this paper we propose a novel algorithm for fast and error-free (infinite-precision) implementation of H.264 based on algebraic integer-encoding scheme. The proposed algorithm has regular structure. Simulation results show that this algorithm will result in reduction of computation complexity while enhancing the quality of obtained image simultaneously. Determining the quality of an image is an open problem that is highly dependent on the specific application that this image will be used for. We propose new measuring quantities for image quality.",2010,0,
98,99,Detection of Rotor Faults in Squirrel-Cage Induction Motors using Adjustable Speed Drives,"The need for detection of rotor faults at an earlier stage, so that maintenance can be planned ahead, has pushed the development of monitoring methods with increasing sensitivity and noise immunity. Addressing diagnostic techniques based on current signatures analysis (MCSA), the characteristic components introduced by specific faults in the current spectrum are investigated and a diagnosis procedure correlate the amplitudes of such components to the fault extent. In this paper, the impact of feedback control on asymmetric rotor cage induction machine behavior is analyzed. It is shown that the variables usually employed in diagnosis procedures assuming open-loop operation are no longer effective under closed-loop operation. Simulation results show that signals already present at the drive are suitable to effective diagnostic procedure. The utilization of the current regulator error signals and the influence of the regulators gains on their utilization in rotor failure detection are the aim of the present work. The use of a band-pass filter bank to detect the presence of sidebands is also proposed in the paper",2006,0,
99,100,FPGA Implementation of Wideband IQ Imbalance Correction in OFDM Receivers,"This paper describes the implementation of a digital compensation scheme, called CSAD, for correcting the effects of wideband gain and phase imbalances in dual-branch OFDM receivers. The proposed scheme is implemented on a Xilinx Virtex-4 field programmable gate array (FPGA). The flexible architecture of the implementation makes it readily adaptable for different broadband applications, such as DVB-T/H, WLAN, and WiMAX. The proposed correction scheme is resilient against multipath fading and frequency offset. When applied to DVB-T, it is shown that an 11-bit arithmetic precision is sufficient to achieve the required BER of 2x10<sup>-4</sup> at an SNR of 16.5 dB. Using this bit-precision, the implementation consumes 1686 Virtex-4 slices equivalent to about 42600 gates.",2008,0,
100,101,Error prediction for multi-classification,"This paper describes an error prediction mechanism for multiclassification systems. First, a multiclassification system is constructed by combining a suite of two-class classifiers. While training, each sub-classifier does not utilize all the training data and the remaining data are used for testing purpose. Thus, the classification system can predict its own performance after training. We have tested this mechanism on several well-known benchmark datasets. Experimental results are demonstrated for its effectiveness.",2005,0,
101,102,Automatic detection and correction of purple fringing using the gradient information and desaturation,"This paper proposes a method to automatically detect and correct purple fringing that is one of the color artifacts due to characteristics of charge coupled device sensors in a digital camera. The proposed method consists of two steps. In the first step, we detect purple fringed regions that satisfy specific properties: hue characteristics around highlight regions with large gradient magnitudes. In the second step, color correction of the purple fringed regions is made by desaturating the pixels in the detected regions. The proposed method is able to detect purple fringe artifacts more precisely than Kang's method. It can be used as a post processing in a digital camera.",2008,0,
102,103,SOA-Based Alarm Integration for Multi-Technology Network Fault Management,"In order for the service provider to offer better quality of telecom services to customers, one of the possible ways is to monitor and control all kinds of deployed network resources, which are used to support the operation of these services, and to proactively analyze and recover any trouble reported from the network resources. In this study, two system integration scenarios along with the associated interface specifications for multi-technology resource alarm notification and retrieval services have been described. An NGOSS-based development methodology was followed, and a number of useful commercials tools were introduced to facilitate the evolution and transformation of legacy BSS/OSS, so that these BSS/OSS are able to support loosely-coupled interoperability by using standards-based interfaces based on the service-oriented architecture. The functionalities of multi-technology network fault management were realized by means of JMS and Web Service techniques. The implementation described in this paper shows the feasibility of the proposed development methodology.",2008,0,
103,104,Fault Diagnosis of Bearings in Rotating Machinery Based on Vibration Power Signal Autocorrelation,"Since fault in a great number of bearings commences from a single point defect, research on this category of faults has shared a great deal in predictive diagnosis literature. Single point defects will cause certain characteristic fault frequencies to appear in machine vibration spectrum. In traditional methods, data extracted from frequency spectrum has been used to identify damaged bearing part. Because of impulsive nature of fault strikes, and complex modulations present in vibration signal, a simple spectrum analysis may result in erroneous conclusions. When a shaft rotates at constant speed, strikes due to a single point defect repeat at constant intervals. Each strike shows a high energy distribution around it. This paper considers the time intervals between successive impulses in auto-correlated vibration power signals. The most frequent interval between successive impulses determines the period of defective part. This period is related to fault frequency and therefore shows the defective part. A comparison of results extracted from the traditional and the proposed methods shows the efficiency improvement of the second method in respect of the first one",2006,0,
104,105,A Distributed Fault-Tolerant Algorithm for Event Detection Using Heterogeneous Wireless Sensor Networks,"Distributed event detection using wireless sensor networks has received growing interest in recent years. In such applications, a large number of inexpensive and unreliable sensor nodes are distributed in a geographical region to make firm and accurate local decisions about the presence or absence of specific events based on their sensor readings. However, sensor readings can be unreliable, due to either noise in the sensor readings or hardware failures in the devices, and may cause nodes to make erroneous local decisions. We present a general fault-tolerant event detection scheme that allows nodes to detect erroneous local decisions based on the local decisions reported by their neighbors. This detection scheme does not assume homogeneity of sensor nodes and can handle cases where nodes have different accuracy levels. We prove analytically that the derived fault-tolerant estimator is optimal under the maximum a posteriori (MAP) criterion. An equivalent weighted voting scheme is also derived. Further, we describe two new error models that take into account the neighbor distance and the geographical distributions of the two decision quorums. These models are particularly suitable for detection applications where the event under consideration is highly localized. Our fault-tolerant estimator is simulated using a network of 1024 nodes deployed randomly in a square region and assigned random probability of failures",2006,0,
105,106,Diagnosis of rotor faults in brushless DC (BLDC) motors operating under non-stationary conditions using windowed Fourier ridges,"There are several applications where the motor is operating in continuous non-stationary operating conditions. Actuators in the aerospace and transportation industries are examples of this kind of operation. Diagnostics of faults in such applications is, however, challenging. A novel method using windowed Fourier ridges is proposed in this paper for the detection of rotor faults in BLDC motors operating under continuous non-stationarity. Experimental results are presented to validate the concept and depict the ability of the proposed algorithm to track and identify rotor faults. The proposed algorithm is simple and can be implemented in real-time without much computational burden.",2005,0,
106,107,Supervision and fault management of process-tasks and terminology,"The supervision of technical processes is aimed at showing the present state, indicating undesired or unpermitted states, and taking appropriate actions to avoid damage or accidents. The deviations from normal process behavior result from faults and errors, which can be attributed to many causes. They may result in some shorter or longer time periods with malfunctions or failures if no counteractions are taken. One reason for supervision is to avoid these malfunctions or failures. In the following article the basic tasks of supervision are shortly described.",2007,0,
107,108,An industrial case study of implementing and validating defect classification for process improvement and quality management,"Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements",2005,0,
108,109,Evaluation of soft-bit error sequence generators at the output of the decoding process,"Soft decision decoding algorithms are widely used in modern wireless systems; convolutional and turbo codes are usually adopted as the inner scheme thanks to their capability to correct symbol errors at low SNR values. Such algorithms can achieve high coding gains using soft decoding, and modern digital hardware technology enables efficient and low cost practical implementations. We apply the experience gained in previous work, concerning the simulation of bit error processes (Costamagna, E. et al., Proc. IEEE, vol.90 p.842-59, 2002), to implement soft-bit generative models based on hidden Markov chains and chaotic attractors. Both the input and the output of the demodulation process of a GSM-GPRS and a 3GPP UMTS transceiver are observed, developing our earlier analysis (Costamagna et al., IEEE 58th VTC, 2003), and the quality of the soft-bit sequences generated for the input is evaluated comparing the sequences obtained at the output of the demodulator when simulated or target sequences are supplied at the input. Moreover, the deep significance of some statistical features exhibited by the sequences in order to describe their error burst behavior is briefly discussed.",2004,0,
109,110,Procedure call duplication: minimization of energy consumption with constrained error detection latency,"This paper presents a new software technique for detecting transient hardware errors. The objective is to guarantee data integrity in the presence of transient errors and to minimize energy consumption at the same time. Basically, we duplicate computations and compare their results to detect errors. There are three choices for duplicate computations: (1) duplicating every statement in the program and comparing their results, (2) re-executing procedures with duplicated procedure calls and comparing the results, (3) re-executing the whole program and comparing the final results. Our technique is the combination of (1) and (2): Given a program, our technique analyzes procedure call behavior of the program and determines which procedures should have duplicated statements (choice (1)) and which procedure calls should be duplicated (choice (2)) to minimize energy consumption while controlling error detection latency constraints. Then, our technique transforms the original program into the program that is able to detect errors with reduced energy consumption by re-executing the statements or procedures. In benchmark program simulation, we found that our technique saves over 25% of the required energy on average compared to previous techniques that do not take energy consumption into consideration",2001,0,
110,111,The complexity of adding failsafe fault-tolerance,"In this paper, we focus our attention on the problem of automating the addition of failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. And, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of adding failsafe fault-tolerance in distributed programs is NP-hard. Towards this end, we reduce the 3-SAT problem to the problem of adding failsafe fault-tolerance. We also identify a class of specifications, monotonic specifications and a class of programs, monotonic programs. Given a (positive) monotonic specification and a (negative) monotonic program, we show that failsafe fault-tolerance can be added in polynomial time. We note that the monotonicity restrictions are met for commonly encountered problems such as Byzantine agreement, distributed consensus, and atomic commitment. Finally, we argue that the restrictions on the specifications and programs are necessary to add failsafe fault-tolerance in polynomial time; we prove that if only one of these conditions is satisfied, the addition of failsafe fault-tolerance is still NP-hard.",2002,0,
111,112,Comparisons of error control techniques for wireless video multicasting,"This paper explores three different methods, employed separately and in combination, to improve the quality of video delivery on wireless local area networks. The approaches are: leader-driven multicast (LDM), monitoring MAC layer unicast (re)transmissions by other receivers; application-level forward error correction (FEC) using block erasure codes; negative feedback from selected receivers in the form of extra parity requests (EPR). The performance of these three methods is evaluated using both experiments on a mobile computing testbed and simulation. The results indicate that, while LDM is helpful in improving the raw packet reception rate, the combination of FEC and EPR is most effective in improving the frame delivery rate",2002,0,
112,113,Research of remote fault diagnostic system based on Grid,"So far, methods of fault diagnosis have been numerous. But as the complexity of modern equipment, the variability of fault, as well as trends in global manufacturing, it is difficult for any separate organization of independent to complete all the work of fault diagnosis. This article reviewed the equipment fault diagnostic technology in the course of development, in particular, remote fault diagnostic system based on Internet technology, and then on this basis, this paper proposed a new fault diagnostic method, that is grid-based remote fault diagnosis, which integrates the resources of related organization to work together. The architecture of this diagnostic system is proposed, and also the work flow of this system is described.",2009,0,
113,114,Finding Faults: Manual Testing vs. Random+ Testing vs. User Reports,"The usual way to compare testing strategies, whether theoretically or empirically, is to compare the number of faults they detect. To ascertain definitely that a testing strategy is better than another, this is a rather coarse criterion: shouldn't the nature of faults matter as well as their number? The empirical study reported here confirms this conjecture. An analysis of faults detected in Eiffel libraries through three different techniques-random tests, manual tests, and user incident reports-shows that each is good at uncovering significantly different kinds of faults. None of the techniques subsumes any of the others, but each brings distinct contributions.",2008,0,
114,115,Adaptive Fault Management of Parallel Applications for High-Performance Computing,"As the scale of high-performance computing (HPC) continues to grow, failure resilience of parallel applications becomes crucial. In this paper, we present FT-Pro, an adaptive fault management approach that combines proactive migration with reactive checkpointing. It aims to enable parallel applications to avoid anticipated failures via preventive migration and, in the case of unforeseeable failures, to minimize their impact through selective checkpointing. An adaptation manager is designed to make runtime decisions in response to failure prediction. Extensive experiments, by means of stochastic modeling and case studies with real applications, indicate that FT-Pro outperforms periodic checkpointing, in terms of reducing application completion times and improving resource utilization, by up to 43 percent.",2008,0,
115,116,Fault-accommodating thruster force allocation of an AUV considering thruster redundancy and saturation,"A new approach to the fault-accommodating allocation of thruster forces of an autonomous underwater vehicle (AUV) is investigated in this paper. This paper presents a framework that exploits the excess number of thrusters to accommodate thruster faults during operation. First, a redundancy resolution scheme is presented that considers the presence of an excess number of thrusters along with any thruster faults and determines the reference thruster forces to produce the desired motion. This framework is then extended to incorporate a dynamic state feedback technique to generate reference thruster forces that are within the saturation limit of each thruster. Results from both computer simulations and experiments are provided to demonstrate the viability of the proposed scheme",2002,0,
116,117,Designing Real-Time and Fault-Tolerant Middleware for Automotive Software,"Automotive software development poses a great deal of challenges to automotive manufacturers since an automobile is inherently distributed and subject to fault-tolerance and real-time requirements. Middleware is a software layer that can handle the intrinsic complexities of distributed systems and arises as an indispensable run-time platform for automotive systems. This paper explains the concept of middleware by enumerating its functions and categorizes middleware according to adopted communication models. It also extracts five essential requirements of automotive middleware and proposes a middleware design for automotive systems based on the message-oriented middleware (MOM) structure. The proposed middleware effectively addresses the derived requirements and includes many essential features such as real-time guarantee, fault-tolerance, and a global time base",2006,0,
117,118,Design of Energy-Efficient High-Speed Links via Forward Error Correction,"In this brief, we show that forward error correction (FEC) can reduce power in high-speed serial links. This is achieved by trading off the FEC coding gain with specifications on transmit swing, analog-to-digital converter (ADC) precision, jitter tolerance, receive amplification, and by enabling higher signal constellations. For a 20-in FR4 link carrying 10-Gb/s data, we demonstrate: 1) an 18-mW/Gb/s savings in the ADC; 2) a 1-mW/Gb/s reduction in transmit driver power; 3) up to 6?? improvement in transmit jitter tolerance; and 4) a 25- to 40-mV improvement in comparator offset tolerance with 3?? smaller swing.",2010,0,
118,119,Fault classification for distance protection,This paper presents an overview of power system fault classification methods and challenges. It also contains some ideas about structured testing.,2002,0,
119,120,On-line Fault Diagnosis Model of the Hydropower Units Based on MAS,"The paper introduced a novel on-line fault diagnosis system model of the hydropower units based on multi-agent system. In allusion to the classical MAS-based fault diagnosis model, it proposes a new function of information interactive between the mission-controlled subsystem and the task decomposition subsystem to increase the transmission rate of control signals and designs the status-monitoring subsystem to detect the abnormal signals directly from local to increase the fault diagnostic sensitivity. In the fault-diagnosis subsystem, a multi-agent interactive parallel structure is designed to meet the requirements of the high reliability and good real-time. A Java-based language so called as JAFMAS is used to build a multi-agent cooperation platform. Experimental results show the effectiveness and feasibility of the proposed method.",2009,0,
120,121,Non-differential protection of a generator's stator utilizing fault transients,"This paper presents a novel protection scheme for detecting faults on the stator of a generator unit which is directly connected to a distribution system. In the scheme, a multi-channel fault transient detection unit, using the outputs of the current transformers (CTs) at the output of the generator terminal, is employed to extract the fault-generated transient current signals. The detector unit is tuned to extract two bands of fault generated transient signals with different center frequencies. A spectral comparison technique is applied to firstly compute the spectral energies of the two band signals, and then the fault diagnosis determines whether it is an internal and external fault by comparing the ratio of the two signals with a predefined threshold. The scheme offers advantages of immunity to CT saturation, and is capable of detecting both low level and interturn faults. In addition, the protection scheme is also simple in application, and is cost-effective in that it only requires one set of CTs. Simulation studies show that the proposed technique can give correct responses for various fault conditions",2001,0,
121,122,Towards fault tolerance pervasive computing,"Pervasive computing exists in the user's environment, the technology is sustainable if it is invisible to the user and does not intrude the user's consciousness. This requires that functioning of the multitude of devices in the environment be oblivious to the user. Therefore, the system has to be resilient to various kinds of faults and should be able to function despite faults. In addition, pervasive computing provides a platform for context-aware computing that enables automatic configuration of a pervasive system based on the environment context. The aim of this article is to highlight the various challenges and issues that confront fault tolerance pervasive computing, discuss their implications, prevent some solutions to these problems, and describe how some of these solutions are implemented in our system.",2005,0,
122,123,Globally optimal uneven error-protected packetization of scalable code streams,"In this paper, we present a family of new algorithms for rate-fidelity optimal packetization of scalable source bit streams with uneven error protection. In the most general setting where no assumption is made on the probability function of packet loss or on the rate-fidelity function of the scalable code stream, one of our algorithms can find the globally optimal solution to the problem in O(N<sup>2</sup>L<sup>2</sup>) time, compared to a previously obtained O(N<sup>3</sup>L<sup>2</sup>) complexity, where N is the number of packets and L is the packet payload size. If the rate-fidelity function of the input is convex, the time complexity can be reduced to O(NL<sup>2</sup>) for a class of erasure channels, including channels for which the probability function of losing n packets is monotonically decreasing in n and independent erasure channels with packet erasure rate no larger than N/2(N + 1). Furthermore, our O(NL<sup>2</sup>) algorithm for the convex case can be modified to rind an approximation solution for the general case. All of our algorithms do away with the expediency of fractional bit allocation, a limitation of some existing algorithms.",2004,0,
123,124,A Fault Recovery Approach in Fault-Tolerant Processor,"A fault recovery scheme of a fault-tolerant processor for embedded systems is introduced in this paper. The microarchitecture of the fault-tolerant processor called RSED is modified from superscalar processor architecture. The fault-tolerant mechanism of RSED is implemented mainly using temporal redundancy technique. Fault recovery scheme is an important part of the fault-tolerant mechanism. In order to resolve the problem of possible single point of failures, a novel TMR approach is adopted to generate re-execution instruction address. Compared with similar works, the fault recovery scheme proposed can recover processor execution more reliably.",2009,0,
124,125,Reaching efficient fault-tolerance for cooperative applications,"Cooperative applications are widely used, e.g. as parallel calculations or distributed information processing systems. Whereby such applications meet the users demand and offer a performance improvement, the susceptibility to faults of any used computer node is raised. Often a single fault may cause a complete application failure. On the other hand, the redundancy in distributed systems can be utilized for fast fault detection and recovery. So, we followed an approach that is based an duplication of each application process to detect crashes and faulty functions of single computer nodes. We concentrate on two aspects of efficient fault-tolerance-fast fault detection and recovery without delaying the application progress significantly. The contribution of this work is first a new fault detecting protocol for duplicated processes. Secondly, we enhance a roll forward recovery scheme so that it is applicable to a set of cooperative processes in conformity to the protocol",2000,0,
125,126,Straight-Edge Extraction in Distorted Images Using Gradient Correction,"Many camera lenses, particularly low-cost or wide-angle lenses, can cause significant image distortion. This means that features extracted naively from such images will be incorrect. A traditional approach to dealing with this problem is to digitally rectify the image to correct the distortion, and then to apply computer vision processing to the corrected image. However, this is relatively expensive computationally, and can introduce additional interpolation errors. We propose instead to apply processing directly to the distorted image from the camera, modifying whatever algorithm is used to correct for the distortion during processing, without a separate rectification pass. In this paper we demonstrate the effectiveness of this approach using the particular classic problem of gradient-based extraction of straight edges. We propose a modification of the Burns line extractor that works on a distorted image by correcting the gradients on the fly using the chain rule, and correcting the pixel positions during the line-fitting stage. Experimental results on both real and synthetic images under varying distortion and noise show that our gradient-correction technique can obtain approximately a 50% reduction in computation time for straight-edge extraction, with a modest improvement in accuracy under most conditions.",2009,0,
126,127,Automatic Spelling Correction Rule Extraction and Application for Spoken-Style Korean Text,"Nowadays, spoken-style text is prevailing because lots of information are being written in spoken-style such as Short-Message-Service(SMS) messages. However, the spokenstyle text contains more spelling errors than the traditional written-style text. In this paper, we propose a rule-based spelling correction system which can automatically extract spelling correction rules from the correction corpus and apply extracted rules to spelling errors of input sentences. In order to preserve both high precision and high recall, we devise a candidate-elimination algorithm which determines appropriate context size of spelling correction rules based on rule accuracy. Experimental results showed that the proposed system can extract 42,537 spelling correction rules and apply the rules to correct spelling errors on the test corpus and thus, the rate of precision is increased from 31.08% to 79.04% on the basis of message unit.",2007,0,
127,128,Value-based scheduling of distributed fault-tolerant real-time systems with soft and hard timing constraints,We present an approach for scheduling of fault-tolerant embedded applications composed of soft and hard real-time processes running on distributed embedded systems. The hard processes are critical and must always complete on time. A soft process can complete after its deadline and its completion time is associated with a value function that characterizes its contribution to the quality-of-service of the application. We propose a quasi-static scheduling algorithm to generate a tree of fault-tolerant distributed schedules that maximize the application's quality value and guarantee hard deadlines.,2010,0,
128,129,Trend analysis techniques for incipient fault prediction,"This paper extends the application of the Laplace Test Statistic for trend analysis and prediction of incipient faults for power systems. The extensions proposed in this paper consider the situation where two parameters believed to contribute explicitly to the eventual failure are monitored. The developed extensions applied to actual incipient failure events provide promising results for prediction of the impending failure. It is demonstrated that by incorporating two parameters in the trend analysis, the robustness to outliers is increased and the flexibility is augmented by increasing the degrees of freedom in the generation of the alarm signal.",2009,0,
129,130,Notice of Retraction<BR>Comprehensive Evaluation of Certain Power Vehicle Fault Based on Rough Sets,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>With the advent of intelligence, people put forward higher request to the fault diagnosis of weapon. As the base of certain weapon system, the power vehicle can work well or not is directly relate to the tasks of the battle and training can complete smoothly or not. Firstly, the paper introduces the fundamental conception of Rough Sets and gives the method of ascertaining the evaluation value and the weights. Secondly, based on the operation characteristics of certain power vehicle, the paper analyses and creates an index system of the power vehicle, then RS is used to impersonally distribute weights. Finally, the comprehensive evaluation model based RS is given out, and the fault sequence and evaluating grade are educed. The computing results show this practical model is very significant to the weapon maintenance and offers a reference for the evaluation of civil equipment fault.",2009,0,
130,131,Fisher Discriminance of Fault Predict for Decision-Making Systems,"A new technology of fault prediction was presented based on the neural network and Fisher discriminance in statistics. First, many enough character of running situation of decision-making were extracted from the real-time observation data. Secondly, the FP software systems were designed and the algorithm of FP of decision-making systems was presented. Finally, a simply example indicated that the algorithm is effectively.",2009,0,
131,132,FAIL-MPI: How Fault-Tolerant Is Fault-Tolerant MPI?,"One of the topics of paramount importance in the development of cluster and grid middleware is the impact of faults since their occurrence in grid infrastructures and in large-scale distributed systems is common. MPI (message passing interface) is a popular abstraction for programming distributed and parallel applications. FAIL (FAult Injection Language) is an abstract language for fault occurrence description capable of expressing complex and realistic fault scenarios. In this paper, we investigate the possibility of using FAIL to inject faults in a fault-tolerant MPI implementation. Our middleware, FAIL-MPI, is used to carry quantitative and qualitative faults and stress testing",2006,0,
132,133,An Integrated Silicon Carbide (SiC) Based Single Phase Rectifier with Power Factor Correction,"Silicon carbide (SiC) based power devices exhibit superior properties such as very low switching losses, fast switching behavior, improved reliability and high temperature operation capabilities. These properties contribute toward the ability to increase switching frequency, decrease the size of passive components and switches, and reduce the need for cooling, thus making the devices an excellent candidate for AC/DC power supplies. In this paper a SiC based integrated single phase rectifier with power factor correction (PFC) is presented. The proposed topology has many advantages including fewer semiconductor components; the presence of AC side inductor resulting in reduced EMI interference, and higher performance. This approach takes advantage of the superior properties of SiC devices and the reduced number of devices in the proposed converter to achieve higher efficiency, smaller size and better performance at high temperature. A performance and efficiency evaluation of the rectifier is presented and the results are compared with benchmark Si solutions",2005,0,
133,134,Neutron Soft Errors in Xilinx FPGAs at Lawrence Berkeley National Laboratory,The Lawrence Berkeley National Laboratory cyclotron offers broad-spectrum neutrons for single event effects testing. We discuss results from this beamline for neutron soft upsets in Xilinx Virtex-4 and -5 field-programmable-gate-array (FPGA) devices.,2008,0,
134,135,TIP-OPC: a new topological invariant paradigm for pixel based optical proximity correction,"As the 193 nm lithography is likely to be used for 45 nm and even 32 nm processes, much more stringent requirement will be posed on optical proximity correction (OPC) technologies. Currently, there are two OPC approaches - the model-based OPC (MB-OPC) and the inverse lithography technology (ILT). MB-OPC generates masks which is less complex compared with ILT. But ILT produces much better results than MB-OPC in terms of contour fidelity because ILT is a pixel based method. Observing that MB-OPC preserves the mask shape topologies which leads to a lower mask complexity, we combine the strengths of both methods - the topology invariant property and the pixel based mask representation. To the best of our knowledge, it is the first time that this topological invariant pixel based OPC (TIP-OPC) paradigm is proposed, which fills the critical hole of the OPC landscape and potentially has many new applications. Our technical novelty includes the lithography friendly mask topological invariant operations, the efficient fast Fourier transform based cost function sensitivity computation and the TIP-OPC algorithm. The experimental results show that TIP-OPC can achieve much better post OPC contours compared with MB-OPC while maintaining the mask shape topologies.",2007,0,
135,136,A New Iterative Approach to the Corrective Security-Constrained Optimal Power Flow Problem,"This paper deals with techniques to solve the corrective security-constrained optimal power flow (CSCOPF) problem. To this end, we propose a new iterative approach that comprises four modules: a CSCOPF which considers only a subset of potentially binding contingencies among the postulated contingencies, a (steady-state) security analysis (SSSA), a contingency filtering (CF) technique, and an OPF variant to check post-contingency state feasibility when taking into account post-contingency corrective actions. We compare performances of our approach and its possible variants with classical CSCOPF approaches such as the direct approach and Benders decomposition (BD), on three systems of 60, 118, and 1203 buses.",2008,0,
136,137,Systems Reliability Analysis and Fault Diagnosis Based on Bayesian Networks,"This paper presents the application of Bayesian networks(BN) to the reliability analysis and fault diagnosis of systems. For systems, it is essential to do reliability analysis. Also it is necessary to do fault diagnosis when a system failed, but the better way is to do fault diagnosis before the system has failed. Bayesian networks have many special characteristics, one of them is that they are parallel structures, so the time of solving is much shorter than that of many common methods. Bayesian networks permit not only computing the reliability indices of a system but also presenting the effect of each component or some components on the system reliability to distinguish the unsubstantial part of the system, so we can know which part is the weakest one of the system. Two examples proved the validity and superiority of the method in the application of the reliability analysis and fault diagnosis of system.",2009,0,
137,138,Prediction Error Prioritizing Strategy for Fast Normalized Partial Distortion Motion Estimation Algorithm,"A prediction error prioritizing-based normalized partial distortion search algorithm for fast motion estimation is proposed in this letter. The distortion behavior of each pixel in a macroblock is first analyzed to point out the priority/order of sum of absolute difference calculation. Afterward, the normalized partial distortion search algorithm is applied for half-stop of the distortion calculation. In addition, a dynamic search range decision algorithm is adopted for automatically changing the size of the search range to further increase the motion estimation speed. The computational complexity can be reduced significantly through the proposed algorithm, though leaving a PSNR degradation that could be dismissed.",2010,0,
138,139,Fault-Tolerant Policy for Optical Network Based Distributed Computing System,"The optical network based distributed computing system has been thought as a promising technology to support large-scale data-intensive distributed applications. For such a system with so many heterogeneous resources and middlewares involved, faults seem to be inevitable. However, for those applications that need to be finished before the given deadline, a fault in the system will lead to the failure of the application. Therefore, fault-tolerant policy is necessary to improve the performance of the system when faults could happen. In this paper, we address to the fault-tolerant problem for the optical network based distributed computing system. We first propose an overlay approach which applies the existing fault-tolerant policies for distributed computing and optical network. Then we present a joint fault-tolerant policy which takes into account the fault tolerance for computing resource and network resource in the same time. We compare the performances of different polices by simulation. The simulation results show that the joint fault-tolerant policy achieves much better performances compared to overlay approaches.",2008,0,
139,140,Silicon Wafer Defect Extraction Based on Morphological Filter and Watershed Algorithm,"Defect extraction techniques are studied regarding the silicon wafer surface defect. We design a new filter based on multiple structuring elements and suggest an improved marker-based and region merging watershed. To begin with, the filter which generalized close-opening and open-closing filter based on the morphological filter with multiple structuring elements is introduced to eliminate the noise and simplify the image and morphological gradient image while preserving the details. And then in order to reduce the over-segmentation of the watershed algorithm, this paper suggests an improved marker-based and region merging method, region average gray value and edge strength criterion is used in merging operation and has a good effect on segmentation. Finally, the improved watershed algorithm is applied to the filtered gradient image to get the defect contours. The experiments show that this method can eliminate the noises and extract accurately location and closed region contours, which lays a good foundation for defect feature extraction and selection.",2008,0,
140,141,Diagnostic fault detection & intelligent reconfiguration of fuel delivery systems,"The reliable operation of an engines fuel delivery system is fundamental. A failure in the fuel system that impacts the ability to deliver fuel to the engine will have an immediate effect on system performance and safety. There are very few diagnostic systems that monitor the health of the fuel system and even fewer that can accommodate for detected faults. Current diagnostic techniques call for careful maintenance of fuel system components. These techniques tend to be backward thinking in that they are based on previous experience which is not always a good indicator for future systems. This paper describes a technique developed at the Penn State Applied Research Laboratories Condition Based Maintenance Department for fault detection and reconfiguration for fuel delivery system components. This technique has been applied to a diesel engine test rig. The test rig is fully instrumented with sensors including those for fuel pressure. Even though this technique is being applied on a diesel engine, the approach is fully compatible to any fuel delivery system",2005,0,
141,142,Evaluation of security and fault tolerance in mobile agents,"The reliable execution of a mobile agent is a very important design issue in building a mobile agent system and many fault-tolerant schemes have been proposed so far. Security is a major problem of mobile agent systems, especially when money transactions are concerned . Security for the partners involved is handled by encryption methods based on a public key authentication mechanism and by secret key encryption of the communication. In this paper, we examine qualitatively the security considerations and challenges in application development with the mobile code paradigm. We identify a simple but crucial security requirement for the general acceptance of the mobile code paradigm, and evaluate the current status of mobile code development in meeting this requirement. We find that the mobile agent approach is the most interesting and challenging branch of mobile code in the security context. Therefore, we built a simple agent- based information retrieval application, the Traveling Information Agent system, and discuss the security issues of the system in particulars.",2008,0,
142,143,"Comprehensive Analysis of Performance, Fault-Tolerance and Scalability in Grid Resource Management System","The management of the large scale heterogeneous resources is a critical issue in grid computing. The resource management system (RMS) is an essential component of grids. To ensure the QoS of the upper layer service, it raises high requirement for the performance, fault-tolerance and scalability of RMS. In this paper, we study three typical structures of RMS, including centralized, hierarchical and peer-to-peer structures, and make a comprehensive analysis of performance, fault tolerance and scalability. We put forward the performance, fault tolerance and scalability evaluation metrics of the RMS, and give the mathematical expressions and detailed calculation processes. Besides, we make further discussions on the interactions of the performance, fault-tolerance and scalability, and make a comparison of the RMSs with the three typical structures. We believe that the results of this work will help system architects make informed choices for building the RMS.",2009,0,
143,144,Recent improvements on the specification of transient-fault tolerant VHDL descriptions: a case-study for area overhead analysis,"We present a new approach to design reliable complex circuits with respect to transient faults in memory elements. These circuits are intended to be used in harmful environments like radiation. During the design flow, this methodology is also used to perform an early-estimation of the obtained reliability level. Usually, this reliability estimation step is performed in the laboratory, by means of radiation facilities (particle accelerators). By doing so, the early-estimated reliability level is used to balance the design process into a trade-off between maximum area overhead due to the insertion of redundancy and the minimum reliability required for a given application. This approach is being automated through the development of a CAD tool (FT-PRO). Finally, we present also a case-study of a simple microprocessor used to analyze the FT-PRO performance in terms of the area overhead required to implement the fault-tolerant circuit.",2000,0,
144,145,Error calculation techniques and their application to the Antenna Measurement Facility Comparison within the European Antenna Centre of Excellence,"This paper gives an overview of the ongoing activities under the Antenna Measurement activity of the Antenna Centre of Excellence (ACE) network within the EU 6th framework research program. In particular, in this work an attempt is made to establish a common uncertainty estimation criteria in spherical near field and far field antenna measurement systems. The results from this activity are important instruments to verify the measurements accuracies for antenna measurement ranges as well as to investigate and evaluate possible improvements in measurement set-ups and procedures. These results will be used in the facility comparison campaigns in order to calculate a reference pattern for each of the high accuracy reference antennas (VAST 12, SATIMO SH800 and SATIMO SH2000) measured during the last 4 years by different institutions in Europe and US.",2007,0,
145,146,GNSS pseudorange error density tracking using Dirichlet Process Mixture,"In satellite navigation system, classical localization algorithms assume that the observation noise is white-Gaussian. This assumption is not correct when the signal is reflected on the surrounding obstacles. That leads to a decrease of accuracy and of continuity of service. To enhance the localization performances, a better observation noise density can be use in an adapted filtering process. This article aims to show how the Dirich-let Process Mixture can be employed to track the observation density on-line. This sequential estimation solution is adapted when the noise is non-stationary. The approach will be tested under a simulation scenario with multiple propagation conditions. Then, this density modeling will be used in Rao-Blackwellised Particle Filter.",2010,0,
146,147,On handling dependent evidence and multiple faults in knowledge fusion for engine health management,"Diagnostic architectures that fuse outputs from multiple algorithms are described as knowledge fusion or evidence aggregation. Knowledge fusion using a statistical framework such as Dempster-Shafer (D-S) has been used in the context of engine health management. Fundamental assumptions made by this approach include the notion of independent evidence and single fault. In most real world systems, these assumptions are rarely satisfied. Relaxing the single fault assumption in D-S based knowledge fusion involves working with a hyper-power set of the frame of discernment. Computational complexity limits the practical use of such extension. In this paper, we introduce the notion of mutually exclusive diagnostic subsets. In our approach, elements of the frame of discernment are subsets of faults that cannot be mistaken for each other, rather than failure modes. These subsets are derived using a systematic analysis of connectivity and causal relationship between various components within the system. Specifically, we employ a special form of reachability analysis to derive such subsets. The theory of D-S can be extended to handle dependent evidence for simple and separable belief functions. However, in the real world the conclusions of diagnostic algorithms might not take the form of simple or separable belief functions. In this paper, we present a formal definition of algorithm dependency based on three metrics: the underlying technique an algorithm is using, the sensors it is using, and the feature of the sensor that the algorithm is using. With this formal definition, we partition evidence into highly dependent, weakly dependent and independent evidence. We present examples from a Honeywell auxiliary power unit to illustrate our modified D-S method of evidence aggregation",2006,0,
147,148,Transmission Line Fault Location Using Two-Terminal Data Without Time Synchronization,"This letter presents a new transmission line fault location method that uses current and voltage sinusoidal phasors at both ends, without necessity of data synchronization. The main difference among the classical Johns method resides in the fact that the proposed method is based on magnitude of fault point voltage and does not demand exact phase angles of the acquired signals. Simulated and real case results are presented, showing that the proposed algorithm is robust, accurate, and provides adequate performance. Practical applications confirm that the synchronization is not really necessary, making the method faster and easier to apply than classical methods in many real situations",2007,0,
148,149,Measurement-based frame error model for simulating outdoor Wi-Fi networks,"We present a measurement-based model of the frame error process on a Wi-Fi channel in rural environments. Measures are obtained in controlled conditions, and careful statistical analysis is performed on the data, providing information which the network simulation literature is lacking. Results indicate that most network simulators use a frame loss model that can miss important transmission impairments even at a short distance, particularly when considering antenna radiation pattern anisotropy and multi-rate switching.",2009,0,
149,150,A new design technique for optimum logic filter using matrix type-B error correcting coding,"A brief examination in digital communications gives that the receiver has to decide and distinguish between a number of discrete signals in background noise. For this case an optimum filter is designed and some techniques are developed as f.i. the MAP (Maximum a posteriori probability), the maximum likelihood - ML, the matched filter, the Kalman filter, etc. In this paper we introduce a new design technique, which we called the optimum logic filter (OLF), using sophisticated matrix type-B error correcting coding.",2005,0,
150,151,Incremental fault-tolerant design in an object-oriented setting,"With the increasing emphasis on dependability in complex, distributed systems, it is essential that system development can be done gradually and at different levels of detail. We propose an incremental treatment of faults as a refinement process on object-oriented system specifications. An intolerant system specification is a natural abstraction from which a fault-tolerant system can evolve. With each refinement step a fault and its treatment are introduced, so the fault-tolerance of the system increases during the design process. Different kinds of faults are identified and captured by separate refinement relations according to how the tolerant system relates to abstract properties of the intolerant one in terms of safety, and liveness. The specification language utilized is object-oriented and based upon first-order predicates on communication traces. Fault-tolerance refinement relations are formalized within this framework",2001,0,
151,152,Fault-tolerant control of PMSM drive unit,"Since the Fuel-Cell Vehicle's demonstration in the public transport, its fault diagnosis and fault tolerant control strategy become more and more important. This paper studies on the PMSM drive of FCV and presents a sensorless control algorithm in the fault mode based analytical redundancy. Simulation analysis and experiment verification are presented to compare the control algorithm using Expanded Kalman Filter (EKF) and Phase Locked Loop.",2009,0,
152,153,The Learning with Errors Problem (Invited Survey),"In this survey we describe the Learning with Errors (LWE) problem, discuss its properties, its hardness, and its cryptographic applications.",2010,0,
153,154,Group communication protocols under errors,"Group communication protocols constitute a basic building block for highly dependable distributed applications. Designing and correctly implementing a group communication system (GCS) is a difficult task. While many theoretical algorithms have been formalized and proved for correctness, only few research projects have experimentally assessed the dependability of GCS implementations under complex error scenarios. This paper describes a thorough error-injection experimental campaign conducted on Ensemble, a popular GCS. By employing synthetic benchmark applications, we stress selected components of the GCS $the group membership service, the FIFO-ordered reliable multicast - under various error models, including errors in the memory (text and heap segments) and in the network messages. The data show that about 5-6% of the failures are due to an error escaping Ensemble's error-containment mechanism and manifesting as a fail silence violation. This constitutes an impediment to achieving high dependability, the natural objective of GCSs. Our results are derived for a particular system (Ensemble), and more investigation involving other GCSs is required to generalize the conclusions. Nevertheless, through an accurate analysis of the failure causes and the error propagation patterns, this paper offers insights into the design and the implementation of robust GCSs.",2003,0,
154,155,Extended fault modeling used in the space shuttle PRA,"A probabilistic risk assessment (PRA) has been completed for the space shuttle with NASA sponsorship and involvement. This current space shuttle PRA is an advancement over past PRAs conducted for the space shuttle in the technical approaches utilized and in the direct involvement of the NASA centers and prime contractors. One of the technical advancements is the extended fault modeling techniques used. A significant portion of the data collected by NASA for the space shuttle consists of faults, which are not yet failures but have the potential of becoming failures if not corrected. This fault data consists of leaks, cracks, material anomalies, and debonding faults. Detailed, quantitative fault models were developed for the space shuttle PRA which involved assessing the severity of the fault, detection effectiveness, recurrence control effectiveness, and mission-initiation potential. Each of these attributes was transformed into a quantitative weight to provide a systematic estimate of the probability of the fault becoming a failure in a mission. Using the methodology developed, mission failure probabilities were estimated from collected fault data. The methodology is an application of counter-factual theory and defect modeling which produces consistent estimates of failure rates from fault rates. Software was developed to analyze all the relevant fault data collected for given types of faults in given systems. The software allowed the PRA to be linked to NASA's fault databases. This also allows the PRA to be updated as new fault data is collected. This fault modeling and its implementation with FRAS was an important part of the space shuttle PRA.",2004,0,
155,156,DMTracker: finding bugs in large-scale parallel programs by detecting anomaly in data movements,"While software reliability in large-scale systems becomes increasingly important, debugging in large-scale parallel systems remains a daunting task. This paper proposes an innovative technique to find hard-to-detect software bugs that can cause severe problems such as data corruptions and deadlocks in parallel programs automatically via detecting their abnormal behaviors in data movements. Based on the observation that data movements in parallel programs typically follow certain patterns, our idea is to extract data movement (DM)-based invariants at program runtime and check the violations of these invariants. These violations indicate potential bugs such as data races and memory corruption bugs that manifest themselves in data movements. We have built a tool, called DMTracker, based on the above idea: automatically extract DM-based invariants and detect the violations of them. Our experiments with two real-world bug cases in MVAPICH/MVAPICH2, a popular MPI library, have shown that DMTracker can effectively detect them and report abnormal data movements to help programmers quickly diagnose the root causes of bugs. In addition, DMTracker incurs very low runtime overhead, from 0.9% to 6.0%, in our experiments with High Performance Linpack (HPL) and NAS Parallel Benchmarks (NPB), which indicates that DMTracker can be deployed in production runs.",2007,0,
156,157,Lightweight Fault-Tolerance for Peer-to-Peer Middleware,"We address the problem of providing transparent, lightweight, fault-tolerance mechanisms for generic peer-to-peer middleware systems. The main idea is to use the peer-to-peer overlay to provide for fault-tolerance rather than support it higher up in the middleware architecture, e.g. in the form of services. To evaluate our approach we have implemented a fault-tolerant middleware prototype that uses a hierarchical peer-to-peer overlay in which the leaf peers connect to sensors that provide data streams. Clients connect to the root of the overlay and request streams that are routed upwards through intermediate peers in the overlay up to the client. We report encouraging preliminary results for latency, jitter and resource consumption for both the non-faulty and faulty cases.",2010,0,
157,158,Automated Support for Propagating Bug Fixes,"We present empirical results indicating that when programmers fix bugs, they often fail to propagate the fixes to all of the locations in a code base where they are applicable, thereby leaving instances of the bugs in the code. We propose a practical approach to help programmers to propagate many bug fixes completely. This entails first extracting a programming rule from a bug fix, in the form of a graph minor of an enhanced procedure dependence graph. Our approach assists the programmer in specifying rules by automatically matching simple rule templates; the programmer may also edit rules or compose them from scratch. A graph matching algorithm for detecting rule violations is then used to locate the places in the code base where the bug fix is applicable. Our approach does not require that rules occur repeatedly in the code base. We present empirical results indicating that the approach nevertheless exhibits good precision.",2008,0,
158,159,Bandwidth effect on distance error modeling for indoor geolocation,"In this paper we introduce a model for the distance error measured from the estimated time of arrival (TOA) of the direct path (DP) between the transmitter and the receiver in a typical multipath indoor environment. We use the results of a calibrated Ray tracing software in a sample office environment. First we divide the whole floor plan into LOS and Obstructed LOS (OLOS), and then we model the distance error in each environment considering the variation of bandwidth of the system. We show that the behavior of the distance error in LOS environment can be modeled as Gaussian, while behavior of the OLOS is a mixture of Gaussian and exponential distribution. We also related the statistics of the distributions to the bandwidth of the system.",2003,0,
159,160,Impact of solid-state fault current limiters on protection equipment in transmission and distribution systems,"Solid-state fault current limiters (SSFCLs) offer a number of benefits when incorporated within transmission and distribution systems. SSFCLs can limit the magnitude of a fault current seen by a system using different methods, such as inserting a large impedance in the current path or controlling the voltage applied to the fault. However, these two methods can introduce a few problems when SSFCLs are used in a system along with other protection equipment such as protective relays and sensors. An experiment was designed and implemented to evaluate the behavior of the protective relays in a mimic distribution system with a SSFCL. This paper introduces the details of the experiment and the result shows that the distorted current and voltage waveforms resulting from the action of the SSFCL disturb the protective equipment.",2010,0,
160,161,Considering Fault Correction Lag in Software Reliability Modeling,"The fault correction process is very important in software testing, and it has been considered into some software reliability growth models (SRGMs). In these models, the time-delay functions are often used to describe the dependency of the fault detection and correction processes. In this paper, a more direct variable ""correction lag"", which is defined as the difference between the detected and corrected fault numbers, is addressed to characterize the dependency of the two processes. We investigate the correction lag and find that it appears Bell-shaped. Therefore, we adopt the Gamma function to describe the correction lag. Based on this function, a new SRGM which includes the fault correction process is proposed. And the experimental results show that the new model gives better fit and prediction than other models.",2008,0,
161,162,Effects of Defects on the Thermal and Optical Performance of High-Brightness Light-Emitting Diodes,"Defects in terms of voids, cracks, and delaminations are often generated in light-emitting diodes (LEDs) devices and modules. During various manufacturing processes, accelerated testing, inappropriate handling, and field applications, defects are most frequently induced in the early stage of process development. One loading is due to the nonuniform loads caused by temperature, moisture, and their gradients. In this research, defects in various cases are modeled by a nonlinear finite-element method (FEM) to investigate the existence of interfaces, interfacial open and contacts in terms of thermal contact resistance, stress force nonlinearity, and optical discontinuity, in order to analyze their effects on the LED's thermal and optical performance. The simulation results show that voids and delaminations in the die attachment would enhance the thermal resistance greatly and decrease the LED's light extraction efficiency, depending on the defects' sizes and locations generated in packaging.",2009,0,
162,163,The minimum worst case error of fuzzy approximators,"The approximation capability of fuzzy systems is an important topic of research when the systems are regarded as input-output maps. By using the notion of information-based complexity (IBC), we derive the minimum worst case error of a fuzzy approximator, which is independent of the detailed construction of the fuzzy rule bases",2001,0,
163,164,Evaluation of risk in canal irrigation systems due to non-maintenance using fuzzy fault tree approach,"The safety and performance of many existing irrigation systems could be improved by doing the preventive maintenance activities. Modeling canal irrigation systems in terms of condition and performance that can be directly correlated with particular canal system maintenance activities. There are two categories of scheduled maintenance activity in irrigation maintenance systems: maintenance may be targeted towards restoring deliveries (""restorative maintenance""), or towards reducing the risk of failures (""preventative maintenance""). This paper covers the latter kind of maintenance scheduling by 'risk analysis'. The purpose of this risk analysis is to forecast the impact of preventative maintenance on deliveries from the main channel systems. After gaining the experience from the preliminary risk analysis through questionnaire based survey and failure history of past record, a 'fuzzy fault tree (FFT)' method is developed for the rapid risk assessment and it is necessitated for the irrigation system manager/engineer. 'Risk analysis' of irrigation systems from the point of view of maintenance is studied and applied to the Tirunelveli Channel Systems located in India. The effectiveness is calculated for each preventative maintenance tasks and it is ranked according to the effectiveness/cost ratio.",2003,0,
164,165,Waveform matching approach for fault diagnosis of a high-voltage transmission line employing harmony search algorithm,"An accurate and effective technology for fault diagnosis of a high-voltage transmission line plays an important role in supporting rapid system restoration. The fault diagnosis of a high-voltage transmission line involves three major tasks, namely fault-type identification, fault location and fault time estimation. The diagnosis problem is formulated as an optimisation problem in this work: the variables involved in the fault diagnosis problem, such as the fault location, and the unknown variables such as ground resistance, are taken into account as optimisation variables; the sum of the discrepancy of the approximation components of the actual and expected waveforms is taken as the optimisation objective. Then, according to the characteristics of the formulated optimisation problem, the harmony search, an effective heuristic optimisation algorithm developed in recent years, is employed to solve this problem. Test results for a sample power system have shown that the developed fault diagnosis model and method are correct and efficient.",2010,0,
165,166,Fault tolerance in autonomic computing environment,"Since the characteristic of current information systems is the dynamic change of their configurations and scales with non-stop provision of their services, the system management should inevitably rely on autonomic computing. Since fault tolerance is one of the important system management issues, it should also be incorporated in an autonomic computing environment. This paper argues what should be taken into consideration and what approach could be available to realize the fault tolerance in such environments.",2002,0,
166,167,Fault Location Using Sparse IED Recordings,"Basic goal of power system is to continuously provide electrical energy to users. Like with any other system, failures in power system can occur. In those situations it is critical that remedial actions are applied as soon as possible. To apply correct remedial actions it is very important that accurate fault condition and location are detected. In this paper, different fault location algorithms followed with description of intelligent techniques used for implementation of corresponding algorithms are presented. New approach for fault location using sparse measurements is examined. According to available data, it decides between different algorithms and selects an optimal one. New approach is developed by utilizing different data structures in order to efficiently implement algorithm decision engine, which is presented in paper.",2007,0,
167,168,A Bio-network Based Fault-Tolerant Architecture for Supervisory System,"Supervisory systems show increasing importance in many plants to ensure the secure and stable production. And the fault tolerance ability is a key problem. Concerned with the problem of low automation level and high operation cost, a layered Bio-Network is studied inspired by biology system, and Bio-Entity as its functional unit is analyzed. In the bottom layer, the communication infrastructure is built on Web Service to hide the difference among underlying heterogeneous systems. In the Bio-System layer, each Bio-Entity is delegated by an agent to gain the functions. Then, in the application layer, various services are encapsulated upon Bio-Entity. The application interface makes the integration with other systems more convenient. Based on Bio-Network, a novel framework of supervisory system for typical waterworks is proposed. The fault tolerant functions are analyzed in the context of Bio-Network. The practical application proves that Bio-Network based system configuration is improved and the cost is reduced.",2008,0,
168,169,Error Analysis of the Complex Kronecker Canonical Form,"In some interesting applications in control and system theory, i.e. in engineering, in ecology (Leslie population model), in financial/actuarial (Leontief multi input - multi output) science, linear descriptor (singular) differential/difference equations with time-invariant coefficients and (non-) consistent initial conditions have been extensively used. The solution properties of those systems are based on the Kronecker canonical form, which is an important component of the Matrix Pencil Theory. In this paper, we present some preliminary results for the error analysis of the complex Kronecker canonical form based on the Euclidean norm. Finally, under some weak assumptions an interesting new necessary condition is also derived.",2010,0,
169,170,Joint Fault-Tolerant Design of the Chinese Space Robotic Arm,"In this paper, joint reliability design for the Chinese space robotic arm has been discussed. Redundant controller unit, redundant can bus communication unit and latch-up power protection unit have been outlined. The fault tree of the joint has been built. Moreover, the new algorithm of auto-adjust thresholding value has been presented for fault detection, and the fault-tolerant strategies of joint have been proposed. Experimental results demonstrate the effectiveness of the joint fault-tolerant design",2006,0,
170,171,Fault tolerance in scalable agent support systems: integrating DARX in the AgentScape framework,"Open multi-agent systems need to cope with the characteristics of the Internet, e.g., dynamic availability of computational resources, latency, and diversity of services. Large-scale multi-agent systems employed on wide-area distributed systems are susceptible to both hardware and software failures. This paper describes AgentScape, a multi-agent system support environment, DARX, a framework for providing fault tolerance in large scale agent systems, and a design for the integration of the two.",2003,0,
171,172,An empirical validation of the relationship between the magnitude of relative error and project size,"Cost estimates are important deliverables of a software project. Consequently, a number of cost prediction models have been proposed and evaluated. The common evaluation criteria have been MMRE, MdMRE and PRED(k). MRE is the basic metric in these evaluation criteria. The implicit rationale of using a relative error measure like MRE, rather than an absolute one, is presumably to have a measure that is independent of project size. We investigate if this implicit claim holds true for several data sets: Albrecht, Kemerer, Finnish, DMR and Accenture-ERP. The results suggest that MRE is not independent of project size. Rather, MRE is larger for small projects than for large projects. A practical consequence is that a project manager predicting a small project may falsely believe in a too low MRE. Vice versa when predicting a large project. For researchers, it is important to know that MMRE is not an appropriate measure of the expected MRE of small and large projects. We recommend therefore that the data set be partitioned into two or more subsamples and that MMRE is reported per subsample. In the long term, we should consider using other evaluation criteria.",2002,0,
172,173,Exact symbol-error probability analysis for orthogonal space-time block codes: two- and higher dimensional constellations cases,"Exact expressions are obtained for the symbol-error probability of orthogonal space-time block codes at the output of the coherent maximum-likelihood decoder in the general case of arbitrary input signal constellation and code. Such expressions are derived for the cases of both deterministic (fixed) and random Rayleigh/Ricean fading channels, and both the two- and higher dimensional constellations.",2004,0,
173,174,Which concurrent error detection scheme to choose ?,"Concurrent error detection (CED) techniques (based on hardware duplication, parity codes, etc.) are widely used to enhance system dependability. All CED techniques introduce some form of redundancy. Redundant systems we subject to common-mode failures (CMFs). While most of the studies of CED techniques focus on area overhead, few analyze the CMF vulnerability of these techniques. In this paper, we present simulation results to quantitatively compare various CED schemes based on their area overhead and the protection (data integrity) they provide against multiple failures and CMFs. Our results indicate that, for the simulated combinational logic circuits, although diverse duplex systems (with two different implementations of the same logic function) sometimes have marginally higher area overhead, they provide significant protection against multiple failures and CMFs compared to other CED techniques like parity prediction",2000,0,
174,175,Optimal scheduling of imprecise computation tasks in the presence of multiple faults,"With the advance of applications such as multimedia, image/speech processing and real-time AI, real-time computing models allowing to express the timeliness versus precision trade-off are becoming increasingly popular. In the imprecise computation model, a task is divided into a mandatory part and an optional part. The mandatory part should be completed by the deadline even under worst-case scenario; however, the optional part refines the output of a mandatory part within the limits of the available computing capacity. A non-decreasing reward function is associated with the execution of each optional part. Since the mandatory parts have hard deadlines, provisions should be taken against faults which may occur during execution. An FT-Optimal framework allows the computation of a schedule that simultaneously maximizes the total reward and tolerates transient faults of mandatory parts. We extend the framework to a set of tasks with multiple deadlines, multiple recovery blocks and precedence constraints among them. To this aim, we first obtain the exact characterization of imprecise computation schedules which can tolerate up to k faults, without missing any deadlines of mandatory parts. Then, we show how to generate FT-Optimal schedules in an efficient way. Our solution works for both linear and general concave reward functions",2000,0,
175,176,Initial Evaluation of the Fracture Behavior of Piezoelectric Single Crystals Due to Artificial Surface Defects,"This study is part of a new research program to develop fundamental understanding of the fracture and fatigue behavior of piezoelectric single crystals through the combination of computational and experimental approaches. In this work we present 1) experimental results on the creation of artificial surface defects in piezoelectric single crystals using a focused ion beam (FIB) system and 2) initial observations on the crystal's fracture behavior under an electrical field. The major advantage of using a FIB is that one can control the size, shape, and orientation of artificial defects precisely, allowing realistic surface defects, e.g., half-penny-shaped, 100 mum long, <1 mum wide, and 50 mum deep. We have demonstrated that multiple artificial defects with varying inclination angles relative to the specimen's crystallographic orientation can be machined in a few hours. In this paper, we report the experimental details of the FIB milling, typical defect shape, and initial results on the effects of high electric field on the fracture behavior of single crystals.",2006,0,
176,177,A parallel and fault tolerant file system based on NFS servers,"One important piece of system software for clusters is the parallel file system. All current parallel file systems and parallel I/O libraries for clusters do not use standard servers, thus it is very difficult to use these systems in heterogeneous environments. However why use proprietary or special-purpose servers on the server end of a parallel file system when you have most of the necessary functionality in NFS servers already? This paper describes the fault tolerance implemented in Expand (Expandable Parallel File System), a parallel file system based on NFS servers. Expand allows the transparent use of multiple NFS servers as a single file system, providing a single name space. The different NFS servers are combined to create a distributed partition where files are stripped. Expand requires no changes to the NFS server and uses RPC operations to provide parallel access to the same file. Expand is also independent of the clients, because all operations are implemented using RPC and NFS protocol. Using this system, we can join heterogeneous servers (Linux, Solaris, Windows 2000, etc.) to provide a parallel and distributed partition. Fault tolerance is achieved using RAID techniques applied to parallel files. The paper describes the design of Expand and the evaluation of a prototype of Expand, using the MPI-IO interface. This evaluation has been made in Linux clusters and compares Expand with PVFS.",2003,0,
177,178,Detection and correction of limit cycle oscillations in second -order recursive digital filter,"In this paper the effects of limit cycle oscillations in recursive second order digital filter is studied and the remedies for curing the problems of limit cycle oscillations are described. Limit cycle deletion using state space representation for second order system implemented with finite word-length register, is depicted. Necessary and sufficient condition to prevent limit cycle oscillation has also been described.",2005,0,
178,179,A Flexible Macroblock Scheme for Unequal Error Protection,"This paper proposes an enhanced error protection scheme using flexible macroblock ordering in H.264/AVC. The algorithm uses a two-phase system. In the first phase, the importance of every macroblock is calculated based on its influence on the current frame and future frames. In the second phase, the macroblocks with the highest impact factor are grouped together in a separate slice group using the flexible macroblock ordering feature of H.264/AVC. By using an unequal error protection scheme, the slice group containing the most important macroblocks can be better protected than the other slice group. The proposed algorithm offers better concealment opportunities than the algorithms which are predefined for flexible macroblock ordering in H.264/AVC.",2006,0,
179,180,Automated defect to fault translation for ASIC standard cell libraries,"Popular generic fault models, which exhibit limited realism for different IC technologies, have been widely misused due to their simplicity and cost-effective implementation. This paper introduces a system for deriving accurate, technology specific fault models that are based on analog defect simulation. The technique is formally defined and a systematic approach is developed. It is supported by a new software tool that provides a push-button solution for the previously tedious task of obtaining accurate ASIC cell defect to fault mappings. Furthermore, upon completion of the cell defect analysis, the tool automatically generates VITAL compliant, defect-injectable, VHDL cell models",2001,0,
180,181,A comparison of neural networks and model-based methods applied for fault diagnosis of electro-hydraulic control systems,The paper aims to investigate two advanced methods used in fault diagnosis of electro-hydraulic (EH) control systems. The theoretical background of the neural network method and model-based approach are presented and the implementation of these methods is summarised with procedures in easy steps to follow for application. The pros and cons of these methods are also analysed based on fault detection capability. It is concluded that a combination of the neural network method and the model-based approach will be beneficial.,2002,0,
181,182,A Predictive Fault Tolerance Agent based on Ubiquitous Computing for A Home Study System,"DOORAE (Distance Object Oriented Collaboration Environment) is a framework for supporting development on multimedia collaborative environment. It provides functions well capable of developing multimedia distance education system for students as well as teachers. It includes session management, access control, concurrency control and handling late comers. There are two approaches to software architecture on which applications for multimedia distance education environment in situation-aware middleware are based. This paper proposes a new model of fault tolerance agent based on situation-aware ubiquitous computing for a multimedia home study system which is based on CARV.",2007,0,
182,183,Model of stator inter-turn short circuit fault in doubly-fed induction generators for wind turbine,"The doubly fed induction generator (DFIG) is an important component of wind turbine systems. It is necessary to identify incipient faults quickly. This paper proposes a complete simulation model of DFIG in wind turbine about inter-turn short circuit fault at stator windings, which is based on multi-circuit theory. A detail analysis about simulation results is presented, especially about short circuit current. By analysis, the apparent 150 Hz, 450 Hz and current phase angle difference are taken as fault features and the fault phase also can be detected by phase angle difference. Both simulated results and experimental results of emulated inter-turn short circuit fault by paralleling a resistance with phase A are carried out. They verify the preceding analysis results. Moreover, their coincidence certificates this model is good and simulation results of inter-turn short circuit fault are correct.",2004,0,
183,184,Self-healing strategies for component integration faults,"Software systems increasingly integrate Off-The-Shelf (OTS) components. However, due to the lack of knowledge about the reused OTS components, this integration is fragile and can cause in the field a lot of failures that result in dramatic consequences for users and service providers, e.g. loss of data, functionalities, money and reputation. As a consequence, dynamic and automatic fixing of integration problems in systems that include OTS components can be extremely beneficial to increase their reliability and mitigate these risks. In this paper, we present a technique for enhancing component-based systems with capabilities to self-heal common integration faults by using a predetermined set of healing strategies. The set of faults that can be healed has been determined from the analysis of the most frequent integration bugs experienced by users according to data in bug repositories available on Internet. An implementation based on AOP techniques shows the viability of this technique to heal faults in real case studies.",2008,0,
184,185,Recursive Evaluation of Fault Tolerance Mechanisms for SLA Management,"Service level agreements (SLAs) have been introduced into the grid in order to build a basis for its commercial uptake. The challenge for Grid providers in agreeing and operating SLA-bound jobs is to ensure their fulfillment even in the case of failures. Hence, fault-tolerance mechanisms are an essential means of the provider's SLA management. The high utilization of commercial operated clusters leads to scenarios in which typically a job migration effects other jobs scheduled. The effects result from the unavailability of enough free resources which would be needed to catch all resource outages. Consequently before initiating a migration, its effects for other jobs have to be compared and the initiation of fault- tolerance (FT-) mechanisms have to be evaluated recursively. This paper presents a measurement for the benefit of initiating a FT-mechanism, the recursive evaluation, and termination condition. Performing such an impact evaluation of an initiated chain of FT-mechanisms is often more profitable than performing a single FT-mechanism and accordingly this is important for the Grid commercialization.",2008,0,
185,186,Corrective control strategies in case of infeasible operating situations,"A new method that deals with power systems infeasible operating situations is proposed in this paper. In case these situations occur, appropriate corrective actions must be efficiently obtained and quickly implemented. In order to accomplish this, it is necessary (a) to quantify the systems unsolvability degree (UD), and (b) to determine a corrective control strategy to pull the system back into the feasible operation region. UD is determined through the smallest distance between the infeasible (unstable) operating point and the feasibility boundary in parameter (load) space. In this paper the control strategies can be obtained by two methods, namely the proportionality method (PM) and the nonlinear programming based method (NLPM). Capacitor banks, tap changing transformers and load shedding are the usual controls available. Simulations have been carried out, for small to large systems, under contingency and heavy load situations, in order to show the efficiency of the proposed method. It can be a very useful tool in operation planning studies, particularly in voltage stability analysis.",2001,0,
186,187,Fault detection using phenomenological models,"There exist many different established approaches to detect system faults. This paper discusses the various system models and the associated fault detection techniques. Specifically, phenomenological models are presented in detail. Fault detection using principal components analysis and the cluster and classify method is illustrated with real operational data from an electrically powered vehicle.",2003,0,
187,188,Using Search Methods for Selecting and Combining Software Sensors to Improve Fault Detection in Autonomic Systems,"Fault-detection approaches in autonomic systems typically rely on runtime software sensors to compute metrics for CPU utilization, memory usage, network throughput, and so on. One detection approach uses data collected by the runtime sensors to construct a convex-hull geometric object whose interior represents the normal execution of the monitored application. The approach detects faults by classifying the current application state as being either inside or outside of the convex hull. However, due to the computational complexity of creating a convex hull in multi-dimensional space, the convex-hull approach is limited to a few metrics. Therefore, not all sensors can be used to detect faults and so some must be dropped or combined with others. This paper compares the effectiveness of genetic-programming, genetic-algorithm, and random-search approaches in solving the problem of selecting sensors and combining them into metrics. These techniques are used to find 8 metrics that are derived from a set of 21 available sensors. The metrics are used to detect faults during the execution of a Java-based HTTP web server. The results of the search techniques are compared to two hand-crafted solutions specified by experts.",2010,0,
188,189,Experimental study on the impact of endoscope distortion correction on computer-assisted celiac disease diagnosis,"The impact of applying barrel distortion correction to endoscopic imagery in the context of automated celiac disease diagnosis is experimentally investigated. For a large set of feature extraction techniques, it is found that contrasting to intuition, no improvement but even significant result degradation of classification accuracy can be observed. For techniques relying on geometrical properties of the image material (shape), moderate improvements of classification accuracy can be achieved. Reasons for this somewhat unexpected results are discussed and ways how to exploit potential distortion correction benefits are sketched.",2010,0,
189,190,A comparison of phase space reconstruction and spectral coherence approaches for diagnostics of bar and end-ring connector breakage faults in polyphase induction motors using current waveforms,"Two signal (waveform) analysis approaches are investigated in this paper for motor drive fault identification-one linear and the other nonlinear. Twenty-one different motor-drive operating conditions including healthy, 1 through 10 broken bars, and 1 through 10 broken end-ring connectors are investigated. Highly accurate numerical simulations of current waveforms for the various operating conditions are generated using the time stepping coupled finite element-state space method for a 208-V, 60-Hz, 2-pole, 1.2-hp, squirrel cage 3-phase induction motor. The linear signal analysis method is based on spectral coherence, whereas the nonlinear signal analysis method is based on stochastic models of reconstructed phase spaces. Conclusions resulting from the comparisons of these two methods are drawn.",2002,0,
190,191,Ontology-based fault diagnosis for industrial control applications,"Traditional fault detection systems in industrial control applications are just able to report occurring faults. Fault diagnosis systems are more desirable for plant operators, as such systems are capable to reduce the number of occurring alarms by elimination of consecutive alarms and prioritization of critical alarms. The disadvantage of those systems is that they have to be implemented anew for every control application, as the system dependencies vary from application to application. Ontology-based fault diagnosis systems do not have this disadvantage. Only the ontology has to be created for the new system, which greatly reduces time and effort for new systems, as old ontologies can be reused for the new system.",2010,0,
191,192,Ground control for the geometric correction of PAN imagery from Indian remote sensing (IRS) satellites,"Efficacy of a hand-held global positioning system (GPS) receiver in stand-alone mode of GPS measurements was investigated by taking measurements on different dates. These measurements were compared to those observed by DGPS and total station for the validation of hand-held GPS receiver accuracy. Ground control point (GCP) coordinates were derived from 1:25,000 and 1:50,000 scale topographic maps, and by using two different types of GPS receivers- a stand-alone hand-held and dual frequency DGPS receivers. GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using affine mapping function. GCPs derived from maps yielded root mean squares (RMS) error from 15 to 35 m respectively. However, GCPs derived by DGPS or stand-alone mode hand-held GPS receiver gave RMS error in the range of 3 to 6 meter, which is very close to spatial resolution of PAN sensor imagery (5.8 m). The mean values of GCP coordinates observed with the help of hand-held GPS receiver in stand-alone mode might prove a cost effective solution for the determination of GCP coordinates in the geometric correction of current high-resolution imagery from IRS satellites.",2003,0,
192,193,Synchronization Probabilities using Conventional and MVDR Beam Forming with DOA Errors,"In this paper, code synchronization probabilities of the direct sequence spread spectrum (DS/SS) system are investigated when the receiver utilizes an adaptive antenna array. Performance studies of three beam forming algorithms in the presence of direction-of-arrival (DOA) errors are presented. The investigated algorithms are the conventional, the minimum variance distortionless response (MVDR), and the MVDR+ADL where the MVDR algorithm is enhanced against DOA error via the adaptive diagonal loading (ADL). The paper includes a large number of analytical and simulation results where the effects of DOA errors are investigated. It can be concluded that the MVDR beam former is more sensitive to DOA errors than the conventional beam former, especially, at large DOA errors and high signal-to-noise-ratio (SNR) values. However, this sensitivity can be reduced notably by using the ADL.",2007,0,
193,194,Fault Tree Reuse Across Multiple Reasoning Paradigms,"The development of a diagnostic model can be a very time-consuming and manually intensive process. One must first analyze the Test Program Set (TPS) to determine the fault tree and then integrate with that any additional knowledge that can be obtained from external data sources (such as test results, maintenance actions from the various maintenance levels, run-time failure information, etc.). The diagnostic models defined in the IEEE Std. 1232-2002 (AI-ESTATE) each define a different method that can be used for a diagnostic reasoner. It has been determined that each of these models utilize the information found in the basic TPS fault tree. As the fault tree represents hard won engineering knowledge that is expensive to reproduce, it is desirable to share the fault tree representations across multiple reasoner models. This paper will layout how each model type in the AI-ESTATE standard utilizes the fault tree to perform diagnostics and how, through the use of the XML representation of the AI-ESTATE fault tree model, that basic fault tree can be shared between reasoner models. It would also be desirable to find a way to gain that fault tree knowledge without having to manually reproduce it. As such, this paper will also describe how that information can at least be semi-automatically extracted from TPS design artifacts.",2006,0,
194,195,Position location error analysis by AOA and TDOA using a common channel model for CDMA cellular environments,"AOA and TDOA are known to be promising methods and are developed separately. Combination or cooperation of the two methods has not been possible due to lack of the applicable channel models. The COST-207 model is utilized to provide the angular information for the AOA method. The angular characteristic of the channel can be obtained from the given temporal channel model, hence fusing of the approaches is now possible. Different properties of the methods are revealed to verify the proposition in literature. As a future research area, the fusing of the two different methods for better reliability and accuracy is mentioned",2000,0,
195,196,"Mutual coupling in microstrip antenna array: evaluation, reduction, correction or compensation","Mutual coupling between the antenna elements in a microstrip antenna array is a potential source of performance degradation, particularly in a highly congested environment. The degradation includes impedance mismatching, increased side-lobe level, deviation of the radiation pattern from the desired one, and decrease of gain due to the excitation of a surface wave. To deal with these problems, the first thing is to evaluate the mutual coupling and to select the element with low mutual coupling. Then, it is still desired to reduce the mutual coupling further by taking some measures. Finally, in certain critical cases, it is necessary to involve the mutual coupling effects accurately through numerical analysis, such as ultra low side lobe arrays and adaptive ing arrays. All these issues are discussed and some numerical examples are given. Due to limited space, the paper focuses mainly on the work done in our laboratory.",2005,0,
196,197,Geometric and shading correction for images of printed materials using boundary,"A novel technique that uses boundary interpolation to correct geometric distortion and shading artifacts present in images of printed materials is presented. Unlike existing techniques, our algorithm can simultaneously correct a variety of geometric distortions, including skew, fold distortion, binder curl, and combinations of these. In addition, the same interpolation framework can be used to estimate the intrinsic illumination component of the distorted image to correct shading artifacts. We detail our algorithm for geometric and shading correction and demonstrate its usefulness on real-world and synthetic data.",2006,0,
197,198,A novel co-evolutionary approach to automatic software bug fixing,"Many tasks in software engineering are very expensive, and that has led the investigation to how to automate them. In particular, software testing can take up to half of the resources of the development of new software. Although there has been a lot of work on automating the testing phase, fixing a bug after its presence has been discovered is still a duty of the programmers. In this paper we propose an evolutionary approach to automate the task of fixing bugs. This novel evolutionary approach is based on co-evolution, in which programs and test cases co-evolve, influencing each other with the aim of fixing the bugs of the programs. This competitive co-evolution is similar to what happens in nature for predators and prey. The user needs only to provide a buggy program and a formal specification of it. No other information is required. Hence, the approach may work for any implementable software. We show some preliminary experiments in which bugs in an implementation of a sorting algorithm are automatically fixed.",2008,0,
198,199,An NN-based atmospheric correction algorithm for Landsat/TM thermal infrared data,"Land surface temperature (LST) is a key variable for studies of global or regional land surface processes, energy and water cycle, and thus, has important applications in various areas. Atmospheric correction is a major issue in LST retrieval using remote sensing data because the presence of the atmosphere always influences the radiation from the ground to the space sensor. Atmospheric correction of thermal infrared (TIR) data for land surface temperature retrieval is to estimate the three atmospheric parameters: transmittance, path radiance and the downward radiance. Typically the atmospheric parameters are obtained using atmospheric profiles combined with a radiative transfer model (RTM). But this approach is time-consuming and expensive, which is impractical for high-speed (near-realtime) operational atmospheric correction. An artificial neural network (NN) based atmospheric correction model for Landsat/TM thermal infrared data is proposed. The multi-layer feed-forward neural network (MFNN) is selected, in which the atmospheric profiles (temperature, humidity and pressure), elevation and scan angle are the input variables, and the atmospheric parameters are the output variables. The MFNN is combined with the radiative transfer simulation, using MODTRAN 4.0 and the latest global assimilated data. Finally, the transmittance and path radiance derived by the MFNN-based algorithm is compared with MODTRAN4.0 results. The RMSE for both parameters are 0.0031 and 0.035 Wm<sup>-2</sup>sr<sup>-1</sup>m<sup>-1</sup>, respectively. The results indicate that the proposed approach can be a practical method for Landsat/TM thermal data in both accuracy and efficiency.",2010,0,
199,200,Fault tree analysis of a fire hazard of a power distribution cabinet with Petri Nets,Motivation of this study is to verify system safety analysis of HAVELSAN Peace Eagle Program developed hardware items for Ground Support Systems. A preliminary hazard analysis for each of the hardware developed items are performed and safety hazard analysis models are constructed with risk assessment of hazards based on their probability of occurrences for future operational and maintenance activities. An example for this kind of analysis the system safety fault tree analysis model of a Ground Support Segment Mission Simulator subsystem Power Distribution Adapter Cabinet design with hazardous risk assessments criteria according to the military standard specifications. Same analysis approach then modeled with Petri Nets that has extensions from fault tree analysis approach and enables the modeler to represent the probability of occurrences in the system design phase. Same model can be built in the specification phase which creates the potential for early validation of the system design behavior.,2010,0,
200,201,Three-Dimensional Pareto-Optimal Design of Inductive Superconducting Fault Current Limiters,"The inductive-type superconducting fault current limiters (LSFCLs) mainly consist of a primary copper coil, a secondary complete or partial superconductor cylinder, and a closed or open magnetic iron core. Satisfactory performance of such device significantly depends on optimal selection of its employed materials and construction dimensions, as well as its electrical, thermal, and magnetic parameters. Therefore, it is very important to identify a comprehensive model describing the LSFCL behavior in a power system prior to its fabrication. When a fault occurs, the dynamic model should essentially characterize the overall phenomena to compare the simulation results by varying LSFCL parameters to maximize the merits of a fault current limiter while minimizing its drawbacks during the normal state. The principle object of this paper is to achieve a feasible and full penetrative approach in 3-D alignments, i.e., a Pareto-optimal design of LSFCLs by means of multicriteria decision-making techniques after defining the LSFCL model in a power system CAD/electromagnetic transients including dc environment.",2010,0,
201,202,Stochastic fault tree analysis with self-loop basic events,"This paper presents an analytical approach for performing fault tree analysis (FTA) with stochastic self-loop events. The proposed approach uses the flow-graph concept, and moment generating function (MGF) to develop a new stochastic FTA model for computing the probability, mean time to occurrence, and standard deviation time to occurrence of the top event. The application of the method is demonstrated by solving one example.",2005,0,
202,203,Coverage gain estimation for multi-burst forward error correction in DVB-H networks,"An approach for increasing the reception robustness of mobile broadcast streaming services has been developed for mobile broadcast systems based on time-slicing, such as DVB-H, employing multi-burst FEC at link or application layer. Multiple bursts will be encoded jointly in order to overcome burst errors caused by signal level variations. The approach shows high potential which can be characterized by a link margin gain due to reduced CNR requirements to cope with fast fading and shadowing. Nevertheless, the achieved gain depends on several system parameters (encoding period and coding rate), the physical environment (correlation of shadowing and multi-path fading) and on the mobility of the users (velocity and trajectory). The paper deals with the coverage estimation and network gain due to multi-burst FEC for vehicular users in a realistic urban scenario. Since the user behavior has to be considered the gain cannot be directly included into the link budget. Thus, a methodology has been developed in order to estimate the coverage of multi-burst FEC services based on dynamic system-level simulations. Results are shown by means of simulations in realistic scenarios and field measurements in urban environments.",2009,0,
203,204,Dynamic strength scaling for delay fault propagation in nanometer technologies,"This paper proposes an algorithm for the detection of resistive delay faults in deep submicron technology using dynamic strength scaling, which is applicable for 45 nm and below. The approach uses an advanced coding system to build logical functions that are sensitive to strength and able to detect even the slightest voltage changes in the circuit. Such changes are caused by interconnection resistive behavior and result in timing-related defects.",2009,0,
204,205,LEON3 ViP: A Virtual Platform with Fault Injection Capabilities,"In addition to functional simulation for validation of hardware/software designs, there are additional robustness requirements that need advanced simulation techniques and tools to analyze the system behavior in the presence of faults. In this paper, we present the design of a fault injection framework for LEON3, a 32bit SPARC CPU based system used by the European Space Agency, described at Transaction Level using System C. First of all an extension of a previous XML formalization of basic binary faults, like memory and CPU registers corruption, is done in order to support TLM2.0transaction's parameters corruptions. Next a novel Dynamic Binary Instrumentation (DBI) technique for C++ binaries is used to insert fault injection wrappers in SystemC transaction path. For binary faults in model components the use of TLM2.0 transport_dbg is proposed. This way each component with fault injection capabilities exposes a standard interface to allow internal component inspection and modification.",2010,0,
205,206,Research on Transformer Fault Diagnosis Expert System Based on DGA Database,"This paper analyzes and designs the transformer fault diagnosis system based on dissolved gas analysis (DGA) database in which DGA data is managed by the Oracle database. The fault diagnosis module includes the single analyzing item and the Integrated analyzing item, such as, improvement three-ratio method, grey relational entropy, fuzzy clustering, artificial neural networks, and so on. They reduce the insufficiency in diagnosis method which is used now. The system realizes each function of the modules by using the lamination method, it is able to diagnose problems existing in oil chromatogram analysis data of transformer, and the accuracy of the system is also testified by practical example.",2009,0,
206,207,Modular fault recovery in timed discrete-event systems: application to a manufacturing cell,"This paper extends the previous results of the authors on fault recovery to timed discrete-event systems (TDES), and discusses the application of the proposed methodology to a manufacturing cell. It is assumed that the plant can be modelled as a TDES, the faults are permanent, and that a diagnosis system is available that detects and isolates faults with a bounded delay (expressed in clock ticks). Thus, the combination of the plant and the diagnosis system, as the system to be controlled, has three modes: normal, transient and recovery. Initially, the plant is in the normal mode. Once a fault occurs, the system enters the transient mode. After the fault is detected and isolated by the diagnosis system, the system enters the recovery mode. This framework does not depend on the diagnosis technique used, as long as lower and upper bounds for diagnosis delay are available. A modular switching supervisory scheme is proposed to satisfy the system specifications. The design consists of a normal-transient supervisor, and multiple recovery supervisors each for recovery from a particular failure mode. The issue of the nonblocking property of the system under supervision, and also supervisor admissibility (controllability), in particular coerciveness, are studied. The proposed approach is applied to a manufacturing cell consisting of two machines and two conveyors. A modular switching supervisor is designed to ensure the specifications in the normal mode are met. In cases of failure, the supervisor sends appropriate recovery commands so that the cell can complete its production cycle",2005,0,
207,208,Managing fault-induced delayed voltage recovery in Metro Atlanta with the Barrow County SVC,"Georgia Transmission Corporation (GTC) commissioned the Barrow County Static Var Compensator (SVC) with a continuous rating of 0 to +260 Mvar in June of 2008. This paper presents the northern Metro Atlanta Georgia area transmission system, the requirements for voltage and var support, the dynamic performance study used to verify performance of the SVC, and provides an overview of the SVC design and control strategy, including the SVC's response to an actual power system disturbance. The Barrow County SVC is connected to the 230 kV bus at the Winder Primary Substation to effectively manage the exposure to fault-induced delayed voltage recovery (FIDVR), where the system voltage remains low (<80%) for several seconds following a disturbance and potentially leading to voltage collapse. The SVC configuration includes two thyristor-switched capacitors for rapid insertion of reactive power following a disturbance to decrease voltage recovery time and control the system's dynamic performance.",2009,0,
208,209,Investigation of fault tolerant of direct torque control in induction motor drive,"AC drives based on direct torque control (DTC) of induction machines are known for their high dynamic performances, obtained with very simple control schemes. So many studies have been performed with ASIC or FPGA DTC implementation. In this paper, we investigate for the tolerance of such drive to sensors defects, when the control algorithm is to be implemented in an FPGA. So, authors specially focus on the influence of the FPGA implementation design on the DTC fault tolerance. Simulations are carried out with system generator (SG) toolbox working in the MATLAB/SIMULINK environment. Results are presented and discussed to evaluate the DTC operating under considered faults.",2004,0,
209,210,Operational Fault Detection in cellular wireless base-stations,"The goal of this work is to improve availability of operational base-stations in a wireless mobile network through non-intrusive fault detection methods. Since revenue is generated only when actual customer calls are processed, we develop a scheme to minimize revenue loss by monitoring real-time mobile user call processing activity. The mobile user call load profile experienced by a base-station displays a highly non-stationary temporal behavior with time-of-day, day-of-the-week and time-of-year variations. In addition, the geographic location also impacts the traffic profile, making each base-station have its own unique traffic patterns. A hierarchical base-station fault monitoring and detection scheme has been implemented in an IS-95 CDMA Cellular network that can detect faults at - base station level, sector level, carrier level, and channel level. A statistical hypothesis test framework, based on a combination of parametric, semi-parametric and non-parametric test statistics are defined for determining faults. The fault or alarm thresholds are determined by learning expected deviations during a training phase. Additionally, fault thresholds have to adapt to spatial and temporal mobile traffic patterns that slowly changes with seasonal traffic drifts over time and increasing penetration of mobile user density. Feedback mechanisms are provided for threshold adaptation and self-management, which includes automatic recovery actions and software reconfiguration. We call this method, Operational Fault Detection (OFD). We describe the operation of a few select features from a large family of OFD features in Base Stations; summarize the algorithms, their performance and comment on future work.",2006,0,
210,211,Fault Tolerance for Manufacturing Components,"This article proposes a multiagent system for industrial production elements that transfers the concept of fault tolerance to the manufacturing levels of the organisation, acting automatically under open protocols when there is degradation or failure of any of the components, ensuring that normal operation is resumed within a delimited time. The main characteristics of this system are the drastic reduction in recovery times, the support for the significant heterogeneity existing in these scenarios and their high level of automation, while practically dispensing with the intervention of system administrators.",2006,0,
211,212,A study of student strategies for the corrective maintenance of concurrent software,"Graduates of computer science degree programs are increasingly being asked to maintain large, multi-threaded software systems; however, the maintenance of such systems is typically not well-covered by software engineering texts or curricula. We conducted a think-aloud study with 15 students in a graduate-level computer science class to discover the strategies that students apply, and to what effect, in performing corrective maintenance on concurrent software. We collected think-aloud and action protocols, and annotated the protocols for a number of behavioral attributes and maintenance strategies. We divided the protocols into groups based on the success of the participant in both diagnosing and correcting the failure. We evaluated these groups for statistically significant differences in these attributes and strategies. In this paper, we report a number of interesting observations that came from this study. All participants performed diagnostic executions of the program to aid program comprehension; however, the participants that used this as their predominant strategy for diagnosing the fault were all unsuccessful. Among the participants that successfully diagnosed the fault and displayed high confidence in their diagnosis, we found two commonalities. They all recognized that the fault involved the violation of a concurrent-programming idiom. And, they all constructed detailed behavioral models (similar to UML sequence diagrams) of execution scenarios. We present detailed analyses to explain the attributes that correlated with success or lack of success. Based on these analyses, we make recommendations for improving software engineering curriculums by better training students how to apply these strategies effectively.",2008,0,
212,213,The feasibility study on the combined equipment between micro-SMES and inductive/electronic type fault current limiter,"The concept of the combined equipment between micro-SMES and inductive/electronic type FCL is proposed in this paper. Having the multifunction for a superconducting device, the new equipment can serve as the protective component for a dual power system. The specification of a testing model was determined and the transient performance was analyzed by Matlab software. The results show that the combined equipment is realizable for a dual power system application, where it has the major function of limiting fault current (FCL function) and the minor function of maintaining power fluctuation (SMES function).",2003,0,
213,214,Effective Static Analysis to Find Concurrency Bugs in Java,"Multithreading and concurrency are core features of the Java language. However, writing a correct concurrent program is notoriously difficult and error prone. Therefore, developing effective techniques to find concurrency bugs is very important. Existing static analysis techniques for finding concurrency bugs either sacrifice precision for performance, leading to many false positives, or require sophisticated analysis that incur significant overhead. In this paper, we present a precise and efficient static concurrency bugs detector building upon the Eclipse JDT and the open source WALA toolkit (which provides advanced static analysis capabilities). Our detector uses different implementation strategies to consider different types of concurrency bugs. We either utilize JDT to syntactically examine source code, or leverage WALA to perform interprocedural data flow analysis. We describe a variety of novel heuristics and enhancements to existing analysis techniques which make our detector more practical, in terms of accuracy and performance. We also present an effective approach to create inter-procedural data flow analysis using WALA for complex analysis. Finally we justify our claims by presenting the results of applying our detector to a range of real-world applications and comparing our detector with other tools.",2010,0,
214,215,A case study of evaluation technique for soft error tolerance on SRAM-based FPGAs,"SRAM-based field programmable gate arrays (FPGAs) are vulnerable to a single event upset (SEU), which is induced by radiation effect. Therefore, the dependable design techniques become important, and the accurate dependability analysis method is required to demonstrate their robustness. Most of present analysis techniques are performed by using full reconfiguration to emulate the soft error. However, it takes long time to analyze the dependability because it requires many times of reconfiguration to complete the soft error injection. In the present paper, we construct the soft error estimation system to analyze the reliability and to reduce the estimation time. Moreover, we apply Monte Carlo simulation to our approach, and identify trade-off between accuracy of error rate and estimation time. As a result of our experimentation for 8-bit full-adder and multiplier, we can show the dependability of the implemented system. Also, the constructed system can reduce the estimation time. According to the result, when performing about 50% circuit Monte Carlo simulation, the error rate is within 20%.",2010,0,
215,216,Effects of finite weight resolution and calibration errors on the performance of adaptive array antennas,"Adaptive antennas are now used to increase the spectral efficiency in mobile telecommunication systems. A model of the received carrier-to-interference plus noise ratio (CINR) in the adaptive antenna beamformer output is derived, assuming that the weighting units are implemented in hardware, The finite resolution of weights and calibration is shown to reduce the CINR. When hardware weights are used, the phase or amplitude step size in the weights can be so large that it affects the maximum achievable CINR. It is shown how these errors makes the interfering signals leak through the beamformer and we show how the output CINR is dependent on power of the input signals. The derived model is extended to include the limited dynamic range of the receivers, by using a simulation model. The theoretical and simulated results are compared with measurements on an adaptive array antenna testbed receiver, designed for the GSM-1800 system. The theoretical model was used to find the performance limiting part in the testbed as the 1 dB resolution in the weight magnitude. Furthermore, the derived models are used in illustrative examples and can be used for system designers to balance the phase and magnitude resolution and the calibration requirements of future adaptive array antennas",2001,0,
216,217,Fault Tolerant Methods for Intermitted Failures in Virtual Large Scale Disks,"Recently, the demand of low cost large scale storages increases. We developed VLSD (Virtual Large Scale Disks) toolkit for constructing virtual disk based distributed storages, which aggregate free spaces of individual disks. However, current implementation of VLSD can mask only stop failure but cannot mask other kinds of failures such as intermitted failure. In this paper, we introduce two classes to VLSD in order to increase the intermitted fault tolerance. One is Retry Disk which retries to read/write at failures, another is VotedRAID1 which masks failures by majority voting. In this paper, we describe these classes in detail and evaluate their fault tolerance.",2010,0,
217,218,Advances in scatter correction for 3D PET/CT,"We report on several significant improvements to the implementation of image-based scatter correction for 3D PET and PET/CT. Among these advances are: a new algorithm to scale the estimated scatter sinogram to the measured data, thereby largely compensating for external scatter; the ability to handle CT image truncation during this scaling; the option to iterate the scatter calculation for improved accuracy; the use of ordered subset estimation maximization (OSEM) reconstruction for the estimated emission images from which the scatter contributions are simulated; reporting of data quality parameters such as scatter and randoms fractions, and noise equivalent count rate (NECR), for each patient bed position; and extensive quality control output. Scatter correction (2 iterations, OSEM) typically requires 15-45 sec per bed. Very good agreement between the estimated scatter and measured emission data for several typical clinical scans is reported for CPS Pico-3D and HiRez LSO PET/CT systems. The data characteristics extracted during scatter correction are useful for patient specific count rate modeling and scan optimization",2004,0,
218,219,Time and frequency domain analyses based expert system for impulse fault diagnosis in transformers,"The presence of insulation failure in the transformer winding is detected using the voltage and current oscillograms recorded during the impulse test. Fault diagnosis in transformers has several parameters such as the severity of fault, the kind of fault and the location of the fault. Detection of major faults involving a large section of the coils have never been a big issue and several visual and computational methods have already been proposed by several researchers. The present paper describes an expert system based on re-confirmative method for the diagnosis of minor insulation failures involving small number of turns in transformers during impulse tests. The proposed expert system imitates the performance of an experienced testing personnel. To identify and locate a fault, an inference engine is developed to perform deductive reasoning based on the rules in the knowledge base and different statistical techniques. The expert system includes both the time-domain and frequency-domain analyses for fault diagnosis. The basic aim of the expert system is to provide a non-expert with the necessary information and interaction in order to make fault diagnosis in a friendly windowed environment. The rules for fault diagnosis have been so designed that these are valid for the range of power transformers used in practice up to a voltage level of 33 kV. The fault diagnosis algorithm has been tested using experimental results obtained for a 3 MVA transformer and simulation results obtained for 5 and 7 MVA transformers",2002,0,
219,220,Fault Detection Using the k-Nearest Neighbor Rule for Semiconductor Manufacturing Processes,"It has been recognized that effective fault detection techniques can help semiconductor manufacturers reduce scrap, increase equipment uptime, and reduce the usage of test wafers. Traditional univariate statistical process control charts have long been used for fault detection. Recently, multivariate statistical fault detection methods such as principal component analysis (PCA)-based methods have drawn increasing interest in the semiconductor manufacturing industry. However, the unique characteristics of the semiconductor processes, such as nonlinearity in most batch processes, multimodal batch trajectories due to product mix, and process steps with variable durations, have posed some difficulties to the PCA-based methods. To explicitly account for these unique characteristics, a fault detection method using the k-nearest neighbor rule (FD-kNN) is developed in this paper. Because in fault detection faults are usually not identified and characterized beforehand, in this paper the traditional kNN algorithm is adapted such that only normal operation data is needed. Because the developed method makes use of the kNN rule, which is a nonlinear classifier, it naturally handles possible nonlinearity in the data. Also, because the FD-kNN method makes decisions based on small local neighborhoods of similar batches, it is well suited for multimodal cases. Another feature of the proposed FD-kNN method, which is essential for online fault detection, is that the data preprocessing is performed automatically without human intervention. These capabilities of the developed FD-kNN method are demonstrated by simulated illustrative examples as well as an industrial example.",2007,0,
220,221,ANN based detection of electrical faults in generator-transformer units,"In the paper a model of decision system based on ANN, will be shown. As protected object the generator-transformer unit has been taking into consideration. The range of detected faults are initially narrows to faults of electromagnetic character (three-phase, two-phase, two-phase to earth and one-phase faults) within the generator - unit transformer - high voltage transmission line configuration.",2004,0,
221,222,Research on remote intelligent fault-diagnosis of CNC lathe based on bayesian networks,"Considering the development of smart machine tools and Internet-based manufacturing and in order to manage the manufacturing process more efficiently, a unit of remote intelligent fault-diagnosis based on Bayesian Networks (BN) was designed and software based on internet was realized as well as the case study concerning CNC lathe. It is a compensation of machine tool's self-detection whose major job is to find the fault of hardware and programming. The case study proved the reliability and advantages of the intelligent model based on BN.",2010,0,
222,223,Soft error optimization of standard cell circuits based on gate sizing and multi-objective genetic algorithm,"A radiation harden technique based on gate sizing and multi-objective genetic algorithm (MOGA) is developed to optimize the soft error tolerance of standard cell circuits. Soft error rate (SER), chip area and longest path delay are selected as the optimization goals and fast fitness evaluation algorithms for the three goals are developed and embedded into the MOGA. All the three goals are optimized simultaneously by optimally sizing the gates in the circuit, which is a complex NP-Complete problem and resolved by MOGA through exploring the global design space of the circuit. Syntax analysis technique is also employed to make the proposed framework can optimize not only pure combinational logic circuit but also the combinational parts of sequential logic circuit. Optimizing experiments carried out on ISCAS'85 and ISCAS'89 standard benchmark circuits show that the proposed optimization algorithm can decrease the SER 74.25% with very limited delay overhead (0.28%). Furthermore, the algorithm can also reduce the area for most of the circuit under test by average 5.23%. The proposed technique is proved to be better than other works in delay and area overhead and suitable to direct the design of soft error tolerance integrated circuits in high reliability realms.",2009,0,
223,224,Nonlinear Systems Fault Diagnosis with Differential Elimination,"The differential elimination algorithm is used to eliminate the non-observed variables of the nonlinear systems. By incorporating the algebraic observability and diagnosability concepts and using numerical differentiation algorithms, another approach to the certain classes of nonlinear systems fault diagnosis problem is presented.",2009,0,
224,225,Fault Tolerant Actuation for Steer-by-Wire Applications,"This paper introduces a R&D project concerned with the development of a fault-tolerant actuation system for steer-by-wire applications. The essential safety and reliability requirements for automotive vehicles are assessed. General redundancy schemes and current practices are examined. The paper then focuses on the use of actuators based on permanent magnetic brushless dc motors, and analyses internal fault-tolerant potentials of the actuator technology with possible control schemes evaluated. Finally key innovations that may provide practical and affordable solutions are discussed.",2007,0,
225,226,A procedure to correct the error in the structure function based thermal measuring methods,In this paper a methodology is presented to correct the systematic error of structure function based thermal material parameter measuring methods. This error stems from the fact that it is practically impossible to avoid parallel heat-flow paths in case of forced one-dimensional heat conduction. With the presented method we show how to subtract the effect of the parallel heat-flow paths from the measured structure function. With this correction methodology the systematic error of structure function based thermal material parameter measuring methods can be practically eliminated. Application examples demonstrate the accuracy increase obtained with the use of the method.,2004,0,
226,227,Backward-compatible robust error protection of JPEG XR compressed video,"The new JPEG XR image encoding standard offers a great compression rate while maintaining a good visual quality. Nonetheless, it has low error robustness, making it unusable in case of unreliable transmission over error prone channels, e.g., wireless channels. An improvement to the standard was developed, which can correct transmission errors, both bit or packet losses, and which is fully compatible with legacy decoders. Data interleaving and channel coding can offer a good protection against transmission errors; different levels of protection can be adopted, in order to trade-off between error protection capabilities and decompressed image quality.",2010,0,
227,228,Application of fuzzy neuro for generator stator earth fault detection,"In this paper the use of a fuzzy neural net for stator earth fault detection is presented. A generator model is simulated using EMTDC software. Earth faults are simulated between 0.1% to 100% distance points from the generator neutral. The combination of both EMTDC simulation and neural network presented in this paper introduces a new, complementary method that performs better in instances where the interpretation of traditional methods is somewhat dubious.",2004,0,
228,229,Research and Realization of Digital Circuit Fault Probe Location Process,"This paper presents three core files relating to circuit fault diagnosis which is generated by LASAR (logic automated stimulus response), i.e. fault dictionary, node truth table and pin connection table, analyses the content of fault dictionary, pin connection table and node truth table, finds the necessary information for fault location, summarizes the procedure of circuit test and fault location. Finally the digital circuit diagnosis system which can locate the fault on the pin of components is designed. With the help of probe, fault location of component pins can be accurately pinpointed.",2008,0,
229,230,New results for fault detection of untimed continuous Petri nets,"In this paper we study fault diagnosis of systems modeled by untimed continuous Petri nets. In particular, we generalize our previous works in this framework where we solved this problem only for special classes of continuous Petri nets, namely state machines and backward conflict free nets. We show that the price to pay for this generalization is that only three diagnosis states can be defined, rather than four. However, this is not a significant restriction because it is in accordance with all the literature on finite state automata.",2009,0,
230,231,Application of Particle Swarm Optimization and RBF Neural Network in Fault Diagnosis of Analogue Circuits,"BP neural network has the shortcoming of over-fitting, local optimal solution, which affects the practicability of BP neural network. RBF neural network is a feedforward neural network, which has the global optimal closing ability. However, the parameters in RBF neural network need determination. Particle swarm optimization is presented to choose the parameters of RBF neural network. The particle swarm optimization-RBF neural network method has high classification performance, and is applied to fault diagnosis of analogue circuits. Finally, the result of fault diagnosis cases shows that the particle swarm optimization - RBF neural network method has higher classification than BP neural network.",2009,0,
231,232,Development of a Testbench for Validation of DMT and DT2 Fault-Tolerant Architectures on SOI PowerPC7448,The purpose of TAFT fault tolerance studies conducted at CNES is to prepare the space community for the significant evolution linked to the usage of COTS components for developing spacecraft supercomputers. CNES has patented the DMT and DT2 fault-tolerant architectures with 'light' features. The development of a DMT/DT2 testbench based on a PowerPC7448 microprocessor from e2v is presented in this paper.,2008,0,
232,233,Compact Power Divider using Defected Ground Structure for Wireless Applications,"Use of different types of defected ground structures (DCS) has been reported in this paper to design compact power dividers in microstrip medium. Unit cell's (of DGS) equivalent circuit has been used to evaluate the performance of power divider. Based on this approach, compact two-way equal power dividers have been designed in GSM (900 MHz) band. Results show a size reduction of 35% and 32% for the power dividers using T shaped DGS and split ring DGS over the conventional power divider.",2008,0,
233,234,Utilisation of motion similarity in Colour-plus-Depth 3D video for improved error resiliency,"Robust 3D stereoscopic video transmission over error-prone networks has been a challenging task. Sustainability of the perceived 3D video quality is essential in case of channel losses. Colour-plus-Depth format on the other hand, has been popular for representing the stereoscopic video, due to its flexibility, low encoding cost compared to left-right stereoscopic video and backwards compatibility. Traditionally, the similarities existing between the colour and the depth map videos are not exploited during 3D video coding. In other words, both components are encoded separately. The similarities include the similarity in motion, image gradients and segments. In this work, we propose to exploit the similarity in the motion characteristics of the colour and the depth map videos by computing only a set of motion vectors and duplicating it for the sake of error resiliency. As the previous research has shown that the stereoscopic video quality is primarily affected by the colour texture quality, especially the motion vectors are computed for the colour video component and the corresponding vectors are used to encode the depth maps. Since the colour motion vectors are protected by duplication, the results have shown that both the colour video quality and the overall stereoscopic video quality are maintained in error-prone conditions at the expense of slight loss in depth map video coding performance. Furthermore, total encoding time is reduced by not calculating the motion vectors for depth map.",2010,0,
234,235,Embryonics+immunotronics: a bio-inspired approach to fault tolerance,"Fault tolerance has always been a standard feature of electronic systems intended for long-term missions. However, the high complexity of modern systems makes the incorporation of fault tolerance a difficult task. Novel approaches to fault tolerance can be achieved by drawing inspiration from nature. Biological organisms possess characteristics such as healing and learning that can be applied to the design of fault-tolerant systems. This paper extends the work on bio-inspired fault-tolerant systems at the University of York. It is proposed that by combining embryonic arrays with an immune inspired network, it is possible to achieve systems with higher reliability",2000,0,
235,236,"Video image based attenuation correction for PETbox, a preclinical PET tomograph","PETBox is a new simplified bench top PET scanner dedicated for pre-clinical imaging of mice. It has only two facing detector heads in a static gantry. Using iterative methods, limited-angle reconstruction of 3D images is possible. The geometry of the PETBox is such that very oblique emission angles are detected traversing significant lengths of tissue, making attenuation correction necessary. To that effect, we have developed a method by which two orthogonal optical views are combined to create a 3-dimensional estimate of the subject. This estimate is used to produce attenuation correction data that significantly improve the quantitative accuracy of the reconstructed images. In this paper, we present the method and evaluate its accuracy.",2009,0,
236,237,Analysis and design of SEPIC converter in boundary conduction mode for universal-line power factor correction applications,"In this paper, a SEPIC converter operated in boundary conduction mode for power factor correction applications with arbitrary output voltage is proposed, analyzed and designed. By developing an equivalent circuit model for the coupled inductor structure, a SEPIC converter with or without coupled inductors (and ripple current steering) can be analyzed and designed in a unified framework. Power factor correction under boundary conduction operation mode can be achieved conveniently using a simple commercially available control IC. Experimental results are provided to validate the circuit design",2001,0,
237,238,Categorization of minimum error forecasting zones using a geostatistic wind model,"In this paper a geostatistic wind direction model is applied to trace a wind speed map, based on data from official measurement weather stations distributed within the region of Andalucia-Spain. Each station's performance is assessed by comparing real measurements to those resulting from the linear interpolation of the rest. Once an error is associated to the station, the error is drawn in a map, in which minimum error zones can be delimited. Frequency and wind speed in each direction are the magnitudes of interest to get a first categorization of wind resources associated to the region. The interest of the method relies in the possibility of forecasting everywhere within the region with an error inside the tolerable margins.",2009,0,
238,239,A novel feature extraction and optimisation method for neural network-based fault classification in TCSC-compensated lines,"The suitability of fault classifiers introduced hitherto to operate correctly under a real TCSC transmission system remains a challenge since the computations are determined based on a number of postulations. This paper describes an alternative approach to fault classification in TCSC tines using artificial neural networks (ANNs). Special emphasis is placed on illustrating a combined wavelet transform and selforganising map (SOM) methodology to extract, validate and optimise the key characteristics of the fault transient phenomena in a TCSC line such that the input features to the ANNs are near optimal. As a result, it is shown that the fault classification proposed provides the ability to accurately classify the fault type, obviating the need for any predefined assumptions. Extensive simulation studies have been made to verify that the proposed method is both powerful and appropriate for fault classification.",2002,0,
239,240,Thermoreflectance imaging of defects in thin-film solar cells,We have identified and characterized various defects in thin-film a-Si and CIGS solar cells with sub-micron spatial resolution using thermoreflectance imaging. A megapixel silicon-based CCD was used to obtain noncontact thermal images simultaneously with visible electroluminescence (EL) images. EL can be indicative of pre-breakdown sites due to trap assisted tunneling and stress induced leakage currents. Physical defects appear at reverse bias voltages of 8 V in a-Si samples. Linear and nonlinear shunt defects are investigated as well as electroluminescent breakdown regions at reverse biases as low as 4.5 V. Pre-breakdown sites with electroluminescence are investigated.,2010,0,
240,241,Induction Motor-Drive Systems with Fault Tolerant Inverter-Motor Capabilities,"A low-cost fault tolerant drive topology for low- speed applications such as ""self-healing/limp-home"" needs for vehicles and propulsion systems, with capabilities for mitigating transistor open-circuit switch and short-circuit switch faults is presented in this paper. The present fault tolerant topology requires only minimum hardware modifications to the conventional off-the-shelf six-switch three-phase drive, with only the addition of electronic components such as triacs/SCRs and fast-acting fuses. In addition, the present approach offers the potential of mitigating not only transistor switch faults but also drive related faults such as rectifier diode short-circuit fault or dc link capacitor fault. In this new approach, some of the drawbacks associated with the known fault mitigation techniques such as the need for accessibility to a motor neutral, overrating the motor to withstand higher fundamental rms current magnitudes above its rated rms level, the need for larger size dc link capacitors, or higher dc bus voltage, are overcome here using the present approach. Given in this paper is a complete set of simulation results that demonstrate the soundness and effectiveness of the present topology.",2007,0,
241,242,On the relationships of faults for Boolean specification based testing,"Various methods of generating test cases based on Boolean specifications have previously been proposed. These methods are fault-based in the sense that test cases are aimed at detecting particular types of faults. Empirical results suggest that these methods are good at detecting particular types of faults. However, there is no information on the ability of these test cases in detecting other types of faults. The paper summarizes the relationships of faults in a Boolean expression in the form of a hierarchy. A test case that detects the faults at the lower level of the hierarchy will always detect the faults at the upper level of the hierarchy. The hierarchy helps us to better understand the relationships of faults in a Boolean expression, and hence to select fault-detecting test cases in a more systematic and efficient manner",2001,0,
242,243,Error sources in in-plane silicon tuning-fork MEMS gyroscopes,"This paper analyzes the error sources defining tactical-grade performance in silicon, in-plane tuning-fork gyroscopes such as the Honeywell-Draper units being delivered for military applications. These analyses have not yet appeared in the literature. These units incorporate crystalline silicon anodically bonded to a glass substrate. After general descriptions of the tuning-fork gyroscope, ordering modal frequencies, fundamental dynamics, force, and fluid coupling, which dictate the need for vacuum packaging, mechanical quadrature, and electrical coupling are analyzed. Alternative strategies for handling these engineering issues are discussed by introducing the Systron Donner/BEI quartz rate sensor, a successful commercial product, and the Analog Device (ADXRS), which is designed for automotive applications.",2006,0,
243,244,Flexible Error Concealment for H.264 Based on Directional Interpolation,"The losses of packets cannot he avoided if real-time video is transported over error prone environments. To conceal missing parts of video pictures, the spatial and temporal correlation feature of natural video sequences is used. However, in some cases - for instance in case of a scene change - there is no temporal correlation available and thus spatial error concealment has to be used. This article proposes flexible spatial error concealment based on directional interpolation method that performs well also if only two neighboring boundaries are used as common for H.264 spatially predicted frames. The proposed method was implemented and tested in a H.264 codec together with other error concealment methods to evaluate their performance",2005,0,
244,245,Nonlinear observers with approximately linear error dynamics: the multivariable case,"Exact error linearization uses nonlinear input-output injection to design observers with linear error dynamics in certain coordinates. This approach can only be applied nongenerically. We propose an observer for a wider class of multivariable systems which uniformly minimizes the nonlinear part of the system that cannot be canceled by nonlinear input-output injection. Our approach is numerical, constructive, and provides locally exponentially stable error dynamics. An example compares our design with a high-gain method",2001,0,
245,246,Time-Varying Network Fault Model for the Design of Dependable Networked Embedded Systems,"Dependability is becoming a key design aspect of today networked embedded systems (NES's) due to their increasing application to safety-critical tasks. Dependability evaluation must be based on modelling and simulation of faulty application behaviors, which must be related to faulty NES behaviors under actual defects. However, NES's behave differently from traditional embedded systems when testing activities are performed on them. In particular, issues arise on the definition of correct behavior, on the best point to observe it, and on the temporal properties of the faults to be injected. The paper describes these issues, discusses some possible solutions and presents a new time-varying network-based fault model to represent failures in a more abstract and efficient way. Finally, the fault model has been used to support the design of a network-based control application where packet losses, end-to-end delay and signal distortion must be carefully controlled.",2009,0,
246,247,Pilot signal-based real-time measurement and correction of phase errors caused by microwave cable flexing in planar near-field tests,Millimeter and submillimeter wave receivers in scanning planar near-field test systems are commonly based on harmonic mixing and thus require at least one flexible microwave cable to be connected to them. The phase errors originated in these cables get multiplied and added to the phase of the final detected signal. A complete submillimeter setup with on-the-fly measurement of phase errors is presented. The novel phase error correction system is based on the use of a pilot signal to measure the phase errors caused by cable flexing. The measured phase error surface in the quiet-zone region of a 310 GHz compact antenna test range (CATR) based on a hologram is shown as an application example. The maximum measured phase error due to the cable within a 8090 cm<sup>2</sup> scan area was 38.,2003,0,
247,248,Research about Software Fault Injection Technology Based on Distributed System,"Firstly, the paper made a contrast between the current domestic and international research condition, and introduced the basic concept of fault injection and distributed system. secondly, it discussed the classification and requirements of fault injection. There are mainly three types of distributed fault, namely the memory fault, CPU fault and correspondence fault. Besides, it discussed the method of distributed software fault injection about DOCTOR and illustrated the comprehensive structure and its respective parts of DOCTOR in detail. Thirdly, it reached a conclusion about the fault model of distributed system of fault injection and its realization method.",2010,0,
248,249,Fault detection and location of open-circuited switch faults in matrix converter drive systems,"Matrix converter based electric vehicles can be effectively applied to military vehicles due to weight and volume reduction as well as high temperature operation with no dc-bus capacitors fragile in a harsh environment. For successful applications for military vehicle areas, satisfactory reliability issues have to be incorporated into the matrix converter drives. This paper proposes a fault diagnostic technique for detecting and locating open-circuited faults in switching components of matrix converter drive systems. In this paper, the fault-mode behaviors of the matrix converter are, in detail, explored under the open-circuited switch fault conditions. Based on the investigated knowledge of the converter behaviors, the proposed scheme enables the matrix converter drive to detect and exactly identify power switches in which open-circuited faults have occurred. The proposed fault diagnostic algorithm is based on monitoring nine voltage errors assigned to nine bi-directional switches of the matrix converter. The voltage error signals are constructed with simple comparison of measured input and output voltages. In case that any of bi-directional switches are associated with open-circuited switch faults, the dedicated voltage error signals rise over a certain threshold value, which can be possible to detect a fault occurrence and locate the faulty switch. Since the developed diagnostic method requires no construction of reference output voltages from the pulsewidth modulation (PWM) reference signals, it can be implemented with simple and robust features. Verification results are presented to demonstrate the feasibility of the proposed technique.",2009,0,
249,250,Provisioning fault-tolerant scheduled lightpath demands in WDM mesh networks,"In this paper, we consider the problem of routing and wavelength assignment (RWA) of fault-tolerant scheduled lightpath demands (FSLDs) in all optical wavelength division multiplexing (WDM) networks under single component failure. In scheduled traffic demands, besides the source, destination, and the number of lightpath demands between a node-pair, their set-up and tear-down times are known, in this paper, we develop integer linear programming (ILP) formulations for dedicated and shared scheduled end-to-end protection schemes under single link/node failure for scheduled traffic demand with two different objective functions: 1) minimize the total capacity required for a given traffic demand while providing 100% protection for all connections; and 2) given a certain capacity, maximize the number of demands accepted while providing 100% protection for accepted connections. The ILP solutions schedule both the primary and end-to-end protection routes and assign wavelengths for the duration of the traffic demands. As the time disjointness that could exist among fault-tolerant scheduled lightpath demands is captured in our formulations, it reduces the amount of global resources required. The numerical results obtained from CPLEX indicate that dedicated scheduled (with set-up and tear-down times) protection provides significant savings (up to 33 %) in capacity utilization over dedicated conventional (without set-up and tear-down times) end-to-end protection scheme; shared scheduled protection provides considerable savings (up to 21 %) in capacity utilization over shared conventional end-to-end protection schemes. Also the numerical results indicate that shared scheduled protection achieves the best performance followed by dedicated scheduled protection scheme, and shared conventional end-to-end protection in terms of the number of requests accepted, for a given network capacity.",2004,0,
250,251,Induced error-correcting code for 2 bit-per-cell multi-level DRAM,"Traditionally, memories employ SEC-DED (Single Error Correcting and Double Error Detecting) Error Correcting Codes (ECC). While such codes have been considered for MLDRAM (Multi-Level Dynamic Random Access Memory), their use is inefficient, due to likely double-bit errors in a single cell. For this reason we propose an induced ECC architecture that uses ECC in such a way that no common error corrupts two bits. Induced ECC allows significant increase in reliability of the MLDRAM",2001,0,
251,252,Design of Timing Error Detectors for Orthogonal Space-Time Block Codes,"We present a method for the design of low complexity timing error detectors in orthogonal space-time block coding (OSTBC) receivers. A general expression for the S-curve of timing error detectors is derived. Based on this result, we obtain sufficient conditions for a difference of threshold crossings timing estimate that is robust to channel fading. A number of timing error detectors for 3- and 4-transmit antenna codes are presented. The performance is evaluated by examining their tracking capabilities within a timing loop of an OSTBC receiver. Symbol-error-rate results are presented showing negligible loss due to timing synchronization. In addition, we study the performance as a function of the timing drift and show that the receiver is able to track up to the normalized timing drift bandwidth of 0.001",2006,0,
252,253,Multi-Agent Fault Diagnosis in Manufacturing Systems Using Soft Computing,"The expeditious and accurate diagnosis of faults in manufacturing systems is essential in order to avoid expensive downtime. Many artificial intelligence approaches to automated fault diagnosis use techniques that are too computationally complex to achieve a diagnosis in real-time or are too inflexible for dynamic systems. Other approaches use either structural or symptom-based reasoning. Functional approaches are unable to provide real-time response due to their computational complexity, whereas, symptom-based approaches are only able to handle situations specifically coded in rules. Current hybrid approaches that combine the two methods are too structured in their approach to switching between the reasoning methods and, therefore fail to provide the flexible, rapid response of humans experts. This paper presents a robust, extensible approach to fault diagnosis that allows unstructured switching between reasoning methods using multiple fuzzy intelligent agents that examine the problem domain from a variety of perspectives.",2007,0,
253,254,Spatial error concealment algorithm based on improved SUSAN operator,"In the transmission of real-time video compressed streams, error concealment method is to restore the damaged or lost data packets. This paper improves the existing spatial error concealment algorithm based on SUSAN detection operator. On the one hand, as recovering the error, the detection pixels were reduced by considering the relationship of nearby pixels. On the other hand, more associated pixels were fully considered. The experimental results show that the proposed algorithm enhances the Peak Signal to Noise Ratio in the case of reducing 4%-8% computational complexity, and is more beneficial to real-time application.",2010,0,
254,255,"Impact of Channel Errors on Decentralized Detection Performance of Wireless Sensor Networks: A Study of Binary Modulations, Rayleigh-Fading and Nonfading Channels, and Fusion-Combiners","We provide new results on the performance of wireless sensor networks in which a number of identical sensor nodes transmit their binary decisions, regarding a binary hypothesis, to a fusion center (FC) by means of a modulation scheme. Each link between a sensor and the fusion center is modeled independent and identically distibuted (i.i.d.) either as slow Rayleigh-fading or as nonfading. The FC employs a counting rule (CR) or another combining scheme to make a final decision. Main results obtained are the following: 1) in slow fading, a) the correctness of using an average bit error rate of a link, averaged with respect to the fading distribution, for assessing the performance of a CR and b) with proper choice of threshold, on/off keying (OOK), in addition to energy saving, exhibits asymptotic (large number of sensors) performance comparable to that of FSK; and 2) for a large number of sensors, a) for slow fading and a counting rule, given a minimum sensor-to-fusion link SNR, we determine a minimum sensor decision quality, in order to achieve zero asymptotic errors and b) for Rayleigh-fading and nonfading channels and PSK (FSK) modulation, using a large deviation theory, we derive asymptotic error exponents of counting rule, maximal ratio (square law), and equal gain combiners.",2008,0,
255,256,A signature-based approach for diagnosis of dynamic faults in SRAMs,"This paper focuses on diagnosis of dynamic faults in SRAMs. The current techniques for fault diagnosis are mainly based on the signature method. Here, we introduce an extension of the signature scheme by taking in account additional information related to the addressing order during March test execution. A first advantage of the proposed approach is its capability to distinguish between static and dynamic faults. Another main feature is the correct identification of the location of the failure in a given memory component: the core-cell array, write drivers, sense amplifiers, address decoders and pre- charge circuits. Moreover, since this approach does not modify the March test, there is no increase of test complexity, conversely to other existing diagnosis techniques.",2008,0,
256,257,Defect tolerance for gracefully-degradable microfluidics-based biochips,"Defect tolerance is an important design consideration for microfluidics-based biochips that are used for safety-critical applications. We propose a defect tolerance methodology based on graceful degradation and dynamic reconfiguration. We first introduce tile-based biochip architecture, which is scalable for large-scale bioassays. A clustered defect model is used to evaluate the graceful degradation method for tile-based biochips. The proposed schemes ensure that the bioassays mapped to a droplet-based microfluidic array during design can be executed on a defective biochip through operation rescheduling and/or resource rebinding. Real-life biochemical procedures, namely polymerase chain reaction (PCR) and multiplexed in-vitro diagnostics on human physiological fluids, are used to evaluate the proposed defect tolerance schemes.",2005,0,
257,258,"Decoding of the (24, 12, 8) extended golay code up to four errors","A new decoder is proposed to decode the (24, 12, 8) binary extended Golay code up to four errors. It consists of the conventional hard decoder for correcting up to three errors, the detection algorithm for four errors and the soft decoding for four errors. For a weight-4 error in a received 24-bit word, Method 1 or 2 is developed to determine all six possible error patterns. The emblematic probability value of each error pattern is then defined as the product of four individual bit-error probabilities corresponding to the locations of the four errors. The most likely one among these six error patterns is obtained by choosing the maximum of the emblematic probability values of all possible error patterns. Finally, simulation results of this decoder in additive white Gaussian noise show that at least 93% and 99% of weight-4 error patterns that occur are corrected if the two E<sub>b</sub>/N<sub>0</sub> ratios are greater than 2 and 5 dB, respectively. Consequently, the proposed method can achieve a better percentage of successful decoding for four errors at variable signal-to-noise ratios than Lu et al.'s algorithm in software. However, the speed of the method is slower than Lu et al.'s algorithm.",2009,0,
258,259,Joint Generalized Antenna Combination and Symbol Detection Based on Minimum Bit Error Rate: A Particle Swarm Optimization Approach,"In order to reduce hardware cost and achieve superior performance in multi-input multi-output (MIMO) systems, this paper proposes a novel scheme for joint antenna combination and symbol detection. More specifically, the new approach simultaneously determines the transformation weighting for antenna combination to lower the RF chains called for and to design the minimum bit error rate (MBER) detector to effectively mitigate the impairment due to interference. The joint decision statistic, however, is highly nonlinear and the particle swarm optimization (PSO) algorithm is employed to reduce the computational overhead. Conducted simulation results show that the new approach yields satisfactory performance with reduced computational overhead compared with pervious works.",2008,0,
259,260,Finite Element Analysis of Switched Reluctance Motor under Dynamic Eccentricity Fault,"This paper describes the results of a two-dimensional finite element analysis carried out on an 8/6 switched reluctance motor for studying the effects of dynamic eccentricity on the static characteristics of the motor. Flux contours, flux-linkage profiles and mutual fluxes are obtained for both healthy and faulty motor. Besides, Static torque profiles of phases are obtained for different degrees of eccentricity and it is shown that at low current; the effect of eccentricity is considerable compared to that of the rated current case. Finally, Fourier analysis of the torque profiles is performed to make their difference visible.",2006,0,
260,261,Effective congestion and error control for scalable video coding extension of the H.264/AVC,"We present an effective congestion and error control mechanism for scalable video coding (SVC) extension of the H.264/AVC video dissemination over Internet. The congestion control is used to determine the appropriate number of SVC video layers based on bandwidth inference congestion (BIC) control protocol for layered multicast scenarios and the error control is achieved by unequal forward error correction (FEC) layered protection using block erasure coding. Through the real Internet streaming experiments, we demonstrate the effectiveness of the proposed layered SVC delivery, in terms of subscription layer, average packet loss rate and PSNRs, under several layered-definition scalabilities.",2008,0,
261,262,Routability estimation of FPGA-based fault injection,"In the past years various approaches to hardware-based fault injection using FPGA-based hardware have been presented. Some approaches insert additional functions at the fault location (any location in the circuit, e.g. I/Os of components or their interconnection nets), while others utilize the reconfigurability of FPGAs. A common feature of each of these methods is the execution of hardware-based fault simulation using the stuck-at fault model at gate level. The expansion of a circuit by insertion of additional functions at the fault location constitutes an overhead of FPGA resources. An optimized mapping of the circuit into an FPGA and a routable placement in the FPGA is difficult to achieve due to the generation of additional functions at the fault locations. Therefore, an optimized assignment of the fault locations to the FPGA-resources (configurable logic blocks, look-up tables, I/O blocks, etc.) precedes and thereby guarantees the mapping and routability of very large circuits in an acceptable runtime. In this paper an approach to node assignment is introduced, which achieves a reduction in FPGA overhead as well as routability of the expanded circuit in a minimal runtime.",2003,0,
262,263,A Research on I.C. Engine Misfire Fault Diagnosis Based on Rough Sets Theory and Neural Network,"A method for diagnosis of misfire fault in internal combustion engine based on exhaust density of HC, CO2, O2 and the engines work parameters are presented in this paper. Rough sets theory is used to simplify attribute parameter reflecting exhaust emission and conditions of internal combustion engine and in which unnecessary properties are eliminated. The engines work parameters, exhaust emission with misfire fault and without fault are tested by the experimentation of CA6100 engine. A diagnosis model which describing the relationship between the misfire degree and the internal combustion engines exhaust emission and work parameters is established based on rough sets theory and RBF neural network. The model reduces the sample size, optimizes the neural network, increase the diagnosis correctness. The model is also trained by test data and MATLAB software. The model has been used to diagnosis internal combustion engine misfire fault, the result illustrates that this diagnosis model is suitable. This system can reduce input node number and overcome some shortcomings, such as neural network scale is too large and the rate of classification is slow.",2010,0,
263,264,High-Intensity Radiated Field fault-injection experiment for a fault-tolerant distributed communication system,"Safety-critical distributed flight control systems require robustness in the presence of faults. In general, these systems consist of a number of input/output (I/O) and computation nodes interacting through a fault-tolerant data communication system. The communication system transfers sensor data and control commands and can handle most faults under typical operating conditions. However, the performance of the closed-loop system can be adversely affected as a result of operating in harsh environments. In particular, High-Intensity Radiated Field (HIRF) environments have the potential to cause random fault manifestations in individual avionic components and to generate simultaneous system-wide communication faults that overwhelm existing fault management mechanisms. This paper presents the design of an experiment conducted at the NASA Langley Research Center's HIRF Laboratory to statistically characterize the faults that a HIRF environment can trigger on a single node of a distributed flight control system.",2010,0,
264,265,On-line fault diagnosis in a Petri Net framework,"The paper addresses the fault detection problem for discrete event systems modeled by Petri nets (PN). Assuming that the PN structure and initial marking are known, faults are modeled by unobservable transitions. The paper recalls a previously proposed diagnoser that works online and employs an algorithm based on the definition and solution of some integer linear programming problems to decide whether the system behavior is normal or exhibits some possible faults. To reduce the on-line computational effort, we prove some results showing that if the unobservable subnet enjoys suitable properties, the algorithm solution may be obtained with low computational complexity. We characterize the properties that the PN modeling the system fault behavior has to fulfill and suitably modify the proposed diagnoser.",2009,0,
265,266,The prediction of fault currents in a large multiwinding reactor transformer,"The fault currents occurring in power transformers are determined by the leakage reactances within the windings. Where the transformers are used in a phase-shifting mode, there is additional coupling between phases which influences the fault current. This work describes the modelling of the self and mutual inductances in a 90 MVA autotransformer with a tertiary winding, on the assumption that the airgap formed by the transformer window dictates the reluctance of the leakage flux paths. Recordings made during a short-circuit between two phases of the tertiary winding show a remarkably close comparison with the predicted waveforms.",2003,0,
266,267,How Long Will It Take to Fix This Bug?,"Predicting the time and effort for a software problem has long been a difficult task. We present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. Our technique leverages existing issue tracking systems: given a new issue report, we use the Lucene framework to search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach using effort data from the JBoss project. Given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating naive predictions by a factor of four.",2007,0,
267,268,Data reduction and clustering techniques for fault detection and diagnosis in automotives,"In this paper, we propose a data-driven method to detect anomalies in operating Parameter Identifiers (PIDs) and in the absence of any anomaly, classify faults in automotive systems by analyzing PIDs collected from the freeze frame data. We first categorize the operating parameter data using automotive domain knowledge. The dataset thus obtained is then analyzed using Principal Component Analysis (PCA) and Independent Component Analysis (ICA) for finding coherence among the PIDs. Then we use clustering algorithms based on both linear distance and information theoretic measures to assign coherent PIDs to the same class or cluster. A comparative analysis of the behavior of PIDs belonging to the same cluster can now be made for detecting anomaly in PIDs. Since a system fault is characterized by the values by of all PIDs across all the clusters, we use the joint probability distribution of the independent components of all PIDs to characterize the fault and find the divergence between the joint distributions of training and test data to classify faults. The proposed method can analyze available parameter data, categorize PIDs into informative or non-informative category, and detect fault condition from the clusters. We demonstrate the algorithm by way of an application to operating parameter data collected during faults in catalytic converters of vehicles.",2010,0,
268,269,Effect of atmospheric correction for different land use on Landsat 7 ETM+ satellite imagery,"Various changes in the atmosphere of the earth and different illuminations resulting from rough terrain change the spectral reflection values of satellite images. Studies making use of real reflection values belonging to the object will provide more accurate data. The atmospheric correction process to be applied in this study is used to prevent the negative effects resulting from atmosphere and different illuminations in order to represent the reflections from the ground on the image in the best way possible. Using atmospheric correction, differentiations in reflection values sensed by different sensors or platforms resulting from atmosphere and some technical problems will be prevented. In this study, the aim is to determine the changes in the spectral reflection values concerning land use following the atmospheric correction to be applied on Landsat image data. For this reason, atmospheric correction was applied on Landsat image data. The relations of each band with each other before and after the correction were determined. The changes between spectral reflection values of all bands before and after correction regarding three different land uses as forest, agricultural area and residential area were examined visually and statistically.",2009,0,
269,270,An Efficient Fault Tolerance Scheme for Preventing Single Event Disruptions in Reconfigurable Architectures,"Reconfigurable architectures are becoming increasingly popular with space related design engineers as they are inherently flexible to meet multiple requirements and offer significant performance and cost savings for critical applications. As the microelectronics industry has advanced, integrated circuit (IC) design and reconfigurable architectures (FPGAs, reconfigurable SoC and etc) have experienced dramatic increase in density and speed. These advancements have serious implications for the reconfigurable architectures when used in space environment where IC is subject to total ionization dose (TID) and single event effects as well. Due to transient nature of single event upsets (SEUs), these are most difficult to avoid in space-borne reconfigurable architectures. We present a unique SEU fault tolerance technique based upon double redundancy with comparison to overcome the overheads associated with the conventional schemes",2006,0,
270,271,Anshan: Wireless Sensor Networks for Equipment Fault Diagnosis in the Process Industry,"Wireless sensor networks provide an opportunity to enhance the current equipment diagnosis systems in the process industry, which have been based so far on wired networks. In this paper, we use our experience in the Anshan Iron and Steel Factory, China, as an example to present the issues from the real field of process industry, and our solutions. The challenges are three fold: First, very high reliability is required; second, energy consumption is constrained; and third, the environment is very challenging and constrained. To address these issues, it is necessary to put systematic efforts on network topology and node placement, network protocols, embedded software, and hardware. In this paper, we propose two technologies i.e. design for reliability and energy efficiency (DRE), and design for reconfiguration (DRC). Using these techniques we developed Anshan, a wireless sensor network for monitoring the temperature of rollers in a continuously annealing line and detecting equipment failures. Project Anshan includes 406 sensor nodes and has been running for four months continuously.",2008,0,
271,272,Heterogeneous Error Protection of H.264/AVC Video Using Hierarchical 16-QAM,"Heterogeneous error protection (HEP) of H.264/AVC coded video is investigated using hierarchical quadrature amplitude modulation (HQAM), which takes into consideration the non- uniformly distributed importance of intracoded frame (I-frame) and predictive coded frame (P-frame) as well as the sensitivity of the coded bitsream against transmission errors. The HQAM constellation are used to give different degrees of error protection of the most important information of the video content. The performance of the transmission system is evaluated under additive Gaussion Noise (AWGN). The simulation results indicate that the strategy produces a high quality of the reconstructed video data compared with uniform protection.",2009,0,
272,273,A design of the novel coupled line bandpass filter using defected ground structure,"In this paper, a novel coupled line bandpass filter with a DGS (Defected Ground Structure) is proposed to realize a compact size with low insertion loss characteristic. The proposed bandpass filter can provide an attenuation pole due to the resonance characteristic of the DGS. The equivalent circuit parameters for the DGS are extracted by using an EM simulation process and the circuit analysis method. The design method for the proposed 3-pole bandpass filter is derived based on coupled line filter theory and the derived equivalent circuit of the DGS. The experimental results show an excellent agreement with theoretical simulation results.",2000,0,
273,274,The use of characteristic features of wireless cellular networks for transmission of GNSS assistance and correction data,"Precise Global Navigation Satellite System (GNSS) positioning using Real Time Kinematics (RTK) correction data is currently utilized in many fields of surveying, mapping and precision agriculture. In the near future, sub decimeter precision data usage is expected to extend to autonomous vehicles navigation and public safety areas. To satisfy this increasing demand of precision positioning correction bandwidth, new techniques and protocols in assistance and correction data transmission are needed. This paper reviews one such possible technique involving sending correction dataset via public wireless cellular networks. The data will be transmitted through a hybrid system integrating correction data broadcasted in the wireless cellular network control plane with AGNSS assistance data and correction metadata in the user plane. Through this system, the bandwidth intensive, low refresh rate data of GNSS system ephemeris, reference station and satellite identification is omitted from the main data stream. Instead, a constant bit rate (CBR) stream for correction data is used and bandwidth is conserved. The results show that the proposed system can achieve scalability required for widespread usage of sub decimeter level positioning data from GNSS.",2010,0,
274,275,An on-line monitoring and multi-layer fault diagnosis system of electrical equipment based on geographic information system,"Automated mapping/facilities management/geographic information system (AM/FM/GIS), which provides a powerful way to process graphic and non-graphic information, can construct a spatial database system with topological structure and analysis function by combining diversified information in power system with geographic position-related graphic information. Based on the AM/FM/GIS and on-line monitoring system, an integrated system is put forward which can implement state monitoring, multi-layer fault diagnosis and assess the faults. By using this integrated system, latent fault and defect can be eliminated, loss due to power cut is reduced and the reliability of running power system is improved. Application indicates it is economical, pragmatic and has excellent performance.",2005,0,
275,276,Coseismic fault rupture detection and slip measurement by ASAR precise correlation using coherence maximization: application to a north-south blind fault in the vicinity of Bam (Iran),"Using the phase differences between satellite radar images recorded before and after an earthquake, interferometry allows mapping the projection along the line of sight (LOS) of the ground displacement. Acquisitions along multiple LOS theoretically allow deriving the complete deformation vector; however, due to the orbit inclination of current radar satellites, precision is poor in the north-south direction. Moreover, large deformation gradients (e.g., fault ruptures) prevent phase identification and unwrapping and cannot be measured directly by interferometry. Subpixel correlation techniques using the amplitude of the radar images allow measuring such gradients, both in slant-range and in azimuth. In this letter, we use a correlation technique based on the maximization of coherence for a radar pair in interferometric conditions, using the complex nature of the data. In the case of highly coherent areas, this technique allows estimating the relative distortion between images. Applied to ASAR images acquired before and after the December 26, 2003 Bam earthquake (Iran), we show that the near-field information retrieved by this technique is useful to constrain geophysical models. In particular, we confirm that the major gradients of ground displacement do not occur across the known fault scarp but approximately 3 km west of it, and we also estimate directly the amplitude of right lateral slip, while retrieving this value from interferometry requires passing through the use of a model for the earthquake fault and slip.",2006,0,
276,277,Reducing cost and tolerating defects in page-based intelligent memory,"Active Pages is a page-based model of intelligent memory specifically designed to support virtualized hardware resources. Previous work has shown substantial performance benefits from off loading data-intensive tasks to a memory system that implements Active Pages. With a simple VLIW processor embedded near each page on DRAM, Active Page memory systems achieve up to 1000X speedups over conventional memory systems. In this study, we examine Active Page memories that share, or multiplex, embedded VLIW processors across multiple physical Active Pages. We explore the trade-off between individual page-processor performance and page-level multiplexing. We find that hardware costs of computational logic can be reduced from 31% of DRAM chip area to 12%, through multiplexing, without significant loss in performance. Furthermore, manufacturing defects that disable up to 50% of the page processors can be tolerated through efficient resource allocation and associative multiplexing",2000,0,
277,278,A defect-to-yield correlation study for marginally printing reticle defects in the manufacture of a 16Mb flash memory device,"This paper presents a defect-to-yield correlation for marginally printing defects in a gate and a contact 4X DUV reticle by describing their respective impact on the lithography manufacturing process window of a 16Mb flash memory device. The study includes site-dependent sort yield signature analysis within the exposure field, followed by electrical bitmap and wafer strip back for the lower yielding defective sites. These defects are verified using both reticle inspection techniques and review of printed resist test wafers. Focus/exposure process windows for defect-free feature and defective feature are measured using both in-line SEM CD data and defect printability simulation software. These process window models are then compared against wafer sort yield data for correlation. A method for characterizing the lithography manufacturing process window is proposed which is robust to both marginally printing reticle defects and sources of process variability outside the lithography module",2000,0,
278,279,PEDS: A Parallel Error Detection Scheme for TCAM Devices,"Ternary content-addressable memory (TCAM) devices are increasingly used for performing high-speed packet classification. A TCAM consists of an associative memory that compares a search key in parallel against all entries. TCAMs may suffer from error events that cause ternary cells to change their value to any symbol in the ternary alphabet ""0"",""1"",""*"". Due to their parallel access feature, standard error detection schemes are not directly applicable to TCAMs; an additional difficulty is posed by the special semantic of the ""*"" symbol. This paper introduces PEDS, a novel parallel error detection scheme that locates the erroneous entries in a TCAM device. PEDS is based on applying an error-detection code to each TCAM entry, and utilizing the parallel capabilities of the TCAM, by simultaneously checking the correctness of multiple TCAM entries. A key feature of PEDS is that the number of TCAM lookup operations required to locate all errors depends on the number of symbols per entry rather than the (orders-of-magnitude larger) number of TCAM entries. For large TCAM devices, a specific instance of PEDS requires only 200 lookups for 100-symbol entries, while a naive approach may need hundreds of thousands lookups. PEDS allows flexible and dynamic selection of trade-off points between robustness, space complexity, and number of lookups.",2009,0,
279,280,Noise identification and fault diagnosis for the new products of the automobile gearbox,"A noise identification and fault diagnosis system for the new products of the automobile gearbox is introduced. The framework of the developed software is described, which includes function modules as data acquisition, feature extracting, time frequency transform, order analysis, learning and training, and so on. The prototype system has been partially put in practice in a certain automobile gear-box manufacture company.",2009,0,
280,281,Soft Defects: Challenge and Chance for Failure Analysis,"Failure analysis on advanced logic and mixed signal ICs more and more has to deal with so called 'soft defects'. In this paper, an analysis flow especially for parameter dependent scan fails is presented. For the two major localization techniques, namely soft defect localization (SDL) and internal signal measurement enhanced activation and localization procedures using test systems are proposed.",2007,0,
281,282,Layout to Logic Defect Analysis for Hierarchical Test Generation,"As shown by previous studies, shorts between the interconnect wires should be considered as the predominant cause of failures in CMOS circuits. Fault models and tools for targeting these defects, such as the bridging fault test pattern generators have been available for a long time. However, this paper proposes a new hierarchical approach based on critical area extraction for identifying the possible shorted pairs of nets on the basis of the chip layout information, combined with logic-level test pattern generation for bridging faults. Experiments on real design layouts will show that only a fraction of all the possible pairs of nets have non-zero shorting probabilities. Furthermore, it will also be proven at the logic-level that nearly all such bridging faults can be tested by a simple and robust one-pattern logic test. The methods proposed in this paper are supported by a design flow implementing existing commercial and academic CAD software.",2007,0,
282,283,A Fault Propagation Approach for Highly Distributed Service Compositions,"Today, the techniques for realizing service compositions (e.g. WS-BPEL) have become mature. Nevertheless, when it comes to execution faults within service compositions, many problems are still unsolved. Especially the propagation and global handling of errors in service compositions yet remains an open issue. In this paper, we describe some preliminary results of our ongoing work in the field of fault propagation and exception handling in service compositions. We provide some service classification criteria and show how they relate to service composition fault handling. Further, we present a fault propagation approach for service compositions.",2008,0,
283,284,Using design based binning to improve defect excursion control for 45nm production,"For advanced device (45 nm and below), we proposed a novel method to monitor systematic and random excursion. By integrating design information and defect inspection results into automated software (DBB), we can identify design/process marginality sites with defect inspection tool. In this study, we applied supervised binning function (DBC) and defect criticality index (DCI) to identify systematic and random excursion problems on 45 nm SRAM wafers. With established SPC charts, we will be able to detect future excursion problem in manufacturing line early.",2007,0,
284,285,Comparison of Outlier Detection Methods in Fault-proneness Models,"In this paper, we experimentally evaluated the effect of outlier detection methods to improve the prediction performance of fault-proneness models. Detected outliers were removed from a fit dataset before building a model. In the experiment, we compared three outlier detection methods (Mahalanobis outlier analysis (MOA), local outlier factor method (LOFM) and rule based modeling (RBM)) each applied to three well-known fault-proneness models (linear discriminant analysis (LDA), logistic regression analysis (LRA) and classification tree (CT)). As a result, MOA and RBM improved Fl-values of all models (0.04 at minimum, 0.17 at maximum and 0.10 at mean) while improvements by LOFM were relatively small (-0.01 at minimum, 0.04 at maximum and 0.01 at mean).",2007,0,
285,286,Algorithm-based fault tolerance for spaceborne computing: basis and implementations,"We describe and test the mathematical background for using checksum methods to validate results returned by a numerical subroutine operating in a fault-prone environment that causes unpredictable errors in data. We can treat subroutines whose results satisfy a necessary condition of a linear form; the checksum tests compliance with this necessary condition. These checksum schemes are called algorithm-based fault tolerance (ABFT). We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent in finite-precision numerical calculations. Two series of tests are described. The first tests the general effectiveness of the linear ABFT schemes we propose, and the second verifies the correct behavior of our parallel implementation of them. We find that under simulated fault conditions, it is possible to choose a fault detection scheme that for average case matrices can detect 99% of faults with no false alarms, and that for a worst-case matrix population can detect 80% of faults with no false alarms",2000,0,
286,287,Bug busters,"One way to deal with bugs is to avoid them entirely. The approach would be wasteful because we'd be underutilizing the many automated tools and techniques that can catch bugs for us. Most tools for eliminating bugs work by tightening the specifications of what we build. At the program code level, tighter specifications affect the operations allowed on various data types, our program's behavior, and our code's style. Furthermore, we can use many different approaches to verify that our code is on track: the programming language, its compiler, specialized tools, libraries, and embedded tests are our most obvious friends. We can delegate bug busting to code. Many libraries come with hooks or specialized builds that can catch questionable argument values, resource leaks, and wrong ordering of function calls. Bugs many be a fact of life, but they're not inevitable. We have some powerful tools to find them before they mess with our programs, and the good news is that these tools get better every year.",2006,0,
287,288,Design and validation of portable communication infrastructure for fault-tolerant cluster middleware,"We describe the communication infrastructure (CI) for our fault-tolerant cluster middleware, which is optimized for two classes of communication: for the applications and for the cluster management middleware. This CI was designed for portability and for efficient operation on top of modern user-level message passing mechanisms. We present a functional fault model for the CI and show how platform-specific faults map to this fault model. Based on this fault model, we have developed a fault injection scheme that is integrated with the CI and is thus portable across different communication technologies. We have used fault injection to validate and evaluate the implementation of the CI itself as well as the cluster management middleware in the presence of communication faults.",2002,0,
288,289,Effect of rotor position error on commutation in sensorless BLDC motor drives,"In this paper, two kinds of commutation modes of the brushless DC(BLDC) motor drives, the delaying commutation and the leading commutation, are discussed in detail. The current of the unexcited phase is calculated under an ideal operation condition, and the condition of circulating current occurring is analyzed. The result with the compensated commutation is provided. The theoretical analysis is confirmed by the experiment results.",2005,0,
289,290,A Fault-Tolerant Active Pixel Sensor to Correct In-Field Hot-Pixel Defects,"Solid-state image sensors develop in-field defects in all common environments. Experiments have demonstrated the growth of significant quantities of hot-pixel defects that degrade the dynamic range of an image sensor and potentially limit low-light imaging. Existing software- only techniques for suppressing hot-pixels are inadequate because these defective pixels saturate at relatively low illumination levels. The redundant fault-tolerant active pixel sensor design is suggested to isolate point-like hot-pixel defects. Emulated hot-pixels have been induced in hardware implementations of this pixel architecture and measurements of pixel response indicate that it generates an accurate output signal throughout the sensor's entire dynamic range, even when standard pixels would be otherwise saturated by the hot defect. A correction algorithm repairs the final image by building a simple look-up table of illumination- response of a working pixel. In emulated hot-pixels, the true illumination value can be recovered with an error of plusmn5% under typical conditions.",2007,0,
290,291,Automatic error recovery in targetless logic emulation,"Targetless logic emulation refers to a verification system in which there are no external hardware targets interfacing with the emulator. In such systems input stimuli to the DUT come either from a user provided vector file or a HDL testbench running on a software simulator and the DUT runs on hardware based logic emulator. Many users use such targetless environment for automated long running verification tests consisting of huge sets of input stimuli, consequently an automatic recovery method is of significant interest in such systems. The automatic error recovery method shall be able to complete the emulation session gracefully skipping error points and subsequently report various errors and mismatch conditions for user debug. The paper presents a novel methodology and verification infrastructure based on periodic checkpointing, which provides a robust way of error condition detection, subsequent restoration of last saved system state and resume emulation run by skipping offending operations. It does not require any special hardware extension and provides a fully customizable checkpoint frequency selection scheme. It is seen to add only a minimal overhead on overall hardware emulation speed.",2009,0,
291,292,Weather radar equation correction for frequency agile and phased array radars,"This paper presents the derivation of a correction to the Probert-Jones weather radar equation for use with advanced frequency agile, phased array radars. It is shown that two additional terms are required to account for frequency hopping and electronic beam pointing. The corrected weather radar equation provides a basis for accurate and efficient computation of a reflectivity estimate from the weather signal data samples. Lastly, an understanding of calibration requirements for these advanced weather radars is shown to follow naturally from the theoretical framework.",2007,0,
292,293,Waveform analysis of the bridge type SFCL during load changing and fault time,"DC reactor type superconducting fault current limiter (SFCL) has drawn the interest of some researchers in developing such device and more research work is being carried out in order to make it practically feasible. We have pointed out one issue that is not properly examined yet on such a device during load changing time. As we know, it is very difficult to introduce DC bias voltage to the reactor coil of the bridge type SFCL and some researchers are developing such device without using DC bias current. In such a case, the voltage drop occurs at the load terminal during the load increasing time caused by the DC reactor's inductance. By using the Electro-Magnetic Transients in DC systems which is the simulator of electric networks (EMTDC) software we carried out analysis of first few half cycles of the voltage and current waveforms after the load is increased. We also performed the same analysis for fault conditions. The peak value of the waveforms is considered in calculating the voltage drop at load terminal during the load changing time. The analysis can be used in selecting an appropriate inductance value for designing such SFCL.",2003,0,
293,294,DuoTracker: Tool Support for Software Defect Data Collection and Analysis,"In today software industry defect tracking tools either help to improve an organizationAs software development process or an individualAs software development process. No defect tracking tool currently exists that help both processes. In this paper we present DuoTracker, a tool that makes possible to track and analyze software defects for organizational and individual software process decision making. To accomplish this, DuoTracker has capabilities to classify defects in a manner that makes analysis at both organizational and individual software processes meaningful. The benefit of this approach is that software engineers are able to see how their personal software process improvement impacts their organization and vice versa. This paper shows why software engineers need to keep track of their program defects, how this is currently done, and how DuoTracker offers a new way of keeping track of software errors. Furthermore, DuoTracker is compared to other tracking tools that enable software developers to record program defects that occur during their individual software processes.",2006,0,
294,295,Investigation of the effects of transmission faults upon a renewable energy generating plant,"In recent years the number of renewable energy generators connected to Ireland's electricity grid has steadily increased. The Republic of Ireland is now expected to source 13.2% of the electricity it consumes from renewables by 2010, which represents a significant challenge to the electricity system operators and planners. This paper describes the modelling and simulation of a small hybrid wind/hydro generating plant connected to the distribution network. The effects upon the plant of transmission network faults and continuous voltage unbalance are investigated.",2005,0,
295,296,Detection of defects in wood slabs by using a microwave imaging technique,"In this paper, an experimental set up based on interrogating microwaves is used to obtain images of the cross section of dielectric cylinders. In particular, a microwave tomographic configuration is used to inspect wood slabs in order to search for defects and voids. The measured data (samples of the scattered electric field) are inverted by using an efficient reconstruction technique, which is able to handle the ill-posedness of the inverse scattering problem. The developed experimental apparatus is validated in this paper by means of several numerical simulations. Preliminary experimental results are also reported.",2007,0,
296,297,Influence of the Transmission Channel Parameters on Error Rates and Picture Quality in DVB Baseband Transmission,"The paper deals with the component analysis of DVB (digital video broadcasting) transmission model in baseband and its source, channel and link coding. The transmission channel model is based on the digital filter design and it can be designed with the variable transmission parameters (e.g. cut-off frequency) and linear distortions with additive noise and reflected signal. Results of achieved BER (bit error rate) and SER (symbol error rate) and corresponding PQE (picture quality evaluation) analysis are presented, including the evaluation of subjective picture quality influence on normalized cut-off frequency of the channel",2006,0,
297,298,Rotor position sensor fault detection Isolation and Reconfiguration of a Doubly Fed Induction Machine control,"In this paper, a Doubly Fed Induction Machine (DFIM) operating in motor mode and supplied by two Voltage Source Inverters (VSI), in stator and rotor sides, is presented. The aim is to analyze the position sensor fault effects on a Direct Torque Control (DTC) of the DFIM. This justifies the necessity of a reconfiguration control when a position sensor fault appears in order to avoid an interruption in system operations. In the other hand, this study emphasizes the close dependency between system performance and the output accuracy of the rotor position sensor. Moreover, simulation results point out the operation system deterioration in case of position sensor fault, which leads in most cases to its shut down in contrast to industrial expectations. This work presents a control reconfiguration for a DFIM speed drive when a position sensor fault occurs, in order to ensure system service continuity. For this purpose, SABER simulation results illustrate the system behavior before and after a position sensor fault. System performance preservation is carried out after control reconfiguration. The proposed solution is relevant especially due to its simplicity.",2009,0,
298,299,Computation and analysis of output error probability for C17 benchmark circuit using bayesian networks error modeling,"The reliability of digital circuits is in question since the new scaled transistor technologies continue to emerge. The major factor deteriorating the circuit performance is the random and dynamic nature of errors encountered during its operation. Output-error probability is the direct measure of circuit's reliability. Bayesian networks error modeling is the approach used to compute error probability of digital circuits. In our paper, we have used this technique to compute and analyze the output error probability of LGSynth's C17 benchmark circuit. The simulations are based on MATLAB and show important relationships among output-error probability, execution time and number of priors involved in the analysis.",2010,0,
299,300,Raising network fault management intelligence,"Most large network management centers have relatively low skilled personnel as their first level operations staff. Many organizations attempt to cope with this situation by restricting the set of problems these people have to deal with to those which are well understood and documented. Several software packages exist which can correlate and filter incoming events from the network and present a select subset to the operator. Unfortunately, programming these fault management applications requires considerable expertise and effort. Often, once the initial development is done, the implementation remains static, while the network itself is dynamic. This paper proposes a methodology for documenting known faults and responses, programming fault correlation engines, continuously examining real behavior, and feeding the result back into the programming process. This results in a continuous improvement in fault management intelligence, with corresponding improvement in network availability and thus value of the network to the organization",2000,0,
300,301,Analysis of Timing Error Aperture Jitter on the Performance of Sigma Delta ADC for Software Radio Mobile Receivers,"Jitter is the limiting effect for high speed analog-to digital converters with high resolution and wide digitization bandwidth, which are required in receivers in order to support high data rates. The rapid development of digital wireless system has led to a need of high resolution and high speed analog to digital converter. The proper selection of data converters, both analog to digital converters and digital to analog converters (DACs) is one of the most challenging steps in designing software radio. The performance of a data converter is dependent upon the accuracy and stability of the clock supplied to the circuits. When data converter employ a high sampling rate, clocking issues become magnified and significant distortion can be result. This paper describes the effect of aperture jitter on the performance of sigma delta ADC and present analytical evaluation of the performance and mean error power spectrum due to aperture jitter application has favored the use of oversampling delta sigma ADC (analog-to-digital converters) due to their better speed-accuracy tradeoff. Delta-sigma modulator is one of the key building blocks, which can be implemented using DT (discrete-time) and CT (continuous-time) techniques. Compared to their DT counterparts, CT delta-sigma modulators have recently attracted more and more attentions due to their advantages in terms of high speed, low power, low noise and intrinsic anti-aliasing capability. In this paper, we concentrate on the discrete implementation. Section 2 presents an aperture jitter effect in SDM in terms of SNR. In the last few years different authors derived formulas to quantify the SNR limiting effect of jitter in ADCs. While Walden used a worst case approach, Kobayashi presented an exact formula which allows calculating the SNR in the presence of an aperture jitter.",2009,0,
301,302,"The Impact of Tower Shadow, Yaw Error, and Wind Shears on Power Quality in a WindDiesel System","To study the impact of aerodynamic aspects of a wind turbine (WT) (i.e., tower shadow, wind shears, yaw error, and turbulence) on the power quality of a wind-diesel system, all electrical, mechanical, and aerodynamic aspects of the WT must be studied. Moreover, the contribution of the diesel generator system and its controllers should be considered. This paper describes how the aerodynamic and mechanical aspects of a WT can be simulated using TurbSim, AeroDyn, and FAST where the electrical parts of WT, diesel generator, its controllers, and electrical loads are modeled by Simulink blocks. Simulation results obtained from the model are used to observe the power and voltage variations at the WT generator terminals under different operating conditions. Furthermore, the effects of tower shadow, wind shears, yaw error, and turbulence on the power quality in a stand-alone wind-diesel system utilizing a fixed-speed WT are studied.",2009,0,
302,303,Based on Compact Type of Wavelet Neural Network Tolerance Analog Circuit Fault Diagnosis,"Based on the classical wavelet neural network, this paper put forward a sort of improved multiple-input multiple-output compact type of wavelet neural network, adopted adaptive learning rate and additional momentum BP algorithm to carry out training, studied its tolerance analog circuit fault diagnosis applications. Simulation results displayed that the compact type of wavelet neural network learning is fast, it can be effective diagnosed and located to tolerance analog circuit fault.",2009,0,
303,304,Lab VIEW based implementation of remedial action for DC arcing faults in a spacecraft,"In this paper remedial action for DC arcing faults in spacecraft has been designed and implemented using Lab VIEW. The Lab VIEW is an innovative graphical programming system designed to facilitate computer controlled data acquisition and analysis. DC arcing faults in spacecraft has been designed and implemented using the experimental data obtained at NASA Glenn research center. It is important to keep the continuity of the power supply and at the same time increase the reliability of spacecraft energy power system. In the frequency domain, fast Fourier transformation (FFT) is used for the feature extraction of the fault signal and odd harmonics frequency components of the phase currents are analyzed.",2003,0,
304,305,A Fault Tolerant Optimization Algorithm based on Evolutionary Computation,"In this paper we describe how an evolutionary algorithm is capable of running on a distributed environment with volatile resources. When executing algorithms in a desktop computing or resource harvesting context, resources can be reclaimed by their owners without warning, which may produce data loss and process to fail. The interest of the algorithm presented in the paper is that although it doesn't keep processes from failing, or data from being lost, it does improve the quality of results because of its design, not employing any special task control, checkpoint/restart or resource redundancies. By means of a series of experiments, we test the performance of the algorithm by studying the number of process failing and the quality of solutions when compared with the classic flavor of the evolutionary algorithm. The new algorithm, which shows its advantages, therefore improve dependability of distributed system",2006,0,
305,306,Efficient Error Correcting Codes for On-Chip DRAM Applications for Space Missions,"New systematic single error correcting codes-based circuits are introduced for random access memories, with ultimate minimal encoding/decoding complexity, low power and high performance. These new, codes-based circuits can be used in combinational circuits and in on-chip random access memories of reconfigurable architectures with high performance and ultimate minimum decoding/encoding complexity. Due to the overhead of parity check bits associated with the error-correcting-codes, there has always been a demand for an efficient and compact code for small memories in terms of data width. The proposed codes give improved performance even for small memories over the other codes. Area and power comparisons have been performed to benchmark the performance index of our codes. The code-centric circuits offer significant advantages over existing error correcting codes-based circuits in the literature in terms of lower size, power and cost which make them suitable for wider range of applications such as those targeting space. The paper describes the new efficient code and associated circuits for its implementation",2005,0,
306,307,Fault Diagnosis for Engine Based on EMD and Wavelet Packet BP Neural Network,"To solve the problem of fault diagnosis for engine, due to the complexity of the equipments and the particularity of the operating environments, generally speaking, there is no one-to-one correspondence between the characteristic parameters and status, so, the methods of diagnosis are very complicated. A novel fault diagnosis method based on empirical mode decomposition (EMD) and wavelet packet BP neural network is proposed in this paper. Firstly, the given signal is analyzed by wavelet packet to remove the noise; Then the de-noised data is decomposed into a number of IMFs by EMD and extract their frequency eigenvectors, then using these eigenvectors as the training samples of the BP network, training the BP network to identify the faults. Finally, the simulation experiments shows that the proposed method for fault diagnosis of engine is effective and the de-nosing process using wavelet packet transform is essential.",2009,0,
307,308,Errors estimation and minimization for the 5-axis milling machine,"This paper presents the tool path optimization algorithms to compute and estimate the non-linear inverse kinematics errors of the 5-axis milling machine. The approach is based on a global approximation of the required surface by a virtual surface constructed from the tool trajectories. Errors are computed from the difference between the required surface and the virtual surface and displayed numerically and graphically through the virtual machine simulator. The simulator is based on 3D representation and employing the inverse kinematics approach to derive the corresponding rotational and translation movement of the mechanism. The simulator makes it possible to estimate the errors of a 3D tool-path based on a prescribed set of the cutter location (CL) points as well as a set of the cutter contact (CC) points with tool inclination angle. Errors, particularly near the vicinity of the large milling errors, are minimized using a discrete algorithm based on a shortest path strategy. Furthermore, the simulator can be used to simulate the milling process, verify the final cut of the actual tool-path before testing with the real machine. Thus, it reduces the cost of iterative trial and errors.",2002,0,
308,309,Soft error assessments for servers,"In order to assess the soft error rate (SER) of a server, it is important to not only quantify the soft error contribution of the individual semiconductor components, but also to account for derating and for SER mitigation like hardening and shielding. Derating describes the fact that not every soft error has an impact. A large number of soft errors vanish based on electrical, logical or timing considerations. They have no impact. Additionally, a server can, to a large degree, be protected from the impact of soft errors by implementing error detection and correction means. In these cases the impact of the soft error is limited to the extra compute time needed for the correction. Summing up the SER contributions from transistors and circuits results in the so-called raw soft error rate, a rate which describes just the bottom layer of the system stack. Powerful protection mechanisms at higher layers can reduce that rate by several orders of magnitude. Awareness of this vertical interaction across the different layers in the system stack leads to servers optimized for robustness.",2010,0,
309,310,On the Use of Dynamic Binary Instrumentation to Perform Faults Injection in Transaction Level Models,"Transaction Level Modelling (TLM) has been widely accepted as systems modelling framework focused in system components communication. This approach allows efficient accurate estimation and rapid design space exploration. Besides of the functional simulation for validation of a hardware/software designs, there are additional reliability requirements that need advanced simulation techniques to analyze the system behaviour in the presence of faults. Several traditional VHDL fault injection mechanisms like mutants or saboteurs have been adapted to SystemC model descriptions. The main drawback of these approaches is the necessity of source code modification to carry out the fault injection campaigns. In this paper, we propose the use of Dynamic Binary Instrumentation (DBI) to perform fault injection in SystemC TLM models. DBI is a technique to intercept software routine calls allowing argument and return value corruption and data structures modification at runtime. This technique needs neither source code modifications nor recompilation of models in order to generate module mutants or in order to insert saboteurs in the signal communication path.",2009,0,
310,311,Online Fault Diagnosis of Discrete Event Systems. A Petri Net-Based Approach,"This paper is concerned with an online model-based fault diagnosis of discrete event systems. The model of the system is built using the interpreted Petri nets (IPN) formalism. The model includes the normal system states as well as all possible faulty states. Moreover, it assumes the general case when events and states are partially observed. One of the contributions of this work is a bottom-up modeling methodology. It describes the behavior of system elements using the required states variables and assigning a range to each state variable. Then, each state variable is represented by an IPN model, herein named module. Afterwards, using two composition operators over all the modules, a monolithic model for the whole system is derived. It is a very general modeling methodology that avoids tuning phases and the state combinatory found in finite state automata (FSA) approaches. Another contribution is a definition of diagnosability for IPN models built with the above methodology and a structural characterization of this property; polynomial algorithms for checking diagnosability of IPN are proposed, avoiding the reachability analysis of other approaches. The last contribution is a scheme for online diagnosis; it is based on the IPN model of the system and an efficient algorithm to detect and locate the faulty state. Note to Practitioners-The results proposed in this paper allow: 1) building discrete event system models in which faults may arise; 2) testing the diagnosability of the model; and 3) implementing an online diagnoser. The modeling methodology helps to conceive in a natural way the model from the description of the system's components leading to modules that are easily interconnected. The diagnosability test is stated as a linear programming problem which can be straightforward programmed. Finally, the algorithm for online diagnosis leads to an efficient procedure that monitors the system's outputs and handles the normal behavior model. This provides an oppo- rtune detection and location of faults occurring within the system",2007,0,
311,312,Impact of correlation errors on the optimum Kalman filter gain identification in a single sensor environment,"The impact of errors in the innovation correlation functions evaluation, related to the suboptimal filter, on the identification of the optimum steady state Kalman filter gains are investigated. This issue arises in all real time applications, where the correlations must be calculated from experimental data. An identification algorithm proposed in the literature, with formal proof of convergence, is revisited and summarized. Based on this algorithm, equations describing this impact are developed. Simulation results are presented and discussed. As contribution, experimental results of the identification algorithm, applied to estimate the states of a position servo systems, are presented.",2004,0,
312,313,A Fault Tolerant Control strategy for an unmanned aerial vehicle based on a Sequential Quadratic Programming algorithm,"In this paper a fault tolerant control strategy for the nonlinear model of an unmanned aerial vehicle (UAV) equipped with numerous redundant controls is proposed. Asymmetric actuator failures are considered and, in order to accommodate them, a sequential quadratic programming (SQP) algorithm which takes into account nonlinearities, aerodynamic and gyroscopic couplings, state and control limitations is implemented. This algorithm computes new trims such that around the new operating point, the faulty linearized model remains nearby from the fault free model. For the faulty linearized models, linear state feedback controllers based on an eigenstructure assignment method are designed to obtain soft transients during accommodation. Real time implementation of the SQP algorithm is also discussed.",2008,0,
313,314,"A Framework to Evaluate the Trade-Off among AVF, Performance and Area of Soft Error Tolerant Microprocessors","Because of the increasing susceptibility of the integrated circuits to soft errors, many techniques have been proposed in all the design levels to reduce the AVF (architecturally vulnerable factor) of microprocessors with extra performance and area overheads. These overheads have a negative impact on the reliability. Conventional reliability evaluation frameworks do not take both performance and area overheads into account. A new metric, mMWTF (modified mean work to failure), is proposed in this paper to capture the trade-off among AVF, performance and area. A quantitative approach to evaluate mMWTF is also presented, in which fault injection is used to estimate the AVF. To modify the conventional fault injection methods which inject only SEU (single event upset), a new method is proposed to injects both SEU and MBU (multi bits upset), the latter of which happens more frequently with the shrinking feature size. Because of the new metric and the new fault injection method, the framework presented in this paper is more accurate than conventional ones. As a case study, two control flow checking techniques are proposed and evaluated in this paper. The evaluation results demonstrate that the techniques with better balance among AVF, performance and area can better improve the reliability of microprocessors.",2008,0,
314,315,Falcon: fault localization in concurrent programs,"Concurrency fault are difficult to find because they usually occur under specific thread interleavings. Fault-detection tools in this area find data-access patterns among thread interleavings, but they report benign patterns as well as actual faulty patterns. Traditional fault-localization techniques have been successful in identifying faults in sequential, deterministic programs, but they cannot detect faulty data-access patterns among threads. This paper presents a new dynamic fault-localization technique that can pinpoint faulty data-access patterns in multi-threaded concurrent programs. The technique monitors memory-access sequences among threads, detects data-access patterns associated with a program's pass/fail results, and reports dataaccess patterns with suspiciousness scores. The paper also presents the description of a prototype implementation of the technique in Java, and the results of an empirical study we performed with the prototype on several Java benchmarks. The empirical study shows that the technique can effectively and efficiently localize the faults for our subjects.",2010,0,
315,316,Enhanced error concealment with mode selection,"Delay sensitive video transmission over error prone networks can suffer from packet erasures when channel conditions are not favorable. Use of error concealment (EC) at the video decoder is necessary in such cases to prevent error induced artefacts making the affected video frames visibly intolerable. This paper proposes an EC method that incorporates enhanced temporal and spatial concealment elements, the use of which is controlled by a mode selection (MS) algorithm well matched to the characteristics of the temporal concealment approach. The performance of the individual enhancements and of the MS algorithm are compared with the respective features of the method employed in the H.264 joint model (JM) decoder and with other state of the art methods. The overall performance of the proposed method is shown to offer significant gains (up to 9 dB) compared to that of the JM decoder for a wide range of natural and animation image sequences without any considerable increase in complexity",2006,0,
316,317,A Fault-Location Method for Application With Current Differential Relays of Three-Terminal Lines,This paper presents a new method for locating faults on three-terminal power lines. Estimation of a distance to fault and indication of a faulted section is performed using three-phase current from all three terminals and additionally three-phase voltage from the terminal at which a fault locator is installed. Such a set of synchronized measurements has been taken into consideration with the aim of developing a fault-location algorithm for applications with current differential relays of three-terminal lines. The delivered fault-location algorithm consists of three subroutines designated for locating faults within particular line sections and a procedure for indicating the faulted line section. Testing and evaluation of the algorithm has been performed with fault data obtained from versatile Alternate Transients Program-Electromagnetic Transients Program simulations. The sample results of the evaluation are reported and discussed.,2007,0,
317,318,Bad Words: Finding Faults in Spirit's Syslogs,"Accurate fault detection is a key element of resilient computing. Syslogs provide key information regarding faults, and are found on nearly all computing systems. Discovering new fault types requires expert human effort, however, as no previous algorithm has been shown to localize faults in time and space with an operationally acceptable false positive rate. We present experiments on three weeks of syslogs from Sandia's 512-node ""Spirit"" Linux cluster, showing one algorithm that localizes 50% of faults with 75% precision, corresponding to an excellent false positive rate of 0.05%. The salient characteristics of this algorithm are (1) calculation of nodewise information entropy, and (2) encoding of word position. The key observation is that similar computers correctly executing similar work should produce similar logs.",2008,0,
318,319,Efficient Test Pattern Compression Method Using Hard Fault Preferring,"This paper describes new compression method that is used for test pattern compaction and compression in algorithm called COMPAS, which utilizes a test data compression method based on pattern overlapping. This algorithm reorders and compresses deterministic test patterns previously generated in an ATPG by overlapping them. Independency of COMPAS on used ATPG is discussed and verified. New method improves compression ratio by preprocessing input data to determine the degree of random test resistance for each fault. This information allows the algorithm to reorder test patterns more efficiently and results to 10% compression ratio improvement in average. Compressed data sequence is well suited for decompression by the scan chains in the embedded tester cores.",2008,0,
319,320,Detection of small defects by THz-waves for non-destructive testing in dielectric layered structures,"In this study, the small defects detection in dielectric layered structures by THz waves for nondestructive testing. Finite element method were used for modelling of the structures.",2010,0,
320,321,"On line sensor fault detection, isolation and accommodation in tactical aerospace vehicle","This paper presents on line sensor fault detection, isolation (FDI) and the associated fault tolerant control (FTC) algorithm for a tactical aerospace vehicle. A study on the analytical redundancy and a sensor fault detection scheme (FDI ) into a flight control system has been performed for a tactical aerospace vehicle using the longitudinal model. There are various methods available in the academic literature to apply FDI and FTC schemes to control systems and some have already been applied to real applications. Among these, observer-based approaches have arisen as one of the most widespread. The basic ideas behind observer-based FDI schemes are the generation of residuals, and the use of an optimal threshold function to differentiate faults from disturbances. Generally, the residuals, also known as diagnostic signals, are generated from estimates of the system's measurements obtained by a Luenberger observer or a Kalman filter. The threshold function is then used to 'detect' the fault by separating the residuals from false faults and disturbances. The change in residual signal is used to detect and isolate the fault and corresponding fault tolerant control action is taken to arrest the failure of the aerospace vehicle. A closed-loop simulation with nonlinear 6-degree of freedom (6-DoF) model shows that the above FDI and FTC scheme will be able to reduce the probability of mission failure due to the fault in one of the sensors.",2004,0,
321,322,Induction machines performance evaluator 'torque speed estimation and rotor fault diagnostic',"This paper proposes a new DSP based tool for evaluating the performance of induction motors based on the data extracted from the stator current. In the proposed algorithm, a pattern recognition technique according to Bayes minimum error classifier is developed to detect incipient rotor faults such as broken rotor bars and static eccentricity in induction motors. Also, part of the algorithm is based on the acceleration method presented in the IEEE Std. 112. It helps to calculate the motor's torque using two line currents and voltages. The use of linear and quadratic time-frequency representations is investigated as a viable solution to the task at hand. Speed information is vital in this approach, so an algorithm to track the speed related saliency induced harmonics from the machine's line current spectrogram is presented. Capturing the harmonics gives the rotor speed that can also be used to extract the feature vector for diagnostic. The implementation of the algorithm on TMS320C6000 family of DSP chips is currently underway. The complete algorithm is then be used to obtain the induction motor's performance curves. This is a complete stand-alone panel mounted induction motor diagnostic tool currently being developed in their lab. This package will be used in conjunction with a drive system (inverter) for online performance monitoring and preventing unwanted shutdown of the induction motor. The difficulties encountered, including a limited dynamic range and the presence of cross terms, are addressed and the suggested solution is provided. Experimental results corroborating the proposed algorithm are presented, and a discussion of the advantages and disadvantages of such an approach is touched upon",2002,0,
322,323,Correction of MR k-space data corrupted by spike noise,"Magnetic resonance images are reconstructed from digitized raw data, which are collected in the spatial-frequency domain (also called k-space). Occasionally, single or multiple data points in the k-space data are corrupted by spike noise, causing striation artifacts in images. Thresholding methods for detecting corrupted data points can fail because of small alterations, especially for data points in the low spatial frequency area where the k-space variation is large. Restoration of corrupted data points using interpolations of neighboring pixels can give incorrect results. The authors propose a Fourier transform method for detecting and restoring corrupted data points using a window filter derived from the striation-artifact structure in an image or an intermediate domain. The method provides an analytical solution for the alteration at each corrupted data point. It can effectively restore corrupted k-space data, removing striation artifacts in images, provided that the following 3 conditions are satisfied. First, a region of known signal distribution (for example, air background) is visible in either the image or the intermediate domain so that it can be selected using a window filter. Second, multiple spikes are separated by the full-width at half-maximum of the point spread function for the window filter. Third, the magnitude of a spike is larger than the minimum detectable value determined by the window filter and the standard deviation of k-space random noise.",2000,0,
323,324,Initial Experiences with a New FPGA Based Traveling Wave Fault Recorder Installed on a MV Distribution Network,"This paper presents the initial results obtained from a newly developed FPGA based traveling wave fault recorder installed on a medium voltage (MV) distribution line. The recorder is capable of recording six input signals, simultaneously sampling at 40 mega samples per second (MSPS) and at 14 bit resolution. It uses high bandwidth 17 MHz Rogowski coils connected to the secondary of a current transformer inside the substation to acquire the high frequency traveling wave components. Initial results during the testing phase show that recorder is capable of recording high fidelity signals relating to switching events. It has also highlighted that the distribution line is subject to many other transient phenomena in addition to faults and switching events which must be taken into consideration when choosing a suitable triggering mechanism.",2008,0,
324,325,Evaluation of the Low Frame Error Rate Performance of LDPC Codes Using Importance Sampling,"We present an importance sampling method for the evaluation of the low frame error rate (FER) performance of LDPC codes under iterative decoding. It relies on a combinatorial characterization of absorbing sets, which are the dominant cause of decoder failure in the low FER region. The biased density in the importance sampling scheme is a mean-shifted version of the original Gaussian density, which is suitably centered between a codeword and a dominant absorbing set. This choice of biased density yields an unbiased estimator for the FER with a variance lower by several orders of magnitude than the standard Monte Carlo estimator. Using this importance sampling scheme in software, we obtain good agreement with the experimental results obtained from a fast hardware emulator of the decoder.",2007,0,
325,326,A New Method for Earth Fault Line Detection Based on Two-Dimensional Wavelet Transform in Distribution Automation,"A novel method based on two-dimensional wavelet transform to detect single-phase faults in distribution systems is proposed in this paper. After structuring analytic signals of zero sequence current, the two-dimensional wavelet transform is applied. Thus the analysis of combined signal of amplitude and phase is realized. Compared with the use of single amplitude or single phase, combined signal carries more details of transient signal. Theoretical analysis and MATLAB based simulation show that the presented method can exactly and effectively choose the faulty line in single phase-to-ground fault",2005,0,
326,327,An FFT-based method to evaluate and compensate gain and offset errors of interleaved ADC systems,"Interleaved analog-digital converter (ADC) systems can be used to increase the sampling rate for a given ADC implementation technique. In theory, the maximum sampling rate that can be achieved is limited only by the bandwidth and the practical limits related to the power and space of integrated circuits. In this paper, a solution to increase the sampling rate of a digitizing system based on interleaved ADCs is presented. An error analysis, which takes into consideration offset and gain errors of the different ADC channels, is performed in order to quantify the effect of such errors in the system's performance. A software method based on the fast Fourier transform is presented for offset and gain error compensation of interleaved ADC associations. Numerical simulations and experimental results are used to validate the theory and the proposed compensation algorithm.",2004,0,
327,328,Fault Diagnosis of Generator Based on D-S Evidence Theory,"It is difficult to identify the fault type with the signal gathered from the sensors. In this paper, a new fusion algorithm based on the Dempster-Shafer theory of evidence and neural networks is brought forward. This method combines the advantages of D-S evidence theory and the BP neural network. Neural networks are used to pretreated the data gathered from the embedded sensors in the monitoring system of hydropower plant. Compared with the approaches that only adopt D-S evidence theory or neural networks, the accuracy of diagnostic results is obviously improved, and the signals analysis proved this conclusion. This method has been applied in the monitoring system of JiLin FengMan hydropower plant successfully.",2008,0,
328,329,Improved error bounds for the erasure/list scheme: the binary and spherical cases,We derive improved bounds on the error and erasure rate for spherical codes and for binary linear codes under Forney's erasure/list decoding scheme and prove some related results.,2004,0,
329,330,Faulted phase selection on double circuit transmission line using wavelet transform and neural network,"Modern numerical relays often incorporate the logic for combined single and three-phase auto-reclosing scheme; single phase to earth faults initiate single-phase tripping and reclosure, and all the other faults initiate three-phase tripping and reclosure. Accurate faulted phase selection is required for such a scheme. This paper presents a novel scheme for detection and classification of faults on double circuit transmission line. The proposed approach uses combination of wavelet transform and neural network, to solve the problem. While wavelet transform is a powerful mathematical tool which can be employed as a fast and very effective means of analyzing power system transient signals, artificial neural network has a ability to classify non-linear relationship between measured signals by identifying different patterns of the associated signals. The proposed algorithm consists of time-frequency analysis of fault generated transients using wavelet transform, followed by pattern recognition using artificial neural network to identify the faulted phase. MATLAB/Simulink software is used to generate fault signals and verify the correctness of the algorithm. The adaptive discrimination scheme is tested by simulating different types of fault and varying fault resistance, fault location and fault inception time, on a given power system model. The simulation results show that the proposed phase selector scheme is able to identify faulted phase on the double circuit transmission line rapidly and correctly.",2009,0,
330,331,Algorithm-Based Fault Tolerance for Fail-Stop Failures,"Fail-stop failures in distributed environments are often tolerated by checkpointing or message logging. In this paper, we show that fail-stop process failures in ScaLAPACK matrix-matrix multiplication kennel can be tolerated without checkpointing or message logging. It has been proved in previous algorithm-based fault tolerance that, for matrix-matrix multiplication, the checksum relationship in the input checksum matrices is preserved at the end of the computation no mater which algorithm is chosen. From this checksum relationship in the final computation results, processor miscalculations can be detected, located, and corrected at the end of the computation. However, whether this checksum relationship can be maintained in the middle of the computation or not remains open. In this paper, we first demonstrate that, for many matrix matrix multiplication algorithms, the checksum relationship in the input checksum matrices is not maintained in the middle of the computation. We then prove that, however, for the outer product version algorithm, the checksum relationship in the input checksum matrices can be maintained in the middle of the computation. Based on this checksum relationship maintained in the middle of the computation, we demonstrate that fail-stop process failures (which are often tolerated by checkpointing or message logging) in ScaLAPACK matrix-matrix multiplication can be tolerated without checkpointing or message logging.",2008,0,
331,332,Maintaining Consistency between Loosely Coupled Services in the Presence of Timing Constraints and Validation Errors,"Loose coupling is often cited as a defining characteristic of service-oriented architectures. Interactions between services take place via messages in an asynchronous environment where communication and processing delays can be unpredictable; further, interacting parties are not required to be on-line at the same time. Despite loose coupling, many service interactions have timing and validation constraints. For example, business interactions that take place using RosettaNet partner interface processes (PIPs) such as request price and availability, request purchase order, notify of invoice, etc. have to meet several timing and message validation constraints. A failure to deliver a valid message within its time constraint could cause mutually conflicting views of an interaction. For example, one party can regard it as timely whilst the other party regards it as untimely, leading to application level inconsistencies. The paper describes how business interactions, such as PIPs can be wrapped by simple handshake synchronisation protocols to provide bilateral consistency, thereby simplifying the task of coordinating peer-to-peer business processes",2006,0,
332,333,Compact Microstrip Quasi-Elliptic Bandpass Filter Using Open-Loop Dumbbell Shaped Defected Ground Structure,"A novel square open-loop dumbbell-shaped defected ground structure (DGS) unit is proposed. This unit provides a quasi-elliptic bandpass characteristic and the two transmission zeros near the passband edges can be controlled by the dimensions of DGS. Two quasi-elliptic bandpass filters using one and two DGS units; centered at 1.5 GHz were designed and implemented. Both the simulation and experimental results show that the DGS filter response is in good accordance with ideal quasi-elliptic model. The prototype filter with two DGS units yields higher order quasi-elliptic filtering and reports the measured 0.72 dB as insertion loss, 34 dB as matching, 51.8 % fractional bandwidth and about 20 dB stopband attenuation up to 10 GHz",2006,0,
333,334,Analyzing the soft error resilience of linear solvers on multicore multiprocessors,"As chip transistor densities continue to increase, soft errors (bit flips) are becoming a significant concern in networked multiprocessors with multicore nodes. Large cache structures in multicore processors are especially susceptible to soft errors as they occupy a significant portion of the chip area. In this paper, we consider the impacts of soft errors in caches on the resilience and energy efficiency of sparse linear solvers. In particular, we focus on two widely used sparse iterative solvers, namely Conjugate Gradient (CG) and Generalized Minimum Residuals (GMRES). We propose two adaptive schemes, (i) a Write Eviction Hybrid ECC (WEH-ECC) scheme for the L1 cache and (ii) a Prefetcher Based Adaptive ECC (PBA-ECC) scheme for the L2 cache, and evaluate the energy and reliability trade-offs they bring in the context of GMRES and CG solvers. Our evaluations indicate that WEH-ECC reduces the CG and GMRES soft error vulnerability by a factor of 18 to 220 in L1 cache, relative to an unprotected L1 cache, and energy consumption by 16%, relative to a cache with strong protection. The PBA-ECC scheme reduces the CG and GMRES soft error vulnerability by a factor of 9 A 10<sup>3</sup> to 8.6 A 10<sup>9</sup>, relative to an unprotected L2 cache, and reduces the energy consumption by 8.5%, relative to a cache with strong ECC protection. Our energy overheads over unprotected L1 and L2 caches are 5% and 14% respectively.",2010,0,
334,335,Application-driven co-design of fault-tolerant industrial systems,"This paper presents a novel methodology for the HW/SW co-design of fault tolerant embedded systems that pursues the mitigation of radiation-induced upset events (which are a class of Single Event Effects - SEEs) on critical industrial applications. The proposal combines the flexibility and low cost of Software Implemented Hardware Fault Tolerance (SIHFT) techniques with the high reliability of selective hardware replication. The co-design flow is supported by a hardening platform that comprises an automatic software hardening environment and a hardware tool able to emulate Single Event Upsets (SEUs). As a case study, we selected a soft-micro (PicoBlaze) widely used in FPGA-based industrial systems, and a fault tolerant version of the matrix multiplication algorithm was developed. Using the proposed methodology, the design was guided by the requirements of the application, leading us to explore several trade-offs among reliability, performance and cost.",2010,0,
335,336,Defect-Tolerant Design and Optimization of a Digital Microfluidic Biochip for Protein Crystallization,"Protein crystallization is a commonly used technique for protein analysis and subsequent drug design. It predicts the 3-D arrangement of the constituent amino acids, which in turn indicates the specific biological function of a protein. Protein crystallization experiments are typically carried out in well-plates in the laboratory. As a result, these experiments are slow, expensive, and error-prone due to the need for repeated human intervention. Recently, droplet-based AdigitalA microfluidics have been used for executing protein assays on a chip. Protein samples in the form of nanoliter-volume droplets are manipulated using the principle of electrowetting-on-dielectric. We present the design of a multi-well-plate microfluidic biochip for protein crystallization; this biochip can transfer protein samples, prepare candidate solutions, and carry out crystallization automatically. To reduce the manufacturing cost of such devices, we present an efficient algorithm to generate a pin-assignment plan for the proposed design. The resulting biochip enables control of a large number of on-chip electrodes using only a small number of pins. Based on the pin-constrained chip design, we present an efficient shuttle-passenger-like droplet manipulation method and test procedure to achieve high-throughput and defect-tolerant well loading.",2010,0,
336,337,Influence of the AC system faults on HVDC system and recommendations for improvement,"The interaction between AC and DC systems in a long distance bulk power transmission system is very complicated. In this paper, taking some cases in China Southern Power Grid as examples, the main functions of the DC control system operating during the AC faults is discussed. During the AC faults, the protection and monitoring system for DC converter and the protection system for converter transformers and auxiliary transformers may work incorrectly, reasons for these cases are analyzed and recommendations are given to solve the defects. Experience from these cases will help us to improve the ability of operation and maintenance to insure the safety and provide useful references for the design of HVDC and the coordination of AC/DC system in China.",2009,0,
337,338,Dense error correction via l1-minimization,"We study the problem of recovering a non-negative sparse signal x isin Ropf<sup>n</sup> from highly corrupted linear measurements y = Ax+e isin Ropf<sup>m</sup>, where e is an unknown (and unbounded) error. Motivated by an observation from computer vision, we prove that for highly correlated dictionaries A, any non-negative, sufficiently sparse signal x can be recovered by solving an lscr<sup>1</sup>-minimization problem: min ||x||<sub>1</sub> + ||e||<sub>1</sub> subject to y = Ax + e. If the fraction rho of errors is bounded away from one and the support of x grows sublinearly in the dimension m of the observation, for large m, the above lscr<sup>1</sup>-minimization recovers all sparse signals x from almost all sign-and-support patterns of e. This suggests that accurate and efficient recovery of sparse signals is possible even with nearly 100% of the observations corrupted.",2009,0,
338,339,Efficient stimuli generators for detection of path delay faults,"This paper presents a way to construct accumulator based test vector generators intended for efficient detection of path delay faults. Experiments conducted using our path delay fault simulator, GFault, shows that our proposed generator can give as much as 30times reduction in test time for circuits in the ISCAS85 benchmark suite compared to an accumulator based pseudo random generator",2005,0,
339,340,A fault analysis and design consideration of pulsed-power supply for high-power laser,"According to the requirements of driving flashlamps, the design of a pulsed-power supply (PPS), based on capacitors as energy storage elements, is presented. Special consideration is given to some possible faults such as capacitor internal short-circuit, bus bar breakdown to ground, flashlamp sudden short or break (open circuit), and closing switch restrike in the preionization branch. These faults were analyzed in detail, and both fault current and voltage waveforms are shown through circuit simulation. Based on the analysis and computation undertaken, the pulsed-power system design and protection requirements are proposed. The preliminary experiments undertaken after circuit simulation demonstrated that the design of the PPS met the project requirements.",2003,0,
340,341,A fault tolerant control architecture for automated highway systems,A hierarchical controller for dealing with faults and adverse environmental conditions on an automated highway system is proposed. The controller extends a previous control hierarchy designed to work under normal conditions of operation. The faults are classified according to the capabilities remaining on the vehicle or roadside after the fault has occurred. Information about these capabilities is used by supervisors in each of the layers of the hierarchy to select appropriate fault handling strategies. We outline the strategies needed by the supervisors and give examples of their detailed operation,2000,0,
341,342,Analysis of interconnect crosstalk defect coverage of test sets,"This paper addresses the problem of evaluating the effectiveness of test sets to detect crosstalk defects in interconnects of deep sub-micron circuits. The fast and accurate estimation technique will enable: (a) evaluation of different existing tests, like functional, scan, logic BIST, and delay tests, for effective testing of crosstalk defects in interconnects, and (b) development of crosstalk tests if the existing tests are not sufficient, thereby minimizing the cost of interconnect testing. Based on a covering relationship we establish between transition tests in detecting crosstalk defects, we develop an abstract crosstalk fault model for circuit interconnects. Based on this fault model, and the covering relationship, we develop a fast and efficient method to estimate the fault coverage of any general test set. We also develop a simulation-based technique to calculate the probability of occurrence of the defects corresponding to each fault, which enables the fault coverage analysis technique to produce accurate estimates of the actual crosstalk defect coverage of a given test set. The crosstalk test and fault properties, as well as the accuracy of the proposed crosstalk coverage analysis techniques, have been validated through extensive simulation experiments. The experiments also demonstrate that the proposed crosstalk techniques are orders of magnitude faster than the alternative method of SPICE-level simulation. Finally, we demonstrate the practical applicability of the proposed fault coverage analysis technique by using it to evaluate the crosstalk fault coverage of logic BIST tests for the buses in a DSP core",2000,0,
342,343,Atmospheric correction of AMSR-E brightness temperatures for dry snow cover mapping,"Differences between the brightness temperatures (spectral gradient) collected by the Advanced Microwave Scanning Radiometer for EOS (AMSR-E) at 18.7 and 36.5 GHz are used to map the snow-covered area (SCA) over a region including the western U.S. The brightness temperatures are corrected to take into account for atmospheric effects by means of a simplified radiative transfer equation whose parameters are stratified using rawinsonde data collected from a few stations. The surface emissivity is estimated from the model, and the brightness temperatures at the surface are computed as the product of the surface temperature and the computed emissivity. The SCA derived from microwave data is compared with that obtained from the Moderate Resolution Imaging Spectroradiometer for both cases of corrected and noncorrected brightness temperatures. The improvement to the SCA retrievals based on the corrected brightness temperatures shows an average value around 7%",2006,0,
343,344,A low-tech solution to avoid the severe impact of transient errors on the IP interconnect,"There are many sources of failure within a system-on-chip (SoC), so it is important to look beyond the processor core at other components that affect the reliable operation of the SoC, such as the fabric included in every one that connects the IP together. We use ARM's AMBA 3 AXI bus matrix to demonstrate that the impact of errors on the IP interconnect can be severe: possibly causing deadlock or memory corruption. We consider the detection of 1-bit transient faults without changing the IP that connects to the bus matrix or the AMBA 3 standard and without adding extra latency while keeping the performance and area overhead low. We explore what can be done under these constraints and propose a combination of techniques for a low-tech solution to detect these rare events.",2009,0,
344,345,The study of analog circuit fault diagnosis method based on circuit transform function,"This is the paper use Laplace Transfer to compute the analog circuit transform function, and compute the fault transform functions with different kinds of fault to generate the fault diagnosis table. The table is used to complete fault diagnosis. At last the software Multisim is used to simulate an analog circuit to do the fault diagnosis, and this method is verified usefully.",2010,0,
345,346,Efficient Fault-Tolerant Backbone Construction in Tmote Sky Sensor Networks,"In this study, we have investigated the effectiveness of building AFault-Tolerant BackboneA for data dissemination in Tmote Sky sensor networks. Tmote Sky sensors provide programmable and adjustable output power for data transmission. Users can control adequate transmission power for each sensor. Based on our measurements of Tmote Sky, there is a steadily transmitted distance for every power level. For certain power level, successfully-transmitted ratio was approximately 100 percent when the distance between sender and receiver was less than the steadily-transmitted distance. In accordance with the character on Tmote Sky, the ideas of fault-tolerant backbone has been made for constructing a fault-tolerant and stable system for Tmote Sky. The fault-tolerant backbone protocol builds up a connected backbone, in which nodes are endowed with a sleep/awake schedule. Practical experimental results reveal the fast fault recovery and high successfully-transmitted ratio can be fulfilled in the realistic system. The following goals in the implementation have been reached, including self-configurable fault-tolerant groups, automatic backbone construction, automatic failure recovery, and route repair.",2009,0,
346,347,Anomaly detection: A robust approach to detection of unanticipated faults,"This paper introduces a methodology to detect as early as possible with specified degree of confidence and prescribed false alarm rate an anomaly or novelty (incipient failure) associated with critical components/subsystems of an engineered system that is configured to monitor continuously its health status. Innovative features of the enabling technologies include a Bayesian estimation framework, called particle filtering, that employs features or condition indicators derived from sensor data in combination with simple models of the systempsilas degrading state to detect a deviation or discrepancy between a baseline (no-fault) distribution and its current counterpart. The scheme provides the probability of abnormal condition and the probability of false alarm. The presence of an anomaly is confirmed for a given confidence level. The efficacy of the proposed anomaly detection architecture is illustrated with test data acquired from components typically found on aircraft and monitored via a test rig appropriately instrumented.",2008,0,
347,348,Research of Remote Fault Diagnosis System Based on Multi-Agent,"A Multi-agent based Remote fault Diagnosis system is an important system for high speed and automation which can not only monitor the status of the remote device, but serve for the remote device. Remote Fault diagnosis system are vital aspects in automation process, in this sense, remote diagnosis systems should support decision-making tools, the enterprise thinking and flexibility. In this paper a kind of Remote Diagnosis System based on multi-agent is presented. This model is based on a generic framework using multi-agent systems. Specifically, this paper analyses the architecture of Remote Fault Diagnosis System and the collaboration mechanism between Agents. The method brought forward in the paper was generally applicable to a general fault diagnosis.",2010,0,
348,349,Fault diagnosis for a delta-sigma converter by a neural network,The diagnosis of faults in a first order -converter is described. The circuit behaviour of fault-free circuits and circuits containing single faults were simulated and characterized by the output bitstream patterns. The latter were compared with that of the ideal fault-free circuit. A Simplified fuzzy ARTMAP was trained with metrics derived from the bitstreams and their assigned class. A diagnostic accuracy of 93% was achieved using just two of the metrics. The technique might be useful for the diagnosis of other circuits.,2004,0,
349,350,The Impact of Link Error Modeling on the Quality of Streamed Video in Wireless Networks,The influence of channel error characteristics on higher layer protocols or methods which are considering or even exploiting the error statistics is significant especially in wireless networks where fading and interference effects result in error pattern correlation properties (error bursts). In this work we are analysing the impact of the channel properties directly on the quality of streamed video. We are focusing on the quality of transmitted H.264/AVC video streaming over UMTS DCH (Dedicated Channel) and compare the quality of the streamed video simulated over measured link error traces (the measurements performed in a live UMTS network) to simulations with a memoryless channel and to models with enhanced error characteristics. The results show that appropriate modeling of the link layer error characteristics is very important but it can also be concluded that the error correlation properties of the link- or the network-layer model do not have an impact on the quality of the video stream as long as the resulting IP packet error probability remains unchanged.,2006,0,
350,351,A portable gait analysis and correction system using a simple event detection method,"Microcontrollers are widely used in the area of portable control systems, though they are only beginning to be used for portable, unobtrusive Functional Electrical Stimulation (FES) systems. This paper describes the initial prototyping of such a portable system. This has the intended use of detecting time variant gait anomalies in patients with hemiplegia, and correcting for them. The system is described in two parts. Firstly, the portable hardware implementing two independent communicating microcontrollers for low powered parallel processing and secondly the simplified low power software. Both are designed specifically for long term, stable use and also to communicate with PC based visual software for testing and evaluation. The system operates by using bend sensors to defect the angles of the hip, knee and ankle of both legs. It computes an error signal with which to produce a stimulation wave cycle, that is synchronised and timed for the new gait cycle from that in which the error was observed. This system uses a PID controller to correct for the instability inherent with such a large time delay between observation and correction.",2002,0,
351,352,Reaction to errors in robot systems,"The paper analyzes the problem of error (failure) detection and handling in robot programming. First an overview of the subject is provided and later error detection and handling in MRROC++ are described. To facilitate system reaction to the detected failures, the errors are classified and certain suggestions are made as to how to handle those classes of errors.",2002,0,
352,353,Final Prediction Error of Autoregressive Model as a New Feature in the Analysis of Heart Rate Variability,"The aim of this study is to offer a new heart rate variability (HRV) index that increases the accuracy in the discrimination of patients with congestive heart failure (CHF) from the control group. For this purpose, final prediction errors (FPE), which shows the quality of the conformity of autoregressive (AR) model, are calculated for model degrees from 1 to 100. Although the optimal AR model order and FPE values are widely used in the literature, they have not been used as possible HRV indices. In this study, we used FPE as an HRV feature for discriminating the patients with CHF from normal subjects and made a comparison with the other common HRV indices. As a result, we showed that FPE of AR model is a possible significant HRV feature.",2007,0,
353,354,One-Dimensional Variational Retrieval of the Wet Tropospheric Correction for Altimetry in Coastal Regions,"The altimeter range is corrected for tropospheric humidity by means of microwave radiometer measurements (Envisat/MWR, Jason-1/JMR, Jason-2/AMR). Over an open ocean, the altimeter/radiometer combination is satisfactory. However, in coastal areas, radiometer measurements are contaminated by the surrounding land surfaces, and the humidity retrieval method is not appropriate anymore. In this paper, a variational assimilation technique is proposed to retrieve the wet tropospheric correction near coasts. The method is first developed on simulations using the data from a meteorological model. A performance assessment is performed, as well as a comparison with a standard algorithm. The method is then applied on actual measurements, thus evaluating its feasibility.",2010,0,
354,355,Compact Test Generation for Small-Delay Defects Using Testable-Path Information,"Testing for small-delay defects requires fault-effect propagation along the longest testable paths. However, the selection of the longest testable paths requires high CPU time and leads to large pattern counts. Dynamic test compaction for small-delay defects has remained largely unexplored thus far. We propose a path-selection scheme to accelerate ATPG based on stored testable critical-path information. A new dynamic test-compaction technique based on structural analysis is also introduced. Simulation results are presented for a set of ISCAS'89 benchmark circuits.",2009,0,
355,356,Use of fault tree analysis for evaluation of system-reliability improvements in design phase,"Traditional failure mode and effects analysis is applied as a bottom-up analytical technique to identify component failure modes and their causes and effects on the system performance, estimate their likelihood, severity and criticality or priority for mitigation. Failure modes and their causes, other than those associated with hardware, primarily electronic, remained poorly addressed or not addressed at all. Likelihood of occurrence was determined on the basis of component failure rates or by applying engineering judgement in their estimation. Resultant prioritization is consequently difficult so that only the apparent safety-related or highly critical issues were addressed. When thoroughly done, traditional FMEA or FMECA were too involved to be used as a effective tool for reliability improvement of the product design. Fault tree analysis applied to the product as a top down in view of its functionality, failure definition, architecture and stress and operational profiles provides a methodical way of following products functional flow down to the low level assemblies, components, failure modes and respective causes and their combination. Flexibility of modeling of various functional conditions and interaction such as enabling events, events with specific priority of occurrence, etc., using FTA, provides for accurate representation of their functionality interdependence. In addition to being capable of accounting for mixed reliability attributes (failure rates mixed with failure probabilities), fault trees are easy to construct and change for quick tradeoffs as roll up of unreliability values is automatic for instant evaluation of the final quantitative reliability results. Failure mode analysis using fault tree technique that is described in this paper allows for real, in-depth engineering evaluation of each individual cause of a failure mode regarding software and hardware components, their functions, stresses, operability and interactions",2000,0,
356,357,Local magnetic error estimation using action and phase jump analysis of orbit data,"It's been shown in previous conferences that action and phase jump analysis is a promising method to measure normal quadrupole components, skew quadrupole components and even normal sextupole components. In this paper, the action and phase jump analysis is evaluated using new RHIC data.",2007,0,
357,358,Mining Frequent Patterns from Software Defect Repositories for Black-Box Testing,"Software defects are usually detected by inspection, black-box testing or white-box testing. Current software defect mining work focuses on mining frequent patterns without distinguishing these different kinds of defects, and mining with respect to defect type can only give limited guidance on software development due to overly broad classification of defect type. In this paper, we present four kinds of frequent patterns from defects detected by black-box testing (called black-box defect) based on a kind of detailed classification named ODC-BD (Orthogonal Defect Classification for Blackbox Defect). The frequent patterns include the top 10 conditions (data or operation) which most easily result in defects or severe defects, the top 10 defect phenomena which most frequently occur and have a great impact on users, association rules between function modules and defect types. We aim to help project managers, black-box testers and developers improve the efficiency of software defect detection and analysis using these frequent patterns. Our study is based on 5023 defect reports from 56 large industrial projects and 2 open source projects.",2010,0,
358,359,Low error rate LDPC decoders,"Low-density parity-check (LDPC) codes have been demonstrated to perform very close to the Shannon limit when decoded iteratively. However challenges persist in building practical high-throughput decoders due to the existence of error floors at low error rate levels. We apply high-throughput hardware emulation to capture errors and error-inducing noise realizations, which allow for in-depth analysis. This method enables the design of LDPC decoders that operate without error floors down to very low bit error rate (BER) levels. Such emulation-aided studies facilitate complex systems designs.",2009,0,
359,360,Fault-tolerant and energy-efficient permutation routing protocol for wireless networks,"A wireless network (WN) is a distributed system where each node is a small hand-held commodity device called a station. Wireless sensor networks have received increasing interest in recent years due to their usage in monitoring and data collection in a wide variety of environments like remote geographic locations, industrial plants, toxic locations or even office buildings. Two of the most important issues related to a WN are their energy constraints and their potential for developing faults. A station is usually powered by a battery which cannot be recharged while on a mission. Hence, any protocol run by a WN should be energy-efficient. Moreover, it is possible that all stations deployed as part of a WN may not work perfectly. Hence, any protocol designed for a WN should work well even when some of the stations are faulty. We design a protocol which is both energy-efficient and fault-tolerant for permutation routing in a WN.",2003,0,
360,361,A novel approach to architecture of radar fault diagnosis system based on mobile agents,"In order to improve radar fault diagnosis system, a new architecture of fault diagnosis system based on mobile agents is proposed. The architecture is based on an embedded network built-in radar system. It utilizes all kinds of mobile fault diagnostic agents in embedded network to detect shortcomings of distributed subsystems in radar. In the architecture, all MFDAs can migrate in embedded network, and can be centralized a personal computer so as to be updated and retrained conveniently for different batches of radar systems. In this paper, three kinds of start-up modes of fault diagnosis are illustrated, two kinds of multi-agent cooperation diagnostic frameworks are introduced, and a kind of structure of MFDA is addressed.",2010,0,
361,362,A Flexible and efficient bit error rate simulation method for high-speed differential link analysis using time-domain interpolation and superposition,"In this paper, a flexible and efficient time-domain method for calculating the bit error rate of high-speed differential links is presented. The method applies interpolation and superposition to the step response of a channel to construct the jittery data or/and clock waveforms at the receiver. With the statistics of the actual reference-crossing points extracted from the constructed receiver waveforms, the bathtub curves can be derived and extrapolated to get the eye margin at the given bit error rate. A software has been developed and applied for high-speed differential link design using the method. Good correlation has been achieved between the simulated results using this method and the measurement data with a bit error rate tester.",2008,0,
362,363,Do Crosscutting Concerns Cause Defects?,"There is a growing consensus that crosscutting concerns harm code quality. An example of a crosscutting concern is a functional requirement whose implementation is distributed across multiple software modules. We asked the question, ""How much does the amount that a concern is crosscutting affect the number of defects in a program?"" We conducted three extensive case studies to help answer this question. All three studies revealed a moderate to strong statistically significant correlation between the degree of scattering and the number of defects. This paper describes the experimental framework we developed to conduct the studies, the metrics we adopted and developed to measure the degree of scattering, the studies we performed, the efforts we undertook to remove experimental and other biases, and the results we obtained. In the process, we have formulated a theory that explains why increased scattering might lead to increased defects.",2008,0,
363,364,Impact of Transmission Network Reinforcement on Improvement of Power System Voltage Stability and Solving the Dynamic Delayed Voltage Recovery and Motor Stalling Problem After System Faults in the Saudi Electricity in the Western Region,"The Saudi Electricity company power system in the Western Region of the Kingdom (SEC-WR) is unique in its load pattern, growth trends and type, generation recourses and network configuration. The power system of the SEC-WR faced and is facing a high load growth. The high load increase gives rise to a very high loading of the transmission system elements, mainly power transformers and cables. The Western Region load is mainly composed of air conditioner (AC) during high load season. In case of faults this nature of load induces delayed voltage recovery following fault clearing on the transmission system. The sustained low voltage following transmission line faults could cause customer interruptions and may be equipment damage. The integrity of the transmission system may also be affected. The transient stability of the system may be affected. This may also influence the stability of the generating units in the system. The existing dynamic model of SEC-WR System has been described. The response of the model to the actual faults is compared with actual records obtained from the dynamic system monitor (DSM) installed in several locations in the SEC-WR System. The solution of the delayed voltage recovery problem after system faults may be achieved by reinforcement of the system, adding static VAr compensators (SVC) to provide the dynamic reactive power support to the system, reducing the fault clearing time and by under voltage load shedding. This paper analyzes and discusses the first alternative, the system reinforcement",2006,0,
364,365,Higher-order corrections to the pi criterion for the periodic operation of chemical reactors,"The present work develops a method to determine higher-order corrections to the pi criterion, derived from basic results of Center Manifold theory. The proposed method is based on solving the Center Manifold PDE via power series. The advantage of the proposed approach is the improvement of the accuracy of the pi criterion in predicting performance under larger amplitudes. The proposed method is applied to a continuous stirred tank reactor, where the yield of the desired product must be maximized.",2009,0,
365,366,Practical Criteria for the Separability of Eddy-Current Testing Signals on Multiple Defects,"Practical quantities have been introduced by the authors to characterize the interaction among multiple defects located in a specimen under eddy-current testing (ECT). If these quantities indicate that the interaction between the pairs of defects is negligible, the signal of an ECT probe for multiple defects can be calculated as the superposition of those signals, which are obtained for each defect as if it would be a single one. Conversely, if the criteria holds, the measured ECT signal can be decomposed into signals associated with the individual defects, which essentially simplifies the solution of the inverse problem of defect reconstruction. As an application of the criteria, the design of a novel barcoding system is presented, which is developed for marking metallic parts by laser treating, and where the applicability of linear signal processing methods for the reading out of the barcode is a requirement.",2008,0,
366,367,On-Line Reconfigurable XGFT Network-on-Chip Designed for Improving the Fault-Tolerance and Manufacturability of the MPSoC Chips,"Large System-on-Chip (SoC) circuits will contain an increasing number of processors which will communicate with each other across Networks-on-Chip (NOC). The faulty processors could be replaced with faultless ones, whereas only a single defect in the NOC can make the whole chip unusable. Therefore, the fault-tolerance of the NOC is a crucial component of the fault-tolerance and manufacturability of the SoCs. This paper presents a fault-tolerant extended generalized fat tree (XGFT) NOC developed for future multi-processor SoCs (MPSoC). Its fault-tolerance is improved with a new version of fault-diagnosis-and-repair (FDAR) system, which makes it possible to diagnose and repair the NOC on-line. It detects such static, dynamic and transient faults which block packets or produce bit errors, and reconfigures the faulty switches to operate correctly. Processors can also use it for reconfiguring the faulty switch nodes after the faults are located with other test methods. Simulation and synthesis results show that slightly defected XGFTs are able to achieve good performance after they are repaired with the FDAR while the costs of the FDAR remain tolerable",2006,0,
367,368,Attitude correction algorithm using GPS measurements for flight vehicles,For flight systems with an on-board seeker the attitude error is the major factor to determine the seeker pointing error at the time of object acquisition. To achieve a desired mission it must be minimized. The proposed algorithm corrects the attitude error in the guidance computer during flight by taking its position and velocity measurements from GPS or radar. This is possible since navigator's position and velocity states are correlated with attitude state. Computer simulation is shown to prove the proposed algorithm.,2002,0,
368,369,Hardware/software optimization of error detection implementation for real-time embedded systems,"This paper presents an approach to system-level optimization of error detection implementation in the context of fault-tolerant real-time distributed embedded systems used for safety-critical applications. An application is modeled as a set of processes communicating by messages. Processes are mapped on computation nodes connected to the communication infrastructure. To provide resiliency against transient faults, efficient error detection and recovery techniques have to be employed. Our main focus in this paper is on the efficient implementation of the error detection mechanisms. We have developed techniques to optimize the hardware/software implementation of error detection, in order to minimize the global worst-case schedule length, while meeting the imposed hardware cost constraints and tolerating multiple transient faults. We present two design optimization algorithms which are able to find feasible solutions given a limited amount of resources: the first one assumes that, when implemented in hardware, error detection is deployed on static reconfigurable FPGAs, while the second one considers partial dynamic reconfiguration capabilities of the FPGAs.",2010,0,
369,370,Development of Defect Classification Algorithm for POSCO Rolling Strip Surface Inspection System,"Surface inspection system (SIS) is an integrated hardware-software system which automatically inspects the surface of the steel strip. It is equipped with several cameras and illumination over and under the steel strip roll and automatically detects and classifies defects on the surface. The performance of the inspection algorithm plays an important role in not only quality assurance of the rolled steel product, but also improvement of the strip production process control. Current implementation of POSCO SIS has good ability to detect defects, however, classification performance is not satisfactory. In this paper, we introduce POSCO SIS and suggest a new defect classification algorithm which is based on support vector machine technique. The suggested classification algorithm shows good classification ability and generalization performance",2006,0,
370,371,Design of a fault-tolerant coarse-grained,"This paper considers the possibility of implementing low-cost hardware techniques which would allow to tolerate temporary faults in the datapaths of coarse-grained reconfigurable architectures (CGRAs). Our goal was to use less hardware overhead than commonly used duplication or triplication methods. The proposed technique relies on concurrent error detection by using residue code modulo 3 and re-execution of the last operation, once an error is detected. We have chosen the DART architecture as a vehicle to study the efficiency of this approach to protect its datapaths. Simulation results have confirmed hardware savings of the proposed approach over duplication.",2010,0,
371,372,"HGRID: Fault Tolerant, Log2N Resource Management for Grids","Grid resource discovery service is currently a very important focus of research. We propose a scheme that presents essential characteristics for efficient, self-configuring and fault-tolerant resource discovery and is able to handle dynamic attributes, such as memory capacity. Our approach consists of an overlay network with a hypercube topology connecting the grid nodes and a scalable, fault-tolerant, self-configuring search algorithm. By design, the algorithm improves the probability of reaching all working nodes in the system even in the presence of failures (inaccessible, crashed or heavy loaded nodes). We analyze the static resilience of the presented approach, that is to say, how well the algorithm is able to find resources without having to update the routing tables. The results show that the presented approach has a high static resilience.",2009,0,
372,373,Adaptive Error-Resilience Transcoding and Fairness Grouping for Video Multicast Over Wireless Networks,"In this paper, we present a two-pass intra-refresh transcoder for on-the-fly enhancing error resilience of a compressed video in a three-tier streaming system. Furthermore, we consider the problem of multicasting a video to multiple clients with diverse channel conditions. We propose a MINMAX loss rate estimation scheme to determine a single intra- refresh rate for all the clients in a multicast group. For the scenario that a quality variation constraint is imposed on the users, we also propose a grouping method to partition a multicast group of heterogeneous users into a minimal number of sub-groups to minimize the channel bandwidth consumption while meeting the quality variation constraint and achieving fairness among all sub-groups. Experimental results show that the proposed method can effectively mitigate the error propagation due to packet loss as well as achieve fairness not only among all sub-groups and also clients in a multicast group.",2007,0,
373,374,Error Control for IPTV over xDSL Networks,"We discuss the necessity of error control for supporting IPTV over imperfect access networks. In particular, we consider typical DSL environments, and examine the physical-layer impairments and error-mitigation techniques. For these networks, we evaluate the performance of two different application-layer Forward Error Correction (FEC) methods. An overview of hybrid error-control methods and recent developments in standardization is also presented.",2008,0,
374,375,A new approach of halftoning based on error diffusion with rough set filtering,"The rough set filtering makes use of the concepts of indiscernibility relations and approximation spaces to define an equivalence class of neighboring pixels in a processing mask, then utilize the statistical mean of the equivalence classes to replace the gray levels of the central pixel in a processing mask. The error diffusion makes use of the correction factor composed of the weighted errors for these pixels prior to addition of the pixel to be processed to diffuse error over the neighboring pixels in a continuous tone image. Both of a system and an algorithm of implementation of halftoning on error diffusion with rough sets are introduced in the paper",2000,0,
375,376,Outage performance of mrt with unequal-power co-channel interference and channel estimation error,In this letter we investigate the outage performance of maximal ratio transmission (MRT) with unequal-power co-channel interference (CCI) and channel estimation error. The exact expression for the outage probability is presented. Our results are applicable to the MRT systems with arbitrary numbers of transmit and receive antennas.,2007,0,
376,377,Optimal placement of sensor in gearbox fault diagnosis based on VPSO,"The optimization layout of the acceleration sensor and application of particle swarm optimization (PSO) algorithm to solve the fitness problems of such optimization are discussed in this paper. Based on the gearbox finite element modeling and the result of modal analysis, use the particle swarm optimization with adaptive velocity (VPSO) algorithm, and take the two kinds of fitness function as evaluation goal, has realized the optimization and positioning of gearbox sensor layout, analyzed optimization result.",2010,0,
377,378,Study of SINS/GPS/DVL integrated navigation system's fault tolerance,This paper put forward several fault tolerant arithmetic combining engineering applications on the AUV (autonomous underwater vehicle). The arithmetic is based on traditional centralized Kalman Filter and can improve SINS/GPS/DVL integrated navigation system's precision and capability of fault tolerant. The simulation and the vehicle tests validate the arithmetic.,2005,0,
378,379,New method for current and voltage measuring offset correction in an induction motor sensorless drive,This paper presents a new algorithm for electromagnetic torque and flux estimation in a sensorless drive when uncompensated dc offset of current and/or voltage sensors are present. The novel feature of the offset error correction algorithm is an attempt not to eliminate the consequence of problem but to identify its source. The algorithm uses the first harmonic of estimated torque and dc value of estimated stator flux to identify the source and value of the current and/or voltage offset error. Identified values can be used for offset cancelation which improves estimation process.,2010,0,
379,380,The Digital Circuit Fault Diagnosis Interface Design and Realization Based on VXI,"This paper discusses in detail the development process of general interface adapter in the digital circuit fault diagnosis system based on VXI. After introducing VXI bus, the paper gives overall description of fault diagnosis system, presents a method of solving the problem about load matching and interface matching, and realizes the function of identification to read and write on the memory of interface circuit and to control chip selection. The method of identity installation in interface circuit to be selected is to give an ID number and add a memory to interface circuit to ensure the accuracy and effectiveness. The paper also describes the method of self diagnosis in the interface circuit that is the key to whole fault diagnosis system.",2008,0,
380,381,The DVB television signal transmission simulation using the forward error correction codes,The contribution deals with the simulation of the digital video signal transmission through the baseband transmission channel model. The simulation model that covers selected phenomena of DVB (digital video broadcasting) system signal processing is presented. The digital video signal is represented with the digital data of one noncompressed video frame that is channel encoded and protected against errors with the forward error correction (FEC) codes. The transmission channel model has influence on transmitted digital data and its distortion and the pertubative signals affect on the data decoding. The developed interactive simulation software (Matlab application) features are outlined too and the conclusion presents efficiency of the used FEC codes.,2003,0,
381,382,Residual Generators for Fault Diagnosis Using Computation Sequences With Mixed Causality Applied to Automotive Systems,"An essential step in the design of a model-based diagnosis system is to find a set of residual generators fulfilling stated fault detection and isolation requirements. To be able to find a good set, it is desirable that the method used for residual generation gives as many candidate residual generators as possible, given a model. This paper presents a novel residual generation method that enables simultaneous use of integral and derivative causality, i.e., mixed causality, and also handles equation sets corresponding to algebraic and differential loops in a systematic manner. The method relies on a formal framework for computing unknown variables according to a computation sequence. In this framework, mixed causality is utilized, and the analytical properties of the equations in the model, as well as the available tools for algebraic equation solving, are taken into account. The proposed method is applied to two models of automotive systems, a Scania diesel engine, and a hydraulic braking system. Significantly more residual generators are found with the proposed method in comparison with methods using solely integral or derivative causality.",2010,0,
382,383,Online drift correction in wireless sensor networks using spatio-temporal modeling,"Wireless sensor networks are deployed for the purpose of sensing and monitoring an area of interest. Sensors in the sensor network can suffer from both random and systematic bias problems. Even when the sensors are properly calibrated at the time of their deployment, they develop drift in their readings leading to erroneous inferences being made by the network. The drift in this context is defined as a slow, unidirectional, long-term change in the sensor measurements. In this paper we present a novel algorithm for detecting and correcting sensors drifts by utilising the spatio-temporal correlation between neigbouring sensors. Based on the assumption that neighbouring sensors have correlated measurements and that the instantiation of drift in a sensor is uncorrelated with other sensors, each sensor runs a support vector regression algorithm on its neigbourspsila corrected readings to obtain a predicted value for its measurements. It then uses this predicted data to self-assess its measurement and detect and correct its drift using a Kalman filter. The algorithm is run recursively and is totally decentralized. We demonstrate using real data obtained from the Intel Berkeley Laboratory that our algorithm successfully suppresses drifts developed in sensors and thereby prolongs the effective lifetime of the network.",2008,0,
383,384,Applying fault-tolerant solutions of circulant graphs to meshes and hypercubes,"Many important architectures such as rings, meshes and hypercubes can be modeled as circulant graphs. As a result, circulant graphs have received a lot of attention, and a new method was developed for designing fault-tolerant solutions for them. We review this method in this paper, and examine its applications to the design of fault-tolerant solutions for meshes and hypercubes. Our results indicate that these solutions are efficient.",2005,0,
384,385,"Vietnamese spelling detection and correction using Bi-gram, Minimum Edit Distance, SoundEx algorithms with some additional heuristics","The spelling checking problem is considered to contain two main phases: the detecting phase and the correcting phase. In this paper, we present a new approach for Vietnamese spelling checking based on Vietnamese characteristics for each phase. Our research approach includes the use of a syllable Bi-gram in combination with parts of speech (POS) to find out suspected syllables. In the correcting phase, we based on minimum edit distance, SoundEx algorithms and some heuristics to build a weight function for assessing suggestion candidates. The training corpus and the test set were collected from e-newspapers.",2008,0,
385,386,A fault-tolerant control architecture for induction motor drives in automotive applications,"This paper describes a fault-tolerant control system for a high-performance induction motor drive that propels an electrical vehicle (EV) or hybrid electric vehicle (HEV). In the proposed control scheme, the developed system takes into account the controller transition smoothness in the event of sensor failure. Moreover, due to the EV or HEV requirements for sensorless operations, a practical sensorless control scheme is developed and used within the proposed fault-tolerant control system. This requires the presence of an adaptive flux observer. The speed estimator is based on the approximation of the magnetic characteristic slope of the induction motor to the mutual inductance value. Simulation results, in terms of speed and torque responses, show the effectiveness of the proposed approach.",2004,0,
386,387,Fault classification and fault distance location of double circuit transmission lines for phase to phase faults using only one terminal data,"An accurate fault classification algorithm for double end fed parallel transmission lines based on application of artificial neural networks is presented in this paper. The proposed method uses the voltage and current available at only the local end of line. This method is virtually independent of the effects of remote end infeed and is insensitive to the variation of fault inception angle and fault location. The Simulation results show that phase-to-phase faults can be correctly detected, classified and located within one cycle after the inception of fault. Large number of faults simulations using MATLAB<sup>A</sup>7.01 have demonstrated the accuracy and effectiveness of the proposed algorithm. The proposed scheme allows the protection engineers to increase the reach setting i.e. greater portion of line length can be protected as compared to conventional techniques. The technique neither requires a communication link to retrieve the remote end data nor zero sequence current compensation for healthy phases.",2009,0,
387,388,Denoising fluorescence endoscopy - A motion compensated temporal recursive video filter with an optimal minimum mean square error parameterization,"Fluorescence endoscopy is an emerging technique for the detection of bladder cancer. A marker substance is brought into the patient's bladder which accumulates at cancer tissue. If a suitable narrow band light source is used for illumination, a red fluorescence of the marker substance is observable. Because of the low fluorescence photon count and because of the narrow band light source, only a small amount of light is detected by the camera's CCD sensor. This, in turn, leads to strong noise in the recorded video sequence. To overcome this problem, we apply a temporal recursive filter to the video sequence. The derivation of a filter function is presented, which leads to an optimal filter in the minimum mean square error sense. The algorithm is implemented as plug-in for the real-time capable clinical demonstrator platform RealTimeFrame and it is capable to process color videos with a resolution of 768times576 pixels at 50 frames per second.",2009,0,
388,389,Compact multilayer coupled stripline LTCC filter with defected ground structure,"A novel multilayer coupled stripline resonator structure is introduced to realize miniature broadband band-pass filter using low temperature co-fired ceramic (LTCC) process with defected ground structure (DGS). Wide bandwidth and good selectivity are obtained by exploiting four resonators and the filter exhibits a high rejection in stopband by adopting the tapered DGS. Moreover, an inductance feed back between the output and input is introduced to produce transmission zeros. Filter with size of <sub>0</sub> /12  <sub>0</sub> /12  h (<sub>0</sub> is the wavelength at the midband frequency; h is the substrate height) is designed, fabricated and measured. The measured responses agree well with simulation results.",2009,0,
389,390,Dynamic Fault Handling Mechanisms for Service-Oriented Applications,"Dynamic fault handling is a new approach for dealing with fault management in service-oriented applications. Fault handlers, termination handlers and compensation handlers are installed at execution time instead of being statically defined. In this paper we present this programming style and our implementation of dynamic fault handling in JOLIE, providing finally a nontrivial example of its usage.",2008,0,
390,391,Towards Software Quality Economics for Defect-Detection Techniques,"There are various ways to evaluate defect-detection techniques. However, for a comprehensive evaluation the only possibility is to reduce all influencing factors to costs. There are already some models and metrics for the cost of quality that can be used in that context. The existing metrics for the effectiveness and efficiency of defect-detection techniques and experiences with them are combined with cost metrics to allow a more fine-grained estimation of costs and a comprehensive evaluation of defect-detection techniques. The current model is most suitable for directly comparing concrete applications of different techniques",2005,0,
391,392,Fault tolerance based on the publish-subscribe paradigm for the BonjourGrid middleware,"How to federate the machines of all Boinc, Condor and XtremWeb projects? If you believe in volunteer computing and want to share more than one project then BonjourGrid may help. In previous works, we proposed a novel approach, called BonjourGrid, to orchestrate multiple instances of Institutional Desktop Grid middleware. It is our way to remove the risk of bottleneck and failure, and to guarantee the continuity of services in a distributed manner. Indeed, BonjourGrid can create a specific environment for each user based on a given computing system of his choice such as XtremWeb, Condor or Boinc. This work investigates, first, the procedure to deploy Boinc and Condor on top of BonjourGrid and, second, proposes a fault tolerant approach based on passive replication and virtualization to tolerate the crash of coordinators. The novelty resides here in an integrated environment based on Bonjour (publication-subscription mecanism) for both the coordination protocol and for the fault tolerance issues. In particular, it is not so frequent to our knowledge to describe and to implement a fault tolerant protocol according to the pub-sub paradigm. Experiments, conducted on the Grid'5000 testbed, illustrate a comparative study between Boinc (respectively Condor) on top of BonjourGrid and a centralized system using Boinc (respectively Condor) and second prove the robustness of the fault tolerant mechanism.",2010,0,
392,393,An approach to detecting domain errors using formal specification-based testing,"Domain testing, a technique for testing software or portions of software dominated by numerical processing, is intended to detect domain errors that usually arise from incorrect implementations of desired domains. This paper describes our recent work aiming to provide support for revealing domain errors using formal specifications. In our approach, formal specifications serve as a means for domain modeling. We describe a strong domain testing strategy that guide testers to select a set of test points so that the potential domain errors can be effectively detected, and apply our approach in two case studies for test cases generation.",2004,0,
393,394,A Robust Error Detection Mechanism for H.264/AVC Coded Video Sequences Based on Support Vector Machines,"Current trends in wireless communications provide fast and location-independent access to multimedia services. Due to its high compression efficiency, H.264/AVC is expected to become the dominant underlying technology in the delivery of future wireless video applications. The error resilient mechanisms adopted by this standard alleviate the problem of spatio-temporal propagation of visual artifacts caused by transmission errors by dropping and concealing all macroblocks (MBs) contained within corrupted segments, including uncorrupted MBs. Concealing these uncorrupted MBs generally causes a reduction in quality of the reconstructed video sequence.",2008,0,
394,395,Track Down HW Function Faults Using Real SW Invariants,System level functional verification by running real software stack on FPGA prototype is essential for achieving a high quality design. But it is hard to find the exact source of hardware function faults while running large closed source system software fails. This paper proposes the idea of tracking down faults through real system software control flow invariants with current trace output hardware support. It captures qualified control flow invariant trace in reference execution and test trace; and tracks down faults through comparing offline invariant trace and test trace. The approach can deal with both deterministic and nondeterministic execution. We implemented the proof of concept in full system simulator Bochs. Our experimentation with the real closed source MS Windows XP suggests that the approach is effective in tracking down hardware function faults.,2009,0,
395,396,Design of a fault-tolerant satellite cluster link establishment protocol,"The design of a protocol for satellite cluster link establishment and management that accounts for link corruption, node failures, and node re-establishment is presented in this paper. This protocol will need to manage the traffic flow between nodes in the satellite cluster, adjust routing tables due to node motion, allow for sub-networks in the cluster, and similar activities. This protocol development is in its initial stages. Preliminary results with eight nodes demonstrate its operations and potential problems that may arise when significant numbers of channel errors are present",2005,0,
396,397,A novel approach to calculate the severity and priority of bugs in software projects,"Discovering and fixing software bugs is a difficult maintenance task, and a considerable amount of effort is devoted by software developers on this issue. In the world of software one cannot get rid of the bugs, fixes, patches etc. each of them have a severity and priority associated to it. There is not yet any formal relation between these components as both of these either depends on the developer and tester or on customer and project manger to be decided on. On one hand, the priority of a component depends on the cost and the efforts associated with it. While on the other, the severity depends on the efforts required to accomplish a particular task. This research paper proposes a formula that can draw a relationship among severity and priority.",2010,0,
397,398,Reducing the soft-error rate of a high-performance microprocessor,"Single-bit upsets from transient faults have emerged as a key challenge in microprocessor design. Soft errors will be an increasing burden for microprocessor designers as the number of on-chip transistors continues to grow exponentially. Unlike traditional approaches, which focus on detecting and recovering from faults, this article introduces techniques to reduce the probability that a fault will cause a declared error. The first approach reduces the time instructions sit in vulnerable storage structures. The second avoids declaring errors on benign faults. Applying these techniques to a microprocessor instruction queue significantly reduces its error rate with only minor performance degradation",2004,0,
398,399,The Orion GN&C data-driven flight software architecture for automated sequencing and fault recovery,"The Orion Crew Exploration Vehicle (CEV) is being designed to include capabilities that allow significantly more automation than either the Space Shuttle or the International Space Station (ISS). In particular, the vehicle flight software has requirements to accommodate increasingly automated missions throughout all phases of flight. This paper presents the Guidance, Navigation & Control (GN&C) flight software architecture designed to provide evolvable automation capability that sequences through software modes and configurations. This software architecture is required to maintain flexibility to address the maturation of operational concepts over time, permit ground and crew operators to gain trust in the system, and provide capabilities for human override of the automation in `off-nominal' situations. To allow for mission flexibility, reconfigurability and reduce the recertification expense over the life of the program, a data-driven approach is used to load the mission event plan as well as the flight software artifacts associated with the GN&C subsystem. The flight software schema for automated mission sequencing is presented with a concept of operations for interactions with ground and crew members. This data is managed through a prototype database of GN&C level sequencing data, which tracks mission specific parameters to aid in the scheduling of GN&C activities. A prototype architecture for fault detection, isolation and recovery interactions with the automation software is presented as part of the upcoming design maturation to respond with appropriate GN&C and vehicle-level actions in `off-nominal' scenarios.",2010,0,
399,400,An experimental study of security vulnerabilities caused by errors,"The paper presents an experimental study which shows that, for the Intel x86 architecture, single-bit control flow errors in the authentication sections of targeted applications can result in significant security vulnerabilities. The experiment targets two well-known Internet server applications: FTP and SSH (secure shell), injecting single-bit control flow errors into user authentication sections of the applications. The injected sections constitute approximately 2-8% of the text segment of the target applications. The results show that out of all activated errors: (a) 1-2% comprised system security (create a permanent window of vulnerability); (b) 43-62% resulted in crash failures (about 8.5% of these errors create a transient window of vulnerability); and (c) 7-12% resulted in fail silence violations. A key reason for the measured security vulnerabilities is that, in the x86 architecture, conditional branch instructions are a minimum of one Hamming distance apart. The design and evaluation of a new encoding scheme that reduces or eliminates this problem is presented.",2001,0,
400,401,Defect Tolerance Based on Coding and Series Replication in Transistor-Logic Demultiplexer Circuits,"We present a family of defect tolerant transistor-logic demultiplexer circuits that can defend against both stuck-ON (short defect) and stuck-OFF (open defect) transistors. Short defects are handled by having two or more transistors in series in the circuit, controlled by the same signal. Open defects are handled by having two or more parallel branches in the circuit, controlled by the same signals, or more efficiently, by using a transistor-replication method based on coding theory. These circuits are evaluated, in comparison with an unprotected demultiplexer circuit, by: 1) modeling each circuit's ability to tolerate defects and 2) calculating the cost of the defect tolerance as each circuit's redundancy factor R, which is the relative number of transistors required by the circuit. The defect-tolerance model takes the form of a function giving the failure probability of the entire demultiplexer circuit as a function of the defect probabilities of its component transistors, for both defect types. With the advent of defect tolerance as a new design goal for the circuit designer, this new form of performance analysis has become necessary.",2007,0,
401,402,Adaptive Correction of Errors from Segmented Digital Ink Texts in Chinese Based on Context,"Digital ink texts in Chinese can neither be converted into users' desired layouts nor be recognized until their characters, lines, and paragraphs are correctly extracted. There are many errors in automatically segmented digital ink texts in Chinese because they are free forms and mixed with other languages, as well as their Chinese characters have small gaps and complex structures. Paragraphs, lines, and characters (recognizable language symbols) in digital ink may be wrongly extracted. An adaptive approach based on context is proposed to correct wrongly extracted these objects. Each extracted object is first adaptively visualized by color and shape labels according to relations between it and its neighbors. Users use simple gestures naturally and easily to merge and split wrongly extracted objects. Contexts are constructed from users' gestures and objects invoked by them, where users' intensions are identified. We have conducted experiments using real-life segmented digital ink texts in Chinese and compared the proposed approach with others. Experimental results demonstrate that the proposed approach is feasible, flexible, effective, and robust.",2010,0,
402,403,Memory Yield Improvement through Multiple Test Sequences and Application-Aware Fault Models,"In this paper, we propose a way to improve the yield of memory products by selecting the appropriate test strategy for memory Built- in Self-Test (BIST). We argue that by testing the memory through a sequence of test algorithms which differ in their fault coverage, it is possible to bin the memory into multiple yield bins and increase the yield and product revenue. Further, the test strategy must take into consideration the usage model of the memory. Thus, a number of video and audio buffers are used in sequential access mode, but are overtested using conventional memory test algorithms which model a large number of defects which do not impact the operation of the buffers. We propose a binning strategy where memory test algorithms are applied in different order of strictness such that bins have a specific defect / fault grade. Depending on the applications some of these bins need not be discarded but sold at a lower price as the functionality would never catch the fault due to its usage of memory. We introduce the notion of a test map for the on-chip memories in a SoC and provide results of yield simulation on two specific test strategies called ""Most Strict First"" and ""Least Strict First"". Our simulations indicate that significant improvements in yield are possible through the adoption of the proposed technique. We show that the BIST controller area and run-time overheads also reduce when information about the usage model of the memory, such as sequential access, is exploited.",2008,0,
403,404,Correcting asr outputs: Specific solutions to specific errors in French,"Automatic speech recognition (ASR) systems are used in a large number of applications, in spite of the inevitable recognition errors. In this study we propose a pragmatic approach to automatically repair ASR outputs by taking into account linguistic and acoustic information, using formal rules or stochastic methods. The proposed strategy consists in developing a specific correction solution for each specific kind of errors. In this paper, we apply this strategy on two case studies specific to French language. We show that it is possible, on automatic transcriptions of French broadcast news, to decrease the error rate of a specific error by 11.4% in one of two the case studies, and 86.4% in the other one. These results are encouraging and show the interest of developing more specific solutions to cover a wider set of errors in a future work.",2008,0,
404,405,The Design of Fault Diagnosis Expert System about Temperature Adjustment System Based on CLIPS,"The fault diagnosis of a certain launching unit's temperature controller is researched with the object-oriented programming method based on expert system theory, the fault-diagnosis expert-system software is designed and developed with VC++ 2008 and CLIPS. The structure of the system is firstly analyzed; The representation of knowledge, the design of database and the production rule are discussed then; lastly the diagnosis flow is studied up and a demonstration of the fault diagnosis is given.",2009,0,
405,406,A highly selective super-wide bandpass filter by cascading HMSIW with asymmetric defected ground structure,"The half mode substrate integrated waveguide (HMSIW) possesses the highpass characteristic of SIW but the size is nearly half reduced. A recently proposed asymmetric defected ground structure (ADGS), composed of two square headed slots connected with a rectangular slot transversely under a microstrip line, exhibits quasi-elliptic-function band-reject characteristics around 3 GHz with high selectivity. Based on the circuit model, the structure of the ADGS is modified to perform well at about 16 GHz. By combining the HMSIW and the modified ADGS, a super-wide bandpass filter operating at about 8-16 GHz with high selectivity at both upper and lower band is proposed. Both simulated and measured results have been presented to demonstrate the validity of the proposed wideband filter.",2010,0,
406,407,A real-time fault tolerant intra-body network,"This paper designs an intra-body network (IBN) of nodes, consisting of small sensors and processing elements (SPEs) placed at different locations within the body and a personal digital assistant placed externally but in close proximity to the body. The sensors measure specific physiological attributes such as electrophysiological and biochemical changes in the myocardium (action potentials of cells), glucose level, blood viscosity etc. and forward them to the processing element. Communication protocols for configuration and data access protocols are proposed. The privacy of the IBN data, fault tolerance and real-time data acquisition are addressed.",2002,0,
407,408,Evaluating speech recognition in the context of a spoken dialogue system: critical error rate,"Evaluating a speech recognition system is a key issue towards understanding its deficiencies and focusing potential improvements on useful aspects. When a system is designed for a given application, it is particularly relevant to have an evaluation procedure that reflects the role of the system in this application. Evaluating continuous speech recognition through word error rate is not completely appropriate when the speech recognizer is used as spoken dialogue system input. Some errors are particularly harmful, when they concern content words for example, while some others do not have any impact on the following comprehension step. The attempt is not to evaluate natural language understanding but to propose a more appropriate evaluation of speech recognition, by making use of semantic information to define the notion of critical errors.",2001,0,
408,409,Detecting VLIW Hard Errors Cost-Effectively through a Software-Based Approach,"Research indicates that as technology scales, hard errors such as wear-out errors are increasingly becoming a critical challenge for microprocessor design. While hard errors in memory structures can be efficiently detected by error correction code, detecting hard errors for functional units cost-effectively is a challenging problem. In this paper, we propose to exploit the idle cycles of the under-utilized VLIW functional units to run test instructions for detecting wear-out errors without increasing the hardware cost or significantly impacting performance. We also explore the design space of this software-based approach to balance the error detection latency and the performance for VLIW architectures. Our experimental results indicate that such a software-based approach can effectively detect hard errors with minimum impact on performance for VLIW processors, which is particularly useful for reliable embedded applications with cost constraints.",2007,0,
409,410,Evaluating the effectiveness of a software fault-tolerance technique on RISC- and CISC-based architectures,"This paper deals with a method able to provide a microprocessor-based system with safety capabilities by modifying the source code of the executed application, only. The method exploits a set of transformations which can automatically be applied, thus greatly reducing the cost of designing a safe system, and increasing the confidence in its correctness. Fault Injection experiments have been performed on a sample application using two different systems based on CISC and RISC processors. Results demonstrate that the method effectiveness is rather independent of the adopted platform",2000,0,
410,411,Experimental validation of fault injection analyses by the FLIPPER tool,The paper discusses the experimental validation of fault injection analyses accomplished with the FLIPPER tool. Validation has been accomplished through accelerated proton testing of a benchmark design provided by the European Space Agency.,2009,0,
411,412,A primary exploration of three-dimensional echocardiographic intra-cardiac virtual reality visualization of atrial septal defect: in vitro validation,"To evaluate the diagnostic value of three-dimensional echocardiography (3-DE) in congenital heart disease such as atrial septal defect (ASD) by virtual reality (VR), ten ASDs with different size and shape were created in ten fresh explained porcine hearts. HP SONOS 5500 imaging system was employed for 3-DE reconstructed and visualized by virtual reality computing techniques. The results showed that all ASDs were successfully reconstructed. The site, geometry were well appraised in its true form. The area, maximum and minimum diameter of ASD were measured on 3D reconstruction and compared with independently measured anatomic date. Good correlation was obtained (r>0.95, P<0.01). In conclusion, VR open an exciting opportunity in the field of diagnosis of 3-DE in congenital heart disease",2005,0,
412,413,A Fault-Tolerant Active Pixel Sensor for Mitigating Hot Pixel Defects,"Hot pixel defects are unavoidable in many solid-state image sensors. Affected pixels accumulate dark signal over the course of an exposure, grossly diminishing dynamic range and often rendering measurements unusable. Experiments suggest the mechanisms causing hot pixels are highly localized and the defect will be confined to a single pixel. A redundant, fault-tolerant active pixel sensor architecture that has previously been applied to other defect types is investigated for the suppression of hot pixels. A recovery scheme using minimal computational power is also described.",2007,0,
413,414,Testing for interconnect crosstalk defects using on-chip embedded processor cores,"Crosstalk effects degrade the integrity of signals traveling on long interconnects and must be addressed during production testing. External testing for crosstalk is expensive due to the need for high-speed testers. Built-in self-test, while eliminating the need for a high-speed tester, may lead to excessive test overhead as well as overly aggressive testing. To address this problem, we propose a new software-based self-test methodology for system-on-chip (SoC) devices based on embedded processors. It enables an on-chip embedded processor core to test for crosstalk in system-level interconnects by executing a self-test program in the normal operational mode of the SoC. We have demonstrated the feasibility of this method by applying it to test the interconnects of a processor-memory system. The defect coverage was evaluated using a system-level crosstalk defect simulation method.",2001,0,
414,415,Adaptive Fuzzy Prediction of Low-Cost Inertial-Based Positioning Errors,"Kalman filter (KF) is the most commonly used estimation technique for integrating signals from short-term high performance systems, like inertial navigation systems (INSs), with reference systems exhibiting long-term stability, like the global positioning system (GPS). However, KF only works well under appropriately predefined linear dynamic error models and input data that fit this model. The latter condition is rather difficult to be fulfilled by a low-cost inertial measurement unit (IMU) utilizing microelectromechanical system (MEMS) sensors due to the significance of their long- and short-term errors that are mixed with the motion dynamics. As a result, if the reference GPS signals are absent or the Kalman filter is working for a long time in prediction mode, the corresponding state estimate will quickly drift with time causing a dramatic degradation in the overall accuracy of the integrated system. An auxiliary fuzzy-based model for predicting the KF positioning error states during GPS signal outages is presented in this paper. The initial parameters of this model is developed through an offline fuzzy orthogonal-least-squares (OLS) training while the adaptive neuro-fuzzy inference system (ANFIS) is implemented for online adaptation of these initial parameters. Performance of the proposed model has been experimentally verified using low-cost inertial data collected in a land vehicle navigation test and by simulating a number of GPS signal outages. The test results indicate that the proposed fuzzy-based model can efficiently provide corrections to the standalone IMU predicted navigation states particularly position.",2007,0,
415,416,Fault tolerant PVFS2 based on data replication,"Aggregating the capacity and bandwidth of the commodity disks in the nodes of a cluster provides cost effective and high performance storage systems. Nevertheless, this strategy could be a feasible approach only if the mean time to failure of disks and nodes is faced. The number of failures increases with the nodes and it is especially important in parallel file systems, like PVFS, because having a file striped over server disks increases the probability of failures. This work proposes a strategy to include data replication in the second version of PVFS in order to provide fault tolerance. We also analyze the performance of the implementation of this approach.",2010,0,
416,417,Development of a motion correction system for respiratory-gated PET study,"A respiratory motion during whole-body imaging has been recognized as a source of image quality degradation and reduces the quantitative accuracy of positron emission tomography (PET) study. The aim of this study is to evaluate respiratory gating system and to develop a respiratory motion correction system using trigger generating device built in-house and gated-PET data acquisition mode. We utilized a commercially available laser optical sensor to detect respiratory motion during PET scanning. Each respiratory cycle is divided into 4 bins defined from average peak interval and irregular peak within the breathing motion. The acquired data within the time bins correspond to different positions within the breathing cycle and stored for the post motion correction. Motion data of diaphragm and chest wall was calculated by CT image acquisition during the normal inspiration and expiration position. In the images of a phantom, the blurring artifact due to breathing motion was reduced by our correction method. This technique improves the quantitative specific activity of the tracer which is distorted because of the respiratory motion.",2004,0,
417,418,Fault Diagnosis Method for Mobile Robots Using Multi-CMAC Neural Networks,"Multi-CMAC (cerebellar model articulation controller) neural networks based fault detection and diagnosis (FDD) method for mobile robots are proposed. Three failure types (system fault, sensor fault, and combined fault) are handled. Mobile robot system consists of several functional modules belonging to different module groups, which execute different tasks. According to the consistency among sensors information between the neighbor modules in the same module group, the method of fault diagnosis is studied. Then, multiple CMAC neural networks are used to implement the diagnosis. One CMAC neural network is set to one module group. In the neural network, the sensor information is used as the inputs and the fault signals are used as the outputs. As an example, the method is implemented on a drive system of a wheeled mobile robot. The simulation results show the effectiveness of the proposed technique.",2007,0,
418,419,Fault Evaluator: A tool for experimental investigation of effectiveness in software testing,"The specifications for many software systems, including safety-critical control systems, are often described using complex logical expressions. It is important to find effective methods to test implementations of such expressions. Analyzing the effectiveness of the testing of logical expressions manually is a tedious and error prone endeavor, thus requiring special software tools for this purpose. This paper presents Fault Evaluator, which is a new tool for experimental investigation of testing logical expressions in software. The goal of this tool is to evaluate logical expressions with various test sets that have been created according to a specific testing method and to estimate the effectiveness of the testing method for detecting specific faulty variations of the original expressions. The main functions of the tool are the generation of complete sets of faults in logical expressions for several specific types of faults; gaining expected (Oracle) values of logical expressions; testing faulty expressions and detecting whether a test set reveals a specific fault; and evaluating the effectiveness of a testing approach.",2010,0,
419,420,Calculation of correction factors to compensate for the reference electric field nonuniformity,"The inaccuracy of the reference electric field nonuniformity assessment is identified, which unnecessarily increases the measurement uncertainty of standardized systems for electric field-meters calibration, making them inadequate for calibration of modern, precision field-meters. By means of numerical field calculation, the correction factors are computed which allow compensation for the reference field nonuniformity. The uncertainty of that calculation is also estimated",2001,0,
420,421,Correction for continuous motion in small animal PET,"In small animal PET imaging experiments, animals are generally required to be anaesthetized to avoid motion artifacts. However, anaesthesia can alter biochemical pathways within the brain, thus affecting the physiological parameters under investigation. The ability to image conscious animals would overcome this problem and open up the possibility of entirely new investigational paradigms.",2008,0,
421,422,Short-circuit fault mitigation methods for interior PM synchronous machine drives using six-leg inverters,"This paper characterizes six-leg inverters to mitigate short-circuit faults for interior permanent magnet (IPM) synchronous machines. Key differences between bus structures in six-leg inverters are identified. For six-leg inverters employing two isolated DC links, it is shown that up to 75% of rated output power could be produced following a single-switch short-circuit fault. A magnet flux ing control method is proposed as a response to stator winding type short-circuit faults. This control method results in a zero-torque fault response by the motor. The important influence of the zero sequence in both the motor and inverter structure is identified and developed for this class of fault. Simulation and experimental results are presented verifying the proposed magnet flux ing control method.",2004,0,
422,423,Automatic red-eye detection and correction,"""Red-eye"" is a phenomenon that causes the eyes of flash photography subjects to appear unnaturally reddish in color. Though commercial solutions exist for red-eye correction, all of them require some measure of user intervention. A method is presented to automatically detect and correct redeye in digital images. First, faces are detected with a cascade of multi-scale classifiers. The red-eye pixels are then located with several refining masks computed over the facial region. The masks are created by thresholding per-pixel metrics, designed to detect red-eye artifacts. Once the redeye pixels have been found, the redness is attenuated with a tapered color desaturation. A detector implemented with this system corrected 95% of the red-eye artifacts in 200 tested images.",2002,0,
423,424,Implementing Probabilistic Risk Assessment with Fault Trees to support space exploration missions,This paper seeks to illustrate the implementation of a Probabilistic Risk Assessment (PRA) methodology as a foundation for space mission support risk assessment and management process. Identifying the risks to delivering expected spacecraft data services to a mission is only the first part of the risk assessment. Arriving at a quantified probability (Likelihood) of the manifestation of these risks is the desired outcome of the process.,2010,0,
424,425,Multi-View Video Coding Using Color Correction,The color variations between multi-view video sequences may degrade the inter-view prediction and result in low coding efficiency. In this paper we propose an efficient multi-view video coding scheme using dominant basic color mapping based color correction. The experimental coding results show that color correction has the potential to make multi-view video coding more efficient.,2008,0,
425,426,"We're Finding Most of the Bugs, but What are We Missing?","We compare two types of model that have been used to predict software fault-proneness in the next release of a software system. Classification models make a binary prediction that a software entity such as a file or module is likely to be either faulty or not faulty in the next release. Ranking models order the entities according to their predicted number of faults. They are generally used to establish a priority for more intensive testing of the entities that occur early in the ranking. We investigate ways of assessing both classification models and ranking models, and the extent to which metrics appropriate for one type of model are also appropriate for the other. Previous work has shown that ranking models are capable of identifying relatively small sets of files that contain 75-95% of the faults detected in the next release of large legacy systems. In our studies of the rankings produced by these models, the faults not contained in the predicted most fault prone files are nearly always distributed across many of the remaining files; i.e., a single file that is in the lower portion of the ranking virtually never contains a large number of faults.",2010,0,
426,427,Fault-Tolerant Algorithm for Distributed Primary Detection in Cognitive Radio Networks,"This paper attempts to identify the reliability of detection of licensed primary transmission based on cooperative sensing in cognitive radio networks. With a parallel fusion network model, the correlation issue of the received signals between the nodes in the worst case is derived. Leveraging the property of false sensing data due to malfunctioning or malicious software, the optimizing strategy, namely fault-tolerant algorithm for distributed detection (FTDD) is proposed, and quantitative analysis of false alarm reliability and detection probability under the scheme is presented. In particular, the tradeoff between licensed transmissions and user cooperation among nodes is discussed. Simulation experiments are also used to evaluate the fusion performance under practical settings. The model and analytic results provide useful tools for reliability analysis for other wireless decentralization-based applications (e.g., those involving robust spectrum sensing).",2009,0,
427,428,China's Research Status Quo and Development Trend of Power Grid Fault Diagnosis,"Fault diagnosis is the basic condition for smart grid to achieve the self-healing function, and it is also one of the important research topics of the intelligent dispatching decision support system. On the basis of analyzing its concept and aiming at the current status of China's studies, this paper reviewed and summarized several intelligent fault diagnosis methods, including expert system, artificial neural networks, rough set theory, data mining techniques, multi-agent technology and the entropy theory, then pointed out their application characteristics and existing problems, and finally the prospects of further development in this field were presented.",2010,0,
428,429,"Transparent, Incremental Checkpointing at Kernel Level: a Foundation for Fault Tolerance for Parallel Computers","We describe the software architecture, technical features, and performance of TICK (Transparent Incremental Checkpointer at Kernel level), a system-level checkpointer implemented as a kernel thread, specifi- cally designed to provide fault tolerance in Linux clusters. This implementation, based on the 2.6.11 Linux kernel, provides the essential functionality for transparent, highly responsive, and efficient fault tolerance based on full or incremental checkpointing at system level. TICK is completely user-transparent and does not require any changes to user code or system libraries; it is highly responsive: an interrupt, such as a timer interrupt, can trigger a checkpoint in as little as 2.5s; and it supports incremental and full checkpoints with minimal overhead-less than 6% with full checkpointing to disk performed as frequently as once per minute.",2005,0,
429,430,Algorithmic Cholesky factorization fault recovery,"Modeling and analysis of large scale scientific systems often use linear least squares regression, frequently employing Cholesky factorization to solve the resulting set of linear equations. With large matrices, this often will be performed in high performance clusters containing many processors. Assuming a constant failure rate per processor, the probability of a failure occurring during the execution increases linearly with additional processors. Fault tolerant methods attempt to reduce the expected execution time by allowing recovery from failure. This paper presents an analysis and implementation of a fault tolerant Cholesky factorization algorithm that does not require checkpointing for recovery from fail-stop failures. Rather, this algorithm uses redundant data added in an additional set of processors. This differs from previous works with algorithmic methods as it addresses fail-stop failures rather than fail-continue cases. The implementation and experimentation using ScaLAPACK demonstrates that this method has decreasing overhead in relation to overall runtime as the matrix size increases, and thus shows promise to reduce the expected runtime for Cholesky factorizations on very large matrices.",2010,0,
430,431,An automated methodology to diagnose geometric defect in the EEPROM cell,"The objective of this paper is to present an automated geometric defect diagnosis methodology for EEPROM cell (AGDE). This method focuses on speeding up the diagnosis process of geometric defects. It is based on a mathematical model generated with a ""design of simulation"" (DOS) technique. The DOS technique takes as input, simulations results of a floating gate transistor with different given geometries and produces, as output, a polynomial equation of the threshold voltage in function of the cell's geometric parameters. The diagnosis process is realized by comparing the measured threshold voltages of an EEPROM cell with the dynamically computed ones. From this comparison, the potentially defective geometric parameters are automatically extracted.",2002,0,
431,432,Multiple transient faults in logic: an issue for next generation ICs?,"In this paper, we first evaluate whether or not a multiple transient fault (multiple TF) generated by the hit of a single cosmic ray neutron can give rise to a bidirectional error at the circuit output (that is an error in which all erroneous bits are 1s rather than 0s, or vice versa, within the same word, but not both). By means of electrical level simulations, we show that this can be the case. Then, we present a software tool that we have developed in order to evaluate the likelihood of occurrence of such bidirectional errors for very deep submicron (VDSM) ICs. The application of this tool to benchmark circuits has proven that such a probability can not be neglected for several benchmark circuits. Finally, we evaluate the behavior of conventional self-checking circuits (generally designed accounting only for single TFs) with respect to such events. We show that the modifications generally introduced to their functional blocks in order to avoid output bidirectional errors due to single TFs (as required when an AUED code is implemented) can significantly reduce (up to the 40%) also the probability to have bidirectional errors because of multiple TFs.",2005,0,
432,433,Comparing fail-silence provided by process duplication versus internal error detection for DHCP server,"This paper uses fault injection to compare the ability of two fault-tolerant software architectures to protect an application from faults. These two architectures are Voltan, which uses process duplication, and Chameleon ARMORs, which use self-checking. The target application is a Dynamic Host Configuration Protocol (DHCP) server, a widely used application for managing IP addresses. NFTAPE, a software-based fault injection environment, is used to inject three classes of faults, namely random memory bit-flip, control-flow and high-level target specific faults, into each software architecture and into baseline Solaris and Linux versions",2001,0,
433,434,Quantification of PET and CT Data Misalignment Errors in Cardiac PET/CT:Clinical and Phantom Studies,"PET/CT units with high temporal resolution (particularly with 64-slice CT capability) are increasingly used as in clinical diagnosis and prognosis of cardiovascular disease. Since the CT sub-system in the combined PET/CT unit is used to perform attenuation correction of acquired PET data, misalignments between patient positioning for both scans can cause artifacts in the myocardial PET images potentially resulting in false positive artifacts. The aim of this study is to evaluate the misalignment effect (induced by spurious or physiological patient motion in-between the two modalities) on regional and global uptake values in the myocardial region. In this study, we used both phantom (RSD thorax phantom) and clinical studies (two FDG and one NH<sub>3</sub> rest/stress). Manual shifts between the CT and PET images ranging from 0 to 20 mm in six different directions were applied. Thereafter, attenuation correction was applied to the emission data using the manually shifted CT images in order to model patient motion between PET and CT. The reconstructed PET images using shifted CT images for attenuation correction were compared with the PET images corrected with the hypothetically misalignment free original CT image. The criteria and figures of merit used included VOI and linear regression analysis. The analysis was performed using 500 VOIs located within the myocardial wall in each PET dataset. The VOIs were uniformly distributed across all myocardial wall regions to assess the overall influence of PET and CT misalignment. The absolute percentage relative difference increased in all simulated movements with increasing misalignments for both phantom and clinical studies (up to 30% in some regions for the 20 mm shift). In conclusion, increasing the misalignment between PET and CT studies resulted in increased changes in the tracer uptake value within the myocardium both on a regional and global basis with respect to the reference as revealed by the various figures of meri- t used. The variation was more significant for right and down movements versus left and up directions.",2009,0,
434,435,Analysis of the Timing System Error of the Constellation Automatic Navigation,"System integrated clock (SIC) plays an important role in implementing the high-accuracy constellation automatic time synchronization and information exchange. In the establishment of SIC, error and noise are unavoidably introduced. In the paper, various error sources in the process are analyzed at first, and then an error-reduction method under the model of two-way plus common view time comparison is put forward and analyzed. Theory research and simulation experiment show that the constellation time synchronization error is below 10 seconds.",2007,0,
435,436,Formal guides for experimentally verifying complex software-implemented fault tolerance mechanisms,"Describes a framework allowing the experimental verification of complex software-implemented fault-tolerance algorithms and mechanisms (FTAMs). This framework takes into account two of the most important aspects which are increasingly required in newly-developed fault-tolerant systems: the considerations of COTS (commercial off-the-shelf) based architectures and the compliance with severe safety certification procedures. The strategy proposed shows how a rigorous FTAM specification, based on a multiple-viewpoint architectural description, may help to mechanically monitor the verification of its implementation under real conditions. The proposed strategy has been instantiated using two mechanized techniques: model checking and fault injection. The preliminary conclusions of the application of this automated approach to a small part of a commercial fault-tolerant system help us clarify its usage and its suitability for validating complex dependable systems",2001,0,
436,437,An improved fault locating system of distribution network based on fuzzy identification,"Fault locating system, which is designed for the fast power recovery, is very important in the economical operating of the distribution network. But, for the uncertainty of the fault information, the incorrect conclusion may be obtained by the traditional fault location calculation, so the most fault locating system can not be employed in the distribution network. In this paper, an improved fault locating system is proposed, which is composed of the fault signal acquisition unit and the fault location analysis center. Fuzzy identification is employed in the fault location analysis center to deal with the uncertainty of fault information. The failure and mistake rates of indicator action are used as the fuzzy parameters to calculate the fuzzy difference of the fault sequence and the standard fault set. The fault indicator is the primary device of fault information acquisition. The radio frequency and GPRS technology construct the communication channel of fault signal acquisition unit, which cuts down the construction cost and also ensures the obtaining accuracy of the fault information. The fault location system is working on the distribution network and operating well. With the accurate fault location, power supply recovers fast. The loss of power failure is reduced effectively.",2010,0,
437,438,"Pattern recognition-a technique for induction machines rotor fault detection ""eccentricity and broken bar fault""","A pattern recognition technique based on Bayes minimum error classifier is developed to detect broken rotor bar faults and static eccentricity in induction motors at the steady state. The proposed algorithm uses stator currents as input without any other sensors. First, rotor speed is estimated from stator currents, then appropriate features are extracted. The produced feature vector is normalized and fed to the trained Bayes minimum error classifier to determine if motor is healthy or has incipient faults (broken bar fault, static eccentricity or both). Only number of poles and rotor slots are needed as pre-knowledge information. Theoretical approach together with experimental results derived from a 3 hp AC induction motor show the strength of this method. In order to cover many different motor load conditions data are derived from 10% to 130% of the rated load for both a healthy induction motor and an induction motor with a rotor having 4 broken bars and/or static eccentricity.",2001,0,
438,439,Sinogram-based motion correction of PET images using optical motion tracking system and list-mode data acquisition,"A head motion during brain imaging has been recognized as a source of image degradation and introduces distortion in positron emission tomography (PET) image. There are several techniques to correct the motion artifact, but these techniques cannot correct the motion during scanning. The aim of this study is to develop a sinogram-based motion correction (SBMC) method to correct directly the head motion during PET scanning using a motion tracking system and list-mode data acquisition. This method is a rebinning procedure by which the lines of response (LOR) are geometrically transformed according to the current values of the six-dimensional motion data. Michelogram was recomposed using rebinned LOR and motion corrected sinogram was generated. In the motion corrected image, the blurring artifact due to motion was reduced by SBMC method.",2002,0,
439,440,Multi-Layer Immune Model for Fault Diagnosis,"Inspired by the multi-layer defense mechanism and incorporates the feedback mechanism in the nature immune system, the paper proposes a multi-layer immune model for fault diagnosis. In the multi-layer model, inherent immune layer direct recognition of known fault that could not cause influence to other nodes; propagation immune layer adopt the structure of the B- lymphocyte network to construct the fault propagation network for the fault localization; Adaptive immune layer learn the unknown fault pattern. Simulation results show that the multilayer immune diagnosis system has the properties of recognition, learning and memory.",2008,0,
440,441,Bit error rate of a digital radio eavesdropper on computer CRT monitors,"An eavesdropper on computer CRT (cathode ray tube) monitors can be used to intercept video information. Its anti-noise performance is analyzed in this paper. Baseband transmission models of digital signals are established according to the operating principle of the eavesdropper. The relationship between the eavesdropper's bit error rate and some parameters, such as intercept distance, superposition times and noise power, is discussed under the circumstances of ISI and ISI-freedom. Good agreement is obtained between experimental results and theoretical analysis.",2004,0,
441,442,High Continuous Availability Digital Information System Based on Stratus Fault-Tolerant Server,"With the construction of harmonious society, health improvement and the rapid development of information technology, People put forward higher requirements for the hospital. Hospital information system as an online services system requires continuous operation. Server system is the key to support hospital operations. System paralyzed accident caused by Server system failure is also not uncommon. Aiming at the problem of insufficient reliability of the traditional Cluster cluster server system, The article made a in-depth technical analysis on the performance of the Stratus fault-tolerant server. Combing with the characteristics of hospital information system, it proposed the digital hospital information system structure based on Stratus fault-tolerant server and explored and analyzed the economic and technical advantages of the program. The application effect demonstrates that the program is of the economic good and can realize continuous availability.",2010,0,
442,443,Estimation of Defects Based on Defect Decay Model: ED^{3}M,"An accurate prediction of the number of defects in a software product during system testing contributes not only to the management of the system testing process but also to the estimation of the product's required maintenance. Here, a new approach called ED<sup>3</sup>M is presented that computes an estimate of the total number of defects in an ongoing testing process. ED<sup>3</sup>M is based on estimation theory. Unlike many existing approaches the technique presented here does not depend on historical data from previous projects or any assumptions about the requirements and/or testers' productivity. It is a completely automated approach that relies only on the data collected during an ongoing testing process. This is a key advantage of the ED<sup>3</sup>M approach, as it makes it widely applicable in different testing environments. Here, the ED<sup>3</sup>M approach has been evaluated using five data sets from large industrial projects and two data sets from the literature. In addition, a performance analysis has been conducted using simulated data sets to explore its behavior using different models for the input data. The results are very promising; they indicate the ED<sup>3</sup>M approach provides accurate estimates with as fast or better convergence time in comparison to well-known alternative techniques, while only using defect data as the input.",2008,0,
443,444,A fault tolerant approach to microprocessor design,"We propose a fault-tolerant approach to reliable microprocessor design. Our approach, based on the use of an online checker component in the processor pipeline, provides significant resistance to core processor design errors and operational faults such as supply voltage noise and energetic particle strikes. We show through cycle-accurate simulation and timing analysis of a physical checker design that our approach preserves system performance while keeping area overheads and power demands low. Furthermore, analyses suggest that the checker is a fairly simple state machine that can be formally verified, scaled in performance, and reused. Further simulation analyses show virtually no performance impacts when our simple checker design is coupled with a high-performance microprocessor model. Timing analyses indicate that a fully synthesized unpipelined 4-wide checker component in 0.25 m technology is capable of checking Alpha instructions at 288 MHz. Physical analyses also confirm that costs are quite modest; our prototype checker requires less than 6% the area and 1.5% the power of an Alpha 21264 processor in the same technology. Additional improvements to the checker component are described which allow for improved detection of design, fabrication and operational faults.",2001,0,
444,445,Tolerating faults while maximizing reward,"The imprecise computation (IC) model is a general scheduling framework that is capable of expressing the precision vs. timeliness tradeoff involved in many current real-time applications. In that model, each task comprises mandatory and optional parts. While allowing greater scheduling flexibility, the mandatory parts in the IC model still have hard deadlines, and hence they must be completed before the task's deadline, even in the presence of faults. In this paper, we address fault-tolerant (FT) scheduling issues for IC tasks. First, we propose two recovery schemes, namely immediate recovery and delayed recovery. These schemes can be readily applied to provide fault tolerance to the mandatory parts by scheduling the optional parts appropriately for recovery operations. After deriving the necessary and sufficient conditions for both schemes, we consider the FT-optimality problem, i.e. generating a schedule which is FT and whose reward is maximum among all possible FT schedules. For immediate recovery, we present and prove the correctness of an efficient FT-optimal scheduling algorithm. For delayed recovery, we show that the FT-optimality problem is NP-hard, and thus is intractable",2000,0,
445,446,Incorporating fault tolerance in analog-to-digital converters (ADCs),The reliability of ADCs used in highly critical systems can be increased by applying a two-step procedure starting with sensitivity analysis followed by redesign. The sensitivity analysis is used to identify the most sensitive blocks which could then be redesigned for better reliability by incorporating fault tolerance. This paper illustrates the steps involved in incorporating fault tolerance in an ADC. Two redesign techniques to improve the reliability of a circuit are presented. Novel selective node resizing algorithms for increased tolerance against -particle induced transients are discussed.,2002,0,
446,447,Multiwave interaction analysis of a coaxial Bragg structure with a localized defect introduced in sinusoidal corrugations,"A multiwave interaction formulation is presented to investigate the effects of a localized defect on the reflective spectrum of a coaxial Bragg structure with sinusoidal corrugations. Good agreement has been achieved between the theoretical results obtained by the present formulation and those simulated by the software HFSS, which confirms the validity and the significance of the multiwave interaction formulation. It is found that, the localized defect creates defected eigenmodes within each reflective band gap of the initial standard Bragg structure, which the parameter can be controlled by the location of the localized defect.",2009,0,
447,448,Coupled field-circuit-mechanical model of an electromagnetic actuator operating in error actuated control system,"An algorithm of coupled field-circuit simulation of the dynamics of an electromagnetic linear actuator operating in error actuated control system is presented. The software consists of three main parts: (a) numerical model of the actuator dynamics which includes equations of a transient electromagnetic field in a non-linear conducting and moving medium, (b) discrete model of electric circuit and (c) optimization solver. Numerical implementation is based on the finite elements. The influence of the PID controller settings on the actuator operation is shown. In order to find optimal parameters of the system the genetic algorithm is applied. The simultaneous optimization of both: actuator structure and regulator settings has been carried out.",2008,0,
448,449,A comprehensive evaluation of capture-recapture models for estimating software defect content,"An important requirement to control the inspection of software artifacts is to be able to decide, based on more objective information, whether the inspection can stop or whether it should continue to achieve a suitable level of artifact quality. A prediction of the number of remaining defects in an inspected artifact can be used for decision making. Several studies in software engineering have considered capture-recapture models to make a prediction. However, few studies compare the actual number of remaining defects to the one predicted by a capture-recapture model on real software engineering artifacts. The authors focus on traditional inspections and estimate, based on actual inspections data, the degree of accuracy of relevant state-of-the-art capture-recapture models for which statistical estimators exist. In order to assess their robustness, we look at the impact of the number of inspectors and the number of actual defects on the estimators' accuracy based on actual inspection data. Our results show that models are strongly affected by the number of inspectors, and therefore one must consider this factor before using capture-recapture models. When the number of inspectors is too small, no model is sufficiently accurate and underestimation may be substantial. In addition, some models perform better than others in a large number of conditions and plausible reasons are discussed. Based on our analyses, we recommend using a model taking into account that defects have different probabilities of being detected and the corresponding Jackknife Estimator. Furthermore, we calibrate the prediction models based on their relative error, as previously computed on other inspections. We identified theoretical limitations to this approach which were then confirmed by the data",2000,0,
449,450,Implementing a reflective fault-tolerant CORBA system,"The use of reflection is becoming popular today for the implementation of non-functional mechanisms such as fault tolerance. The main benefits of reflection are separation of concerns between the application and the mechanisms and transparency from the application programmer point of view. Unfortunately, metaobject protocols (MOPs) available today are not satisfactory with respect to necessary features needed for implementing fault tolerance mechanisms. Previously, we proposed a specialised MOP based on Corba, well adapted for such mechanisms (M.-O. Killijian and J.C. Fabre, 1998). We deliberately focus on the implementation of this metaobject protocol using compile-time reflection and its use for implementing distributed fault tolerance. We present the design and the implementation of a fault-tolerant Corba system using this metaobject together with some preliminary experimental results. From the lessons learnt from this work, we briefly address the benefits of reflection in other layers of a system for dependability issues",2000,0,
450,451,The Optimized Combination of Fault Location Technology Based on Traveling Wave Principle,"The accuracy and the reliability of modern D-type double-ended and A-type single-ended traveling wave fault location principles used for transmission lines is comprehensively evaluated. Based on the evaluation, this paper presents the idea of optimized combination of location based on these two traveling wave principles, and successfully applies the idea in actual fault analysis of transient traveling waves. Compared with the traveling wave location schemes based on D-type or A-type principle alone, this scheme has the greatest advantages of utilizing the A-type traveling wave principle to verify and correct the location results obtained with the D-type traveling wave principle, so that both the location reliability and accuracy are enhanced. Practical applications showed that the optimized combination of traveling wave location schemes is feasible, and the location precision is improved significantly.",2009,0,
451,452,Autonomous cooperation technique to achieve fault tolerance in service oriented community system,"The advancement of mobile telecommunication and wireless technologies is required to provide local but familiar services in daily life, which has not been satisfied through the global services on the Internet. In retail business under evolving market, users request access to unknown but appropriate services based on their preference and situation, and retailers need to be aware of the current requirements of the majority of consumers in specific local trade areas. Because of the transience of the requirements of the users in their trade-areas, the services require being temporary and having the time limit. Therefore the areas of the services need to become narrower with the time. The concept of the service oriented community has been proposed to satisfy both the users and the retailers requirements. It consists of members in the specified area based on services, and they cooperate with each other in order to get mutual benefits. For realization of the service oriented community, the systems require flexibility for the effective provision of the services and fault tolerance for the stable service. In the service oriented community system, the Time Distance has been introduced as the efficient measure of the distance between the users and the retailers. The Time Distance Oriented Service System architecture has been proposed to satisfy these requirements for flexible and stable services, where the nodes are autonomously distribute services and reduce the service area based on the time distance. Here autonomous cooperation technique for achieving fault tolerance is proposed in order to satisfy the requirement of high service availability.",2002,0,
452,453,A fault-tolerant approach to network security,"Summary form only given. The increasing use of the Internet, especially for internal and business-to-business applications has resulted in the need for increased security for all networked systems to avoid unauthorized access and use. A failure of network security can effectively close the business, its availability is vital to operations. Vital functions such as firewalls and VPNs must remain in operation without loss of time for fallover, without loss of data and must be able to be placed even at remote locations where support personnel may not be readily available. Network firewalls are the first, and often are the only, line of defense against an attack. However, the firewall can be a double-edged sword. In operation, the firewall protects the network from everything from Denial of Service attacks to the entry of known viruses and unauthorized intrusion. If the firewall falls, there are generally only two options: Leave the network open to all or shut down access by anyone. The default condition is to close everything off, but this can be as disastrous as leaving the network open. Due to the importance of the firewall, most leading firewall software provides some method of establishing a form of fail-over redundancy for high availability. Yet in most cases this means some form of clustering using a secondary system as a backup with specialty software to detect and respond to a failure of the primary firewall. Such a clustered approach introduces additional complexity when establishing and configuring the firewall and additional complexity when upgrading. It also adds dramatically to the cost, not only in the hardware for the firewall, but in additional software copies and in the expertise for clustering support software required to establish and maintain the cluster. The approach we will discuss examines the creation of network security based on a hardware approach to fault tolerance. This approach will dramatically reduce the system complexity, simultaneously eliminating the need for special clustering software and special expertise for configuring the system for the kind of continuous availability that is the objective of the network security application. In addition, because the hardware approach is something that is designed in from the inception of the system, there are additional advantages. The fault tolerance is not an afterthought, but rather the purpose of the hardware, meaning that the system can be made to function very smoothly with very little administration. Failure of a part of the system is seamlessly recovered by the redundant elements, without loss of data in memory or loss of state for the system. In sum, this paper discusses the ability to create network security that reaches the standard of being continuously available, what is often referred to as the ""Holy Grail of reliability,"" 99.999% uptime",2001,0,
453,454,Research of error correction of LEO satellite orbit prediction for vehicle-borne tracking and position device,"Vehicle-borne tracking and position device is used to track LEO satellite. Because of the absence of the target which might be caused by cloud or zenith blind zone, the forecasting data will be used to acquire the target. While orbit prediction has serious errors, the target is always missed. Meanwhile, in its application to the vehicle-borne tracking and position device, due to the base of instability in the tracking process, it will result in significant difference between predicting data and tracking data, so the target will be not tracked rapidly. We applied tracking data to predict satellite orbits by improving Laplace method, and then corrected the error between Predicted data and actual measured data by interpolation method of Lagrange which improves the accuracy of prediction values. The testing data shows the accuracy of predicted data ranging from 3' to 10' for both azimuth and elevation when extrapolated satellite orbit to 7 seconds time.",2010,0,
454,455,Exploiting Mobile Agents for Structured Distributed Software-Implemented Fault Injection,"Embedded distributed real-time systems are traditionally used in safety-critical application areas such as avionics, healthcare, and the automotive sector. Assuring dependability under faulty conditions by means of fault tolerance mechanisms is a major concern in safety-critical systems. From a validation perspective, Software-Implemented Fault Injection (SWIFI) is an approved means for testing fault tolerance mechanisms. In recent work, we have introduced the concept of using mobile agents for distributed SWIFI in time-driven real-time systems. This paper presents a prototypical implementation of the agent platform for the OSEKtime real-time operating system and the FlexRay communication system. It is further shown, how to implement fault injection experiments by means of mobile agents in a structured manner following a classification of faults in terms of domain, persistence, and perception. Based on experiments conducted on ARM-based platforms, selected results are described in detail to demonstrate the potential of mobile agent based fault injection.",2006,0,
455,456,Analytical approach to internal fault simulation in power transformers based on fault-related incremental currents,"A new method for simulating faulted transformers is presented in this paper. Unlike other methods proposed in the literature, this method uses the data obtained from any sound transformer simulation to obtain the damaged condition by simply adding a set of calculated currents. These currents are obtained from the definition of the fault. The model is fully based on determining the incremental values exhibited by the currents in phases and lines from the prefault to the postfault condition. As a consequence, data obtained from simulation of the sound transformer may be readily used to define the damaged condition. The model is described for light and severe faults, introducing this latter feature as a further add-on feature to the low-level faults simulation. The technique avoids the use of complex routines and procedures devoted to specially simulate the internal fault. Of prompt application to relay testing, the proposed analytical model also gives an insight into the fault nature by means of the investigation of symmetrical components. In contrast with its low complexity, the method has shown to present large accuracy for simulating the fault performance.",2006,0,
456,457,Automated Bug Neighborhood Analysis for Identifying Incomplete Bug Fixes,"Although many static-analysis techniques have been developed for automatically detecting bugs, such as null dereferences, fewer automated approaches have been presented for analyzing whether and how such bugs are fixed. Attempted bug fixes may be incomplete in that a related manifestation of the bug remains unfixed. In this paper, we characterize the completeness of attempted bug fixes that involve the flow of invalid values from one program point to another, such as null dereferences, in Java programs. Our characterization is based on the definition of a bug neighborhood, which is a scope of flows of invalid values. We present an automated analysis that, given two versions P and P' of a program, identifies the bugs in P that have been fixed in P', and classifies each fix as complete or incomplete. We implemented our technique for null-dereference bugs and conducted empirical studies using open-source projects. Our results indicate that, for the projects we studied, many bug fixes are not complete, and thus, may cause failures in subsequent executions of the program.",2010,0,
457,458,Applying FIRMAMENT to test the SCTP communication protocol under network faults,"How to apply a fault injector to evaluate the dependability of a network protocol implementation is the main focus of this paper. In the last years, we have been developing FIRMAMENT, a tool to inject faults directly into messages that pass through the kernel protocol stack. SCTP is a promising new protocol over IP that, due its enhanced reliability, is competing with TCP where dependability has to be guaranteed. Using FIRMAMENT we evaluate the error coverage and the performance degradation of SCTP under faults. Performing a complete fault injection campaign over a third party software give us a deep insight about the additional test strategies that are needed to reach significant dependability measures.",2009,0,
458,459,Performance Evaluation of Probe-Send Fault-tolerant Network-on-chip Router,"With increasing reliability concerns for current and next generation VLSI technologies, fault-tolerance is fast becoming an integral part of system-on-chip and multi-core architectures. Another trend for such architectures is network-on-chip (NoC) becoming a standard for on-chip global communication. In an earlier work, a generic fault-tolerant routing algorithm in the context of NoCs has been presented. The proposed routing algorithm works in two phases, namely path exploration (PE) and normal communication. This paper presents fundamental insights into various novel PE approaches, their feasibility and performance trade-offs for k-ary 2-cube NoCs. The dependence of the normal communication phase on the probability of finding paths and their quality in the first phase emphasizes the PE's significance. One major contribution of this work is the investigation of application of constrained randomness to PE for optimizing the quality of paths. Another contribution is the proposed use of merging of traffic to reduce the reconfiguration time by a large amount (73.8% on an average).",2007,0,
459,460,Impact of Error Characteristics of an Indoor 802.11g WLAN on TCP Retransmissions,"In this paper we present the results of extensive measurements made over an experimental wired-to-wireless testbed, which consisted of a TCP protocol combined with a real-world indoor IEEE 802.11g WLAN. We investigated the effects of signal attenuation due to client distance from the AP on the 802.11 frame error rates (FER) and consequently on the segment loss rates and retransmission behavior of TCP at the sender in the fixed network. Specifically, we experimented with different modulation schemes belonging to the OFDM 802.11g PHY in order to gauge differences in performance between them, comparing real-world FERs calculated from actual frame captures against SNR, for both the forward and reverse WLAN channel directions. We also present real-world distributions of frame retransmissions made over the WLAN by the AP, with useful findings. Our results confirm that the reverse channel of a WLAN possesses a higher FER than the forward channel, and poses a greater threat to TCP's retransmission mechanism.",2008,0,
460,461,Statistical feature representations for automatic wood defects recognition research and applications,"In this paper, we introduce the non-negative matrix factorization (NMF) to decompose the wood images and structure the feature spaces. Local binary pattern (LBP) is used to extract the original spatial local structure features, such as curly edges, etc. and they have better luminance adaptability. Simultaneously, dual-tree complex wavelet transform (DTCWT) is used to extract the energy based statistical features from different directions and frequencies and they can maintain better time-frequency localized characteristics and finite data redundancy. We integrate the features together to choose the proper features to describe the discrepancies between sound woods and defects and then propose an automatic detection system for wood defects recognition. After many cross experiments, we received a better identification rate of more than 90% with good research values and potential applications.",2009,0,
461,462,Error concealment for stereoscopic images using mode selection,"In this paper, a novel error concealment (EC) method for compressed stereoscopic image pairs is presented, which contains a new binocular EC mode and an improved monocular EC mode. The proposed algorithm selects appropriate EC mode to conceal the error block (EB) in the stereoscopic image according to the local characteristic of the EB. The experimental results demonstrate that the proposed scheme has good subjective and objective EC performance in stereoscopic images as compared to monocular mode.",2010,0,
462,463,Fault tolerance of CNC software based on artificial neural network,"This paper proposes an efficient method for realizing the fault tolerance of CNC software with the introduction of artificial neural network (ANN) to the design filed of CNC software. In addition, function aspects (velocity, acceleration, chord error, real time, prediction accuracy) from the experiment on Non-Uniform Rational B-Spline (NURBS) interpolator based on ANN were evaluated in detail. Our experimental results showed that the NURBS interpolation based on ANN not only meet the requirements of function aspects, but also can realize fault tolerance technology, which may provide a new strategy in the improvement of the reliability of CNC software.",2010,0,
463,464,PMSG Wind Turbine Performance Analysis During Short Circuit Faults,"Due to the increasing price of fossil fuels and the security concerns of the nuclear energy, electricity generation using wind turbines has recently attracted significant attention after a period of neglect. Among different types of wind turbine generators, PM synchronous generators (PMSG) offer better performance due to higher efficiency and less maintenance since they do not have rotor current and could be used without a gear box. In addition, the utilization of a double conversion results in higher flexibility compared with other wind turbine systems. This paper develops the model of a PMSG wind turbine and then simulates short circuits to evaluate the performance of the system during short circuit fault. Since PMSG wind turbine systems use a double conversion converter, this paper develops two methods for controlling the converter, a new protection method for capacitor over voltage is also evaluated in this paper.",2007,0,
464,465,A novel fault diagnosis algorithm for K-connected distributed clusters,"In this paper, we propose an on-line two phase (TPD) fault diagnosis algorithm for distributed clusters that follows an arbitrary network topology with connectivity k. Intermediate nodes communicate heartbeat messages between different source destination pairs. The algorithm addresses a realistic fault model considering crash and value faults in the cluster nodes. The algorithm is shown to produce a time complexity of O(l) and message complexity of O(n. e) respectively. The algorithm has been simulated using discrete event simulation techniques and the results show that the algorithm is feasible for large distributed clusters.",2010,0,
465,466,The effect of the time window width of correlation method on single-ended modern traveling wave based fault location principles,"In the technique of single-ended modern traveling wave based fault location principles for transmission lines, traveling wave correlation method is a classical algorithm applied to detect the fault reflected surge. But in actual application, the lack of an effective way to choose the time window results in the limitation of correlation method - the time window width affects the correlation coefficient value which is an important indicator to show the similarity of the waveforms. This paper presents a new conception called optimal time window width of correlation method, and analyzes different factors probably affecting the width by means of EMTP-ATP and Matlab applied to simulations. Further more, the basic idea of new correlation method based on multi-time-window is proposed, which could be applied to improve the reliability of fault location technique.",2008,0,
466,467,Hierarchical application aware error detection and recovery,"Proposed is a four-tired approach to develop and integrate detection and recovery support at different levels of the system hierarchy. The proposed mechanisms exploit support provided by (i) embedded hardware, (ii) operating system, (iii) compiler, and (iv) application.",2004,0,
467,468,A Statistical Approach for Estimating the Correlation between Lightning and Faults in Power Distribution Systems,"The paper deals with the subject of the source-identification of transient voltage disturbances in distribution system buses. In particular, a statistical procedure is proposed for the evaluation of the probability that a lightning flash detected by a lightning location system (LLS) could cause a fault and, therefore, relay interventions, generally associated with voltage dips. The proposed procedure is based on the coordinated use of the information provided by the LLS and the availability of an advanced simulation tool for the accurate simulation of lightning-induced voltages on complex power systems, namely the LIOV-EMTP code. The uncertainty levels of the stroke location and of the peak current estimations provided by the LLS are discussed and their influence on the lightning-fault correlation is analyzed",2006,0,
468,469,Fault-tolerant five-phase permanent magnet motor drives,"In this paper, a control strategy that provides fault tolerance to five-phase permanent magnet motors is introduced. In this scheme, the five-phase permanent magnet (PM) motor continues operating safely under loss of up to two phases without any additional hardware connections. This feature is very important in traction and propulsion applications where high reliability is of major importance. The five-phase PM motors with sinusoidal and quasi-rectangular back-EMFs have been considered. To obtain the new set of phase currents to be applied to the motor during fault in stator phases or inverter legs, the torque producing MMF by the stator is kept constant under healthy and faulty conditions for both cases. Simulation and experimental results are provided to verify that the five-phase motor continues operating continuously and steadily under faulty conditions.",2004,0,
469,470,"Performance of Multicode DS/CDMA With Noncoherent <formula formulatype=""inline""> <img src=""/images/tex/964.gif"" alt=""M""> </formula>-ary Orthogonal Modulation in the Presence of Timing Errors","This paper derives an accurate approximation to the bit error rate (BER) of multicode direct-sequence code-division multiple access (DS/CDMA) with noncoherent <i>M</i>-ary modulation in wideband fading channels when timing errors are made at the receiver employing equal-gain combining (EGC). This reflects the practical scenario where the path delays are estimated imperfectly, leading to synchronization errors between the correlation receivers and the received signals. The analysis can be applied to any type of a fading distribution for both independent and correlated diversity branches. It is shown that the derived analytical expressions are in close agreement with the Monte Carlo system simulations, particularly in the case of small timing errors.",2008,0,
470,471,Fault tolerance as an aspect using JReplica,"Reliability and availability are very important trends in the development process of distributed systems. In order to improve these features, object replication mechanisms have been introduced. Programming replication policies for a given application is not an easy task, and this is the reason why transparency for the programmer has been one of the most important properties offered by all replication models. However, this transparency for the programmer is not always desirable. In this paper we present a replication model, JReplica, based on Aspect Oriented Programming (AOP). JReplica allows the separated specification of the replication code from the functional behaviour of objects, providing not only a high degree of transparency, as done by previous models, but also the possibility for programmers to introduce new behaviour to specify different fault tolerance requirements. Moreover, the replication aspect has been introduced at design time, and in this way, UML has been extended in order to consider replication issues separately when designing fault tolerance systems",2001,0,
471,472,A novel transmission line test and fault location methodology using pseudorandom binary sequences,"A novel pulse echo test methodology, using pseudorandom binary sequence (PRBS) excitation, is presented in this paper as an alternative to time domain reflectometry (TDR) for transmission line fault location and identification. The essential feature of this scheme is the cross correlation (CCR) of the fault response echo with the PRBS test input stimulus input which results in a unique signature for identification of the fault type, if any, or load termination present as well as its distance from the point of test stimulus injection. This fault identification method can used in a number of key industrial applications incorporating printed circuit boards, overhead transmission lines and underground cables in inaccessible locations which rely on a pathway for power transfer or signal propagation. As an improved method PRBS fault identification can be performed over several cycles at low amplitude levels online to reject normal signal traffic and extraneous noise pickup for the purpose of multiple fault coverage, resolution and identification. In this paper a high frequency co-axial transmission line model is presented for transmission line behavioural simulation with PRBS stimulus injection under known load terminations to mimic fault conditions encountered in practice for proof of concept. Simulation results, for known resistive fault terminations, with measured CCR response demonstrate the effectiveness of the PRBS test method in fault type identification and location. Key experimental test results are also presented for a co-axial cable, under laboratory controlled conditions, which substantiates the accuracy of PRBS diagnostic CCR method of fault recognition and location using a range of resistive fault terminations. The accuracy of the method is further validated through theoretical calculation via known co-axial cable parameters, fault resistance terminations and link distances in transmission line experimental testing.",2008,0,
472,473,Lightweight Fault-Tolerance Mechanism for Distributed Mobile Agent-Based Monitoring,"Thanks to asynchronous and dynamic natures of mobile agents, a certain number of mobile agent-based monitoring mechanisms have actively been developed to monitor large scale and dynamic distributed networked systems adaptively and efficiently. Among them, some mechanisms attempt to adapt to dynamic changes in various aspects such as network traffic patterns, resource addition and deletion, network topology and so on. However, failures of some domain managers are very critical to providing correct, real-time and efficient monitoring functionality in a large-scale mobile agent-based distributed monitoring system. In this paper, we present a novel fault- tolerance mechanism to have the following advantageous features appropriate for large-scale and dynamic hierarchical mobile agent-based monitoring organizations. It supports fast failure detection functionality with low failure-free overhead by each domain manager transmitting heart-beat messages to its immediate higher-level manager. Also, it minimizes the number of non-faulty monitoring managers affected by failures of domain managers. Moreover, it allows consistent failure detection actions to be performed continuously in case of agent creation, migration and termination, and is able to execute consistent takeover actions even in concurrent failures of domain managers.",2009,0,
473,474,A Superstring Galaxy Associative Memory Model with Expecting Fault-Tolerant Fields,"The synthesis problems of associative memory models are not better solved until now. Learning from the idea of superstring theory, a design method of superstring galaxy associative memory model with expecting fault-tolerant field is proposed by making the sphere mapping and giving the algorithm of galaxy covering. The method better solves a difficult synthesis problem of associative memory models. The superstring galaxy associative memory model designed by the method can have expecting fault-tolerant fields of the samples.",2009,0,
474,475,A Model-based Simulation Approach to Error Analysis of IT Services,"Utility computing environments provide on-demand IT services to customers. Such environments are dynamic in nature and continuously adapt to changes in requirements and system state. Errors are an important category of environment state changes as such environments consist of a large number of components, and hence, are subject to errors. In this paper, we design and implement a model-based simulation framework that leverages information about existing service components and their interactions, and provides concrete service behavior in presence of a variety of errors. To evaluate the framework, experiments are conducted on a virtualized blade-server based environment. Results show that the framework is effective and practical in analyzing error impacts on IT services.",2007,0,
475,476,Power quality improvement using a new structure of fault current limiter,"In this paper, power quality improvement by using a new structure of non superconducting fault current limiter (NSFCL) is discussed. This structure prevents voltage sags on Point of Common Coupling (PCC) just after fault occurrence, because of its fast operation. On the other hand, previously used structures produce harmonics on load voltage and have ac losses in normal operation. New structure has solved this problem by using dc voltage source. The proposed structure is simulated using PSCAD/EMTDC software and simulation results are presented to validate the effectiveness of this structure.",2010,0,
476,477,3 Faults Tolerant Orthogonal RAID for Large Storage,"Recently, the demand of low cost large scale storage increases. We developed VLSD (Virtual Large Scale Disks) toolkit for constructing virtual disk based distributed storages, which aggregate free spaces of individual disks. However, in order to construct large-scale storage, more than or equal to 3 fault tolerant RAID is important. In this paper, we propose MeshRAID that is 3 fault tolerant orthogonal RAID. And, we implement MeshRAID using VLSD. It is easy to implement MeshRAID using various classes in VLSD. From the viewpoint of its features, MeshRAID is addressed between RAID55 and NaryRAID.",2010,0,
477,478,Responsive Fault-Tolerant Computing in the Era of Terascale Integration State of Art Report,"Scaling in hardware integration process results in IC-process geometry reductions, lower operating voltages and increased clock speeds. This paper first surveys the reliability obstacles these developments give rise to and then points out that computing systems can no longer be safely assumed to fail only by crashing. Yet this assumption is at the core of primary-backup replication which the literature presents as the appropriate, and hence the most widely used, strategy for time-critical fault-tolerant applications. The paper then observes that building computing nodes with announced crash failure mode is a promising way forward to deal with the emerging reliability challenges. Work carried out to assure such a failure mode has also been briefly surveyed.",2008,0,
478,479,Formal Analysis of a Distributed Fault Tolerant Clock Synchronization Algorithm for Automotive Communication Systems,"A synchronized time base is indispensable for a time- triggered system since all activities in such a system are triggered by the passage of time. Distributed fault-tolerant clock synchronization algorithms are normally used to achieve the synchronized time base. As a state-of-the-art representative of the time-triggered systems for automotive applications, FlexRay uses a fault-tolerant mid-point algorithm to achieve the synchronized time base. Correctness of the algorithm plays a crucial role as most of the protocol services rely on the fact that there exists a synchronized time base in the system. Due to the distinguished characteristics of the algorithm, we propose a case-analysis based technique for the formal analysis of the algorithm. We show that the case analysis technique can greatly facilitate our formal analysis of the algorithm. Mechanical support with Isabelle/HOL, a theorem prover, is also discussed.",2008,0,
479,480,Experimental analysis of the errors induced into Linux by three fault injection techniques,"The main goal of the experimental stud), reported in this paper is to investigate to what extent distinct fault injection techniques lead to similar consequences (errors and failures). The target system we are using to carry out our investigation is the Linux kernel as it provides a representative operating system. It is featuring full controllability and observability thanks to its open source status. Three types of software-implemented fault injection techniques are considered, namely: i) provision of invalid values to the parameters of the kernel calls, ii) corruption of the parameters of the kernel calls, and iii) corruption of the input parameters of the internal functions of the kernel. The workload being used for the experiments is tailored to activate selectively each functional component. The observations encompass typical kernel failure modes (e.g., exceptions and kernel hangs) as well as a detailed analysis of the reported error codes.",2002,0,
480,481,A New Neural-Network-Based Fault Diagnosis Approach for Analog Circuits by Using Kurtosis and Entropy as a Preprocessor,"This paper presents a new fault diagnosis method for analog circuits. The proposed method extracts the original signals from the output terminals of the circuits under test (CUTs) by a data acquisition board and finds the kurtoses and entropies of the signals, which are used to measure the high-order statistics of the signals. The entropies and kurtoses are then fed to a neural network as inputs for further fault classification. The proposed method can detect and identify faulty components in an analog circuit by analyzing its output signal with high accuracy and is suitable for nonlinear circuits. Preprocessing based on the kurtosis and entropy of signals for the neural network classifier simplifies the network architecture, reduces the training time, and improves the performance of the network. The results from our examples showed that the trochoid of the entropies and kurtoses is unique when the faulty component's value varies from zero to infinity; thus, we can correctly identify the faulty components when the responses do not overlap. Applying this method for three linear and nonlinear circuits, the average accuracy of the achieved fault recognition is more than 99%, although there are some overlapping data when tolerance is considered. Moreover, all the trochoids converge to one point when the faulty component is open-circuited, and thus, the method can classify not only soft faults but also hard faults.",2010,0,
481,482,A NURBS-based error concealment technique for corrupted images from packet loss,"An error concealment using non-uniform rational B-spline (NURBS) is proposed. NURBS has been employed by many CAD/CAM systems as a fundamental geometry representation. Despite the fact that NURBS has gained tremendous popularity from the CAD/CAM and computer graphics community, its application on exploring the image problem only received little attention. On the other hand, the image contents might be corrupted or lost during transmission. Although there are quite a few existing techniques, yet developing an effective approach to conceal the error remains as one of the hottest research topics. Thus the aim of this study is to develop an image reconstruction technique using NURBS. The key idea is to use NURBS to represent the portion of image data without corruption. By accomplishing this, a single-hidden layer neural network is employed to learn the appropriate control points of NURBS. After learning, NURBS is then used to render the corrupted image data. Experimental results indicate that the proposed approach exhibits promising performance.",2002,0,
482,483,Adaptive Cancellation of a Sinusoidal Disturbance with Rapidly Varying Frequency Using an Augmented Error Algorith,This paper considers a compensator for a sinusoidal disturbance with known but rapidly varying frequency. The compensator is obtained as an adaptive feedforward cancellation algorithm using an augmented error. The system is shown to be Lyapunov stable and equivalent to a linear time-varying controller that includes an internal model of the disturbance. The stability and robustness properties of the augmented error algorithm are validated by simulation results,2005,0,
483,484,In-system partial run-time reconfiguration for fault recovery applications on spacecrafts,"This paper presents a methodology for partially reconfiguring a field programmable gate array (FPGA) device using only limited onboard resources. This paper also seeks to provide a roadmap to developing necessary tools and technologies to help design self-sufficient partial run-time reconfigurable systems for spacecraft avionic systems. To provide a vision for the technology, this paper recommends a few possible applications in spacecraft avionic systems, in fault tolerance and space-saving hardware. In addition, some previous work done on the research for reconfigurable, modular avionics are also presented at the end as an example of applications.",2005,0,
484,485,Application of Aircraft Fuel Fault Diagnostic Expert System Based on Fuzzy Neural Network,Theories of expert system and fuzzy artificial neural network (ANN) are applied to solve the problem of fault diagnosis in the aircraft fuel system. A multilayer neural network model of the aircraft fuel system is put forward and the integrated aircraft fuel fault diagnostic expert system which solves the problems of knowledge representation and knowledge acquisition of traditional expert system is realized. The hardware-in-loop simulation results show that the expert system diagnoses the fault in accessories rapidly and accurately and it is proved that the expert system is significative and helpful for further development in the aircraft fuel fault diagnosis.,2009,0,
485,486,Inverse wave field extrapolation: a different NDI approach to imaging defects,"Nondestructive inspection (NDI) based on ultrasound is widely used. A relatively recent development for industrial applications is the use of ultrasonic array technology. Here, ultrasonic beams generated by array transducers are controlled by a computer. This makes the use of arrays more flexible than conventional single-element transducers. However, the inspection techniques have principally remained unchanged. As a consequence, the properties of these techniques, as far as characterization and sizing are concerned, have not improved. For further improvement, in this paper we apply imaging theory developed for seismic exploration of oil and gas fields on the NDI application. Synthetic data obtained from finite difference simulations is used to illustrate the principle of imaging. Measured data is obtained with a 64-element linear array (4 MHz) on a 20-mm thick steel block with a bore hole to illustrate the imaging approach. Furthermore, three examples of real data are presented, representing a lack of fusion defect, a surface breaking crack, and porosity",2007,0,
486,487,A decomposition approach to the inverse problem-based fault diagnosis of liquid rocket propulsion systems,"The health monitoring of propulsion systems has being been one of the most challenging issues in space launch vehicles, particularly for the manned space missions. The development of an advanced health monitoring system involves many technical aspects, such as failure detection and fault diagnosis as well as the integration of hardware and algorithms, for improving the safety and reliability of propulsion systems. The inverse problem-based strategy provides a new solution to the design of model-based fault diagnosis methods for monitoring the health of propulsion systems. This paper presents a decomposition approach to the inverse problem-based fault diagnosis for a class of liquid rocket propulsion systems. Simulation results are provided for demonstrating the effectiveness of the proposed approach to the inverse problem-based fault diagnosis.",2004,0,
487,488,A Unified Environment for Fault Injection at Any Design Level Based on Emulation,"Sensitivity of electronic circuits to radiation effects is an increasing concern in modern designs. As technology scales down, Single Event Upsets (SEUs) are made more frequent and probable, affecting not only space applications, but also applications at earth's surface, like automotive applications. Fault injection is a method widely used to evaluate the SEU sensitivity of digital circuits. Among the existing fault injection techniques, those based on FPGA emulation have proven to be the fastest ones. In this paper a unified emulation environment which combines two fault injection techniques based on FPGA emulation is proposed. The new emulation environment provides both, a high speed tool for quick fault detection, and a medium speed tool for in-depth analysis of SEUs propagation. The experiments presented here show that the two techniques can be successfully applied in a complementary manner.",2007,0,
488,489,Enhancing Fault Tolerance And Reliability In GAIAOS Through Structured Overlay Network,"GAIAOS event manager is a distributed event service, based on CORBA event service with a centralized entry point, resulting in limited fault resilience and scalability. In this paper, we have proposed a decentralized event service for GAIAOS through the use of DHT based structured overlay network to overcome these problems. The proposed architecture provides a completely distributed event communication mechanism without any centralized entry point. Incorporation of the structured overlay network in GAIAOS results in higher degree of fault resilience and scalability",2006,0,
489,490,Application of non-parametric statistics of the parametric response for defect diagnosis,"This paper presents a method using only the rank of the measurements to separate a part's elevated response to parametric tests from its non-elevated response. The effectiveness of the proposed method is verified on the 130nm ASIC. Good die responses are correlated for same parametric tests at different conditions such as temperature, voltage and or other stress. Nonparametric correlation methods are used to calculate the intra-die correlation. When intra-die correlation is found to be low the elevated vectors that lower correlation are extracted and input to IDDQ-based diagnostic tools. Monte-Carlo simulations are described to obtain confidence bounds of the correlation for good die test response.",2009,0,
490,491,Multi-rate receiver design with IF sampling and digital timing correction,This contribution deals with a fully digital multirate radio receiver suitable for vehicular applications. Timing correction and sample rate conversion are performed by a polynomial interpolator. Three different receiver configurations are considered in terms of computational complexity and BER performance. Careful selection of the intermediate frequency turns out to play a crucial role. System parameters are provided yielding good BER performance for all considered symbol rates. Results are verified by computation of the BER degradation as compared to an analog receiver with synchronized symbol-rate sampling.,2003,0,
491,492,An improved neural network algorithm for classifying the transmission line faults,"This study introduces a new concept of artificial intelligence based algorithm for classifying the faults in power system networks. This classification identifies the exact type and zone of the fault. The algorithm is based on unique type of neural network specially developed to deal with a large set of highly dimensional input data. An improvement of the algorithm is proposed by implementing various steps of input signal preprocessing, through the selection of parameters for analog filtering, and values for the data window and sampling frequency. In addition, an advanced technique for classification of the test patterns is discussed and the main advantages compared to previously used nearest neighbor classifier are shown.",2002,0,
492,493,An error resilience scheme for layered video coding,"Layered video coding combined with prioritized transmission is widely recognized as one of the schemes for providing error resilience in video transport system. We examine the error performance of data partitioning coded MPEG-2 video bitstreams transmitted over channel subject to bit errors. While base layer errors cannot be tolerated, only a limited amount of errors in the enhancement layer is acceptable. Further improvements on bit error resilience can be achieved using the EREC coder in the enhancement layer. Our results show the PSNR gain of up to 3 dB with EREC coded enhancement layer and no errors in the base layer.",2005,0,
493,494,SIED: software implemented error detection,"This paper presents a new error detection technique called software implemented error detection (SIED). The proposed method is based on a new control check flow scheme combined with software redundancy. The distinctive advantage of the SIED approach over other fault tolerance techniques is the fault coverage. SIED is able to cope with faults affecting data and the program control flow. By-applying the proposed approach on several benchmark programs, we evaluate the error detection capabilities by means of several fault injection experiments. Experimental results underline very good error detection capabilities for the obtained hardened version of selected benchmark programs.",2003,0,
494,495,Fault detection techniques for effective line side asset monitoring,"In this paper the results of current research into the state-of-the-art in predictive fault detection and diagnosis methods for railway line-side assets is presented. Research to date has mainly focussed on point machines, track circuits and level crossing systems. It will be argued that, through the use of examples, that the most appropriate method for robust fault detection is based around generic models that are tuned for a particular instance of an asset. Furthermore, once a fault has been detected, it is necessary to have an a priori knowledge of the symptoms that are observable under fault conditions to reliably diagnose faults.",2005,0,
495,496,A Family of Electronic Ballasts Integrating Power Factor Correction and Power Control Stages to Supply HPS Lamps,"This paper presents a family of high power factor electronic ballasts applied to the public lighting system. Flyback, buck-boost, boost or SEPIC converter is employed in the power factor correction stage, integrated to the power control stage through a single active switch. The use of a half-bridge inverter, to supply the lamp, becomes possible through the employment of a flyback converter in the lamp power control stage. The lamp is supplied in a low frequency voltage square waveform in order to guarantee the safe lamp operation, regarding to the acoustic resonance phenomenon. The presented solutions to supply HPS lamps take the advantage of low cost and simplicity. The shared switch characteristics are analyzed and discussed during this work. A comparative analysis among the presented electronic ballasts is performed",2006,0,
496,497,Respiratory motion correction of PET using motion parameters from MR,"Respiratory motion during PET acquisition from the chest/abdomen leads to significant image degradation. Combined PET/MR scanners open up the opportunity to correct motion using MR data acquired simultaneously with PET. As simultaneous human chest/abdomen PET/MR images are currently unobtainable, in this preliminary study we determined motion parameters from respiratory-gated MR and then used these to correct pseudo-PET images generated from the MR. The gated MR images were segmented to typical organ FDG SUV values, smoothed to mimic PET resolution, forward projected into the GE advance geometry and reconstructed separately using OSEM. The MR images were registered using a combined affine and non-rigid B-splines algorithm, with mutual information used as the cost function in a multi-resolution approach. Motion corrected images from both post-reconstruction registration and 4D image reconstruction are shown to be superior to those without motion compensation for most organs.",2009,0,
497,498,Assessing and improving the effectiveness of logs for the analysis of software faults,"Event logs are the primary source of data to characterize the dependability behavior of a computing system during the operational phase. However, they are inadequate to provide evidence of software faults, which are nowadays among the main causes of system outages. This paper proposes an approach based on software fault injection to assess the effectiveness of logs to keep track of software faults triggered in the field. Injection results are used to provide guidelines to improve the ability of logging mechanisms to report the effects of software faults. The benefits of the approach are shown by means of experimental results on three widely used software systems.",2010,0,
498,499,A novel approach to fault diagnostics and prognostics,A novel fault diagnostics and prognostics algorithm based on hidden Markov model (HMM) is proposed. The algorithm combines fault diagnostics and prognostics in a unified framework. The algorithm has been fully tested by using experimental data from a rotating shift testbed in our laboratory.,2003,0,
499,500,Best ANN structures for fault location in single-and double-circuit transmission lines,"The great development in computing power has allowed the implementation of artificial neural networks (ANNs) in the most diverse fields of technology. This paper shows how diverse ANN structures can be applied to the processes of fault classification and fault location in overhead two-terminal transmission lines, with single and double circuit. The existence of a large group of valid ANN structures guarantees the applicability of ANNs in the fault classification and location processes. The selection of the best ANN structures for each process has been carried out by means of a software tool called SARENEUR.",2005,0,
500,501,Defect Data Analysis Based on Extended Association Rule Mining,"This paper describes an empirical study to reveal rules associated with defect correction effort. We defined defect correction effort as a quantitative (ratio scale) variable, and extended conventional (nominal scale based) association rule mining to directly handle such quantitative variables. An extended rule describes the statistical characteristic of a ratio or interval scale variable in the consequent part of the rule by its mean value and standard deviation so that conditions producing distinctive statistics can be discovered As an analysis target, we collected various attributes of about 1,200 defects found in a typical medium-scale, multi-vendor (distance development) information system development project in Japan. Our findings based on extracted rules include: (l)Defects detected in coding/unit testing were easily corrected (less than 7% of mean effort) when they are related to data output or validation of input data. (2)Nevertheless, they sometimes required much more effort (lift of standard deviation was 5.845) in case of low reproducibility, (i)Defects introduced in coding/unit testing often required large correction effort (mean was 12.596 staff-hours and standard deviation was 25.716) when they were related to data handing. From these findings, we confirmed that we need to pay attention to types of defects having large mean effort as well as those having large standard deviation of effort since such defects sometimes cause excess effort.",2007,0,
501,502,A Hybrid Fault-Tolerant Algorithm for MPLS Networks,In this paper we present a novel fault-tolerant algorithm for use in MPLS based networks. The algorithm is employing both protection switching and path rerouting techniques and satisfies four selected performance criteria,2006,0,
502,503,Using software implemented fault inserter in dependability analysis,We investigate program susceptibility to hardware faults in Win32 environment. For this purpose we use the software implemented fault injector FITS. We analyze natural fault resistivity of COTS systems and the effectiveness of various software techniques improving system dependability. The problems of experiment tuning and result interpretation are discussed in context of a wide spectrum of applications.,2002,0,
503,504,Fault-Tolerant Discrete Dynamical Systems Over Finite Ring,"It is worked out some general method of fault- tolerant synthesis for implementations of information-looseness dynamical systems over finite ring, based on application of error control codes. Corresponding self-checking systems are designed complexity and some basic characteristics of designed implementations is characterized.",2007,0,
504,505,A Fault-Tolerant Scheme for Complex Transaction Patterns in J2EE,"End-to-end reliability is an important issue in building large-scale distributed enterprise applications based on multi-tier architecture, but the support of reliability as adopted in conventional replication or transactions mechanisms is not enough due to their distinct objectives - replication guarantees the liveness of computational operations by using forward error recovery, while transactions guarantee the safety of application data by using backward error recovery. Combining the two mechanisms for stronger reliability is a challenging task Current solutions, however, are typically on the assumption of simple transaction pattern where a request from a single client executes in the context of exactly one transaction at the middle-tier application server, and seldom think about some complex patterns, such as client transaction enclosing multiple client requests or nested transactions. In this paper, we first identify four transaction pattern classes, and then propose a fault-tolerant scheme that can uniformly provide exactly-once semantic reliability support for these patterns. In this scheme, application servers are passively replicated to endow business logics with high reliability and high availability. In addition, by replicating transaction coordinator, the blocking problem of 2PC protocol during distributed transactions processing is eliminated. We have implemented this approach and integrated it into our own J2EE application server, OnceAS. Also, its effectiveness is discussed in different transaction patterns and the corresponding performance is evaluated",2006,0,
505,506,Tracking the elusive online help topic. Organizing the review process with defect-tracking software,"Online help systems consist of hundreds of help topics. Keeping track of reviewers, comments about each help topic requires a database to do the job effectively. Rather than develop such a database from scratch, it may be possible to adapt the defect-tracking software already in use in the QA department to this task. This paper describes how technical writers can adapt defect-tracking software to organize the online help review process",2001,0,
506,507,Modeling Defect Enhanced Detection at 1550 nm in Integrated Silicon Waveguide Photodetectors,"Recent attention has been attracted by photo-detectors integrated onto silicon-on-insulator (SOI) waveguides that exploit the enhanced sensitivity to subbandgap wavelengths resulting from absorption via point defects introduced by ion implantation. In this paper, we present the first model to describe the carrier generation process of such detectors, based upon modified Shockley-Read-Hall generation/recombination, and, thus, determine the influence of the device design on detection efficiency. We further describe how the model may be incorporated into commercial software, which then simulates the performance of previously reported devices by assuming a single midgap defect level (with properties commensurate with the single negatively charged divacancy). We describe the ability of the model to highlight the major limitations to responsivity, and thus suggest improvements which diminish the impact of such limitations.",2009,0,
507,508,Fault diagnosis technology based on wavelet analysis and resonance demodulation,"The impulse signal is contained in the fault signals of some pivotal components such as gears and axletrees. Extracting weensy impact information is an important method to diagnose equipment. A mathematical model on the technology of resonant demodulation is put forward in this paper. The model provides the theoretical basis on how to use the technology to extract the weensy impulse signal from the normal low-frequency vibrating signal, at the same time, another method that wavelet analysis is used to extract the weensy impulse information is introduced too. Simulation and practical application manifest that both wavelet analysis and demodulation have good effect on extracting the weensy impulse from the mechanical fault caused by gear and axletree.",2004,0,
508,509,Cluster-Based Error Messages Detecting and Processing for Wireless Sensor Networks,"Wireless sensor networks (WSNs) have emerged as a new technology about acquiring and processing messages for a variety of applications. Faults occurring to sensor nodes are common due to lack of power or environmental interference. In order to guarantee the network reliability of service, it is necessary for the WSN to be able to detect and processes the faults and take appropriate actions. In this paper, we propose a novel approach to distinguish and filter the error messages for cluter-based WSNs. The simulation results show that the proposed method not only can avoid frequent re-clustering but also can save the energy of sensor nodes, thus prolong the lifetime of sensor network.",2008,0,
509,510,Error-Correcting Codes Based on Quasigroups,"Error-correcting codes based on quasigroup transformations are proposed. For the proposed codes, similar to recursive convolutional codes, the correlation exists between any two bits of a codeword, which can have infinite length, theoretically. However, in contrast to convolutional codes, the proposed codes are nonlinear and almost random: for codewords with large enough length, the distribution of the letters, pair of letters, triple of letters, and so on, is uniform. Simulation results of bit-error probability for several codes in binary symmetric channels are presented.",2007,0,
510,511,Multimedia processor-based implementation of an error-diffusion halftoning algorithm exploiting subword parallelism,"Multimedia processor-based implementations of digital image processing algorithms have become important since several multimedia processors are now available and can replace special-purpose hardware-based systems because of their flexibility. Multimedia processors increase throughput by processing multiple pixels simultaneously using a subword-parallel arithmetic and logic unit architecture. The error-diffusion halftoning algorithm employs feedback of quantized output signals to faithfully convert a multi-level image to a binary image or to one with fewer levels of quantization. This makes it difficult to achieve speedup by utilizing the multimedia extension. In this study, the error-diffusion halftoning algorithm is implemented for a multimedia processor using three methods: single-pixel, single-line, and multiple-line processing. The single-pixel approach is the closest to conventional implementations, but the multimedia extension is used only in the filter kernel. The single-line approach computes multiple pixels in one scan-line simultaneously, but requires a complex algorithm transformation to remove dependencies between pixels. The multiple-line method exploits parallelism by employing a skewed data structure and processing multiple pixels in different scan-lines. The Pentium MMX instruction set is used for quantitative performance evaluation including run-time overheads and misaligned memory accesses. A speedup of more than ten times is achieved compared to the software (integer C) implementation on a conventional processor for the structurally sequential error-diffusion halftoning algorithm",2001,0,
511,512,Fault Diagnosis Implementation of Induction Machines based on Advanced Digital Signal Processing Techniques,"In this paper, a comprehensive cross correlation-based fault diagnostic method is proposed for real time DSP implementation. It covers both fault monitoring and decision making stages. In practice, a motor driven by an adjustable speed drive is run at various operating points where the frequency, amplitude and phase of the fault signatures varies with time. These dynamic changes are considered as one of the common factor that yields erroneous fault tracking and unstable fault detection. In this paper, the proposed algorithms deals with the operating point dependent ambiguities and threshold issues. It is theoretically and experimentally verified that the motor fault can continuously be tracked when the operating point changes within a limited range.",2009,0,
512,513,Positive Switching Impulse Discharge Performance and Voltage Correction of Rod-Plane Air Gap Based on Tests at High-Altitude Sites,"The Qinghai-Tibet Railway is the highest railway in the world. Up to now, there were no test and service data for the external insulation of the power-supply project of the railway system above 4000 m above sea level (a.s.l.). The ldquogrdquo parameter method recommended by IEC Publication 60.1 (1989) has a limited applicable range. Therefore, based on the former tests carried on the artificial climate chamber (ACC), in this paper, a series of test investigations is conducted on the positive switching impulse (PSI) discharge performance of rod-plane air gaps with gap spacing of 0.25 to 3.0 m at the six high-altitude sites along the Qinghai-Tibet Railway with altitudes of 2820 to 5050 m. With analyses of the mathematical optimization method on the test results, the new correction method of discharge voltage is proposed. They are also checked and compared with the test results obtained from the simulation tests carried out in the ACC. It is indicated that the 50% PSI discharge voltage <i>U</i> <sub>50</sub> of the air gap at high altitude is a power function of gap spacing <i>d</i>, also a power function of relative pressure of dry air and absolute humidity. The influence law of atmospheric parameters on <i>U</i> <sub>50</sub> obtained at high-altitude sites is the same as that obtained in the ACC. <i>U</i> <sub>50</sub> . obtained in the ACC, is about 8.15% higher than that obtained at high-altitude sites due to the influence of nonsimulated factors, such as ultraviolet ray and cosmic radiation. The ldquogrdquo parameter method is not applicable to the regions with an altitude above 2800 m.",2009,0,
513,514,A randomized error recovery algorithm for reliable multicast,"An efficient error recovery algorithm is essential for a liable multicast in large groups. Tree-based protocols (RMTP, TMTP, LBRRM) group receivers into local regions and select a repair server for performing error recovery in each region. Hence a single server bears the entire responsibility of error recovery for a region. In addition, the deployment of repair servers requires topological information of the underlying multicast tree, which is generally not available at the transport layer. This paper presents RRMP, a randomized reliable multicast protocol which improves the robustness of tree-based protocols by diffusing the responsibility of error recovery among all members in a group. The protocol works well within the existing IP multicast framework and does not require additional support from routers. Both analysis and simulation results show that the performance penalty due to randomization is low and can be tuned according to application requirements",2001,0,
514,515,Combined Use of Fuzzy Set-Covering Theory and Mode Identification Technique for Fault Diagnosis in Power Systems,"After a fault occurs in a power system, generally some operating information of protective relays and circuit breakers could be obtained. Because protective relays and circuit breakers might improperly operate or fail to operate, and some errors and distortion may exist in data acquisition and communication, as a result uncertainties could be involved in the received information. A fault diagnosis model based on fuzzy set- covering theory and mode identification technique is proposed in this paper. With the fuzzy technology, the above mentioned uncertainties could be dealt with very well. Meanwhile, as the protective relays and circuit breakers may fail to operate in some cases, there are several different operating modes in protective relays and circuit breakers even for a same electrical device failure. Based on the received information, the proposed model could identify the most possible operating mode, and then the information corresponding to a fault hypothesis could be obtained. In the proposed model, the proposed fault diagnosis problem is described as a 0-1 integer programming one, and thus could be solved by the widely employed search technology, i.e., the well-known Tabu search method. The feasibility and efficiency of the proposed model is demonstrated by a sample power system.",2007,0,
515,516,H<sub></sub> Dynamic observer design with application in fault diagnosis,"Most observer-based methods applied in fault detection and diagnosis (FDD) schemes use the classical two-degrees of freedom observer structure in which a constant matrix is used to stabilize the error dynamics while a post filter helps to achieve some desired properties for the residual signal. In this paper, we consider the use of a more general framework which is the dynamic observer structure in which an observer gain is seen as a filter designed so that the error dynamics has some desirable frequency domain characteristics. This structure offers extra degrees of freedom and we show how it can be used for the sensor faults diagnosis problem achieving detection and estimation at the same time. The use of weightings to transform this problem into a standard H<inf></inf>problem is also demonstrated.",2005,0,
516,517,Practical considerations in making CORBA services fault-tolerant,"This paper examines the CORBA Naming, Event, Notification, Trading, Time and Security Services, with the objective of identifying the issues that must be addressed in order make these services fault-tolerant. The reliability considerations for each of these services involves strategies for replicating the service objects, and for keeping the states of the replicas consistent. Of particular interest are the sources of non-determinism in each of these services, along with the means for addressing the non-deterministic behavior in the interests of ensuring strong fault tolerance",2002,0,
517,518,Analysis - The terrors and the errors [IT Change Management],"According to last month??s Sophos `Security Threat Report??, concern is increasing that computer applications running critical national infrastructures are vulnerable to malevolent hacks. Such hacks could in theory switch control of power and gas supplies, say, to the keyboards of hostile entities, enabling them to wreak damage and disruption. Similar threats face crucial financial computer platforms that underpin national economies, and even emergency services communication channels.",2010,0,
518,519,Innovative airborne inventory and inspection technology for electric power line condition assessments and defect reporting,"A cost-effective and innovative airborne inventory and inspection patrol system for distributed assets such as transmission lines, pipelines, and roadways has been developed and evaluated. Results show that aerial high-resolution digital visual and spectral images tagged by Global Positioning Satellite (GPS) coordinates can be successfully used to cost-effectively identify the majority of conditions/defects on electric power lines. Experiments show that the condition and defect detection rate of the airborne inventory and inspection system is significantly higher than rates derived from traditional patrols and comparable to values achieved from driving patrols. Geographic information systems (GIS) based mapping tools can be used to quickly and efficiently interpret digital images collected from aerial platforms. Digital images provide an archival record of the condition of the distributed assets to estimate the long-term performance of the assets and to define cost-effective maintenance and replacement schedules",2000,0,
519,520,Advanced Cu CMP defect excursion control for leading edge micro-processor manufacturing,"The introduction of yield sensitive, advanced interconnect technology coupled with the requirement for accelerating yield ramp in today's state-of-the-art semiconductor manufacturing facilities, are driving tool monitoring requirements for fast and accurate defect excursion control. In the Copper CMP module the challenge is accentuated by the relative immaturity of this process, the dominance of single wafer excursions and a high count of nuisance defect types relative to the critical yield-limiting defect types. A manufacturing-worthy Copper CMP tool monitor methodology is described here that improves excursion control through detection and tracking of critical, yield-limiting defect types, independent of non-yield-critical nuisance defect types. High-resolution automatic defect review and classification, a critical component of the methodology, is limited to wafers with high critical-defect counts, reducing monitoring cost and time-to-results. A new trigger sampling feature and intelligent image sampling reduces monitoring cost and time-to-results through minimizing defect review overhead. Integration of such a solution into the manufacturing environment is presented in detail and contrasted next to existing traditional defect excursion control model. Ease-of-use considerations are highlighted with use case examples. The paper will approximate the cost savings to manufacturing such as reducing existing levels of false excursion due to nuisance defects and improving the cycle time in the Cu CMP module. Benefits are achieved by integrating functionality into existing inspection hardware. No additional capital equipment was required.",2002,0,
520,521,Hybrid fault-tolerant control of aerospace vehicles,"We describe our recent results (2001) related to the design of hybrid online failure detection and identification and adaptive reconfigurable control algorithms for aerial and space vehicles. Our approach is based on the multiple models, switching and tuning methodology and its extensions, and has been demonstrated as an efficient tool for hybrid fault tolerant control under subsystem and component failures and structural damage",2001,0,
521,522,Active error recovery for reliable multicast,"An error recovery scheme is essential for large-scale reliable multicast. We design, implement, and evaluate an improved active error recovery scheme for reliable multicast (AERM). The AERM uses soft-state storage to facilitate fast error recovery. It has the following features: a simple NAK suppression and aggregation mechanism, an efficient hierarchical RTT measurement mechanism, an effective local recovery and scoped retransmission mechanism, and a periodical ACK mechanism. We implement the AERM and study its characteristics in NS2. We also compare performance with ARM and AER/NCA, both of which are representative active reliable multicast protocols. The results indicate that AERM can achieve considerable performance improvement with limited support from routers. Our work also confirms that active networks can benefit some applications and become a promising network computing platform in the future",2001,0,
522,523,An Error Concealment Scheme for Entire Frame Losses for H.264/AVC,"In this paper, an error concealment scheme is proposed to conceal an entirely lost frame in a compressed video bitstream due to errors introduced during transmission. The proposed scheme targets low bit rate video transmission applications using H.264/AVC. The motion field of the lost frame is first reconstructed by copying the co-located motion vectors and reference indices from the last decoded reference frame. After the motion field estimation of the missing frame, motion compensation is performed to reconstruct the frame. This technique reuses existing modules of the video decoder and it does not incur extra complexity compared to decoding a normal frame. It has also been adopted as a non-normative decoder option to the JM reference software at the JVT meeting in Poznan, Poland in July 2005 [1] and has been incorporated into the SA4 video ad hoc group's toolkit at the 3GPP meeting at Paris [2] in September 2005. Simulation results will show its improved performance over other simple error concealment schemes such as ""frame copy,"" both subjectively and objectively, without significant complexity overhead.",2006,0,
523,524,A new textual/non-textual classifier for document skew correction,A robust approach is proposed for document skew detection. We use Fourier analysis and SVM to classify textual areas from non-textual areas of documents. We also propose a robust method to determine the skew angle from textual areas. Our approach achieves good performance on documents with large area of non-textual contents.,2002,0,
524,525,Evolutionary design and adaptation of digital filters within an embedded fault tolerant hardware platform,"Finite impulse response filters (FIRs) are crucial device for robust data communication and manipulation. Multiplierless filters have been shown to produce high performance systems with fast signal processing and reduced area. Furthermore, the distributed architecture inherent in multiplierless filters makes it a suitable candidate for fault tolerant design. Alternative approaches to the design of fault tolerant systems have been proposed using evolutionary algorithms (EAs) and the concept of evolvable hardware (EHW). This paper presents an evolvable hardware platform for the automated design and adaptation of multiplierless digital filters. Filters are realised within a dedicated programmable logic array (PLA). The platform employs a genetic algorithm to autonomously configure the PLA for a give set of coefficients. The ability of the platform to adapt to increasing numbers of faults was investigated through the evolution of a 31-tap low-pass FIR filter. Results show that the functionality of filters evolved on the PLA was maintained despite an increasing number of faults covering up to 25% of the PLA area. Additionally, three PLA initialisation methods were investigated to ascertain which produced the fastest fault recovery times. It was shown that seeding a population of random configuration-strings with the best configuration currently obtained resulted in a 6 fold increase in fault recovery speed over other methods investigated",2001,0,
525,526,Defect Identification in Large Area Electronic Backplanes,We describe a rapid testing system for active matrix thin-film transistor (TFT) backplanes which enables the identification of many common processing defects. The technique spatially maps the charge feedthrough from TFTs in the pixel and is suited for pixels with switched-capacitor architecture.,2009,0,
526,527,Design and analysis of a fault-tolerant mechanism for a server-less video-on-demand system,"Video-on-demand (VoD) systems have traditionally been built on the client-server architecture, where a video server stores, retrieves, and transmits video data to video clients for playback This paper investigates a radically different approach to building VoD systems, one where the server, and hence the primary bottleneck, is completely eliminated. This server-less architecture comprises homogeneous hosts, called nodes, which serve both as client and as mini-server. Video data are distributed over all nodes and these nodes cooperatively stream video data to one another for playback. However, unlike traditional video server that runs on high-end server hardware in a carefully controlled and protected data centre, a node in a server less system is likely to be far more unreliable. Therefore it is essential that sufficient data and capacity redundancies are incorporated to maintain an acceptable set-vice reliability. This paper presents and analyzes a fault tolerant mechanism based on inter-node striping and erasure correction codes to tackle this challenge. By formulating the system's reliability as a Markov chain model, we obtain insights into the feasible operating region of the system, such as the amount of redundancy required and the node-level reliability that can be tolerated. Numerical results show that a server-less VoD system of 200 nodes can achieve reliability surpassing that of dedicated video server using a redundancy overhead of only 21.2% even though individual nodes are highly unreliable.",2002,0,
527,528,Performance analysis and improvements for a simulation-based fault injection platform,"In this paper, we study and present two techniques to improve the performance of a simulation-based fault injection platform that inserts bit flips in order to model soft errors on digital circuits. The platform is based on the ESA Data Systems Divisionpsilas SEE simulation tool. In contrast with methods based on emulation, the proposed approach reduces the complexity and costs, supplying a test environment with the same reliability as emulation systems. Only one disadvantage appears when comparing both methodologies: the lower performance of the simulation in cases where the fault injection campaigns are very large. Two proposals have been developed in order to address this drawback: the first one is based on software (through checkpoints) and the second one uses parallel computation.",2008,0,
528,529,Autonomous Decentralized VoD System Architecture and Fault-Tolerant Technology to Assure Continuous Service,"In distributed and ubiquitous computing systems, not only the composing subsystems and their functions, but also the system structure would be changed constantly under the evolving situation. With the advances of compression, storage and network technologies, Video on Demand (VoD) service is becoming more and more popular. However, it is difficult for conventional systems to meet the continuous and heterogeneous requirements from service providers and users simultaneously. This paper introduces an autonomous decentralized VoD system sustained by mobile agents for information service provision and utilization. Under the proposed architecture, autonomous fault detection and recovery technologies are proposed to assure continuous service. The effectiveness of the proposed technology is proved by the simulation. The results show that an average of 30% improvement in recovery time and users' video service can be recovered without stopping compared with the conventional system.",2009,0,
529,530,A novel framework for robust video streaming based on H.264/AVC MGS coding and unequal error protection,"We present a novel framework to provide robust video streaming service over time-varying error-prone network. The scheme is based on the medium granularity scalability (MGS) video coding of the H.264/AVC standard, which adopts a hierarchical prediction structure for the group-of-pictures (GOP). We determine the optimal allocation of protection strength for different network abstraction layer (NAL) units according to their individual importance to the end-to-end video quality. To analyse the importance of the NAL units, we emulate the error concealment if one frame is considered as lost and take into account the propagation distortion within the GOP. An efficient algorithm is proposed to account for the non-convex rate-distortion characteristics associated with the NAL units in the hierarchical GOP. With this framework, we can provide robust video streaming for the range of packet loss rates from 0% to 40% with about 30% additional channel bit-rate for the channel coding. The simulation results demonstrate high flexibility and efficiency of the proposed framework, which can effectively prevent frequent loss of frames.",2009,0,
530,531,Application of an automated PD failure identification system for EMC-assessment strategies of multiple PD defects at HV-insulators,"EMC-assessment of field emission of high voltage insulators can be performed using phase-angle-resolved partial discharge diagnosis. However, with conventional PD-detection systems no satisfying statements about multiple PD defects are possible because of the highly dynamic apparent charge values for different discharge phenomena. This measurement problem can be solved by a new approach. For this purpose phase resolved pulse sequence analysis methods are suitable diagnosis tools without using the apparent charge as a dominating influence. A recently developed feature extraction method based on consecutive u/ values shows good classification results. The problem of multiple PD defects, which occur at the same time, is a new challenge for PD diagnosis systems. For the investigation two reference databases are generated. With the database, which takes these multiple PD defects into account, the diagnosis system WinTED of the University of Wuppertal is able to identify with high reliability actual measurements made by the University of Dortmund",2000,0,
531,532,Design aspects and pattern prediction for phased arrays with subarray position errors,"In modern array design, the antenna elements are often grouped into mechanical units such as printed antenna boards and mechanical subarrays/multipacks. This contributes to a more cost efficient manufacturing process and facilitates integration, handling, reuse and exchange of units, but it also makes the antenna element position errors correlated. Classical papers predict the statistical sidelobe level based on the assumption of uncorrelated errors, but using this for the general case, the statistical sidelobe level is under estimated. In this paper, the statistical sidelobe level for arrays with correlated position errors is predicted. Furthermore, rules of thumb relating antenna element position tolerances and mechanical array design to antenna array performance (sidelobe level) are given. Finally, array design aspects are discussed.",2010,0,
532,533,Double Redundant Fault-Tolerance Service Routing Model in ESB,"With the development of the Service Oriented Architecture (SOA), the Enterprise Service Bus (ESB) is becoming more and more important in the management of mass services. The main function of it is service routing which focuses on delivery of message among different services. At present, some routing patterns have been implemented to finish the messaging, but they are all static configuration service routing. Once one service fails in its operation, the whole service system will not be able to detect such fault, so the whole business function will also fail finally. In order to solve this problem, we present a double redundant fault tolerant service routing model. This model has its own double redundant fault tolerant mechanism and algorithm to guarantee that if the original service fails, another replica service that has the same function will return the response message instead automatically. The service requester will receive the response message transparently without taking care where it comes from. Besides, the state of failed service will be recorded for service management. At the end of this article, we evaluated the performance of double redundant fault tolerant service routing model. Our analysis shows that, by importing double redundant fault tolerance, we can improve the fault-tolerant capability of the services routing apparently. It will solve the limitation of existent static service routing and ensure the reliability of messaging in SOA.",2009,0,
533,534,Stator Current and Motor Efficiency as Indicators for Different Types of Bearing Faults in Induction Motors,"This paper proposes a new approach to use stator current and efficiency of induction motors as indicators of rolling-bearing faults. After a presentation of the state of the art about condition monitoring of vibration and motor current for the diagnostics of bearings, this paper illustrates the experimental results on four different types of bearing defects: crack in the outer race, hole in the outer race, deformation of the seal, and corrosion. The first and third faults have not been previously considered in the literature, with the latter being analyzed in other research works, even if obtained in a different way. Another novelty introduced by this paper is the analysis of the decrease in efficiency of the motor with a double purpose: as alarm of incipient faults and as evaluation of the extent of energy waste resulting from the lasting of the fault condition before the breakdown of the machine.",2010,0,
534,535,Noise-Related Radiometric Correction in the TerraSAR-X Multimode SAR Processor,"Synthetic aperture radar (SAR) image intensity is disturbed by additive system noise. During SAR focusing, pattern corrections that are adapted to the characteristics of the wanted signal, but not to the characteristics of the noise, influence the spatial distribution of the noise power. Particularly in the case of ScanSAR, a distinct residual noise pattern in low backscatter areas results. This necessitates a noise-adapted radiometric correction of the focused image for almost all applications except interferometry. In this paper, we thoroughly investigate this topic. Based on signal theoretical and stochastic considerations, we develop a radiometric correction scheme. Simulations and the application of the algorithm to TerraSAR-X datatakes support the theoretical results.",2010,0,
535,536,Robot fault-tolerance using an embryonic array,"Fault-tolerance, complex structure management and reconfiguration are seen as valuable characteristics. Embryonic arrays represent one novel approach that takes inspiration from nature to improve upon standard techniques. An existing BAE SYSTEMS RASCALTM robot has been augmented so as to improve the motor control system reliability through two biologically-inspired systems: an embryonic array and an artificial immune system. This paper is concerned with the embryonic array; this is novel in that it supports datapath-wide arithmetic and logic functions. The array is configured to provide an autonomous self-repairing hardware motor controller and is realized using a standard Xilinx Virtex FPGA. As with previous embryonic systems, the logic requirement of the array is greater than that of a conventional FPGA or standard modular-redundancy approach. However, the array offers the advantages of both conventional FPGAs and modular-redundancy techniques. It is a reconfigurable computing platform that provides inherent fault-tolerance through its distributed self-repair mechanism.",2003,0,
536,537,Comparing code reading techniques applied to object-oriented software frameworks with regard to effectiveness and defect detection rate,"This paper first reasons on understanding software frameworks for defect detection, and then presents an experimental research for comparing the effectiveness and defect detection rate of code-reading techniques, once applied to C++ coded object-oriented frameworks. We present and discuss the functionality-based approach to framework understanding. Then, we present an experiment that compared three reading techniques for inspection of software frameworks. Two of those reading techniques, namely checklist-based reading, and systematic order-based reading, were adopted from scientific literature, while the third one, namely functionality-based reading, was derived from the functionality-based approach. The results of the experiment are that (1) functionality-based reading is much more effective and efficient than checklist based reading. (2) Functionality-based Reading is significantly more effective and efficient than systematic order-based reading. (3) Systematic order-based reading performs significantly better than checklist based reading for what concerns defect detection rate. However, because we used checklist-based reading and systematic order-based reading quite as they are, with limited adaptation to frameworks, it is too early to draw strong conclusions from the experiment results and improving and replicating this study is strongly recommended.",2004,0,
537,538,Fault detection in IP-based process control networks using data mining,"Industrial process control IP networks support communications between process control applications and devices. Communication faults in any stage of these control networks can cause delays or even shutdown of the entire manufacturing process. The current process of detecting and diagnosing communication faults is mostly manual, cumbersome, and inefficient. Detecting early symptoms of potential problems is very important but automated solutions do not yet exist. Our research goal is to automate the process of detecting and diagnosing the communication faults as well as to prevent problems by detecting early symptoms of potential problems. To achieve our goal, we have first investigated real-world fault cases and summarized control network failures. We have also defined network metrics and their alarm conditions to detect early symptoms for communication failures between process control servers and devices. In particular, we leverage data mining techniques to train the system to learn the rules of network faults in control networks and our testing results show that these rules are very effective. In our earlier work, we presented a design of a process control network monitoring and fault diagnosis system. In this paper, we focus on how the fault detection part of this system can be improved using data mining techniques.",2009,0,
538,539,Investigating effects of neutral wire and grounding in distribution systems with faults,"In some applications, like fault analysis, fault location, power quality studies, safety analysis, loss analysis, etc., knowing the neutral wire and ground currents and voltages could be of particular interest. In order to investigate the effects of neutrals and system grounding on the operation of distribution feeders with faults, a hybrid short circuit algorithm is generalized. In this novel use of the technique, the neutral wire and assumed ground conductor are explicitly represented. Results obtained from several case studies using the IEEE 34-node test network are presented and discussed.",2004,0,
539,540,On Line Fault Detection and an Adaptive Algorithm to Fast Distance Relaying,"This paper presents the design of an hybrid scheme of wavelet transforms and an adaptive Fourier filtering technique for on line fault detection and phasor estimation to fast distance protection of transmission lines. The wavelet transform is used as a signal processing tool. The sampled voltage and current signals at the relay location are decomposed using wavelet transform-Multi Resolution Analysis (MRA). The decomposed signals are used for the fault detection and as input to the phasor estimation algorithm. The phasor estimation algorithm possesses the advantage of recursive computing and a decaying dc offset component is removed from fault signals by using an adaptive compensation method. Fault detection index and a variable data window scheme are embedded in the algorithm. The proposed scheme provides capability for fast tripping decision, taking accuracy into account. Extensive simulation tests and comparative evaluation presented prove the efficacy of the proposed scheme in distance protection.",2008,0,
540,541,Parametric fault trees with dynamic gates and repair boxes,"A new approach is proposed to include s-dependencies in fault tree (FT) models. With respect to previous techniques, the approach presented in this paper is based on two peculiar powerful features. First, adopting a parameterization technique, referred to as parametric FT (PFT), to fold equal subtrees (or basic events) in order to resort to a more compact FT representation. It is shown that parameterization can be conveniently adopted as well for dynamic gates. Second, PFT can be modularized and each module translated into a high level colored Petri net in the form of a stochastic well-formed net (SWN). SWN generate a lumped Markov chain and the saving in the dimension of the state space can be very substantial with respect to standard (non colored) Petri nets. Translation of PFT modules into SWN has proved to be very flexible, and various kinds of new dependencies can be easily accommodated. In order to exploit this flexibility a new primitive, called repair box, is introduced. A repair box, attached to an event, causes the starting of a repair activity of all the components that failed as the event occurs. In contrast to all the previous FT based models, the addition of repair boxes enables the approach to model cyclic behaviors. The proposed approach as dynamic repairable PFT (DRPFT) was referred to. A tool supporting DRPFT is briefly described and the tool is validated by analyzing a benchmark proposed recently in the literature for quantitative comparison [H. Zhu et al., 2001].",2004,0,
541,542,An Enhanced Fault-Tolerant Routing Algorithm for Mesh Network-on-Chip,"Fault-tolerant routing is the ability to survive failure of individual components and usually uses several virtual channels (VCs) to overcome faulty nodes or links. A well-known wormhole-switched routing algorithm for 2-D mesh interconnection network called f-cube3 uses three virtual channels to pass faulty regions, while only one virtual channel is used when a message does not encounter any fault. One of the integral stages of designing network-on-chips (NoCs) is the development of an efficient communication system in order to provide low latency networks. We have proposed a new fault-tolerant routing algorithm based on f-cube3 as a solution to reduce the delay of network packets which uses less number of VCs in comparison with f-cube3. Moreover, in this method we have improved the use of VCs per each physical link by reducing required channels to two. Furthermore, simulations of both f-cube3 and our algorithm based on same conditions have been presented.",2009,0,
542,543,Path diversity with forward error correction (PDF) system for packet switched networks,"Packet loss and end-to-end delay limit delay sensitive applications over the best effort packet switched networks such as the Internet. In our previous work, we have shown that substantial reduction in packet loss can be achieved by sending packets at appropriate sending rates to a receiver from multiple senders, using disjoint paths, and by protecting packets with forward error correction. In this paper, we propose a path diversity with forward error correction (PDF) system for delay sensitive applications over the Internet in which, disjoint paths from a sender to a receiver are created using a collection of relay nodes. We propose a scalable, heuristic scheme for selecting a redundant path between a sender and a receiver, and show that substantial reduction in packet loss can be achieved by dividing packets between the default path and the redundant path. NS simulations are used to verify the effectiveness of PDF system.",2003,0,
543,544,New Approach for Defect Inspection on Large Area Masks,"Besides the mask market for IC manufacturing, which mainly uses 6 inch sized masks, the market for the so called large area masks is growing very rapidly. Typical applications of these masks are mainly wafer bumping for current packaging processes, color filters on TFTs, and Flip Chip manufacturing. To expose e.g. bumps and similar features on 200 mm wafers under proximity exposure conditions 9 inch masks are used, while in 300 mm wafer bumping processes 14 inch masks are handled. Flip Chip manufacturing needs masks up to 28 by 32 inch. This current maximum mask dimension is expected to hold for the next 5 years in industrial production. On the other hand shrinking feature sizes, just as in case of the IC masks, demand enhanced sensitivity of the inspection tools. A defect inspection tool for those masks is valuable for both the mask maker, who has to deliver a defect free mask to his customer, and for the mask user to supervise the mask behavior conditions during its lifetime. This is necessary because large area masks are mainly used for proximity exposures. During this process itself the mask is vulnerable by contacting the resist on top of the wafers. Therefore a regular inspection of the mask after 25, 50, or 100 exposures has to be done during its whole lifetime. Thus critical resist contamination and other defects, which lead to yield losses, can be recognized early. In the future shrinking feature dimensions will require even more sensitive and reliable defect inspection methods than they do presently. Besides the sole inspection capability the tools should also provide highly precise measurement capabilities and extended review options.",2007,0,
544,545,On the bit-error probability of differentially encoded QPSK and offset QPSK in the presence of carrier synchronization,"We investigate the differences between allowable differential encoding strategies and their associated bit-error probability performances for quadrature phase-shift keying (QPSK) and offset QPSK modulations when the carrier demodulation reference signals are supplied by the optimum (motivated by maximum a posteriori estimation of carrier phase) carrier-tracking loop suitable for that modulation. In particular, we show that in the presence of carrier-synchronization phase ambiguity but an otherwise ideal loop, both the symbol and bit-error probabilities in the presence of differential encoding are identical for the two modulations. On the other hand, when in addition the phase error introduced by the loop's finite signal-to-noise ratio is taken into account, it is shown that the two differentially encoded modulations behave differently, and their performances are no longer equivalent. A similar statement has previously been demonstrated for the same modulations when the phase ambiguity was assumed to have been perfectly resolved by means other than differential encoding.",2006,0,
545,546,Robust sensor fault estimation for tolerant control of a civil aircraft using sliding modes,This paper proposes a sensor fault tolerant control scheme for a large civil aircraft. It is based on the application of a robust method for sensor fault reconstruction using sliding mode theory. The novelty lies in the application of the sensor fault reconstruction scheme to correct the corrupted measured signals before they are used by the controller and therefore the controller does not need to be reconfigured to adapt to sensor faults,2006,0,
546,547,High level net models: a tool for permutation mapping and fault detection in multistage interconnection network,"This paper aims at structurising the detection of different types of stuck-at faults for a wide range of multistage interconnection networks (MINs). The results reported so far in this respect have been mainly based on direct combinatorial analysis of the concerned networks with very little consideration towards the modelling aspects. Graphical representation coupled with well-defined semantics allowing formal analysis has already established the Petri net as an effective tool for modelling dynamic systems. However, the existing variants of high level nets had certain limitations in modelling the dynamic behaviour of mapping a permutation through the MIN and further analysis of the same. This has inspired the authors to propose a couple of new high level net models, called MP-net and S-net in their earlier works. The S-net model uses tokens to hold and propagate information apart from controlling the firing of events. It uses two different types of places and transitions each as has been defied subsequently. In this paper, we have concentrated on the detection of faults in MINs using this S-net model",2000,0,
547,548,From Fireflies to Fault-Tolerant Swarms of Robots,"One of the essential benefits of swarm robotic systems is redundancy. In case one robot breaks down, another robot can take steps to repair the failed robot or take over the failed robot's task. Although fault tolerance and robustness to individual failures have often been central arguments in favor of swarm robotic systems, few studies have been dedicated to the subject. In this paper, we take inspiration from the synchronized flashing behavior observed in some species of fireflies. We derive a completely decentralized algorithm to detect non-operational robots in a swarm robotic system. Each robot flashes by lighting up its on-board light-emitting diodes (LEDs), and neighboring robots are driven to flash in synchrony. Since robots that are suffering catastrophic failures do not flash periodically, they can be detected by operational robots. We explore the performance of the proposed algorithm both on a real-world swarm robotic system and in simulation. We show that failed robots are detected correctly and in a timely manner, and we show that a system composed of robots with simulated self-repair capabilities can survive relatively high failure rates.",2009,0,
548,549,Implications of Rent's Rule for NoC Design and Its Fault-Tolerance,"Rent's rule is a powerful tool for exploring VLSI design and technology scaling issues. This paper applies the principles of Rent's rule to the analysis of networks-on-chip (NoC). In particular, a bandwidth-version of Rent's rule is derived, and its implications for future NoC scaling examined. Hop-length distributions for Rent's and other traffic models are then applied to analyse NoC router activity. For fault-tolerant design, a new type of router is proposed based on this analysis, and it is evaluated for mutability and its impact on congestion by further use of the hop-length distributions. It is shown that the choice of traffic model has a significant impact on scaling behaviour, design and fault-tolerant analysis",2007,0,
549,550,Analysis on fault voltage and secondary arc current of single phase refusing-shut of the 500kV extra high voltage transmission line,"It is common knowledge that 500 kV extra high voltage and long distant transmission line join a shunt reactor and a neutral grounding via small reactor; This paper analysis systematically an possible condition of the frequency-regulating resonance over-voltage on single phase cut fault to refusing-shut of the 500 kV extra high voltage transmission line which join a shunt reactor, the system compose an complex series resonance circuits, and present a rational mode of reactive compensation. This paper also build rational mathematic mode on systemic parameter of 500 kV ci-yong transmission line, and resolute detailedly its power frequency component, low frequency component and its DC component of single phase cut fault voltage and secondary arc current by the mean of Laplacian transformation ruling formula. All the this is to offer an farther analysis on switching over-voltage and secondary arc current interrupter of long distant transmission line. In the end, this system also implemented using MATLAB software, compute the transient process on single phase cut fault voltage and secondary arc current.",2009,0,
550,551,BOAs: backoff adaptive scheme for task allocation with fault tolerance and uncertainty management,"We propose the backoff adaptive scheme (BOAs) as a new technique for the automatic allocation of tasks amongst a team of heterogeneous mobile robots. It is an optimal, decentralized decision making scheme that utilizes explicit communication between the agents. A structured and unified framework is also proposed for task specification. This scheme is fault tolerant (to robot malfunctions) and allows for uncertainty in the nature of task specification in terms of the actual number of robots required. Team demography may change without the need for the respecification of tasks. The adaptive feature in BOAs further improves the flexibility of the team. Realistic simulations are carried out to verify the effectiveness of the scheme.",2004,0,
551,552,Fault-tolerant vibration control in a networked and embedded rocket fairing system,"Active vibration control using piezoelectric actuators in a networked and embedded environment has been widely applied to solve the rocket fairing vibration problem. However, actuator failures may lead to performance deterioration or system dysfunction. To guarantee the desired system performance, the remaining actuators should be able to coordinate with each other to compensate for the damaging effects caused by the failed actuator in a timely manner. Further, in the networked control environment, timing issues such as sampling jitter and network-induced delay should be considered in the controller design. In this study, a timing compensation approach is implemented in an adaptive actuator failure compensation controller to maintain the fairing system performance by also considering the detrimental effects from real-time constraints. In addition, time-delay compensation in the networked control system is discussed, which is able to reduce damaging effects of network-induced delays.",2004,0,
552,553,Techniques to enable FPGA based reconfigurable fault tolerant space computing,Reconfigurable computing using field programmable gate arrays (FPGAs) offer significant performance improvements over traditional space based processing solutions. The application of commercial-off-the-shelf (COTS) FPGA processing components requires radiation-effect detection and mitigation strategy to compensate for the FPGAs' susceptibility to single event upsets (SEUs) and single event functional interrupts (SEFIs). A reconfigurable computing architecture that uses external triple modular redundancy (TMR) via a radiation-hardened ASIC provides the most robust approach to SEU and SEFI detection and mitigation. Honeywell has designed a TMR Voter ASIC with an integrated FPGA configuration manager that can automatically reconfigure an upset FPGA upon TMR error detection. The automatic configuration manager also has features to support resynchronizing the upset FPGA with the remaining two FPGAs operating in a self checking pair (SCP) mode. Automating and minimizing reconfiguration times and re synchronization times enables high performance FPGA-based processors to provide high system availability with minimal software/system controller intervention,2006,0,
553,554,A Predictive Method for Providing Fault Tolerance in Multi-agent Systems,"The growing importance of multi-agent applications and the need for a higher quality of service in these systems justify the increasing interest in fault-tolerant multi-agent systems. In this article, we propose an original method for providing dependability in multi- agent systems through replication. Our method is different from other works because our research focuses on building an automatic, adaptive and predictive replication policy where critical agents are replicated to avoid failures. This policy is determined by taking into account the criticality of the plans of the agents, which contain the collective and individual behaviors of the agents in the application. The set of replication strategies applied at a given moment to an agent is then fine-tuned gradually by the replication system so as to reflect the dynamicity of the multi-agent system. We report on experiments assessing the efficiency of our approach.",2006,0,
554,555,Design of a fault-tolerant voter for safety related analog inputs,"This paper introduces a voting scheme for safety-related analog input module to arbitrate between the results of redundant channels in fault-tolerant system. The design approach is a distributed system using a sophisticated form of duplication. For each running process, there is a backup process running on a different CPU. The voter is responsible for checkpointing its state to duplex CPUs. In order to increase the dependability for safety-related controllers, the I/O modules use redundancy to reduce the risk associated with relying upon any single component operating flawlessly. The 1oo2D voting principle is commonly used in fault tolerant I/O modules to provide passive redundancy for masking runtime faults at hardware and software levels, respectively. A dual architecture (1oo2D) which provides high safety integrity to a rating of SIL 3 is presented. The outputs from two identical channels operating in parallel with the same inputs are supplied to a voting unit that arbitrates between them to produce an overall output. Based on the hardware logic model and FPGA technique, the study adopts the hardware voter which has much more advantage in the velocity and reliability. Finally, using modelsim simulations, we verify the effectiveness of the proposed voter design in preserving the hazard-free property of the response of an analog inputs module.",2010,0,
555,556,Research on Multi-agent System Model of Diesel Engine Fault Diagnosis by Case-Based Reasoning,"Oil monitoring technology is a useful method in condition monitoring and fault diagnosis for the machine, especially for low-speed, heavy-load, reciprocated and lubricated diesel engine equipment. But it is difficult to implement intelligent diagnosis because monitored information lacks logical relationship in oil monitoring. To solve this problem, the theory and method of case-based reasoning is adopted for the data processing and fault analysis in oil monitoring with a multi-agent system structure. Detailed definitions of agents in the system were proposed, and the multi-agent system framework was established finally. Multi-agent mechanism brings flexible for case based reasoning. It enhances the capability of solving complicated question in new system, and overcomes the shortcoming of the fault knowledge difficult to update in traditional systems",2006,0,
556,557,Predicting and controlling FPGA Device Heat using System monitor and IBERT (internal bit error ratio tester),The aim of this paper is to present a new methodology and the tools used to predict and control the FPGA Device Heat before starting the design. Knowing that the FPGA silicon heat is crucial as they all have a temperature above and under which their functionalities is not longer guaranteed. The silicon temperature is linked to the different options and strategies used to implement the design. Many tools such ldquouse Xpowerrdquo from Xilinx allows the user to have an estimation of the power consumption. This paper will present a primitive called System monitor which is present in every Virtex 5 to monitor the environment around the FPGA. Monitoring the device environment maximises the probability of getting the FPGA work after implementing required design.,2009,0,
557,558,Traveling Wave Fault Location for Power Cables Based on Wavelet Transform,"In this paper, traveling wave fault location equipment for power cables is designed, and the characteristic waveforms of cable fault point broken down and not broken down are simulated respectively. Then a new traveling wave fault location method based on wavelet transform is presented. Wavelet transform have good performance in denoising and singularity detection, which well solved the difficulty in identifying the initial point of the reflected traveling wave because of the local time-frequency characteristic. The fault distance can be calculated by the round-trip times which the traveling wave spends in the cable. The only required parameter is the length of cable. With the method, the result of fault location is not influenced by the change of propagation velocity of traveling wave. The correctness and effectiveness of this method are analyzed by computer simulation. The obtained results show an acceptable degree of accuracy for fault location.",2007,0,
558,559,"A note on inconsistent axioms in Rushby's ""systematic formal verification for fault-tolerant time-triggered algorithms""",We describe some inconsistencies in John Rushby's axiomatization of time-triggered algorithms that he presented in these transactions and that he formally specifies and verifies in the mechanical theorem-prover PVS. We present corrections for these inconsistencies that have been checked for consistency in PVS,2006,0,
559,560,Fault Tolerant Service Composition in Service Overlay Networks,"In a service overlay network, the services provided by different service providers might span multiple Internet domains. A service provider failure may cause significant performance deterioration. Thus, it is desirable to provide fault tolerant service composition solutions such that the service composition can be switched to the backup service composition solution in case of a service provider failure. To provide 100% protection against a single service provider failure, fault tolerant service composition essentially requires to partition service providers into two disjoint sets, each of them can provide a service composition solution. We study a generalized fault tolerant service composition which aims to find two service composition solutions for each request to minimize the number of shared service providers. Subject to such a primary objective, we also aim to minimize the total service composition cost. We firstly prove that the problem is NP-Complete, and formulate the problem as an integer linear program. We then propose heuristic algorithms to efficiently solve the problem. Simulation results demonstrate the effectiveness of the proposed heuristic algorithms.",2008,0,
560,561,Application of Taguchi technique to reduce positional error in two degree of freedom rotary-rotary planar robotic arm,"In present work, positional accuracy of robotic arm has been discussed. The factors considered in the experiment were the length of links, the mass of both links, the velocity of end point and torque on both links. A considerable reduction in performance variation can be obtained by Taguchi technique. Through simple multifactorial experiments on manipulator, controlled factors can be isolated to provide centering and variance control for a process variable. The primary objective in present work is to investigate the effect of process parameter on performance variation to improve positional accuracy. An attempt has been made to introduce a small variation to current approaches broadly called Taguchi parametric design method. In these methods, there are two broad categories of problems associated with simultaneously minimizing performance variations and bringing the mean on target, viz. Type 1- minimizing variations in performance caused by variations in noise factors (uncontrolled parameters); Type 2-minimizing variations in performance caused by variations in control factors (design variables).",2007,0,
561,562,Fault Tolerant Permanent Magnet Motor Drive Topologies for Automotive X-By-Wire Systems,"Future automobiles will be equipped with by-wire systems to improve reliability, safety and performance. The fault tolerant capability of these systems is crucial due to their safety critical nature. Three fault tolerant inverter topologies for permanent magnet brushless dc motor drives suitable for automotive x-by-wire systems are analyzed. A figure of merit taking into account both cost and post-fault performance is developed for these drives. Simulation results of the two most promising topologies for various inverter faults are presented. The drive topology with the highest post-fault performance and cost effectiveness is built and evaluated experimentally.",2008,0,
562,563,Using memory errors to attack a virtual machine,"We present an experimental study showing that soft memory errors can lead to serious security vulnerabilities in Java and .NET virtual machines, or in any system that relies on type-checking of untrusted programs as a protection mechanism. Our attack works by sending to the JVM a Java program that is designed so that almost any memory error in its address space will allow it to take control of the JVM. All conventional Java and .NET virtual machines are vulnerable to this attack. The technique of the attack is broadly applicable against other language-based security schemes such as proof-carrying code. We measured the attack on two commercial Java virtual machines: Sun's and IBM's. We show that a single-bit error in the Java program's data space can be exploited to execute arbitrary code with a probability of about 70%, and multiple-bit errors with a lower probability. Our attack is particularly relevant against smart cards or tamper-resistant computers, where the user has physical access (to the outside of the computer) and can use various means to induce faults; we have successfully used heat. Fortunately, there are some straightforward defenses against this attack.",2003,0,
563,564,Error propagation of the robotic system for liver cancer coagulation therapy,"The goal of this paper is to establish the error propagation model of the ultrasound-guided robot for liver cancer coagulation therapy, which consists of ultrasound machine, image-guided software subsystem, position tracking unit and needle-driven robot. The target of tumor is transformed to robot coordinate frame to let the robot move to the target. The transformation includes three dimension ultrasound construction, registration between pre-operative model and intra-operative physical body, coordinate transformation from position tracking unit to robot. The factors affecting the system accuracy can be expressed by the sum of target mapping error and robot positioning error. Then, the propagation model of target mapping error on the Euclidean motion group is established. At last, the simulations of the propagation model of target mapping error and the experiment of the system accuracy are carried out and the results show our proposed error propagation model is efficient and the system accuracy can satisfy the need of coagulation therapy for liver cancer.",2009,0,
564,565,Evaluating the Performance of Adaptive Fault-Tolerant Routing Algorithms for Wormhole-Switched Mesh Interconnect Networks,"One of the fundamental problems in parallel computing is how to efficiently perform routing in a faulty network each component of which fails with some probability. This paper presents a comparative performance study of ten prominent adaptive fault-tolerant routing algorithms in wormhole-switched 2D mesh interconnect networks. These networks carry a routing scheme suggested by Boppana and Chalasani as an instance of a fault-tolerant method. The suggested scheme is widely used in the literature to achieve high adaptivity and support inter-processor communications in parallel computer systems due to its ability to preserve both communication performance and fault-tolerant demands in these networks. The performance measures studied are the throughput, average message latency and average usage of virtual channels per node. Results obtained through simulation suggest two classes of presented routing schemes as high performance candidate in most faulty networks.",2007,0,
565,566,Analysis of Single-Phase-to-Ground Fault Generated Initial Traveling Waves,"Analysis of fault generated traveling waves is the base to implement traveling waves based protection and fault location. However, the structure at fault point is not asymmetrical under single-phase to ground fault condition in multi-phase power system, so that traveling waves analysis method of single circuit can not be applied. The paper at first analyzes initial traveling wave at fault point generated by the fault through resistance, according to superimposed theory and using phase-to-module transformation method, then considers the fault generated traveling waves' characteristics at the relay point. At last, EMTP is implemented to verify the correctness of analysis of single-phase-to-ground fault generated initial traveling waves",2005,0,
566,567,Real-time correction of distortion image based on FPGA,"Correcting infrared camera distortion is necessary in target tracking and object recognition system. The existent FPGA algorithm didn't utilize sufficiently the advantage of the parallel processing and leaded to the results that a great deal of the system resources were consumed and the running speed was slowed down. The paper analyzed the existing problems such as serial structure in other algorithms, proposed a new parallel algorithm and realized it with the lowest resources. The experiments carried on the chip-virtex5 produced by Xilinx company show that the proposed algorithm has a good real-time performance, use less resource than previous structure and realize the correction of distortion on FPGA on line.",2010,0,
567,568,Executable assertions for detecting data errors in embedded control systems,"In order to be able to tolerate the effects of faults, we must first detect the symptoms of faults, i.e. the errors. This paper evaluates the error detection properties of an error detection scheme based on the concept of executable assertions aiming to detect data errors in internal signals. The mechanisms are evaluated using error injection experiments in an embedded control system. The results show that using the mechanisms allows one to obtain a fairly high detection probability for errors in the areas monitored by the mechanisms. The overall detection probability for errors injected to the monitored signals was 74%, and if only errors causing failure are taken into account we have a detection probability of over 99%. When subjecting the target system to random error injections in the memory areas of the application, i.e., not only the monitored signals, the detection probability for errors that cause failure was 81%",2000,0,
568,569,Simultaneous optimization for wind derivatives based on prediction errors,"Wind power energy has been paid much attention recently for various reasons, and the production of electricity with wind energy has been increasing rapidly for a few decades. In this work, we will propose a new type of weather derivatives based on the prediction errors for wind speeds, and estimate their hedge effect on wind power energy businesses. At first, we will investigate the correlation of prediction errors between the power output and the wind speed in a Japanese wind farm. Then we will develop a methodology that will optimally construct a wind derivative based on the prediction errors using nonparametric regressions. A simultaneous optimization technique of the loss and payoff functions for wind derivatives is demonstrated based on the empirical data.",2008,0,
569,570,An empirical study of fault localization for end-user programmers,"End users develop more software than any other group of programmers, using software authoring devices such as e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been little research on finding ways to help these programmers with the dependability of their software. We have been addressing this problem in several ways, one of which includes supporting end-user debugging activities through fault localization techniques. This paper presents the results of an empirical study conducted in an end-user programming environment to examine the impact of two separate factors in fault localization techniques that affect technique effectiveness. Our results shed new insights into fault localization techniques for end-user programmers and the factors that affect them, with significant implications for the evaluation of those techniques.",2005,0,
570,571,A stochastic model of fault introduction and removal during software development,"Two broad categories of human error occur during software development: (1) development errors made during requirements analysis, design, and coding activities; (2) debugging errors made during attempts to remove faults identified during software inspections and dynamic testing. This paper describes a stochastic model that relates the software failure intensity function to development and debugging error occurrence throughout all software life-cycle phases. Software failure intensity is related to development and debugging errors because data on development and debugging errors are available early in the software life-cycle and can be used to create early predictions of software reliability. Software reliability then becomes a variable which can be controlled up front, viz, as early as possible in the software development life-cycle. The model parameters were derived based on data reported in the open literature. A procedure to account for the impact of influencing factors (e.g., experience, schedule pressure) on the parameters of this stochastic model is suggested. This procedure is based on the success likelihood methodology (SLIM). The stochastic model is then used to study the introduction and removal of faults and to calculate the consequent failure intensity value of a small-software developed using a waterfall software development",2001,0,
571,572,Detecting faults in technical indicator computations for financial market analysis,"Many financial trading and charting software packages provide users with technical indicators to analyze and predict price movements in financial markets. Any computation fault in technical indicator may lead to wrong trading decisions and cause substantial financial losses. Testing is a major software engineering activity to detect computation faults in software. However, there are two problems in testing technical indicators in these software packages. Firstly, the indicator values are updated with real-time market data that cannot be generated arbitrarily. Secondly, technical indicators are computed based on a large amount of market data. Thus, it is extremely difficult, if not impossible, to derive the expected indicator values to check the correctness of the computed indicator values. In this paper, we address the above problems by proposing a new testing technique to detect faults in computation of technical indicators. We show that the proposed technique is effective in detecting computation faults in faulty technical indicators on the MetaTrader 4 Client Terminal.",2010,0,
572,573,Dynamic node management and measure estimation in a state-driven fault injector,"The following topics were dealt with: visual querying and data exploration; graphs and hierarchies; taxonomies, frameworks and methodology; document visualization and collaborative visualization; algorithm visualization; and 3D navigation",2000,0,
573,574,Very high-resistance fault on a 525 kV transmission line - Case study,"This paper analyzes a 300 ohm primary ground fault, which is an unusually high value for a 525 kV transmission line in southeastern Brazil. This case study emphasizes the techniques used by the analysts. Considering that the fault impedance was larger than those usually observed in single-phase faults on extra-high-voltage (EHV) lines, this paper discusses the probable cause of the fault and mentions an analysis technique to evaluate such faults. The protective relaying community lacks information regarding the causes and values of fault resistances to ground on high-voltage (HV) and EHV transmission lines. The objectives of this paper are to stimulate research and contribute to the collection of very high-resistance fault information. The analysis techniques are presented using symmetrical components and fault calculations to arrive at fault parameter values that are very close to the ones provided by protective relays. The performance of the line protection is evaluated for the specific fault conditions, with calculation of the observed impedances and currents. The importance of the ground over- current directional protection on a pilot directional comparison scheme is shown. Speculation on the widespread use of differential protection for transmission lines should stimulate discussions of line protection philosophies and applications. The criteria for the resistive reach setting of the quadrilateral ground distance characteristic are presented to show an evolution of past criteria and to open discussion about the setting limits. The conclusions of this paper highlight the importance of present event report analysis techniques regarding fault calculation software and the need for appropriate settings criteria for the resistive ground distance element threshold. This paper also supports the use of ground directional overcurrent protection with a pilot scheme for HV and EHV transmission line protection, while proposing the widespread use of differential functions f- r transmission lines, even for the most extensive cases.",2009,0,
574,575,Circuit-level modeling of soft errors in integrated circuits,"This paper describes the steps necessary to develop a soft-error methodology that can be used at the circuit-simulation level for accurate nominal soft-error prediction. It addresses the role of device simulations, statistical simulation, analytical soft-error rate (SER) model development, and SER-model calibration. The resulting approach is easily automated and generic enough to be applied to any type of circuit for estimation of the nominal SER.",2005,0,
575,576,Development of a technique for calculation of the influence of generator design on power system balanced fault behaviour,"This paper presents the development of a method for quantitatively determining the potential impact that the design of a single generator may have upon the performance of power system under fault conditions. Initially it is illustrated that the impact that a single generator may have on network fault behaviour is limited by the configuration of the existing network to which the new generator is connected. These constraints are then used to develop a quantitative measure of variability in network-wide fault currents and the subsequent voltage disturbances that can be produced under balanced fault conditions by changing the design of a new generator, irrespective of its point of connection. Finally comparisons with the observed variation in network fault behaviour obtained from the simulation in PSS/E of a realistic 600-bus transmission network are used to demonstrate the technique's apparent effectiveness.",2002,0,
576,577,Fast Enhancement of Validation Test Sets for Improving the Stuck-at Fault Coverage of RTL Circuits,"A digital circuit usually comprises a controller and datapath. The time spent for determining a valid controller behavior to detect a fault usually dominates test generation time. A validation test set is used to verify controller behavior and, hence, it activates various controller behaviors. In this paper, we present a novel methodology wherein the controller behaviors exercised by test sequences in a validation test set are reused for detecting faults in the datapath. A heuristic is used to identify controller behaviors that can justify/propagate pre-computed test vectors/responses of datapath register-transfer level (RTL) modules. Such controller behaviors are said to be <i>compatible</i> with the corresponding precomputed test vectors/responses. The heuristic is fairly accurate, resulting in the detection of a majority of stuck-at faults in the datapath RTL modules. Also, since test generation is performed at the RTL and the controller behavior is predetermined, test generation time is reduced. For microprocessors, if the validation test set consists of instruction sequences then the proposed methodology also generates instruction-level test sequences.",2009,0,
577,578,Fault tolerant error coding and detection using reversible gates,"In recent years, reversible logic has emerged as one of the most important approaches for power optimization with its application in low power CMOS, quantum computing and nanotechnology. Low power circuits implemented using reversible logic that provides single error correction - double error detection (SEC-DED) is proposed in this paper. The design is done using a new 4times4 reversible gate called 'HCG' for implementing hamming error coding and detection circuits. A parity preserving HCG (PPHCG) that preserves the input parity at the output bits is used for achieving fault tolerance for the hamming error coding and detection circuits.",2007,0,
578,579,Design of fault simulation training system for a certain tank,"It is necessary to carry out simulation driving training before forces trainees conduct real vehicle driving training for tanks, which can save training funds and raise the level of military modernization. A fault simulation training system for a certain tank is designed and its hardware and software platform is introduced. The dynamics model of tank in linear motion is derived. The simulation shows that the system has good interaction between the trainees on the tank driving platform and the instructors on the ground master platform. The functions of driving simulation and fault exclusion have been realized initially.",2010,0,
579,580,On errors-in-variables regression with arbitrary covariance and its application to optical flow estimation,"Linear inverse problems in computer vision, including motion estimation, shape fitting and image reconstruction, give rise to parameter estimation problems with highly correlated errors in variables. Established total least squares methods estimate the most likely corrections Acirc and bcirc to a given data matrix [A, b] perturbed by additive Gaussian noise, such that there exists a solution y with [A + Acirc, b +bcirc]y = 0. In practice, regression imposes a more restrictive constraint namely the existence of a solution x with [A + Acirc]x = [b + bcirc]. In addition, more complicated correlations arise canonically from the use of linear filters. We, therefore, propose a maximum likelihood estimator for regression in the general case of arbitrary positive definite covariance matrices. We show that Acirc, bcirc and x can be found simultaneously by the unconstrained minimization of a multivariate polynomial which can, in principle, be carried out by means of a Grobner basis. Results for plane fitting and optical flow computation indicate the superiority of the proposed method.",2008,0,
580,581,Vehicle localization in outdoor woodland environments with sensor fault detection,"This paper describes a 2D localization method for a differential drive mobile vehicle on real forested paths. The mobile vehicle is equipped with two rotary encoders, Crossbow's NAV420CA inertial measurement unit (IMU) and a NAVCOM SF-2050M GPS receiver (used in StarFire-DGPS dual mode). Loosely-coupled multisensor fusion and sensor fault detection issues are discussed as well. An extended Kalman filter (EKF) is used for sensor fusion estimation where a GPS noise pre-filter is used to avoid introducing biased GPS data (affected by multi-path). Normalized innovation squared (NIS) tests are performed when a GPS measurement is incorporated to reject GPS data outliers and keep the consistency of the filter. Finally, experimental results show the performance of the localization system compared to a previously measured ground truth.",2008,0,
581,582,The design and implementation of a microcontroller-based single phase on- line uninterrupted power supply with power factor correction,"In this study, the design and implementation of a microcontroller-based single phase on-line UPS (Uninterrupted Power Supply) with PFC (Power Factor Correction) were made practically. SP-320-24 SMPS (Switch Mode Power Supply) module was used to correct the input power factor. Input power factor value was held at the desired value in uninterrupted power supply topologies. In the realized system, two PIC16F876 were used as microcontroller. One of them was used to generate sinusoidal PWM (Pulse Width Modulation) signals that are used to drive n-channel MOSFETs in push pull inverter and to assure feedback control. Other one was used to control and display units. Harmonics were eliminated and output filter was simplified by using sinusoidal PWM technology.",2009,0,
582,583,Fault-recovery Non-FPGA-based Adaptable Computing System Design,"Reconfigurability with fault-tolerance is one of the most desirable hardware combinations for space computing systems. This paper introduces an adaptable computing architecture that includes random and delay- fault recovering capability for avionics and space applications. A micro-architecture level fault handling and recovering scheme that can immunize random/delay errors is presented as a means of overcoming the limitations of gate-level fault tolerance. The fault- recovery flexible architecture was developed based on a pure-ASIC-based retargetable computing system. The retargetable system also offers sufficient flexibility without employing programmable devices. This adaptable system reasserts different signal patterns for random/delay faults by rerouting micro-operations of the operation that caused the faults. Different sequences of bit-pattern generated by the retargetable system avoid the same faulty situation in high-speed VLSI circuits, while continuously supporting seamless modification and migration of underlying hardware and software after fabrication of retargetable systems.",2007,0,
583,584,Soft errors in Flash-based FPGAs: Analysis methodologies and first results,"The paper presents the development of three different analysis methodologies in order to evaluate soft errors effects in flash-based FPGAs. They are complementary and can be used in different design stages, from the device characterization up to the design sensitiveness estimation. First results are very promising, proving that such methodologies are valid and open new ways of investigation. In particular, we are going to upgrade the experimental setup in order to support higher frequencies (up to 250 MHz) for further characterizing SEE effects. Moreover, a benchmark circuit should be defined in order to correctly predict the expected number of SETs for real circuits, taking into account other side effects, like broadening and logical masking. We expect that from the analysis results we will able to delight suitable hardening techniques that will undergo to both radiation test and prediction analysis.",2009,0,
584,585,"Reliable 3D surface acquisition, registration and validation using statistical error models","We present a complete data acquisition and processing chain for the reliable inspection of industrial parts considering anisotropic noise. Data acquisition is performed with a stripe projection system that was modeled and calibrated using photogrammetric techniques. Covariance matrices are attached individually to points during 3D coordinate computation. Different datasets are registered using a new multi-view registration technique. In the validation step, the registered datasets are compared with the CAD model to verify that the measured part meets its specification. While previous methods have only considered the geometrical discrepancies between the sensed part and its CAD model, we also consider statistical information to decide whether the differences are significant",2001,0,
585,586,Measuring application error rates for network processors,"Faults in computer systems can occur due to a variety of reasons. In many systems, an error has a binary effect, i.e. the output is either correct or it is incorrect. However, networking applications exhibit different properties. For example, although a portion of the code behaves incorrectly due to a fault, the application can still work correctly. Integrity of a network system is often unchanged during faults. Therefore, measuring the effects of faults on the network processor applications require new measurement metrics to be developed. In this paper, we highlight essential application properties and data structures that can be used to measure the error behavior of network processors. Using these metrics, we study the error behavior of seven representative networking applications under different cache access fault probabilities.",2004,0,
586,587,Study on the Features of Loudspeaker Sound Faults,"In this paper, the short-time Fourier transformation (STFT) is adopted to transform the loudspeaker sound signal. By STFT, the one-dimensional loudspeaker response signal is converted into two-dimensional time-frequency figure. Then, the figure is decomposed into a number of areas according to its harmonics distribution. The peak and mean values of every area are computed. Through observation and calculation, the features of loudspeaker defects are found. According to the experiment, this method is very effective and universal for different types of loudspeakers.",2009,0,
587,588,A Cellular Approach to Fault Detection and Recovery in Wireless Sensor Networks,"In the past few years wireless sensor networks have received a greater interest in application such as disaster management, border protection, combat field reconnaissance and security surveillance. Sensor nodes are expected to operate autonomously in unattended environments and potentially in large numbers. Failures are inevitable in wireless sensor networks due to inhospitable environment and unattended deployment. The data communication and various network operations cause energy depletion in sensor nodes and therefore, it is common for sensor nodes to exhaust its energy completely and stop operating. This may cause connectivity and data loss. Therefore, it is necessary that network failures are detected in advance and appropriate measures are taken to sustain network operation. In this paper we extend our cellular architecture and proposed a new mechanism to sustain network operation in the event of failure cause of energy-drained nodes. In our solution the network is partitioned into a virtual grid of cells to perform fault detection and recovery locally with minimum energy consumption. Specifically, the grid based architecture permits the implementation of fault detection and recovery in a distributed manner and allows the failure report to be forwarded across cells. The proposed failure detection and recovery algorithm has been compared with some existing related work and proven to be more energy efficient.",2009,0,
588,589,Engineering knowledge-based condition analyzers for on-board intelligent fault classification: A case study,"In this paper we describe the design of a knowledge-based condition analyzer that performs on-board intelligent fault classification. The system is designed to be deployed as a prototype on E414 locomotives, a series of downgraded highspeed vehicles that are currently employed in standard passenger service. Our goal is to satisfy the requirements of a development scenario in the Integrail project for a condition analyzer that leverages an ontology-based description of some critical E414 subsystems in order to classify faults considering mission and safety related aspects.",2008,0,
589,590,The Dangers of Failure Masking in Fault-Tolerant Software: Aspects of a Recent In-Flight Upset Event,"On 1 August 2005, a Boeing Company 777-200 aircraft, operating on an international passenger flight from Australia to Malaysia, was involved in a significant upset event while flying on autopilot. The Australian Transport Safety Bureau's investigation into the event discovered that ""an anomaly existed in the component software hierarchy that allowed inputs from a known faulty accelerometer to be processed by the air data inertial reference unit (ADIRU) and used by the primary flight computer, autopilot and other aircraft systems."" This anomaly had existed in original ADIRU software, and had not been detected in the testing and certification process for the unit. This paper describes the software aspects of the incident in detail, and suggests possible implications concerning complex, safety- critical, fault-tolerant software.",2007,0,
590,591,A fault line selection algorithm in non-solidly earthed network based on holospectrum,"The methods of line selection today focus on one target of the signal such as amplitude, frequency or phase. A novel method based on holospectrum algorithm to detect single-phase faults in distribution systems is proposed in this paper. After structuring analytic signals of zero sequence current and voltage, the holospectrum algorithm is applied. Thus the analysis of combined signal of amplitude, frequency and phase is realized. Compared with the use of single amplitude, frequency or phase, combined signal carries more details and information of transient signal. Theoretical analysis and simulation based on Simulink of MATLAB show that the presented method can exactly and effectively choose the faulty line in single-phase-to-ground fault.",2010,0,
591,592,Design of Integrated Fault Diagnostic System (FDS),"Early diagnosis of plant faults/deviations is a critical factor for optimized and safe plant operation. Although smart controllers and diagnosis systems are available and widely used in chemical plants, however, some faults couldn't be detected. Major reason is the lack of learning techniques that can learn from operational running data and previous abnormal cases. In addition, operator and maintenance engineer opinions and observations are not well used, and useful diagnosis knowledge is ignored. Providing link between operation management, maintenance management and fault diagnostic and monitoring systems will enable closing such gap where diagnostic and monitoring results can be used more effectively for real time operation support, and optimized plant maintenance. In addition, operation and maintenance findings and discovered knowledge can be used effectively for plant condition monitoring. This research work presents the framework and mechanism for such integrated fault diagnostic system, which is called FDS. The proposed idea will support operation and maintenance planning as well as overall plant safety",2006,0,
592,593,Develop on feed-forward real time compensation control system for movement error in CNC machining,"A theory model of feed-forward compensation controlling system is constructed by the method of precision compensation. A feed-forward compensation hardware control system is designed to MCS51CPU as the core and structure of compensation data processing program. Established components of linear contour error mathematical model, thus determine the amount of feed-forward compensation algorithm. CNC x-y experiment platform simulation results indicate that this design can effectively eliminate the phase lag and amplitude errors of the computer numerical control (CNC) system, and improve the general CNC machining accuracy on the part contour.",2010,0,
593,594,Non-inductive variable reactor design and computer simulation of rectifier type superconducting fault current limiter,"A rectifier type superconducting fault current limiter with noninductive reactor has been proposed by the authors. The concept behind this SFCL is that the high impedance generated during superconducting to normal state of the trigger coil limits the fault current. In the hybrid bridge circuit of the SFCL, two superconducting coils: a trigger coil and a limiting coil are connected in anti-parallel. Both the coils are magnetically coupled with each other and could have the same value of self inductance so that they can share the line current equally. At fault time when the trigger coil current reaches a certain level, the trigger coil changes from superconducting state to normal state. This super to normal transition of the trigger coil changes the current ratio of the coils and therefore the flux inside the reactor is no longer zero. So, the equivalent impedance of both the coils is increased and limits the fault current. We have carried out computer simulation using PSCAD/EMTDC and observed the results. Both the simulation and preliminary experiment shows good results. The advantage of using hybrid bridge circuit is that the SFCL can also be used as circuit breaker.",2005,0,
594,595,An FMO based error resilience method in H.264/AVC and its UEP application in DVB-H link layer,"Flexible Macroblock Ordering (FMO) is one of the new error resilience tools introduced in H264/AVC. Several slice grouping methods have been studied for improving error robustness using FMO. In this paper, a simple and fast slice grouping method for inter frames is introduced. Fast mode decision and early Skip Mode decision are applied for the first encoding pass, and only the features that are available at the stage of early Skip Mode decision are used for the classification. The computation time cost can be reduced by about 50% on average compared to traditional methods. The proposed scheme is tested under the proposed Unequal Error Protection scheme at the DVB-H link layer. The results are compared to the standard MPE-FEC EEP scheme using traditional FMO type `interleaved' at the DVB-H link layer. It is shown that the proposed scheme can provide improved error robustness for high error rate channels in a DVB-H system.",2010,0,
595,596,A PH complex control system built-in correction factor,"Besides the pH deployment process's non-linear, large hysteretic nature, the system's requirement of real-time and accuracy, the traditional control methods can not get to the high quality control results. The fuzzy control does not rely on a mathematical model of the object. It is very difficult to eliminate the steady-state deviation from the root. Because PI control has a very good scavenging effect of the steady-state, therefore the system uses a built-correction factor of the Fuzzy-PI composite control strategy.",2010,0,
596,597,Methodology to support laser-localized soft defects on analog and mixed-mode advanced ICs,"The soft defect localization on analog or mixed-mode ICs is becoming more and more challenging due to their increasing complexity and integration. New techniques based on dynamic laser stimulation are promising for analog and mixedmode ICs. Unfortunately, the considerable intrinsic sensitivity of this kind of devices under laser stimulation makes the defect localization results complex to analyze. As a matter of fact, the laser sensitivity mapping contains not only abnormal sensitive regions but also naturally sensitive ones. In order to overcome this issue by extracting the abnormal spots and therefore localize the defect, we propose in this paper a methodology that can improve the FA efficiency and accuracy. It consists on combining the mapping results with the electrical simulation of laser stimulation impact on the device. First, we will present the concept of the methodology. Then, we will show one case study on a mixed-mode IC illustrating the soft defect localization by using laser mapping technique & standard electrical simulations. Furthermore, we will argument the interest of a new methodology and we will show two simple examples from our experiments to validate it.",2009,0,
597,598,Empirical evaluation of the fault-detection effectiveness of smoke regression test cases for GUI-based software,"Daily builds and smoke regression tests have become popular quality assurance mechanisms to detect defects early during software development and maintenance. In previous work, we addressed a major weakness of current smoke regression testing techniques, i.e., their lack of ability to automatically (re)test graphical user interface (GUI) event interactions - we presented a GUI smoke regression testing process called daily automated regression tester (DART). We have deployed DART and have found several interesting characteristics of GUI smoke tests that we empirically demonstrate in this paper. We also combine smoke tests with different types of test oracles and present guidelines for practitioners to help them generate and execute the most effective combinations of test-case length and test oracle complexity. Our experimental subjects consist of four GUI-based applications. We generate 5000-8000 smoke tests (enough to be run in one night) for each application. Our results show that: (1) short GUI smoke tests with certain test oracles are effective at detecting a large number of faults; (2) there are classes of faults that our smoke test cannot detect; (3) short smoke tests execute a large percentage of code; and (4) the entire smoke testing process is feasible to do in terms of execution time and storage space.",2004,0,
598,599,Automatically translating dynamic fault trees into dynamic Bayesian networks by means of a software tool,"This paper presents a software tool allowing the automatic analysis of a dynamic fault tree (DFT) exploiting its conversion to a dynamic Bayesian network (DBN). First, the architecture of the tool is described, together with the rules implemented in the tool, to convert dynamic gates in DBNs. Then, the tool is tested on a case of system: its DFT model and the corresponding DBN are provided and analyzed by means of the tool. The obtained unreliability results are compared with those returned by other tools, in order to verify their correctness.",2006,0,
599,600,Average Error Performance of M-ary Modulation Schemes in Nakagami-q (Hoyt) Fading Channels,"Presented are exact-form expressions for the average error performance of various coherent, differentially coherent, and noncoherent modulation schemes in Nakagami-q (Hoyt) fading channels. The expressions are given in terms of the Lauricella hypergeometric function, F<sub>D</sub> <sup>(n)</sup>; for nges1, which can be evaluated numerically using its integral or converging series representation. It is shown that the derived expressions reduce to some existing results for Rayleigh fading as special cases",2007,0,
600,601,Analysis of the soft error effects on CAN network controller,"In this article, the effects of the single event upset on a Controller Area Network (CAN) controller and its effects on the network is being evaluated. The experiment is done using SINJECT fault injection tool in a simulation based environment. Three mail modules of the controller are used in three independent set of experiments in one of the CAN controllers of the network. The results show that the main cause of the network failure is the bit stream processor. 6.7% of the injected faults in the bit stream processor led to the network failure. On the other hand, the registers sub-module of the controller showed to be most fault tolerant. The experiment showed that 0.3% of the faults in the registers module results in network failure, and the bit timing module is responsible for the failure of the whole network in 3.2% of the injected single event upset faults.",2010,0,
601,602,Modeling of cable fault system,"Modeling is the essential part of implementing the prediction and location of three-phase cable fault. To predict and locate cable fault, a model of three-phase cable fault system is constructed based on a great deal of measured validation data by choosing BP neural network that has nonlinear characteristic and using the unproved BP algorithm, Levenberg-Marquardt data-optimized method. It is shown by the simulation using MATLAB software that the parameters of the model converge rapidly, and the simulated output of the neural network model and the measured output of cable fault system are approximately equal, and the mean value of the relatively predictive error of the fault distance is smaller than 0.3%, so that the model quality is reliable.",2004,0,
602,603,Asymmetries in soft-error rates in a large cluster system,"Early in the deployment of the ASC Q cluster supercomputer system, an unexpectedly high rate of soft errors were observed in the board-level cache subsystems of the constituent AlphaServer ES45 systems that make up the compute component of this large cluster. A series of tests and experiments was undertaken to validate the hypothesis that this frequency was consistent with the high level of terrestrial secondary cosmic-ray neutron flux resulting from the high elevation of its installation site. The overall success of this effort is reported elsewhere in this issue. This paper reports on three secondary phenomena that were observed during these tests and experiments: Error logs were collected from all servers during a representative period and examined for nonrandom event rates, which would indicate a systematic cause. The only significant result of this exploration was the discovery of a latent soft-error discovery effect, and a self-shielding effect, whereby the servers positioned physically higher in their racks suffered disproportionately higher soft-error rates. This excess was examined and found to be consistent with established shielding effect of the high-Z composition of the constituents of the overlying systems. Experiments with individual ES45 systems in an artificial neutron beam at the Los Alamos Neutron Science Center facility have established that the soft-error rates observed in the SRAM parts is significantly dependent on the incident direction of the neutrons in the beam. These asymmetries could be exploited as part of a strategy for mitigating the frequency of soft errors in future computer systems.",2005,0,
603,604,Sandra - A New Concept for Management of Fault Isolation in Aircraft Systems,"The embedded Fault Isolation functionality in the Saab JAS39 Gripen aircraft has been designed to accurately and reliably provide the technician with proposed maintenance procedures. A previously identified drawback and built in limitation has been the significant lead time for Fault Isolation functional changes based on aircraft operational statistics and line experience. With the Fault Isolation executing as compiled source code, changes and corrections require adaptation of the regular onboard systems computer software and careful planning of code and documentation releases, implying not only significant delays, but also high costs for necessary updates. The ""Sandra"" project aims at even further refine - and to introduce a state of the art - fault isolation maintenance concept for the Saab JAS39 Gripen aircraft. Based on an easy-to-use PC based graphical tool, Fault Isolation on dedicated aircraft monitoring and safety check result data is specified. Output in the form of design documentation artifacts, such as flowcharts and technical publications, is generated. The contained Fault Isolation object data is updated in parallel with the regular onboard computer software development process and the corresponding Loadable Data File will be delivered when convenient. The PC application constitutes the maintenance engineer's primary Fault Isolation design tool. The tool enables the maintenance engineer to select dedicated settings via a graphical user interface and use logical expressions to propose detailed and specific maintenance actions to be performed by the aircraft technician. The tool is capable of verifying a complete set of design documents towards the content of a generated loadable file. Thus, a generated output file with a minimum of additional verification can be delivered to be loaded into the aircraft. This new approach implies that the lead time for a Fault Isolation functional change can be reduced by as much as 80 %. The cost for the corresponding- functional change will decrease by more than 50 %.",2007,0,
604,605,Development of simulation model based on directed fault propagation graph,"A new method of simulation model is presented in this paper in order to deal with system based fault mode and effect analysis model in modern complex system with large structure. Directed fault propagation graph model based on fault influence degree is proposed and fault propagation model is put forward. With the definition of direct fault propagation influence degree and indirect fault propagation influence degree is introduced, the algorithm of propagation and search method for fault propagation model is discussed. Visualization simulation system based on directed fault propagation graph is developed with object oriented method according to the proposed fault analysis model. The Simulation system can used for fault propagation analysis and fault influence of exist complex system, simulation result can be validated and verified by control area network platform, the method is useful for fault diagnosis and analysis model in modern large complex system.",2010,0,
605,606,Research on Analyse and Compensation Approach Aimed at CNC Machine Geometrical and Kinematic Errors,"The geometrical error and kinematic error are regarded as the prime reasons to bring about CNC machine contour error, which confine the improvement of machining precision further. In the paper, the main error sources that produce geometrical error and kinematic error in CNC machine are researched in detail, and an analyse approach aimed at the main error sources is put forward which adopts the ""arc interpolation motion - arc image method"". Furthermore, the compensation approach aimed at error resources such as the mismatch of position loop servo gains and orthogonal axes out of the vertical is developed in CNC software. Finally, the analyse and compensation approach is tested on a CNC experiment table. The experimentation result reveals that the developed analyse and compensation approach aimed at the main error sources can enhance machine contour precision greatly. Consequently, the research is helpful to improve and keep CNC machine high precision long-time.",2009,0,
606,607,Hybrid Error Concealment with Automatic Error Detection for Transmitted MPEG-2 Video Streams over Wireless Communication Network,"This work presents a complete error concealment system, for overcoming visible distortions in video sequences which are transmitted over a lossy communication network. The system we propose provides an error concealment solution from the point of receiving the transmitted sequence by the decoder, until it is presented to viewers, without human interference. The system is composed of an automatic error detection algorithm, and a decision tree error concealment algorithm. The performance of the detection algorithm is estimated, along with a performance evaluation of the decision tree algorithm by comparing it to the other three error concealment methods. The results are evaluated using two quality measures. We show that our error concealment method achieves the highest quality compared to the other methods for most of the conducted tests.",2006,0,
607,608,On the Distribution of Software Faults,"The Pareto principle is often used to describe how faults in large software systems are distributed over modules. A recent paper by Andersson and Runeson again confirmed the Pareto principle of fault distribution. In this paper, we show that the distribution of software faults can be more precisely described as the Weibull distribution.",2008,0,
608,609,Accelerating learning from experience: avoiding defects faster,"All programmers learn from experience. A few are rather fast at it and learn to avoid repeating mistakes after once or twice. Others are slower and repeat mistakes hundreds of times. Most programmers' behavior falls somewhere in between: They reliably learn from their mistakes, but the process is slow and tedious. The probability of making a structurally similar mistake again decreases slightly during each of some dozen repetitions. Because of this a programmer often takes years to learn a certain rule-positive or negative-about his or her behavior. As a result, programmers might turn to the personal software process (PSP) to help decrease mistakes. We show how to accelerate this process of learning from mistakes for an individual programmer, no matter whether learning is currently fast, slow, or very slow, through defect logging and defect data analysis (DLDA) techniques",2001,0,
609,610,Towards high-precision lens distortion correction,"This paper points out and attempts to remedy a serious discrepancy in results obtained by global calibration methods: The re-projection error can be rendered very small by these methods, but we show that the optical distortion correction is far less accurate. This discrepancy can only be explained by internal error compensations in the global methods that leave undetected the inadequacy of the distortion model. This fact led us to design a model-free distortion correction method where the distortion can be any image domain diffeomorphism. The obtained precision compares favorably to the distortion given by state of the art global calibration and reaches a RMSE of 0.08 pixels. Nonetheless, we also show that this accuracy can still be improved.",2010,0,
610,611,Research on code pattern automata-based code error pattern automatic detection technique,"Nowadays, many defects, e.g., obscure error generation-scenario and lacking of formalization which is the basis for the automatic error detection, exist in field of code error research. Furthermore, the automation of error detection will greatly affect the quality and efficiency of software testing. Therefore, more deeply research on code errors need to be done. At first, this paper presents the definition of code error pattern based on definition of pattern. Secondly, it investigates the formalization description of code error pattern. Then, it studies the automatic error pattern detecting technique based on non-determinate finite state automata and treats the matching technique of error pattern as the key problem. Finally, some case studies are given. The preliminary results show the rationality of code error pattern definition and the effectiveness of error pattern formalization description and error pattern matching technique.",2009,0,
611,612,Multichamber Tunable Liquid Microlenses with Active Aberration Correction,"A design approach and new manufacturing technique for a novel type of stacked fluidic multi-chamber tunable lenses is presented. The design offers flexibility and extensibility, leading to fully functional miniature tunable optical lens systems with the ability for low order aberration control.",2009,0,
612,613,A New Fault-Information Model for Adaptive & Minimal Routing in 3-D Meshes,"In this paper, we rewrite the minimal-connected-component (MCC) model in 2-D meshes in a fully-distributed manner without using global information so that not only can the existence of a Manhattan-distance-path be ensured at the source, but also such a path can be formed by routing-decisions made at intermediate nodes along the path. We propose the MCC model in 3-D meshes, and extend the corresponding routing in 2-D meshes to 3-D meshes. We consider the positions of source & destination when the new faulty components are constructed. Specifically, all faulty nodes will be contained in some disjoint fault-components, and a healthy node will be included in a faulty component only if using it in the routing will definitely cause a non-minimal routing-path. A distributed process is provided to collect & distribute MCC information to a limited number of nodes along so-called boundaries. Moreover, a sufficient & necessary condition is provided for the existence of a Manhattan-distance-path in the presence of our faulty components. As a result, only the routing having a Manhattan-distance-path will be activated at the source, and its success can be guaranteed by using the information of boundary in routing-decisions at the intermediate nodes. The results of our Monte-Carlo-estimate show substantial improvement of the new fault-information model in the percentage of successful Manhattan-routing conducted in 3-D meshes.",2008,0,
613,614,Current fault management trends in NASA's planetary spacecraft,"Fault management for today's space missions is a complex problem, going well beyond the typical safing requirements of simpler missions. Recent missions have experienced technical issues late in the project lifecycle, associated with the development and test of fault management capabilities, resulting in both project schedule delays and cost overruns. Symptoms seem to become exaggerated in the context of deep space and planetary missions, most likely due to the need for increased autonomy and the limited communications opportunities with Earth-bound operators. These issues are expected to cause increasing challenges as the spacecraft envisioned for future missions become more capable and complex. In recognition of the importance of addressing this problem, the Discovery and New Frontiers Program Office hosted a Fault Management Workshop on behalf of NASA's Science Mission Directorate, Planetary Science Division, to bring together experts in fault management from across NASA, DoD, industry and academia. The scope of the workshop was focused on deep space and planetary robotic missions, with full recognition of the relevance of, and subsequent benefit to, Earth-orbiting missions. Three workshop breakout sessions focused the discussions to target three topics: 1) fault management architectures, 2) fault management verification and validation, and 3) fault management development practices, processes and tools. The key product of this three-day workshop is a NASA White Paper that documents lessons learned from previous missions, recommended best practices, and future opportunities for investments in the fault management domain. This paper summarizes the findings and recommendations that are captured in the white paper.",2009,0,
614,615,Extraction of Tectonic Faults of Longmen Mountain Based on DEM,"According to the analysis of the tectonic characteristics of thrust belt in the Longmen Mountain, the present study aims to build a methodology to extract liner fault structures in the study area. The methodology is an approach which includes automatic extraction of major faults based on combined calculation of landform factors from the SRTM-DEM and revision of the automatic extraction result according to remote sensing images and geologic data. Therein, these landform factors including elevation, slope, aspect and variation of aspect, slope of slope (SOS) and slope of aspect (SOA). The compound method, including the spatial analysis techniques based on SRTM-DEM, interpretation of remote sensing images, and some geosciences' researches, provides strong technical support to achieve the quantization of the morphotectonics research.",2009,0,
615,616,"A Comparison of Cascading Horizontal and Vertical Menus with Overlapping and Traditional Designs in Terms of Effectiveness, Error Rate and user Satisfaction","In this study, effectiveness, efficiency and user satisfaction of different menu designs were investigated. 24 graduate students voluntarily participated to the study. The results indicate that horizontal menus are more effective than vertical menus in terms of selecting sub menu items, overall task completion time is not related to menu design, horizontal overlapping menu design is the most effective one in terms of preventing user errors. Lastly, user satisfaction doesn't vary according to menu designs.",2007,0,
616,617,A new method in reducing the overcurrent protection response times at high fault currents to protect equipment from extended stress,"This paper describes a new method for protecting power equipment from extended stresses during high fault current conditions. This was achieved using a universal protection device with a software platform that can facilitate designing time-current characteristic (TCC) curves of different shapes, all in the same hardware. When combined, recloser control and relay response times are faster and more accurate than with conventional means. Reduced device response times are achieved by combining different overcurrent TCCs. A coordination example is presented for a typical distribution system loop scheme containing new multifunction relays and reclosers. The advantages of using integrated device functions over standard overcurrent relays and recloser controls are illustrated. A comparative analysis is presented to quantify the reduction in let-thru I<sup>2</sup>t values and equipment stress that can be realized using this method during high fault current conditions",2001,0,
617,618,Fault-tolerance for exascale systems,"Periodic, coordinated, checkpointing to disk is the most prevalent fault tolerance method used in modern large-scale, capability class, high-performance computing (HPC) systems. Previous work has shown that as the system grows in size, the inherent synchronization of coordinated checkpoint/restart (CR) limits application scalability; at large node counts the application spends most of its time checkpointing instead of executing useful work. Furthermore, a single component failure forces an application restart from the last correct checkpoint. Suggested alternatives to coordinated CR include uncoordinated CR with message logging, redundant computation, and RAID-inspired, in-memory distributed checkpointing schemes. Each of these alternatives have differing overheads that are dependent on both the scale and communication characteristics of the application. In this work, using the Structural Simulation Toolkit (SST) simulator, we compare the performance characteristics of each of these resilience methods for a number of HPC application patterns on a number of proposed exascale machines. The result of this work provides valuable guidance on the most efficient resilience methods for exascale systems.",2010,0,
618,619,Detection and correction of abnormal pixels in Hyperion images,"Hyperion images are currently processed to level 1a (from level 0 or raw data). These level 1a images are files of radiometrically corrected data in units of either watts/(sr  micron  m<sup>2</sup>)  40 for VNIR bands or watts/(sr  micron  m<sup>2</sup>)  80 for SWIR bands. Each distributed Hyperion level 1a image tape contains a log file, called ""(EO-1 identifier).fix.log"", that reports the bad or corrupted pixels (called known bad pixels) found during the pre-flight checking, and details how they were fixed. All bad pixels should be corrected in a level 1a image. However, bad pixels are still evident. In addition, there are dark vertical stripes in the image that are not reported in the log file. In this paper, we introduce a method to detect and correct the bad pixels and vertical stripes (we will refer to these occurrences as abnormal pixels). Images from the Greater Victoria Watershed and other EVEOSD test sites are used to determine how stationary the locations of the abnormal pixels are. After abnormal pixel correction a Hyperion image is ready for geometric correction, atmospheric correction, and further analysis.",2002,0,
619,620,Fault diagnosis for transformer based on fuzzy entropy,"Power transformers are one of the key equipments of the power system, so it is valuable to discover the incipient fault timely and truly. Code deficiency exists in the gas ratio method by the IEC/DEEE standard and fault diagnosis for power transformers. A model based on fuzzy entropy for power transformer faults diagnosis is put forward, which expand coding bound of original IEC three-ratio. At the same time, the method has some contain fault ability in a certain degree. It also shows the probability and disposes lost or false power transformer fault symptoms. That shows the validity of the method for power transformer fault diagnosis by dissolved gas-in-oil analysis.",2007,0,
620,621,Enhanced Fault Ride-Through Method for Wind Farms Connected to the Grid Through VSC-Based HVDC Transmission,"This paper describes a new control approach for secure fault-ride through of wind farms connected to the grid through a voltage source converter-based high voltage DC transmission. On fault occurrence in the high voltage grid, the proposed control initiates a controlled voltage drop in the wind farm grid to achieve a fast power reduction. In this way overvoltages in the DC transmission link can be avoided. It uses controlled demagnetization to achieve a fast voltage reduction without producing the typical generator short circuit currents and the related electrical and mechanical stress to the wind turbines and the converter. The method is compared to other recent FRT methods for HVDC systems and its superior performance is demonstrated by simulation results.",2009,0,
621,622,Error Resilient Video Coding Using B Pictures in H.264,"Since the quality of compressed video is vulnerable to errors, video transmission over unreliable Internet is very challenging today. Multi-hypothesis motion-compensated prediction (MHMCP) has been shown to have error resilience capability for video transmission, where each macroblock is predicted by a linear combination of multiple signals (hypotheses). B picture prediction is a special case of MHMCP. In H.264/AVC, the prediction of B pictures is generalized such that both of the two predictions can be selected from the past pictures or from the subsequent pictures. The multiple reference picture framework in H.264/AVC also allows previously decoded B pictures to be used as references for B picture coding. In this paper, we will discuss the error resilience characteristics of the generalized B pictures in H.264/AVC. Three prediction patterns of B pictures are analyzed in terms of their error-suppressing abilities. Both theoretical models (picture level error propagation) and simulation results are given for the comparison.",2009,0,
622,623,Managing Post-Development Fault Removal,"In this paper, we manage fault removal by classifying and prioritizing fault warnings reported by a static analysis tool. We present our findings from analyzing three cross-platform industrial code bases at Yahoo! totaling approximately 3.6+ MLOC. The tool found 1.2K potential fault warnings as follows: 52.29% true faults and 47.71% false/noise. The 52.29% correctly reported faults were prioritized based on severity. Additionally, we connected the tool classification to a standard software weakness schema, Common Weakness Enumeration (CWE) to standardized discourse. The results from creating a management system for post-development fault removal are intended to be shifted back into earlier stages of software development.",2009,0,
623,624,A Probabilistic Method for Aligning and Merging Range Images with Anisotropic Error Distribution,"This paper describes a probabilistic method of aligning and merging range images. We formulate these issues as problems of estimating the maximum likelihood. By examining the error distribution of a range finder, we model it as a normal distribution along the line of sight. To align range images, our method estimates the parameters based on the expectation maximization (EM) approach. By assuming the error model, the algorithm is implemented as an extension of the iterative closest point (ICP) method. For merging range images, our method computes the signed distances by finding the distances of maximum likelihood. Since our proposed method uses multiple correspondences for each vertex of the range images, errors after aligning and merging range images are less than those of earlier methods that use one-to-one correspondences. Finally, we tested and validated the efficiency of our method by simulation and on real range images.",2006,0,
624,625,New resonance type Fault Current Limiter,"This paper proposes a new parallel LC resonance type Fault Current Limiter (FCL). This structure has low cast because of using dry capacitor and non-superconducting inductor and fast operation. The proposed FCL is able to limit fault current in constant value near to pre-fault condition value against series resonance type FCL. In this way, the voltage of point of common coupling (PCC) will not change during fault. Analytical analysis is presented in detail and simulation results are involved to validate the effectiveness of this structure.",2010,0,
625,626,ConvexFit: an optimal minimum-error convex fitting and smoothing algorithm with application to gate-sizing,"Convex optimization has gained popularity due to its capability to reach global optimum in a reasonable amount of time. Convexity is often ensured by fitting the table data into analytically convex forms such as posynomials. However, fitting the look-up tables into the posynomial forms with minimum error itself may not be a convex optimization problem and hence excessive fitting errors may be introduced. In this paper, we propose to directly adjust the look-up table values into a numerically convex look-up table without explicit analytical form. We show that numerically ""convexifying"" the table data with minimum perturbation can be formulated as a convex semidefinite optimization problem and hence optimality can be reached in polynomial time. Without an explicit form limitation, we find that the fitting error is significantly reduced while the convexity is still ensured. As a result, convex optimization algorithms can still be applied. Furthermore, we also develop a ""smoothing"" algorithm to make the table data smooth and convex to facilitate the optimization process. Results from extensive experiments on industrial cell libraries demonstrate that our method reduces 30 fitting error over a well-developed posynomial fitting algorithm. Its application to circuit tuning is also presented.",2005,0,
626,627,Stereoscopic video error concealment for missing frame recovery using disparity-based frame difference projection,"At low bit-rate video communications, packet loss may easily cause whole-frame loss that, in return, leads to annoying frame drop phenomenon. In this paper, a novel error concealment algorithm is specifically developed for stereoscopic video, called the disparity-based frame difference projection (DFDP), to recover the lost frames at the decoder. The proposed DFDP contains three key components: 1) change detection; 2) disparity estimation; and 3) frame difference projection, which exploits both the intra-view frame difference from one view and interview correlation to estimate the lost frame in another view. The change region computed on the correctly received frame will be used to predict the change region between current missing frame and its previous frame through the estimated disparity, which is the summation of the estimated global disparity and the estimated local disparity. Experimental results have shown that the proposed stereoscopic video error concealment method can effectively restore the lost frames at the decoder and deliver attractive performance, in terms of objective measurement (in peak signal-to-noise ratio) and subjective visual quality.",2009,0,
627,628,Fault-tolerant static scheduling for grids,"While fault-tolerance is desirable for grid applications because of the distributed and dynamic nature of grid resources, it has seldom been considered in static scheduling. We present a fault-tolerant static scheduler for grid applications that uses task duplication and combines the advantages of static scheduling, namely no overhead for the fault-free case, and of dynamic scheduling, namely low overhead in case of a fault. We also give preliminary experimental results on our scheme.",2008,0,
628,629,Charge sharing and interaction depth corrections in a wide energy range for small pixel pitch CZT detectors,"The CSTD project aims at developing a high resolution pixel gamma detector based on CdZnTe for Compton imaging applications. Our research group has been working recently on the design and characterization of a new pixel detector with specifications focused at high energy SPECT for medical imaging applications. The detector pitch, 0.3 mm, and its thickness, 5 mm, allows to reach high spatial resolution and high detector efficiency. Non-ideal performance appears with more strength in small pixel pitch CdZnTe detectors, below 1 mm, affecting at the spectroscopic results. In order to recuperate the shared charge, the customized ASIC simultaneously collects the charge in the triggering pixel and its eight neighboring pixels per event. The detector design, readout electronics, acquisition software and data analysis have been completed at CIEMAT. Data has been taken by irradiating the CdZnTe detector with high and low energy gamma-ray sources. The high energy events of the <sup>137</sup>Cs source suffer from a great proportion of charge sharing in the neighboring pixels. Two <sup>137</sup>Cs spectra, with and without energy correction, are shown and compared. To obtain the corrected spectra offline, the collected charge at the neighboring pixels is added to the trigger pixel collected charge. The corrected spectra show that the 662 keV photopeak is reconstructed. Interaction depth correction follows to improve the energy resolution by data segmentation of the 662 keV energy peak according to fifty cathode to pixel ratios. The computed interaction depth correction profile is the inverse of the charge collection efficiency. Energy resolution can be improved discarding the segmented data which do not achieve an acceptable energy resolution. Several interaction depth correction profiles at 81, 356 and 662 keV are shown and reveal a second correlation between the charge collecting efficiency and the collecting energy.",2010,0,
629,630,Automatic Diagnosis of Defects of Rolling Element Bearings Based on Computational Intelligence Techniques,"This paper presents a method, based on classification techniques, for automatic detection and diagnosis of defects of rolling element bearings. We used vibration signals recorded by four accelerometers on a mechanical device including rolling element bearings: the signals were collected both with all faultless bearings and after substituting one faultless bearing with an artificially damaged one. We considered four defects and, for one of them, three severity levels. In all the experiments performed on the vibration signals represented in the frequency domain we achieved a classification accuracy higher than 99%, thus proving the high sensitivity of our method to different types of defects and to different degrees of fault severity. We also assessed the degree of robustness of our method to noise by analyzing how the classification performance varies on variation of the signal-to-noise ratio and using statistical classifiers and neural networks. We achieved very good levels of robustness.",2009,0,
630,631,Cloud Model-Based Security-Aware and Fault-Tolerant Job Scheduling for Computing Grid,"The uncertainties of grid nodes security are main hurdle to make the job scheduling secure, reliable and fault-tolerant. The fixed fault-tolerant strategy in jobs scheduling may utilize excessive resources. In this paper, the job scheduling decides which kinds of fault-tolerance strategy will be applied to each individual job for more reliable computation and shorter makespan. And we discuss the fuzziness or uncertainties between TL and SD attributes by the subjective judgment of human beings. Cloud model is a model of the uncertain transition between qualitative concept and its quantitative representation. Based on the cloud model, We propose a security-aware and fault-tolerant jobs scheduling strategy for grid (SAFT), which makes the assess of SD and SL to become more flexible and more reliable. Meanwhile, the different fault-tolerant strategy has been applied in grid job scheduling algorithm by the SD and job workload. Moreover, much more important, we are able to set up some rules and active each qualitative rule to select a suitable fault-tolerant strategy for a scheduling job by input value (the SD and job workload) to realize the uncertainty reasoning. The results demonstrate that our algorithm has shorter makespan and more excellent efficiencies on improving the job failure rate than the fixed fault-tolerant strategy selection.",2010,0,
631,632,SWIFT: software implemented fault tolerance,"To improve performance and reduce power, processor designers employ advances that shrink feature sizes, lower voltage levels, reduce noise margins, and increase clock rates. However, these advances make processors more susceptible to transient faults that can affect correctness. While reliable systems typically employ hardware techniques to address soft-errors, software techniques can provide a lower-cost and more flexible alternative. This paper presents a novel, software-only, transient-fault-detection technique, called SWIFT. SWIFT efficiently manages redundancy by reclaiming unused instruction-level resources present during the execution of most programs. SWIFT also provides a high level of protection and performance with an enhanced control-flow checking mechanism. We evaluate an implementation of SWIFT on an Itanium 2 which demonstrates exceptional fault coverage with a reasonable performance cost. Compared to the best known single-threaded approach utilizing an ECC memory system, SWIFT demonstrates a 51% average speedup.",2005,0,
632,633,On-line diagnosis of interconnect faults in FPGA-based systems,"This paper presents an on-line diagnosis approach for locating the interconnect faults in field programmable gate arrays (FPGAs)-based systems. The diagnosis proposed approach consists of two phases. Phase one is locating the faulty tile through partitioning the FPGA-based system into self-checking tiles. The faulty tile can be detected concurrently with the normal system operation. This operation is performed prior to scheduling and allocating the circuit. The proposed partitioning approach was applied on certain circuits as a case study, and has been implemented using Xilinx foundation CAD tool with FPGA chip XC4010. The simulation study proved that our partitioning scheme reduces the test complexity and produces lower overheads. Upon locating a faulty tile and by the aid of a proposed path-list file per tile created during the routing process, the second phase of the diagnosis approach is applied only on the utilized interconnection of that tile for locating the faulty wires and switches. Therefore, the diagnosis approach is considered to be simplified.",2004,0,
633,634,Application of Empirical Bayes Estimation in Error Model Identification of Two Orthometric Accelerometers,"The high accuracy accelerometer can be demarcated in multiposition tumbling experiment under 1g gravitational field. The g<sup>2</sup> observation model of Two Orthometric Accelerometers can eliminate the corner error. there is a serious multicollinearity exit in this system because some model coefficient mix together. In view of above question, this article has given the arithmetic of Empirical Bayes Estimation(EB), and applied this method in model which is mentioned above. The result of the simulation and the experiment shows that compared with the conventional least squares method and the generalized diagonal ridge estimation, the Empirical Bayes Estimation can overcome the influence of the multicollinearity and can separate two coefficients which are The Second-Order Terms and the cross-coupling terms.",2010,0,
634,635,Multi Gigabit Transceiver Configuration RAM Fault Injection Response,"High performance processing and memory systems require enormous amounts of I/O bandwidth. Wide parallel bus architectures have reached their practical limits for high bandwidth transport. High speed serial interfaces that support 10's of Gbps are now displacing wide shared bus architectures for many systems. Xilinx FPGAs serial links support this transition by providing more than 10 Gbps in their multi gigabit transceiver (MGT) I/Os. For space applications, these links are susceptible to single event effects (SEE). Many of these effects are due to upsets in the FPGAs configuration RAM that control the many features and functions of the I/O. This paper details the functional effects of configuration RAM upsets in Xilinx MGTs. These effects are realized by injecting upsets in the FPGA configuration RAM while monitoring MGT functional operation. Configuration RAM upset effects are described and functional upset rates due to configuration RAM upsets are calculated for an example orbit. The results of this work provide insight into the on-orbit upset rate and effects of Xilinx multigigabit transceivers",2005,0,
635,636,Statistical software debugging: From bug predictors to the main causes of failure,"Detecting latent errors is a key challenging issue in the software testing process. Latent errors could be best detected by bug predictors. A bug predictor manifests the effect of a bug on the program execution state. The aim has been to find the smallest reasonable subset of the bug predictors, manifesting all possible bugs within a program. In this paper, a new algorithm for finding the smallest subset of bug predictors is presented. The algorithm, firstly, applies a LASSO method to detect program predicates which have relatively higher effect on the termination status of the program. Then, a ridge regression method is applied to select a subset of the detected predicates as independent representatives of all the program predicates. Program control and data dependency graphs can be best applied to find the causes of bugs represented by the selected bug predictors. Our proposed approach has been evaluated on two well-known test suites. The experimental results demonstrate the effectiveness and accuracy of the proposed approach.",2009,0,
636,637,ERCOT's experience in identifying parameter and topology errors using State Estimator,"The State Estimator is an important tool that ERCOT relies on to monitor the real time state of power grid. As the parameter and topology errors are critical to the quality of state estimator results, operations engineers in ERCOT are using multiple tools to detect and identify the topology and parameter errors in ERCOT EMS network model. This paper will present ERCOT experience in detecting and identifying the topology and parameter errors using state estimator monitoring tools and by analyzing SE results.",2010,0,
637,638,Performance of fault-tolerant distributed shared memory on broadcast- and switch-based architectures,"This paper presents a set of distributed-shared-memory protocols that provide fault tolerance on broadcast-based and switch-based architectures with no decrease in performance. These augmented DSM protocols combine the data duplication required by fault tolerance with the data duplication that naturally results in distributed-shared-memory implementations. The recovery memory at each backup node is continuously maintained consistent and is accessible by all processes executing at the backup node. Simulation results show that the additional data duplication necessary to create fault-tolerant DSM causes no reduction in system performance during normal operation and eliminates most of the overhead at checkpoint creation. Data blocks which are duplicated to maintain the recovery memory are also utilized by the DSM protocol, reducing network traffic, and increasing the processor utilization significantly. We use simulation and multiprocessor address trace files to compare the performance of a broadcast architecture called the SOME-Bus to the performance of two representative switch architectures.",2005,0,
638,639,"Image steganalysis based on moments of characteristic functions using wavelet decomposition, prediction-error image, and neural network","In this paper, a general blind image steganalysis system is proposed, in which the statistical moments of characteristic functions of the prediction-error image, the test image, and their wavelet subbands are selected as features. Artificial neural network is utilized as the classifier. The performance of the proposed steganalysis system is significantly superior to the prior arts.",2005,0,
639,640,Supporting fault tolerance in a data-intensive computing middleware,"Over the last 2-3 years, the importance of data-intensive computing has increasingly been recognized, closely coupled with the emergence and popularity of map-reduce for developing this class of applications. Besides programmability and ease of parallelization, fault tolerance is clearly important for data-intensive applications, because of their long running nature, and because of the potential for using a large number of nodes for processing massive amounts of data. Fault-tolerance has been an important attribute of map-reduce as well in its Hadoop implementation, where it is based on replication of data in the file system. Two important goals in supporting fault-tolerance are low overheads and efficient recovery. With these goals, this paper describes a different approach for enabling data-intensive computing with fault-tolerance. Our approach is based on an API for developing data-intensive computations that is a variation of map-reduce, and it involves an explicit programmer-declared reduction object. We show how more efficient fault-tolerance support can be developed using this API. Particularly, as the reduction object represents the state of the computation on a node, we can periodically cache the reduction object from every node at another location and use it to support failure-recovery. We have extensively evaluated our approach using two data-intensive applications. Our results show that the overheads of our scheme are extremely low, and our system outperforms Hadoop both in absence and presence of failures.",2010,0,
640,641,Enhancing Motion Picture Lens Performance by Digital Calibration and Correction,"To some degree, all lenses used by the motion picture industry exhibit certain distortions which can detract from the ideal viewing experience. This paper presents a lens calibration and correction system which enables these problems to be resolved digitally in post production.  For some time lenses such as the Cook 4i and now 5i provide the necessary metadata on focus and aperture settings to enable digital post corrections to be applied. Cameras such the Alexa and RED are capable of capturing this metadata. Many other lenses however may be adapted to provide the necessary metadata by means of a simple encoder. The paper presents how this is achieved followed by a presentation of the digital correction system for enhancing such scenarios as extreme focus pulls. Using digital high definition camera systems the calibration of each individual lens is presented along with the automatic derivation of the required lens database. The full post production work flow through to final image generation is presented.",2010,0,
641,642,The study of fault diagnosis in rotating machinery,"This project presents a detail review of the subject fault diagnosis; feature extraction, dimensionality reduction and fault classification are being discussed. This project focuses on the faulty bearing which mainly caused by mass imbalance and axis misalignment. By analyzing the vibration signal obtained from the test rigs (rigs that are built to demonstrate the effect of faults in rotating machinery), it gives solid information concerning any faults within the rotating machinery.",2009,0,
642,643,Multiobjective Optimization for HTS Fault-Current Limiters Based on Normalized Simulated Annealing,"This paper presents an improved simulated annealing (SA) algorithm for multiobjective optimization, which is a positive approach in the design of high-temperature superconducting (HTS) fault-current limiters (SFCLs).The main goal of this paper is to achieve an effective and feasible approach in the structural design of HTS FCLs by means of multiobjective decision-making techniques, based on normalized SA. The combination of electrical and thermal models of a purpose-designed resistive-type HTS FCL is defined as a component in PSCAD/EMTDC simulations from which the proposed method will be used to optimize the selective parameters of the SFCL. The above requires the need of advanced numerical techniques for simulation studies by PSCAD on a sample distribution system for determining a global optimum HTS FCL, by considering individual parameters and accounting for the constraints, which is the main motivation for initiating this paper.",2009,0,
643,644,A Multi-agent System for Complex Vehicle Fault Diagnostics and Health Monitoring,"This paper presents a multi-agent system(MAS_VFD&HM) developed for complex vehicle fault diagnosis and health monitoring. The MAS_VFD&HM consists of signal diagnostic agents, special case agents, and a vehicle diagnostic/monitoring agent. A signal agent is responsible for the fault diagnosis or monitoring of one particular signal using either a single signal or multiple signals depending on the complexity of signal faults. Special case agents are those trained to detect specific component faults. All these agents are autonomous and report their results to the Vehicle System Agent. A computational framework is presented for agent learning and agent operation. The proposed MAS_VFD&HM is scalable, versatile, and has the capability of dealing complex problems such as multiple faults in a vehicle system. Although our focus was on the automotive diagnostics, the proposed MAS_VFD&HM is applicable to complex engineering diagnostic problems beyond vehicles.",2010,0,
644,645,Channel capacity and average error rates in generalised-K fading channels,"In the present study, the performance of digital communication systems operating over a composite fading channel modelled by the generalised-<i>K</i> distribution is analysed and evaluated. Novel closed-form expressions for the outage performance, the average bit error probabilities of several modulation schemes and the channel capacity under four different adaptive transmission schemes are derived. The analytical expressions are used to investigate the impact of different fading parameters of this composite fading channel model on the average bit error rate performance for a variety of digital modulation schemes and the spectral efficiency of different adaptive transmission policies.",2010,0,
645,646,Stochastic change detection based on an active fault diagnosis approach,The focus in this paper is on stochastic change detection applied in connection with active fault diagnosis (AFD). An auxiliary input signal is applied in AFD. This signal injection in the system will in general allow to obtain a fast change detection/isolation by considering the output or an error output from the system. The classical CUSUM (cumulative sum) method will be modified such that it will be able to detect change in the signature from the auxiliary input signal in the (error) output signal. It will be shown how it is possible to apply both the gain as well as the phase change of the output vector in the CUSUM test.,2007,0,
646,647,Identification of faulted section in TCSC transmission line based on DC component measurement,"This paper presents an analysis of possibility of detection of a fault position with respect to the compensating bank in a series compensating transmission line. The algorithm designed for this purpose is based on determining the contents of dc components in the distance relay input currents. Fuzzy logic technique is applied for making the decision whether a fault is in front of the compensating bank or behind it. The delivered algorithm has been tested and evaluated with use of the fault data obtained from versatile ATP-EMTP simulations of faults in the test power network containing the 400 kV, 300 km transmission line, compensated with the aid of TCSC (Thyristor Controlled Series Capacitor) bank installed at mid-line. The results of the evaluation are reported and discussed.",2009,0,
647,648,Research on Web-Based Multi-Agent System for Aeroengine Fault Diagnosis,"On the analysis of current state of aeroengine remote diagnosis, collaborative mechanism based on multi-agent was introduced to overcome the obstacles of conventional remote fault diagnosis. The model of aeroengine remote collaborative diagnosis based on multi-agent was put forward on analysis of the positional relationship of all agents in the collaborative environment and the relationship between collaborative agents and roles in the course of collaboration. Some key technologies such as coordination mechanism, task assignment mechanism, agent interaction mechanism, case-based reasoning (CBR) in treatment agent, and the analytic hierarchy process (AHP) in decision analysis were discussed and specific methods of realization were given concretely. Based on these, a Web-based prototype system for aeroengine fault diagnosis was developed on the JADE (Java Agent DEvelopment Framework) platform. The process of system implementation and a case example of fault diagnosis were presented to illustrate and prove the proposed system's applicability. Running results show the feasibility and reliability of the framework, which will be helpful to integrate the aeroengine diagnosis knowledge, improve the diagnosis efficiency effectively and decrease the aeroengine diagnosis cost remarkably.",2008,0,
648,649,A perceptual Sensitivity Based Redundant Slices Coding Scheme for Error-Resilient Transmission H.264/AVC Video,"In this paper, redundant slices feature of the H.264/AVC codec is evaluated. In order to trade off compression efficiency and error robustness of a H.264/AVC codec with redundant slice capability, we propose a novel perceptual sensitivity based redundant slices coding scheme. The perceptually sensitive regions are determined by using a simple yet effective perceptual sensitivity analysis technique, which analyzes both the motion and the texture structures in the original video sequence. The experimental results show that our proposed algorithm can remarkably improve the reconstructed video quality in the packet lossy network",2006,0,
649,650,Application of methods of 3D surface reconstruction for characterization of pitting defects,In this paper a possibility of application of two different methods of pitting visualization is discussed.,2009,0,
650,651,A State Machine for Detecting C/C++ Memory Faults,"Memory faults are major forms of software bugs that severely threaten system availability and security in C/C++ program. Many tools and techniques are available to check memory faults, but few provide systematic full-scale research and quantitative analysis. Furthermore, most of them produce high noise ratio of warning messages that require many human hours to review and eliminate false-positive alarms. And thus, they cannot locate the root causes of memory faults precisely. This paper provides an innovative state machine to check memory faults, which has three main contributions. Firstly, five concise formulas describing memory faults are given to make the mechanism of the state machine simple and flexible. Secondly, the state machine has the ability to locate the cause roots of the memory faults. Finally, a case study applying to an embedded software, which is written in 50 thousand lines of C codes, shows it can provide useful data to evaluate the reliability and quality of software",2005,0,
651,652,A different view of fault prediction,"We investigated a different mode of using the prediction model to identify the files associated with a fixed percentage of the faults. The tester could ask the tool to identify which files are likely to contain the bulks of faults, with the tester selecting any desired percentage of faults. Again the tool would return a list ordered in decreasing order of the predicted numbers of faults in the files the model expects to be most problematic. If the number of files identified is too large, the tester could reselect a smaller percentage of faults. This would make the number of files requiring particular scrutiny manageable. We expect both modes to be valuable to professional software testers and developers.",2005,0,
652,653,Insulation fault detection in a PWM controlled induction motor-experimental design and preliminary results,"To investigate feature extraction methods for early detection of insulation degradation in low voltage (under 600 V), 3-phase, PWM controlled induction motors, a series of seeded fault tests was planned on a 50 HP, 440 V motor. In this paper, the background and rationale for the test plan are described. The instrumentation and test plan are then detailed. Finally, preliminary test experiences are related",2000,0,
653,654,Clinic: A Service Oriented Approach for Fault Tolerance in Wireless Sensor Networks,"With the size and complexity of modern Wireless Sensor Networks (WSNs) systems, a system's ability to recover from faults is becoming more important. A self-healing system is one that has the capability to recover from faults without human intervention during execution. Since WSNs are inherently fault-prone and since their on-site maintenance is infeasible, scalable self-healing is crucial for enabling the deployment of large-scale sensor network applications. Previous work has typically dealt with single faults in isolation, has imposed constraints on systems, or required new protocol elements. In this paper, we attempt to solve some of these problems through the use of service-oriented architecture. We propose a service-oriented self-healing approach, called Clinic, that works with existing network components, e.g. routing protocols, and resources without adding extra overhead on the network. In Clinic, different network capabilities are viewed as services of the network instead of being isolated capabilities of individual nodes. This view of the network promotes collaboration among nodes and information reuse by sharing information collected by one service with other network services. Preliminary evaluation showed that Clinic achieved fault tolerance while keeping low communication overhead by reusing only the information collected by other network services to heal from faults.",2010,0,
654,655,Developing Fault Injection Environment for Complex Experiments,"The paper addresses the problem of creating a comprehensive fault injection environment, which integrates and improves various simulation and supplementary functions. This is illustrated with experimental results.",2008,0,
655,656,Neural network methods for error canceling in human-machine manipulation,"A neural network technique is employed to cancel hand motion error during microsurgery. A cascade-correlation neural network trained via extended Kalman filtering was tested on 15 recordings of hand movement collected from 4 surgeons. The neural network was trained to output the surgeon's desired motion, suppressing erroneous components. In experiments this technique reduced the root mean square error (rmse) of the erroneous motion by an average of 39.5%. This was 9.6% greater than the reduction achieved in earlier work, which followed the complementary approach of estimating the error rather than the desired component. Preliminary results are also presented from tests in which training and testing data were taken from different surgeons.",2001,0,
656,657,Exploration of beam fault scenarios for the Spallation Neutron Source target,"The Spallation Neutron Source (SNS) accelerator systems will provide a 1 GeV, 1.44 MW proton beam to a liquid mercury target for neutron production. In order to ensure adequate lifetime of the target system components, requirements on several beam parameters must be maintained. A series of error studies was performed to explore credible fault scenarios which can potentially violate the various beam-on-target parameters. The response of the beam-on-target parameters to errors associated with the phase-space painting process in the ring and field setpoint errors in all the ring-to-target beam transport line elements were explored and will be presented. The plan for ensuring beam-on-target parameters will also be described.",2003,0,
657,658,On undetectable faults in partial scan circuits,We provide a definition of undetectable faults in partial scan circuits under a test application scheme where a test consists of primary input vectors applied at-speed between scan operations. We also provide sufficient conditions for a fault to be undetectable under this test application scheme. We present experimental results on finite-state machine benchmarks to demonstrate the effectiveness of these conditions in identifying undetectable faults.,2002,0,
658,659,Analytical Modeling Approach to Detect Magnet Defects in Permanent-Magnet Brushless Motors,The paper presents a novel approach to detect magnet faults such as local demagnetization in brushless permanent-magnet motors. We have developed a new form of analytical model that solves the Laplacian/quasi-Poissonian field equations in the machine's air-gap and magnet element regions. We verified the model by using finite-element software in which demagnetization faults were simulated and electromotive force was calculated as a function of rotor position. We then introduced the numerical data of electromotive force into a gradient-based algorithm that uses the analytical model to locate demagnetized regions in the magnet as simulated in the finite-element package. The fast and accurate convergence of the algorithm makes the model useful in magnet fault diagnostics.,2008,0,
659,660,Error-rate analysis for multirate DS-CDMA transmission schemes,"We analyze and compare the error performance of a dual-rate direct-sequence code-division multiple-access (DS-CDMA) system using multicode (MCD) and variable-spreading gain (VSG) transmission in the uplink. Specifically, we present two sets of results. First, we consider an ideal additive white Gaussian noise channel. We show that the bit-error rate (BER) of VSG users is slightly lower than that of MCD users if the number of low-rate interferers is smaller than a specific threshold. Otherwise, they exhibit similar error performance. Second, we look at multipath fading channels. We show that with diversity RAKE reception, the VSG user suffers from a larger interference power than the MCD user if the channel delay spread is small. The reverse is true for a large delay spread. However, a larger interference power in this case does not necessarily lead to higher error probability. Essentially, our results for both cases show that: 1) in addition to the signal-to-interference ratio (SIR), the difference in error performance between the two systems strongly depends on the distributions of multiple-access and multipath interference; 2) for practical cellular communications, performances for both systems are expected to be similar most of the time.",2003,0,
660,661,An AS-DSP for forward error correction applications,"An application specific digital signal processor for channel coding is presented. The vector operations can improve both the performance of memory accesses and program code density. The special function units and datapaths for channel decoding accelerate the decoding speed and facilitate algorithm implementation. The processor had been fabricated in a 0.18 m CMOS 1P6M technology. The chip size is 7.73 mm<sup>2</sup> including 18k bits embedded memory, and the power consumption is 141 mW while decoding Reed-Solomon code and convolutional code. In contrast with general purpose processor designs, the results show this chip has at least 50% improvement in code density and 66% data rate enhancement.",2005,0,
661,662,Compensation of inertia error in brake dynamometer testing,"Loss in terms of windage and bearing friction is an important origin of inertia error to be compensated in brake dynamometer testing, acquisition of which has always been a troublesome problem. An indirect method of loss measurement using speed data under null pipeline pressure is described in this paper. Mathematical model of resistance torque or energy loss is calculated by regression of collected speed data using SPSS software. Error compensation of two inertia simulating methods, torque control method and energy compensation method, is discussed. Experiments of the former are conducted on NT11 brake dynamometer, which proves it to be effective in eliminating inertia error.",2009,0,
662,663,Rate-Distortion Optimal Video Transport Over IP with Bit Errors,"In this paper we propose a method for video delivery over bit error channels. In particular, we propose a rate distortion optimal method for slicing and unequal error protection (UEP) of packets over bit error channels. The proposed method performs full frame based search using a novel dynamic programming approach to determine the optimal slicing configuration in a practically short time. Also we propose a rate and distortion estimation technique that decreases the time to evaluate the objective function for a slice configuration. The proposed method can perform rate-distortion UEP that can be used over forward error correction (FEC) capable channels. We show that the proposed method successfully exploit the local dynamics of a video frame and perform more than 1 dB better than common methods.",2006,0,
663,664,Fault-Tolerant Overlay Protocol Network,"Voice over Internet Protocol (VoIP) and other time critical communications require a level of availability much higher than the typical transport network supporting traditional data communications. These critical command and control channels must continue to operate and remain available in the presence of an attack or other network disruption. Even disruptions of short duration can severely damage, degrade, or drop a VoIP connection. Routing protocols in use today can dynamically adjust for a changing network topology. However, they generally cannot converge quickly enough to continue an existing voice connection. As packet switching technologies continue to erode traditional circuit switching applications, some methodology or protocol must be developed that can support these traditional requirements over a packet-based infrastructure. We propose the use of a modified overlay tunneling network and associated routing protocols called the fault tolerant overlay protocol (FTOP) network. This network is entirely logical; the supporting routing protocol may be greatly simplified due to the overlays's ability to appear fully connected. Therefore, ensuring confidentiality and availability are much simpler using traditional cryptographic isolation and VPN technologies. Empirical results show for substrate networks, convergence time may be as high as six to ten minutes. However, the FTOP overlay network has been shown to converge in a fraction of a second, yielding an observed two order of magnitude convergence time improvement. This unique ability enhances availability of critical network services allowing operation in the face of substrate network disruption caused by malicious attack or other failure",2006,0,
664,665,Adaptive error protection for Scalable Video Coding extension of H.264/AVC,"This paper presents an adaptive error protection method which provides different packet correction capacities by using only one Reed-Solomon code. The proposed method can be applied separately for each data part in a bit stream. The adaption of the error correction capacity works on-the-fly and only based on the way of data interleaving. In this work, the error protection is applied unequally to data units in the Network Abstraction Layer (NAL) of the Scalable Video Coding (SVC) extension of H.264/AVC. Simulation results show that the video quality increases 6 dB in average with the total overhead of ca. 9%. The advantage of our method is the simpleness and flexibility to apply. Therefore, it is suitable for real-time streaming applications.",2008,0,
665,666,Power factor correction and efficiency investigation of AC-DC converters using forced commutation techniques,"In this paper the power factor and the efficiency of a suggested AC-DC converter topology is studied via Mathlab/Simulink simulation. This converter topology consists of four MOSFET elements in bridge form and: a) two antiparallel IGBT elements between the bridge and the AC grid, b) one MOSFET element between the bridge and the DC load. These switching elements control the conduction time intervals of the bridge by a hysteresis current controller in order to achieve an AC current waveform in phase with the AC voltage as well as a very low content of higher harmonics. This way the values of the power factor and the efficiency become very high (e.g. 0,98... 0,99).",2005,0,
666,667,The Application of Safety Simulation Technology in the Fault Diagnosis of the Chemical Process,"With the development of information and computational technology, the safety simulation technique is becoming more and more useful in the chemical process hazard assessment, hazard identification, and safety control system design and operating personnel training etc.The fault diagnosis of the gravity water tank is studied by using dynamic simulation of HYSYS (Hyprotech System for Engineers). The simulation results presents the method need not design problem-specific observer to estimate unmeasured state variables, and can identification and diagnosis faults simultaneously as well. The parameters of the chemical process are updated via on-line correction.",2008,0,
667,668,On the relation between design contracts and errors: a software development strategy,"When designing a software module or system, a systems engineer must consider and differentiate between how the system responds to external and internal errors. External errors cannot be eliminated and must be tolerated by the system, while the number of internal errors should be minimized and the resulting faults should be detected and removed. This paper presents a development strategy based on design contracts and a case study of an industrial project in which the strategy was successfully applied. The goal of the strategy is to minimize the number of internal errors during the development of a software system while accommodating external errors. A distinction is made between weak and strong contracts. These two types of contracts are applicable to external and internal errors, respectively. According to the strategy, strong contracts should be applied initially to promote the correctness of the system. Before releasing, the contracts governing external interfaces should be weakened and error management of external errors enabled. This transformation of a strong contract to a weak one is harmless to client modules",2002,0,
668,669,Defect control methods for SIMOX SOI wafer manufacture and processing,"The layered structure of thin film silicon-on-insulator (SOI) wafers introduces new considerations for defect detection, particularly for optical metrology tools used to characterize and control SOI wafer processing. Multi-layer interference, as well as subsurface features of the material, can complicate the detection of surface defects. Non-particle defect types which scatter light, such as mounds, pits (including so-called HF defects), and slip lines, can be efficiently detected and classified with advanced operating modes of state-of-the art optical metrology tools. Such capabilities facilitate improvements in the wafer manufacturing process, and result in improved defect detection capabilities and material quality. This work describes defect characterization of SIMOX-SOI wafers using the KLA-Tencor Surfscan 6420 and SP1<sup>TBI</sup>",2000,0,
669,670,Usage of Weibull and other models for software faults prediction in AXE,There are several families for software quality prediction techniques in development projects. All of them can be classified in several subfamilies. Each of these techniques has its own distinctive feature and it may not give correct prediction of quality for a scenario different from the one for which the technique was designed. All these techniques for software quality prediction are dispersed. One of them is statistical and probabilistic technique. The paper deals with software quality prediction techniques in development projects. Four different models based on statistical and probabilistic approach is presented and evaluated for prediction of software faults in very large development projects.,2008,0,
670,671,Fault Emulation for Dependability Evaluation of VLSI Systems,"Advances in semiconductor technologies are greatly increasing the likelihood of fault occurrence in deep-submicrometer manufactured VLSI systems. The dependability assessment of VLSI critical systems is a hot topic that requires further research. Field-programmable gate arrays (FPGAs) have been recently pro posed as a means for speeding-up the fault injection process in VLSI systems models (fault emulation) and for reducing the cost of fixing any error due to their applicability in the first steps of the development cycle. However, only a reduced set of fault models, mainly stuck-at and bit-flip, have been considered in fault emulation approaches. This paper describes the procedures to inject a wide set of faults representative of deep-submicrometer technology, like stuck-at, bit-flip, pulse, indetermination, stuck-open, delay, short, open-line, and bridging, using the best suitable FPGA- based technique. This paper also sets some basic guidelines for comparing VLSI systems in terms of their availability and safety, which is mandatory in mission and safety critical application contexts. This represents a step forward in the dependability benchmarking of VLSI systems and towards the definition of a framework for their evaluation and comparison in terms of performance, power consumption, and dependability.",2008,0,
671,672,Delay Constraint Error Control Protocol for Real-Time Video Communication,"Real-time video communication over wireless channels is subject to information loss since wireless links are error-prone and susceptible to noise. Popular wireless link-layer protocols, such as retransmission (ARQ) based 802.11 and hybrid ARQ methods provide some level of reliability while largely ignoring the latency issue which is critical for real-time applications. Therefore, they suffer from low throughput (under high-error rates) and large waiting-times leading to serious degradation of video playback quality. In this paper, we develop an analytical framework for video communication which captures the behavior of real-time video traffic at the wireless link-layer while taking into consideration both reliability and latency conditions. Using this framework, we introduce a delay constraint packet embedded error control (DC-PEEC) protocol for wireless link-layer. DC-PEEC ensures reliable and rapid delivery of video packets by employing various channel codes to minimize fluctuations in throughput and provide timely arrival of video. In addition to theoretically analyzing DC-PEEC, the performance of the proposed scheme is analyzed by simulating real-time video communication over ldquorealrdquo channel traces collected on 802.11 b WLANs using H.264/AVC JM14.0 video codec. The experimental results demonstrate performance gains of 5-10 dB for different real-time video scenarios.",2009,0,
672,673,Evolutionary design of lifting scheme wavelet-packet adaptive filters for elevator fault detection,"An evolutionary-based procedure for designing adaptive filters based on second-generation wavelet (lifting scheme) packet decomposition for industrial fault detection is presented. The proposed procedure is validated by an experimental case study for induction motor fault diagnosis in an elevator system. Preliminary results on two typologies of faults, broken rotor bars and static air gap eccentricity, are discussed by showing encouraging performance.",2010,0,
673,674,Job Migration and Fault Tolerance in SLA-Aware Resource Management Systems,"Contractually fixed service quality levels are mandatory prerequisites for attracting the commercial user to Grid environments. Service level agreements (SLAs) are powerful instruments for describing obligations and expectations in such a business relationship. At the level of local resource management systems, checkpointing and restart is an important instrument for realizing fault tolerance and SLA- awareness. This paper highlights the concepts of migrating such checkpoint datasets to achieve the goal of SLA- compliant job execution.",2008,0,
674,675,A particle swarm optimization approach for automatic diagnosis of PMSM stator fault,"Permanent magnet synchronous motors (PMSM) are frequently used to high performance applications. Accurate diagnosis of small faults can significantly improve system availability and reliability. This paper proposes a new scheme for the automatic diagnosis of interturn short circuit faults in PMSM stator windings. Both the fault location and fault severity are identified using a particle swarm optimization (PSO) algorithm. The performance of the motor under the fault conditions is simulated through lumped-parameter models. Waveforms of the machine phase currents are monitored, based on which a fitness function is formulated and PSO is used to identify the fault location and fault size. The proposed method is simulated in MATLAB environment. Simulation results provide preliminary verification of the diagnosis scheme",2006,0,
675,676,Efficient techniques for reducing error latency in on-line periodic BIST,"With transient and intermittent operational faults becoming a dominant failure mode in modern digital systems, the deployment of on-line test technology is becoming a major design objective. On-line periodic BIST is a testing method for the detection of operational faults in digital systems. The method applies a near-minimal deterministic test sequence periodically to the circuit under test and checks the circuit responses to detect the existence of operational faults. On-line periodic BIST is characterized by full error coverage, bounded error latency, moderate space and time redundancy. In this paper, we present various techniques to minimize the error latency without sacrificing the full error coverage. These techniques are primarily based on the reordering the test vectors or the selective repetition of test vectors. Our analytical and preliminary experimental results demonstrate that our techniques lead to a significant reduction in the error latency.",2009,0,
676,677,A multi-path routing protocol with fault tolerance in mobile ad hoc networks,"In recent years many researches have focused on ad-hoc networks, mainly because of their independence to any specific structure. These networks suffers from frequent and rapid topology changes that cause many challenges in their routing. Most of the routing protocols try to find a path between source and destination nodes because any path will expire, offer a short period, the path reconstruction may cause the network inefficiency. The proposed protocol build two paths between source and destination and create backup paths during the route reply process, route maintenance process and local recovery process in order to improve the data transfer and the fault tolerance. The protocol performance is demonstrated by using the simulation results obtain from the global mobile simulation software(Glomosim). The experimental results show that this protocol can decrease the packet loss ratio rather than DSR and SMR and it is useful for the applications that need a high level of reliability.",2009,0,
677,678,Cleansing Test Suites from Coincidental Correctness to Enhance Fault-Localization,"Researchers have argued that for failure to be observed the following three conditions must be met: 1) the defect is executed, 2) the program has transitioned into an infectious state, and 3) the infection has propagated to the output. Coincidental correctness arises when the program produces the correct output, while conditions 1) and 2) are met but not 3). In previous work, we showed that coincidental correctness is prevalent and demonstrated that it is a safety reducing factor for coverage-based fault localization. This work aims at cleansing test suites from coincidental correctness to enhance fault localization. Specifically, given a test suite in which each test has been classified as failing or passing, we present three variations of a technique that identify the subset of passing tests that are likely to be coincidentally correct. We evaluated the effectiveness of our techniques by empirically quantifying the following: 1) how accurately did they identify the coincidentally correct tests, 2) how much did they improve the effectiveness of coverage-based fault localization, and 3) how much did coverage decrease as a result of applying them. Using our better performing technique and configuration, the safety and precision of fault-localization was improved for 88% and 61% of the programs, respectively.",2010,0,
678,679,Videoendoscopic distortion correction and its application to virtual guidance of endoscopy,"Modern video based endoscopes offer physicians a wide-angle field of view (FOV) for minimally invasive procedures, Unfortunately, inherent barrel distortion prevents accurate perception of range. This makes measurement and distance judgment difficult and causes difficulties in emerging applications, such as virtual guidance of endoscopic procedures. Such distortion also arises in other wide FOV camera circumstances. This paper presents a distortion correction technique that can automatically calculate correction parameters, without precise knowledge of horizontal and vertical orientation. The method is applicable to any camera-distortion correction situation. Based on a least-squares estimation, the authors' proposed algorithm considers line fits in both FOV directions and gives a globally consistent set of expansion coefficients and an optimal image center. The method is insensitive to the initial orientation of the endoscope and provides more exhaustive FOV correction than previously proposed algorithms. The distortion-correction procedure is demonstrated for endoscopic video images of a calibration test pattern, a rubber bronchial training device, and real human circumstances. The distortion correction is also shown as a necessary component of an image-guided virtual-endoscopy system that matches endoscope images to corresponding rendered three-dimensional computed tomography views.",2001,0,
679,680,Test Generation and Diagnostic Test Generation for Open Faults with Considering Adjacent Lines,"In order to ensure high quality of DSM circuits, testing for the open defect in the circuits is necessary. However, the modeling and techniques for test generation for open faults have not been established yet. In this paper, we propose a method for generating tests and diagnostic tests based on a new open fault model. Firstly, we show a new open fault model with considering adjacent lines [9]. Under the open fault model, we reveal more about the conditions to excite the open fault. Next we propose a method for generating tests for open faults by using a stuck-at fault test with don't cares. We also propose a method for generating a diagnostic test that can distinguish the pair of open faults. Finally, experimental results show that (1) the proposed method is able to achieve 100% fault coverages for almost all benchmark circuits and (2) the proposed method is able to reduce the number of indistinguished open fault pairs.",2007,0,
680,681,Minimum Zone Evaluation of Sphericity Error Based on Ant Colony Algorithm,"In this paper, based on the analysis of existent evaluation methods for sphericity errors, an intelligent evaluation method is provided. The evolutional optimum model and the calculation process are introduced in detail. According to characteristics of sphericity error evaluation, ant colony optimization (ACO) algorithm is proposed to evaluate the minimum zone error. Compared with conventional optimum evaluation methods such as simplex search and Powell method, it can find the global optimal solution, and the precision of calculating result is very high. Then, the objective function calculation approaches for using the ACO algorithm to evaluate minimum zone error are formulated. Finally, the control experiment results evaluated by different method such as the least square, simplex search, Powell optimum methods and GA, indicate that the proposed method can provide better accuracy on sphericity error evaluation, and it has fast convergent speed as well as using computer expediently and popularizing application easily.",2007,0,
681,682,Development and evaluation of a model of programming errors,"Models of programming and debugging suggest many causes of errors, and many classifications of error types exist. Yet, there has been no attempt to link causes of errors to these classifications, nor is there a common vocabulary for reasoning about such causal links. This makes it difficult to compare the abilities of programming styles, languages, and environments to prevent errors. To address this issue, this paper presents a model of programming errors based on past studies of errors. The model was evaluated with two observational of Alice, an event-based programming system, revealing that most errors were due to attentional and strategic problems in implementing algorithms, language constructs, and uses of libraries. In general, the model can support theoretical, design, and educational programming research.",2003,0,
682,683,A Comparative Study of Voice Over Wireless Networks Using NS-2 Simulation with an Integrated Error Model,"Wireless communication is the fastest growing field and with the emergence of IEEE 802.11 based devices, wireless access is becoming more popular. Many multimedia applications for IP networks have been developed and thus the demand for quality of service (QoS) has increased. In this paper our primary objective is to evaluate 802.11e EDCF framework for video, voice and background traffic all at the same time. Our assessment is based on an error model called E-model, MOS for VoIP and PSNR for video. We also studied the effects of random uniform error model on various types of traffic. As expected, wireless networks are more prone to errors than wired networks",2006,0,
683,684,Adaptive Causal Models for Fault Diagnosis and Recovery in Multi-Robot Teams,"This paper presents an adaptive causal model method (adaptive CMM) for fault diagnosis and recovery in complex multi-robot teams. We claim that a causal model approach is effective for anticipating and recovering from many types of robot team errors, presenting extensive experimental results to support this claim. To our knowledge, these results show the first, full implementation of a CMM on a large multi-robot team. However, because of the significant number of possible failure modes in a complex multi-robot application, and the difficulty in anticipating all possible failures in advance, our empirical results show that one cannot guarantee the generation of a complete a priori causal model that identifies and specifies all faults that may occur in the system. Instead, an adaptive method is needed to enable the robot team to use its experience to update and extend its causal model to enable the team, over time, to better recover from faults when they occur. We present our case-based learning approach, called LeaF (for learning-based fault diagnosis), that enables robot team members to adapt their causal models, thereby improving their ability to diagnose and recover from these faults over time",2006,0,
684,685,Effects of clipping on the error performance of OFDM in frequency selective fading channels,"Previous studies on the effect of the clipping noise on the error performance of orthogonal frequency-division multiplexing (OFDM) systems in frequency selective fading channels provide pessimistic results. They do not consider the effect of channel fading on the clipping noise. The clipping noise is added at the transmitter and hence fades with the signal. Here, the authors show that the ""bad"" subcarriers that dominate the error performance of the OFDM system are least affected by the clipping noise and, as a result, the degradation in the error performance of OFDM system in fading channels is very small.",2004,0,
685,686,Master Defect Record Retrieval Using Network-Based Feature Association,"As electronic records (e.g., medical records and technical defect records) accumulate, the retrieval of a record from a past instance with the same or similar circumstances, has become extremely valuable. This is because a past record may contain the correct diagnosis or correct solution to the current circumstance. We refer to the two records of the same or similar circumstances as <i>master</i> and <i>duplicate</i> records. Current record retrieval techniques are lacking when applied to this special master defect record retrieval problem. In this study, we propose a new paradigm for master defect record retrieval using network-based feature association (NBFA). We train the master record retrieval process by constructing feature associations to limit the search space. The retrieval paradigm was employed and tested on a real-world large-scale defect record database from a telecommunications company. The empirical results suggest that the NBFA was able to significantly improve the performance of master record retrieval, and should be implemented in practice. This paper presents an overview of technical aspects of the master defect record retrieval problem, describes general methodologies for retrieval of master defect records, proposes a new feature association paradigm, provides performance assessments on real data from a telecommunications company, and highlights difficulties and challenges in this line of research that should be addressed in the future.",2010,0,
686,687,Fault-tolerant scheduling in distributed real-time systems,"In distributed systems, a real-time task has several subtasks which need to be executed at different nodes. Some of these subtasks can be executed in parallel on different nodes without violating their precedence relationships, if any, among them. To better exploit the parallelism, it becomes necessary to assign separate deadlines to subtasks and schedule them independently. We use three subtask deadline assignment policies which we have introduced earlier to develop a bidding-based fault-tolerant scheduling algorithm for distributed real-time systems. A local scheduler which resides on each node, tries to determine a schedule for each subtask according to the primary-backup approach. In this paper we discuss the algorithm and present the results of simulation studies conducted to establish the efficacy of our algorithm",2001,0,
687,688,The Error Reduced ADI-CPML Method for EMC Simulation,"In this paper, convolutional perfectly matched layer (CPML) is developed for the recently proposed error reduced (ER) ADI-FDTD method to solve electromagnetic compatibility problems efficiently. Its numerical results are examined and compared with the conventional ADI-CPML method. It is found that for a CFL number equal to 5, the reflection error of the ER- ADI-CPML is approximately 12 dB better than the conventional ADI-CPML method.",2007,0,
688,689,Identification of Errors in Power Flow Controller Parameters,"Transmission open access allows power transactions to take place between remote parts of an interconnected system. As a result, some parts of the transmission system may experience unusual power flows during certain power transactions. One way to circumvent possible congestion is to use power flow control devices. These devices which are also referred as flexible AC transmission system (FACTS) devices, allow rerouting of power flows in the system. The amount of power flowing through such a device can be controlled via device parameters. Hence, proper monitoring of these parameters is important for reliable operation and system security. In this paper, an identification method for detecting and identifying errors associated with power controller parameters will be presented. The method is based on the available measurements such as the power flows and injections which are used by the state estimators at the control center. Hence, the method can be implemented easily as part of the existing energy management functions",2006,0,
689,690,The use of historical defect imagery for yield learning,"The rapid identification of yield detracting mechanisms through integrated yield management is the primary goal of defect sourcing and yield learning. At future technology nodes, yield learning must proceed at an accelerated rate to maintain current defect sourcing cycle times despite the growth in circuit complexity and the amount of data acquired on a given wafer lot. As integrated circuit fabrication processes increase in complexity, it has been determined that data collection, retention, and retrieval rates will continue to increase at an alarming rate. Oak Ridge National Laboratory (ORNL) has been working with International SEMATECH to develop methods for managing the large volumes of image data that are being generated to monitor the status of the manufacturing process. This data contains an historical record that can be used to assist the yield engineer in the rapid resolution of manufacturing problems. To date there are no efficient methods of sorting and analyzing the vast repositories of imagery collected by off-line review tools for failure analysis, particle monitoring, line width control and overlay metrology. In this paper we will describe a new method for organizing, searching, and retrieving imagery using a query image to extract images from a large image database based on visual similarity",2000,0,
690,691,An iron core probe based inter-laminar core fault detection technique for generator stator cores,"A new technique for detecting incipient interlaminar insulation failure of laminated stator cores of large generators is proposed in this paper. The proposed scheme is a low flux induction method that employs a novel probe for core testing. The new probe configuration, which uses magnetic material and is scanned in the wedge depression area, significantly improves the sensitivity of fault detection as well as user convenience compared to existing methods. Experimental results from various test generators tested in factory, field and lab environments under a number of fault conditions are presented to verify the sensitivity and reliability of the proposed scheme.",2003,0,
691,692,Experiments on Fault-Tolerant Self-Reconfiguration and Emergent Self-Repair,"This paper presents a series of experiments on fault tolerant self-reconfiguration of the ATRON robotic system. For self-reconfiguration we use a previously described distributed control strategy based on meta-modules that emerge, move and stop. We perform experiments on three different types of failures: 1) Action failure: On the physical platform we demonstrate how roll-back of actions are used to achieve tolerance to collision with obstacles and other meta-modules. 2) Module failure: In simulation we show, for a 500 module robot, how different degrees of catastrophic module failure affect the robot's ability to shape-change to support an insecure roof. 3) Robot failure: In simulation we demonstrate how robot faults such as a broken robot bone can be emergent self-repaired by exploiting the redundancy of self-reconfigurable modules. We conclude that the use of emergent, distributed control, action roll-back, module redundancy, and self-reconfiguration can be used to achieve fault tolerant, self-repairing robots",2007,0,
692,693,Runtime Diversity against Quasirandom Faults,"Complex software based systems that have to be highly reliable, are increasingly confronted with fault types whose corresponding failures appear to be random, although they have a systematic cause. This paper introduces and defines these ""quasirandom"" faults. They have certain inconvenient common properties such as their difficulty to be reproduced, their strong state dependence and their likelihood to be found in operational systems after testing. However, these faults are also likely to be detected or tolerated with the help of diversity in software, and even low level diversity which can be achieved during runtime is a promising means against them. The result suggests, that runtime diversity can improve software reliability in complex systems.",2009,0,
693,694,"A secure modular exponential algorithm resists to power, timing, C safe error and M safe error attacks","This paper proposes a method for protecting public key schemes from timing and fault attacks. In general, this is accomplished by implementing critical operations using ""branch-less"" path routines. More particularly, the proposed method provides a modular exponentiation algorithm without any redundant computation does not have a store operation with non-certain destination so that it can protect the secret key from many known attacks.",2005,0,
694,695,Detailed radiation fault modeling of the Remote Exploration and Experimentation (REE) first generation testbed architecture,"The goal of the NASA HPCC Remote Exploration and Experimentation (REE) Project is to transfer commercial supercomputing technology into space. The project will use state of the art, low-power, non-radiation-hardened, COTS hardware chips and COTS software to the maximum extent possible, and will rely on software-implemented fault tolerance to provide the required levels of availability and reliability. We outline the methodology used to develop a detailed radiation fault model for the REE Testbed architecture. The model addresses the effects of energetic protons and heavy ions which cause single event upset and single event multiple upset events in digital logic devices and which are expected to be the primary fault generation mechanism. Unlike previous modeling efforts, this model will address fault rates and types in computer subsystems at a sufficiently fine level of granularity (i.e., the register level) that specific software and operational errors can be derived. We present the current state of the model, model verification activities and results to date, and plans for the future. Finally, we explain the methodology by which this model will be used to derive application-level error effects sets. These error effects sets will be used in conjunction with our Testbed fault injection capabilities and our applications' mission scenarios to replicate the predicted fault environment on our suite of onboard applications",2000,0,
695,696,Topology discovery for network fault management using mobile agents in ad-hoc networks,"Managing today's complex and increasingly heterogeneous networks requires in-depth knowledge and extensive training as well as collection of very large amount of data. Fault management is one of the functional areas of network management that entails detection, identification and correction of anomalies that disrupt services of a network. The task of fault management is even harder in ad-hoc networks where the topology of the network changes frequently. It is very inefficient if not impossible to discover the ad-hoc network topology using traditional practices of network discovery. We propose a mobile multi agent system for topology discovery that will allow fault management functions in ad-hoc network. Comparison to current mobile agent based topology discovery systems is also presented",2005,0,
696,697,Timing-based delay test for screening small delay defects,"The delay fault test pattern set generated by timing unaware commercial ATPG tools mostly affects very short paths, thereby increasing the escape chance of smaller delay defects. These small delay defects might be activated on longer paths during functional operation and cause a timing failure. This paper presents an improved pattern generation technique for transition fault model, which provides a higher coverage of small delay defect that lie along the long paths, using a commercial no-timing ATPG tool. The proposed technique pre-processes the scan flip-flops based on their least slack path and the detectable delay defect size. A new delay defect size metric based on the affected path length and required increase in test frequency is developed. We then perform pattern generation and apply a novel pattern selection technique to screen test patterns affecting longer paths. Using this technique will provide the opportunity of using existing timing unaware ATPG tools as slack based ATPG. The resulting pattern set improves the defect screening capability of small delay defects",2006,0,
697,698,Exploiting Memory Soft Redundancy for Joint Improvement of Error Tolerance and Access Efficiency,"Technology roadmap projects nanoscale multibillion- transistor integration in the coming years. However, on-chip memory becomes increasingly exposed to the dual challenges of device-level reliability degradation and architecture-level performance gap. In this paper, we propose to exploit the inherent memory soft (<i>transient</i>) <i>redundancy</i> for on-chip memory design. Due to the mismatch between fixed cache line size and runtime variations in memory spatial locality, many irrelevant data are fetched into the memory thereby wasting memory spaces. The proposed soft-redundancy allocated memory detects and utilizes these memory spaces for jointly achieving efficient memory access and effective error control. A runtime reconfiguration scheme is also proposed to further enhance the soft-redundancy allocation. Simulation results demonstrate 74.8% average error-control coverage ratio on the SPEC CPU2000 benchmarks with average of 59.5% and 41.3% reduction in memory miss rate and bandwidth usage, respectively, as compared to the existing memory techniques. Furthermore, the proposed technique is fully scalable with respect to various memory configurations.",2009,0,
698,699,Towards Identifying the Best Variables for Failure Prediction Using Injection of Realistic Software Faults,"Predicting failures at runtime is one of the most promising techniques to increase the availability of computer systems. However, failure prediction algorithms are still far from providing satisfactory results. In particular, the identification of the variables that show symptoms of incoming failures is a difficult problem. In this paper we propose an approach for identifying the most adequate variables for failure prediction. Realistic software faults are injected to accelerate the occurrence of system failures and thus generate a large amount of failure related data that is used to select, among hundreds of system variables, a small set that exhibits a clear correlation with failures. The proposed approach was experimentally evaluated using two configurations based on Windows XP. Results show that the proposed approach is quite effective and easy to use and that the injection of software faults is a powerful tool for improving the state of the art on failure prediction.",2010,0,
699,700,Fault tolerance of feed-forward artificial neural network architectures targeting nano-scale implementations,"Several circuit architectures have been proposed to overcome logic faults due to the high defect densities that are expected to be encountered in the first generations of nanoelectronic systems. How feed-forward artificial neural networks can possibly be exploited for the purpose of conceiving highly reliable Boolean gates is the topic of this paper. Computer simulations show that feed-forward artificial neural networks can be trained to absorb faults while implementing Boolean functions of various complexity. Using this approach, it can be shown that very high device failure rates (up to 20%) can be accommodated. The cost is to be paid in terms of hardware overhead, which is comparable to the area cost of conventional hardware redundancy measures.",2007,0,
700,701,Fault tolerance evaluation using two software based fault injection methods,"A silicon independent C-Based model of the TTP/C protocol was implemented within the EU-founded project FIT. The C-based model is integrated in the C-Sim simulation environment. The main objective of this work is to verify whether the simulation model of the TTP/C protocol behaves in the presence of faults in the same way as the existing hardware prototype implementation. Thus, the experimental results of the software implemented fault injection applied in the simulation model and in the hardware implementation of the TTP/C network have been compared. Fault injection experiments in both the hardware and the simulation model are performed using the same configuration setup, and the same fault injection input parameters (fault injection location, fault type and the fault injection time). The end result comparison has shown a complete conformance of 96.30%, while the cause of the different results was due to hardware specific implementation of the built-in-self-test error detection mechanisms.",2002,0,
701,702,Error detection and unit conversion,"The article discusses the accuracy mathematical modeling languages (MML) for biomedicine, for example in cardiac electrophysiology. Unit balance checking is showed that it can be automated. The implemented example is JSim (http://www.physiome.org/ jsim/), which is general and can be applied to other systems in which units can be specified and checked. ODE-based simulator Physiome CellML Environment is also discussed.",2009,0,
702,703,Implementation of Web-Based Fault Diagnosis Using Improved Fuzzy Petri Nets,"According to the current application and maintenance situation of numerical control equipment (NCE), a novel remote fault diagnosis expert system is designed to prevent fault occurrence and quicken the recovering process by online real-time monitoring the working state of NCEs. The article addresses the overall framework and relevant application technology of fault diagnosis system (FDS) and emphasizes on the establishment of fuzzy expert system (FES). Improved fuzzy Petri nets (FPNs) model and concurrent reasoning algorithm are applied to handle the fuzziness and concurrency of fault and inadequate and uncertain information. Utilization of simple matrix operation to realize complicated reasoning process that simplifies the diagnostic reasoning decision-making process. Meanwhile, it can be realized easily by computer programming. Finally, a practical fault instance is presented to demonstrate the feasibility and validity of this method.",2009,0,
703,704,An object-based approach to optical proximity correction,"As the feature sizes of integrated circuits have been continually reducing to below exposure wavelength, some correcting techniques, such as OPC and PSM are indispensable to compensate for the distortions on wafer images. In this paper, we describe an object-based approach to OPC named OPCM, which is a model-based OPC tool. Also, a rule-based OPC has been adopted to enhance the practicability of the software",2001,0,
704,705,Generator dynamics influence on currents distribution in fault condition,"Current flow calculation results along the elements of a complex power system are analyzed in this work, during a three-phase short-circuit taking into account relative rotor swing. Analysis is implemented on the examples when the infinite bus fault point is supplied by one or more generators. It is shown that generator swing neglected during short-circuits, essentially changing current distribution in the system, can lead to impermissible mistakes in calculation results.",2000,0,
705,706,"An automated fault analysis system for SP energy networks: Requirements, design and implementation","The proliferation of monitoring equipment on modern electrical power transmission networks is causing an increasing amount of monitoring data to be captured by transmission network operators. Traditional manual data analysis techniques fail to meet the analysis and reporting requirements of the utilities which have chosen to invest in monitoring. The volume of monitoring data, the complexities in analysing multiple related data sources and the preparation of internal reports based on that analysis, render timely manual analysis impractical, if not intractable. In 2006, the authors reported on the first online trials of the protection engineering diagnostics agents (PEDA) system, an automated fault diagnosis system which integrated legacy intelligent systems for the analysis of SCADA and digital fault recorder (DFR) data in order to provide automatic post fault assessment of protection system performance. In this paper the authors revisit the requirements of the TNO where PEDA was trialled. Based on a new formal specification of requirements carried out in 2008, the authors discuss the requirements met by the current version of PEDA and how PEDA could be augmented to meet these new requirements highlighted in this latest analysis of the utilities' requirements.",2009,0,
706,707,Impact of correlation errors on optimum Kalman filter matrices gains identification in multicoordinate systems,"This paper investigates the impact that errors in the innovation correlation calculations has upon the steady-state Kalman filter gain identification. This issue arises in all real time applications, where the correlations must he calculated from experimental data. The algorithm proposed by [L. Hong (1991)] is considered and equations describing the impact are established. Simulation results are presented and discussed. Finally, experimental results for the algorithm in [L. Hong (1991)], applied to estimate the states of a servo system, are presented.",2005,0,
707,708,Optimal Cluster-Cluster Design for Sensor Network with Guaranteed Capacity and Fault Tolerance,"Sensor networks have recently gained a lot of attention from the research community. To ensure scalability sensor networks are often partitioned into clusters, each managed by a cluster head. Since sensors self organize in the form of clusters within a hierarchal wireless sensor network, it is necessary for a sensor node to perform target tracking cooperating with a set of sensors that belong to another cluster. The increased flexibility allows for efficient and optimized use of sensor nodes. While most of the previous research focused on the optimal communication of sensors in one cluster, very little attention has been paid to the efficiency of cooperation among the clusters. This paper proposes a heuristic algorithm of designing optimal structure across clusters to allow the inter-cluster flow of communication and resource sharing under reliability constraints. Such a guarantee simultaneously provides fault tolerance against node failures and high capacity through multi-path routing.",2007,0,
708,709,Fault treatment with net condition/event systems: a first approach,"The paper presents a preliminary report on modeling parts of a modular production system, their dedicated controllers, and the appropriate methods of fault treatment on the level of net condition/event systems (NCES). To achieve practicability, NCES support a systematic and modular way of modeling more complex systems as well as concurrent and non-deterministic behavior which is highly beneficial for modeling and control of DES in failure situations as studies of existing methods show.",2001,0,
709,710,SLICED: Slide-based concurrent error detection technique for symmetric block ciphers,"Fault attacks, wherein faults are deliberately injected into cryptographic devices, can compromise their security. Moreover, in the emerging nanometer regime of VLSI, accidental faults will occur at very high rates. While straightforward hardware redundancy based concurrent error detection (CED) can detect transient and permanent faults, it entails 100% area overhead. On the other hand, time redundancy based CED can only detect transient faults with minimum area overhead but entails 100% time overhead. In this paper we present a general time redundancy based CED technique called SLICED for pipelined implementations of symmetric block cipher. SLICED SLIdes one encryption over another and compares their results for CED as a basis for protection against accidental faults and deliberate fault attacks.",2010,0,
710,711,A Support System for Teaching Computer Programming Based on the Analysis of Compilation Errors,"A system was developed to support teaching computer programming to a group of students who have common questions and make common mistakes on practice computer programs. The system extrapolates the causes and syntaxes of students' compilation errors by analyzing the trends of past compilation errors and presents the extrapolated result to the teacher in real time. By using the system, a teacher can understand in real time students' programming mistakes when they are writing computer programs, and can appropriately teach computer programming to a group of students who have common problems",2006,0,
711,712,The research on a new fault wave recording device in generator-transformer units,"This paper presents a new kind of distributed fault recorder including the design of the system structure, the hardware and software design of the recorder. The recorder adopts NI CompaceRIO series programmable automation controller (PAC) in the hardware while virtual instrument technology in the software. The network communication based on TCP/IP between the client and the server is adopted in the power plant. Moreover, an improved frequency tracking algorithm is presented in the monitoring of the electric quantity to improve the detection precision and the processing speed. The detection and operation results show that it has improved the performance greatly and realized authenticity, integrity and reliability and so on.",2009,0,
712,713,Rotor fault detection using the instantaneous power signature,"The aim of this paper is to present a method to detect broken rotor bar faults by estimating a global modulation index which corresponds to the contribution of all detected modulating frequencies in the stator current. We show that additional information carried by instantaneous power improves the detection of the sidebands and consequently the monitoring too. In fact, the instantaneous power method can be interpreted as a modulation operation in the time domain that translates the spectral components specific to the broken rotor bars to the band [0-50]Hz.",2004,0,
713,714,A system level approach in designing dual-duplex fault tolerant embedded systems,"This paper presents an approach for designing embedded systems able to tolerate hardware faults, defined as an evolution of our previous work proposing an hardware/software co-design framework for realizing reliable embedded systems. The framework is extended to support the designer in achieving embedded systems with fault tolerant properties minimizing overheads and limiting power consumption. A reference system architecture is proposed; the specific hardware/software implementation and reliability methodologies (to achieve the fault tolerance properties) are the result of an enhanced hw/sw partitioning process driven by the designer' constraints and by the reliability constraints, set at the beginning of the design process. By introducing also the reliability constraints during specification, the final system can benefit from the introduced redundancy also for performance gains, while limiting area, time, performance and power consumption overheads.",2002,0,
714,715,A 32-bit COTS-based fault-tolerant embedded system,"This paper presents a 32-bit fault-tolerant (FT) embedded system based on commercial off-the-shelf (COTS) processors. This embedded system uses two 32-bit Pentium processors with master/checker (M/C) configuration and an external watchdog processor (WDP) for implementing a behavioral-based error detection scheme called committed instructions counting (CIC). The experimental evaluation was performed using both power-supply disturbance (PSD) and software-implemented fault injection (SWIFI) methods. A total of 9000 faults have been injected into the embedded system to measure the coverage of error detection mechanisms, i.e., the checker processor and the CIC scheme. The results show that the M/C configuration is not enough for this system and the CIC scheme could cover the limitation of the M/C configuration.",2005,0,
715,716,A software fault tolerance method for safety-critical systems: effectiveness and drawbacks,"An automatic software technique suitable for on-line detection of transient errors due to the effects of the environment (radiation, EMC,...) is presented. The proposed approach, particularly well suited for low-cost safety-critical microprocessor-based applications, has been validated through fault injection experiments and radiation testing campaigns. The experimental results demonstrate the effectiveness of the approach in terms of fault detection capabilities. Undetected faults have been analyzed to point out the limitations of the method.",2002,0,
716,717,IFRA: Post-silicon bug localization in processors,"IFRA overcomes challenges associated with an expensive step in post-silicon validation of processors - pinpointing the bug location and the instruction sequence that exposes the bug from a system failure. On-chip recorders collect instruction footprints (information about flows of instructions, and what the instructions did as they passed through various design blocks) during the normal operation of the processor in a post-silicon system validation setup. Upon system failure, the recorded information is scanned out and analyzed off-line for bug localization. Special self-consistency-based program analysis techniques, together with the test program binary of the application executed during post-silicon validation, are used. Major benefits of using IFRA over traditional techniques for post-silicon bug localization are: 1. It does not require full system-level reproduction of bugs, and, 2. It does not require full system-level simulation. Simulation results on a complex super-scalar processor demonstrate that IFRA is effective in accurately localizing electrical bugs with very little impact on overall chip area.",2009,0,
717,718,Parallel computation of configuration space on reconfigurable mesh with faults,"A reconfigurable mesh (RMESH) can be used to compute robotic paths in the presence of obstacles, where the robot and obstacle images are represented and processed in mesh processors. For a non-point-like robot, we can compute the so-called configuration space to expand the obstacles, so that the robot can be reduced to a reference point to facilitate the robot's motion planning. In this paper, we present algorithms to compute the configuration space in a reconfigurable mesh that contains sparsely distributed faulty processors. Robots of rectangular and circular shapes are treated. It is seen that, in terms of computing the configuration space, a reconfigurable mesh can tolerate faulty processors without much extra cost-the computation takes the optimal O(1) time in both fault-free and faulty reconfigurable meshes",2000,0,
718,719,Nonstationary Motor Fault Detection Using Recent Quadratic TimeFrequency Representations,"As the use of electric motors increases in the aerospace and transportation industries where operating conditions continuously change with time, fault detection in electric motors has been gaining importance. Motor diagnostics in a nonstationary environment is difficult and often needs sophisticated signal processing techniques. In recent times, a plethora of new time-frequency distributions has appeared, which are inherently suited to the analysis of nonstationary signals while offering superior frequency resolution characteristics. The Zhao-Atlas-Marks distribution is one such distribution. This paper proposes the use of these new time-frequency distributions to enhance nonstationary fault diagnostics in electric motors. One common myth has been that the quadratic time-frequency distributions are not suitable for commercial implementation. This paper also addresses this issue in detail. Optimal discrete-time implementations of some of these quadratic time-frequency distributions are explained. These time-frequency representations have been implemented on a digital signal processing platform to demonstrate that the proposed methods can be implemented commercially.",2008,0,
719,720,Towards understanding the effects of intermittent hardware faults on programs,"Intermittent hardware faults are bursts of errors that last from a few CPU cycles to a few seconds. They are caused by process variations, circuit wear-out, and temperature, clock or voltage fluctuations. Recent studies show that intermittent fault rates are increasing due to technology scaling and are likely to be a significant concern in future systems. We study the propagation of intermittent faults to programs; in particular, we are interested in the crash behaviour of programs. We use a model of a program that represents the data dependencies in a fault-free trace of the program and we analyze this model to glean some information about the length of intermittent faults and their effect on the program under specific fault and crash models. The results of our study can aid fault detection, diagnosis and recovery techniques.",2010,0,
720,721,A Method of Detecting Vulnerability Defects Based on Static Analysis,"This paper proposes a method for detecting vulnerability defects caused by tainted data based on state machine. It first uses state machine to define various defect patterns. If the states of state machine is considered as the value propagated in dataflow analysis and the union operation of the state sets as the aggregation operation of dataflow analysis, the defect detection can be treated as a forward dataflow analysis problem. To reduce the false positives caused by intraprocedural analysis, the dynamic information of program was represented approximately by abstract value of variables, and then infeasible path can be identified when some variable's abstract value is empty in the state condition. A function summary method is proposed to get the information needed for performing interprocedural defect detection. The method proposed has been implemented in a defect testing tools.",2010,0,
721,722,Distortion correction of LDMOS power amplifiers using hybrid RF second harmonic injection/digital predistortion linearization,"An LDMOS RF power amplifier for RF multichannel wireless systems with improved IMD performance characteristics is presented. The application of two combined linearization methods is being tested with the help of circuit simulation software ADS. The injection of the fundamental signal's second harmonic in the RF amplifier, as well as a digital predistortion technique, is combined in order to achieve IMD improvement. By proper selection of phase and amplitude of the injected second harmonic signal, it is possible to reduce IMD products that have already been reduced by the well established method of digital predistortion",2006,0,
722,723,Identifying the root causes of memory bugs using corrupted memory location suppression,"We present a general approach for automatically isolating the root causes of memory-related bugs in software. Our approach is based on the observation that most memory bugs involve uses of corrupted memory locations. By iteratively suppressing (nullifying) the effects of these corrupted memory locations during program execution, our approach gradually isolates the root cause of a memory bug. Our approach can work for common memory bugs such as buffer overflows, uninitialized reads, and double frees. However, our approach is particularly effective in finding root causes for memory bugs in which memory corruption propagates during execution until an observable failure such as a program crash occurs.",2008,0,
723,724,Towards a Model of Fault Tolerance Technique Selection in Static and Dynamic Agent-Based Inter-Organizational Workflow Management Systems,"Research in workflow management systems design references the mobile agent computing paradigm where agents have been shown to increase the total capacity of a workflow system through the decoupling of execution management from a statically designated workflow engine, although coordinating fault tolerance mechanisms has been shown to be a downside due to increased overall execution times. To address this issue, we develop a model for comparing the effects of two fault tolerance techniques: local and remote checkpointing. The model enables an examination of fault tolerance coordination impacts on execution time while concomitantly taking into account the dynamic nature of a workflow environment. A proposed use for the model includes providing for selecting and configuring agent-based fault tolerance approaches based on changes in environmental variables - an approach that allows the owners of a workflow management system to reap the scaling efficiency benefits of the mobile agent paradigm without being forced to make trade-offs in execution performance.",2005,0,
724,725,Impact of configuration errors on DNS robustness,"During the past twenty years the Domain Name System (DNS) has sustained phenomenal growth while maintaining satisfactory user-level performance. However, the original design focused mainly on system robustness against physical failures, and neglected the impact of operational errors such as mis-configurations. Our measurement efforts have revealed a number of mis-configurations in DNS today: delegation inconsistency, lame delegation, diminished server redundancy, and cyclic zone dependency. Zones with configuration errors suffer from reduced availability and increased query delays up to an order of magnitude. The original DNS design assumed that redundant DNS servers fail independently, but our measurements show that operational choices create dependencies between servers. We found that, left unchecked, DNS configuration errors are widespread. Specifically, lame delegation affects 15% of the measured DNS zones, delegation inconsistency appears in 21% of the zones, diminished server redundancy is even more prevalent, and cyclic dependency appears in 2% of the zones. We also noted that the degrees of mis-configuration vary from zone to zone, with the most popular zones having the lowest percentage of errors. Our results indicate that DNS, as well as any other truly robust large-scale system, must include systematic checking mechanisms to cope with operational errors.",2009,0,
725,726,Detecting defects on planar circuits by using non-contacting magnetic probe,"Recently, the research of non-contacting measurement with magnetic coupling theorem mostly choose CPW(coplanar waveguide) loop-type circuits as probes. It has advantage of low cost, easy to fabricate and simple designing. While moving the probe, different relative position between planar circuit and magnetic probe cause different strength of coupling. Variation of resonance frequency due to changing magnetic coupling and electric coupling from metal strip outline can be observed. The relation between planar circuit and magnetic probe is analyzed by full-wave EM simulation and some simple measurement. Furthermore, the LC equivalent circuit has also been built for analyzing. At last, the possibility of doing quickly defect-detecting by sweeping the circuits at special frequency will be discussed.",2010,0,
726,727,An Application of Semantic Annotations to Design Errors,"As current engineered systems (e.g. aviation systems) have been equipped with automated and computer-based artefacts, human-system interaction (e.g. human computer interaction) has been an important issue. Design errors that are attributable to human-system interaction failures are not pure engineering design issues, but a multidisciplinary subject with related other areas such as management, psychology, physiology or ergonomics. To identify such design errors (called design-induced errors) in accident reports is important for designing more reliable systems. However, the lack of precise definitions of the concept of design-induced error and the diversity of expression of such failures make it difficult to retrieve relevant documents from accident reports. This paper describes how an ontology and annotation scheme can help to overcome such limitations. Engineering designers can be assisted by the developed ontology and annotation scheme to reason on the issues of design induced error",2006,0,
727,728,The effects of Gaussian weighting errors in hybrid SC/MRC combiners,"The paper examines the impact of Gaussian distributed weighting errors (in the channel gain estimates used for coherent combination) on the statistics of the output of hybrid selection/maximal-ratio (SC/MRC) receiver as well as the degradation of the average symbol error rate (ASER) performance from the ideal case. New expressions for the probability density function (PDF), cumulative distribution function (CDF) and moment generating function (MGF) of the coherent hybrid SC/MRC combiner output signal-to-noise ratio (SNR) are derived. The MGF is then used to derive exact closed form ASER formulas for binary and M-ary modulations employing a nonideal hybrid SC/MRC receiver in Rayleigh fading. Results for both SC and MRC are obtained as limiting cases. The effect of the weighting errors on the outage rate of error probability and the average combined SNR are also investigated. These analytical results provide some insights into the trade-off between diversity gain and combination losses with the increasing order of diversity branches in an energy-sharing communication system",2000,0,
728,729,An Extension of Differential Fault Analysis on AES,"In CHES 2006, M. Amir et al. introduced a generalized method of differential fault attack (DFA) against AES-128. Their fault models cover all locations before the 9th round in AES-128. However, their method cannot be applied to AES with other key sizes, such as AES-192 and AES-256. On the differential analysis, we propose a new method to extend DFA on AES with all key sizes. Our results in this study will also be beneficial to the analysis of the same type of other iterated block ciphers.",2009,0,
729,730,A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction,"In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: >75% percentage of correctly classified files, a recall of >80%, and a false positive rate <30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.",2008,0,
730,731,Smoothing Algorithm for Tetrahedral Meshes by Error-Based Quality Metric,"Smoothing or geometrical optimization is one of basic procedures to improve the quality of mesh. This paper first introduces an error-based mesh quality metric based on the concept of optimal Delaunay triangulations, and then examines the smoothing scheme which minimizes the interpolation error among all triangulations with the same number of vertices. Facing to its deficiency, a modified smoothing scheme and corresponding optimization model for tetrahedral mesh that avoid illegal elements are proposed. The optimization model is solved by integrating chaos search and BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm efficiently. Quality improvement for tetrahedral mesh is realized through alternately applying the smoothing approach suggested and topological optimization technique. Testing results show that the proposed approach is effective to improve mesh quality and suitable for combining with topological technique.",2010,0,
731,732,Fault diagnosis based on timed automata: Diagnoser verification,"The paper deals with the supervisory control problem based on vector synchronous product (VSP) of automata. A necessary and sufficient condition for the existence of such a controller is given, which is based on the notion of vs-controllability. Furthermore, a more general framework called vector synchronous product with communication is proposed. In addition, isomorph and homomorph of two VSPs are defined. Some simplified traffic examples are used to illustrate the notions and the result",2006,0,
732,733,Analogous view transfer for gaze correction in video sequences,"This paper provides a framework for doing facial gaze correction in video sequences. The proposed framework involves stages of face registration, face parameter mapping, and face synthesis. We introduce the concept of analogous views, and derive a novel formulation which extends view transfers based on epipolar geometry to cope with non-rigid motion. Additionally, a disparity mapping function is derived which is learned from training data and handles both spatial disparities as well as pixel-value changes. The disparity mapping function generalizes to facial expressions, illumination conditions and individuals not in the training set, as shown by the results obtained.",2002,0,
733,734,A learning-based approach for fault tolerance on grid resources scheduling,"While Grid environment has developed increasingly, unfortunately the importance of fault tolerance has not been remarkable in Grid resource management. On the other hand, the cost of computing by grid is important because grid is an economy-based system. Most organizations intend to spend little on their own computations by grid. Therefore, using a better approach to resource scheduling to avoid fault is necessary. This paper presents a new approach on fault tolerance mechanisms for the resource scheduling on grid by using Case-Based Reasoning technique in a local fashion. This approach applies a specific structure in order to prepare fault tolerance between executer nodes to retain system in a safe state with minimum data transferring. Certainly, this algorithm increases fault tolerant confidence therefore, performance of grid will be high.",2009,0,
734,735,More on general error locator polynomials for a class of binary cyclic codes,"Recently, the general error locator polynomials have been widely used in the algebraic decoding of binary cyclic codes. This paper utilizes the proposed general error locator polynomial to develop an algebraic decoding algorithm for a class of the binary cyclic codes. This general error locator polynomial differs greatly from the previous general error locator polynomial. Each coefficient of the proposed general error locator polynomial is expressed as a binary polynomial in the single syndrome and the degrees of nonzero terms in the binary polynomial satisfy at least one congruence relation.",2010,0,
735,736,A property oriented fault detection approach for link state routing protocol,"This paper proposes a new approach to fault detection for a link state routing system-property oriented analysis and detection (POD). A routing system is modeled as a set of distributed processes. A property is defined as state predicate(s) over system variables. For the link state routing protocol, the high-level overall converging property P is defined as the synchronization among routing information bases maintained by all processes. We decompose the routing protocol into different computation phases. For each phase, we use invariant state predicates (safety property) and the liveness property as our guide for observation and analysis. The goal of the detection algorithm is to construct a validation path based on the history to determine if the fault is natural or malicious once the stable property P is rendered invalid by faults. The contribution of this paper is twofold: first, a new detection approach is proposed that differs from traditional signature-based or profile-based intrusion detection paradigms in the sense that it utilizes the stable property as a starting point, and correlates the history and future to validate changes in the system; second, by exploring the primary concerned system properties, we show that detection effort can be conducted in a more focused and systematic fashion",2000,0,
736,737,On the 2-Adic Complexity and the k-Error 2 -Adic Complexity of Periodic Binary Sequences,"A significant difference between the linear complexity and the 2-adic complexity of periodic binary sequences is pointed out in this correspondence. Based on this observation, we present the concept of the symmetric 2-adic complexity of periodic binary sequences. The expected value of the 2-adic complexity is determined, and a lower bound on the expected value of the symmetric 2-adic complexity of periodic binary sequences is derived. We study the variance of the 2-adic complexity of periodic binary sequences, and the exact value for it is given. Because the k-adic complexity of periodic binary sequences is unstable, we present the concepts of the <i>kappa</i>-error 2-adic complexity and the k-error symmetric 2-adic complexity, and lower bounds on them are also derived. In particular, we give tighter upper and lower bounds for the minimum k-adic complexity of l-sequences by substituting two symbols within one period.",2008,0,
737,738,A Framework for Proactive Fault Tolerance,"Fault tolerance is a major concern to guarantee availability of critical services as well as application execution. Traditional approaches for fault tolerance include checkpoint/restart or duplication. However it is also possible to anticipate failures and proactively take action before failures occur in order to minimize failure impact on the system and application execution. This document presents a proactive fault tolerance framework. This framework can use different proactive fault tolerance mechanisms, i.e., migration and pause/un-pause. The framework also allows the implementation of new proactive fault tolerance policies thanks to a modular architecture. A first proactive fault tolerance policy has been implemented and preliminary experimentations have been done based on system-level virtualization and compared with results obtained by simulation.",2008,0,
738,739,Study of the Dispersion Characteristics of One Dimensional EBG with Defects,"In this paper we propose a simplified model for studying the Brillouin diagrams based on dielectric multilayers inside a parallel plate waveguide. The main objective of this work is the use of a simplified approach in modelling EBG structures with periodical defects. The effect of layer permittivity, defect length and periodicity were studied using simulation software with appropriated periodical boundary conditions. Physical insights and intuitive justifications for the simulation findings and concepts are also presented. It is shown that the two forbidden band-gaps can either be controlled independently by varying the permittivity or the size of the defects",2005,0,
739,740,Poisonedwater: an adaptive approach to reducing the reputation ranking error in P2P networks,"This paper preliminarily proposes a reputation ranking algorithm called ldquoPoisonedwaterrdquo to resist front peer attack - peers that gain high reputation values by always cooperating with other peers and then promote their malicious friends through passing most of their reputation values to those malicious peers. Specifically, we introduce a notion of Poisoned Water (PW) that iteratively floods from identified malicious peers in the reverse direction of the incoming trust links towards other peers. Furthermore, we propose the concept of spreading factor (SF) that is logistically correlated to each peer's PW level. Then, we design the new reputation ranking algorithm seamlessly integrated with peers' recommendation ability (represented as SF), to infer the more accurate reputation ranking for each peer. Simulation results show that, in comparison with Eigentrust, Poisonedwater can significantly reduce the ranking error ratio up to 20%, when P2P systems exist many malicious peers and front peers.",2009,0,
740,741,Distributed Fault Management for Computational Grids,"Grid resources having heterogeneous architectures, being geographically distributed and interconnected via unreliable network media, are at the risk of failure. Grid environment consists of unreliable resources; therefore, fault tolerant mechanisms can not be ignored. Some scientific jobs require long commitments of grid resources whose failures may not be overlooked. We need a flexible management of these failures by considering the failure of fault manager itself. In this paper we propose the concept of distributed management of failures without engaging the resources for this particular task exclusively. Resources performing the fault management may also participate in serving the long running user jobs. Each sub-job of the main user job is inspected by an individual resource. In case of failure inspector resource takes over in place of inspected resource. Contributions of this paper are: elimination of single point of failure and proposed concept's ability to be integrated with variety of grid middleware",2006,0,
741,742,Spherical Near-Field Antenna Measurements: A Review of Correction Techniques,"Following an introductory review of spherical near-field scanning measurements, with emphasis on the general applicability of the technique, we present a survey of the various methods to improve measurement accuracy by correcting the acquired data before performing the transform and by special processing of the resulting data following the transform. A post-processing technique recently receiving additional attention is the IsoFilter technique that assists in suppressing extraneous stray signals due to scattering from antenna range apparatus.",2007,0,
742,743,Filter Hardware Cost Reduction by Means of Error Feedback,"The article presents an uncommon application of the error feedback-improved IIR filter. A simple method to reduce the hardware cost (silicon area) of the biquadratic section implementation by means of error feedback (EF) is described. The optimization method utilizes the fact that the filter with an EF is more resistant to roundoff noise than a filter without it. An iterative method is used to reduce the occupied silicon area. First the standard IIR filter is designed with the requested quantization properties. Then the EF-improved biquadratic section is designed to attain the same roundoff noise properties. The occupied silicon areas of both solutions are compared then. Although implementation of EF results in more arithmetic components and more complex filter control, the resulting structure attaining the same quantization noise is smaller under defined circumstances (filter with poles close to the unit circle). Results show it is possible to spare up to 22% of the occupied silicon area. Our findings are valid for FPGA as well as ASIC implementation of the IIR filters. Our method has an advantage in using a standard and already verified filtering IP core which results in design time reduction.",2007,0,
743,744,The use of PSA for fault detection and characterization in electrical apparatus,"The monitoring of the actual condition of high voltage apparatus has become more and more important in the last years. One well established tool to characterize the actual condition of electric equipment is the measurement and evaluation of partial discharge data. Immense effort has been put into sophisticated statistic software-tools, to extract meaningful analyses out of data sets, without taking care of relevant correlations between consecutive discharge pulses. In contrast to these classical methods of partial discharge analysis the application of the Pulse Sequence Analysis allows a far better insight into the local defects. The detailed analysis of sequences of discharges in a voltage- and a current-transformer shows that the sequence of the partial discharge signals may change with time, because either different defects are active at different measuring times or a local defect may change with time as a consequence of the discharge activity. Hence for the evaluation of the state of degradation or the classification of the type of defect the analysis of short `homogeneous' sequences or sequence correlated data is much more meaningful than just the evaluation of a set of independently accumulated discharge data. This is demonstrated by the evaluation of measurements performed on different commercial hv-apparatus",2000,0,
744,745,Automated fault tree generation and risk-based testing of networked automation systems,"In manufacturing automation domain safety and availability are the most important factors to ensure productivity. In modern software intensive networked automation systems it became quite hard to ensure which non-functional requirements are related to these factors as well as whether these are satisfied or not. This is due to the prevalence of manual efforts in several analyses phases where complexity of the system often makes it hard to obtain comprehensive overview and thus makes it difficult to ascertain the presence of certain undesired consequences. Since design, development and following verification and validation activities are largely dependent upon the result of the analyses the product is largely affected. To address these problems automated fault tree generation is presented in this paper. It uses distinct modeling artifacts and information to automatically compose formal models of the system. Embedding hardware and network failures it is then ascertained through model checking whether the system satisfies certain safety and availability properties or not. This information is used to compose the fault tree. Proposed approach will improve completeness and correctness in fault trees and will consequently help in improving the quality of the system. Furthermore, it is also shown how the artifacts of this analysis can be used to produce test goals and test cases to validate the software constituents of the system and assure traceability between testing activity and safety requirements.",2010,0,
745,746,Quasi-cyclic generalized ldpc codes with low error floors,"In this paper, a novel methodology for designing structured generalized LDPC (G-LDPC) codes is presented. The proposed design results in quasi-cyclic G-LDPC codes for which efficient encoding is feasible through shift-register-based circuits. The structure imposed on the bipartite graphs, together with the choice of simple component codes, leads to a class of codes suitable for fast iterative decoding. A pragmatic approach to the construction of G-LDPC codes is proposed. The approach is based on the substitution of check nodes in the protograph of a low-density parity-check code with stronger nodes based, for instance, on Hamming codes. Such a design approach, which we call LDPC code doping, leads to low-rate quasi-cyclic G-LDPC codes with excellent performance in both the error floor and waterfall regions on the additive white Gaussian noise channel.",2008,0,
746,747,A new H.264/AVC error resilience model based on Regions of Interest,"Video transmission over the Internet can sometimes be subject to packet loss which reduces the end-user's quality of experience (QoE). Solutions aiming at improving the robustness of a video bitstream can be used to subdue this problem. In this paper, we propose a new region of interest-based error resilience model to protect the most important part of the picture from distortions. We conduct eye tracking tests in order to collect the region of interest (RoI) data. Then, we apply in the encoder an intra-prediction restriction algorithm to the macroblocks belonging to the RoI. Results show that while no significant overhead is noted, the perceived quality of the video's RoI, measured by means of a perceptual video quality metric, increases in the presence of packet loss compared to the traditional encoding approach.",2009,0,
747,748,The z990 first error data capture concept,"Superior availability is one of the outstanding features of modern zSeries machines, among the most highly rated of any existing computer platforms in this reqard. Many features contribute to this characteristic, some in hardware, some in software. This paper describes the first error data capture (FEDC) concept in the zSeries 990. The concept is used for both zSeries integration efficiency and its ability to gain field data for problem determination in the user environment. FEDC is not a single function, but part of all internal software (firmware) in the z990. This paper explains the overall concept and implementation details of the various internal functional layers (subsystems).",2004,0,
748,749,Enhanced server fault-tolerance for improved user experience,"Interactive applications such as email, calendar, and maps are migrating from local desktop machines to data centers due to the many advantages offered by such a computing environment. Furthermore, this trend is creating a marked increase in the deployment of servers at data centers. To ride the price/performance curves for CPU, memory and other hardware, inexpensive commodity machines are the most cost effective choices for a data center. However, due to low availability numbers of these machines, the probability of server failures is relatively high. Server failures can in turn cause service outages, degrade user experience and eventually result in lost revenue for businesses. We propose a TCP splice-based Web server architecture that seamlessly tolerates both Web proxy and backend server failures. The client TCP connection and sessions are preserved, and failover to alternate servers in case of server failures is fast and client transparent. The architecture provides support for both deterministic and non-deterministic server applications. A prototype of this architecture has been implemented in Linux, and the paper presents detailed performance results for a PHP-based Webmail application deployed over this architecture.",2008,0,
749,750,An adaptive distance relaying algorithm with a morphological fault detector embedded,"This paper presents an adaptive distance relaying algorithm (ADRA) for transmission line protection. In ADRA, a fault detector designed based on mathematical morphology (MM) is used to determine the occurrence of a fault. The Euclidean norm of the detector output is then calculated for fault phase selection and fault type classification. With respect to a specific type of fault scenario, an instantaneous circuit model applicable to a transient fault process is constructed to determine the position of the fault. The distance between the fault position and the relay is calculated by a differential equation of the instantaneous circuit model which is resolved in a recursive manner within each sampling interval. Due to the feature of recursive calculation, the protection zone of the relay varies from a small length to large, which increases as an augment in the sample window length. ADRA is evaluated on a transmission model based on PSCAD/EMDTC, under a variety of different fault distances, fault types, fault resistances and loading angles, respectively. The simulation results show that in comparison with conventional DFT-based protection methods, by which the fault distance is calculated using phasor measurements of voltage and current signals in a fixed-length window, ADRA requires much fewer samples to achieve a same degree of the accuracy of fault distance calculation, which enables much faster tripping, and its protection zone can be extended as more samples are used.",2009,0,
750,751,Fault-tolerant defect prediction in high-precision foundry,"High-precision foundry production is subjected to rigorous quality controls in order to ensure a proper result. Such exams, however, are extremely expensive and only achieve good results in a posteriori fashion. In previous works, we presented a defect prediction system that achieved a 99% success rate. Still, this approach did not take into account sufficiently the geometry of the casting part models, resulting in higher raw material requirements to guarantee an appropriate outcome. In this paper, we present here a fault-tolerant software solution for casting defect prediction that is able to detect possible defects directly in the design phase by analysing the volume of three-dimensional models. To this end, we propose advanced algorithms to recreate the topology of each foundry part, analyze its volume and simulate the casting procedure, all of them specifically designed for an robust implementation over the latest graphic hardware that ensures an interactive design process.",2010,0,
751,752,Bounds on the Decoding Error Probability of Binary Block Codes over Noncoherent Block AWGN and Fading Channels,"We derive upper bounds on the decoding error probability of binary block codes over noncoherent block additive white Gaussian noise (AWGN) and fading channels, with applications to turbo codes. By a block AWGN (or fading) channel, we mean that the carrier phase (or fading) is assumed to be constant over each block but independently varying from one block to another. The union bounds are derived for both noncoherent block AWGN and fading channels. For the block fading channel with a small number of fading blocks, we further derive an improved bound by employing Gallager's first bounding technique. The analytical bounds are compared to the simulation results for a coded block-based differential phase shift keying (B-DPSK) system under a practical noncoherent iterative decoding scheme proposed by Chen et al. We show that the proposed Gallager bound is very tight for the block fading channel with a small number of fading blocks, and the practical noncoherent receiver performs well for a wide range of block fading channels",2006,0,
752,753,ConfErr: A tool for assessing resilience to human configuration errors,"We present ConfErr, a tool for testing and quantifying the resilience of software systems to human-induced configuration errors. ConfErr uses human error models rooted in psychology and linguistics to generate realistic configuration mistakes; it then injects these mistakes and measures their effects, producing a resilience profile of the system under test. The resilience profile, capturing succinctly how sensitive the target software is to different classes of configuration errors, can be used for improving the software or to compare systems to each other. ConfErr is highly portable, because all mutations are performed on abstract representations of the configuration files. Using ConfErr, we found several serious flaws in the MySQL and Postgres databases, Apache web server, and BIND and djbdns name servers; we were also able to directly compare the resilience of functionally-equivalent systems, such as MySQL and Postgres.",2008,0,
753,754,Diagnostic and protection of inverter faults in IPM motor drives using wavelet transform,"This paper presents a novel faults diagnostic and protection technique for interior permanent magnet (IPM) motor drives using wavelet transform. The proposed wavelet based diagnostic and protection technique for inverter faults is developed and implemented in real-time for a voltage source inverter fed IPM motor. In the proposed technique, the motor currents of different faulted and unfaulted conditions of an IPM motor drive system are preprocessed by wavelet packet transform. The wavelet packet transformed coefficients of motor currents are used as inputs of a three-layer wavelet neural network. The performances of the proposed diagnostic and protection technique are investigated in simulation and experiments. The proposed technique is experimentally tested on a laboratory 1-hp IPM motor drive using the ds1102 digital signal processor board. The test results showed satisfactory performances of the proposed diagnostic and protection technique in terms of speed, accuracy and reliability.",2008,0,
754,755,FTDIS: A Fault Tolerant Dynamic Instruction Scheduling,"In this work, we target the robustness for controller scheduler of type Tomasulo for SEU faults model. The proposed fault-tolerant dynamic scheduling unit is named FTDIS, in which critical control data of scheduler is protected from driving to an unwanted stage using Triple Modular Redundancy and majority voting approaches. Moreover, the feedbacks in voters produce recovery capability for detected faults in the FTDIS, enabling both fault mask and recovery for system. As the results of analytical evaluations demonstrate, the implemented FTDIS unit has over 99% fault detection coverage in the condition of existing less than 4 faults in critical bits. Furthermore, based on experiments, the FTDIS has a 200% hardware overhead comparing to the primitive dynamic scheduling control unit and about 50% overhead in comparision to a full CPU core. The proposed unit also has no performance penalty during simulation. In addition, the experiments show that FTDIS consumes 98% more power than the primitive unit.",2010,0,
755,756,Single-switch power factor correction AC/DC converter with storage capacitor size reduction,"In universal line applications with hold-up time requirement, the single-stage PFC AC/DC converters may not be more attractive than the conventional two-stages approach if the size and cost of the storage capacitor are too high. Furthermore, computer related applications, in which the holdup time is a very important requirement, will have to comply with Class D limits of the low frequency harmonic regulation IEC 61000-3-2. Therefore, for these applications, a not very distorted line current will be required. In this paper, a new single-stage AC/DC converter suitable for universal line applications is proposed. The main difference with other solutions is the low voltage swing on the storage capacitor while the line varies within its universal range. This feature allows reducing the size and cost of the storage capacitor. Additional advantages of the proposed converter are topology simplicity (single-switch converter) and IEC 61000-3-2 Class D compliance. The experimental results confirms the above mentioned advantages.",2003,0,
756,757,A Java API for advanced faults management,"The paper proposes an alternative for modeling managed resources using Java and telecommunication network management standards. It emphasizes functions related to fault management, namely: diagnostic testing and performance monitoring. Based on Java management extension (JMX<sup>TM</sup>), specific extensions are proposed to facilitate diagnostic testing and performance measurements implementation. The new API also called Java fault management extension (JFMX) consists of managed objects that model real resources being tested or monitored and support objects defined for the need of diagnostic testing and performance measurements. The paper discusses four Java implementations of a 3-tier client/server scenario focusing on the SystemUnderTest package of the new API to instrument a minimalist managed system scenario. These implementations are respectively built on top of the following Java based communication infrastructures: JMX/JFMX, RMI, CORBA/Java, and Voyager<sup>TM</sup>. The paper extends the Voyager implementation with JMX/JFMX and uses their dynamic and advanced features to provide a highly efficient solution. The later implementation also uses the mobile agent paradigm to overcome well-known limitations of the RPC based implementations",2001,0,
757,758,Operating system function reuse to achieve low-cost fault tolerance,The aim of this article is to propose a new approach to fault tolerance in single processor embedded systems which is centred on the operating system. Particular attention is put on low-cost techniques that exploit functions already present in the system in a different than-usual way to achieve protection with little or no intervention at the application level. An example is given: the realization of a checkpoint and rollback scheme through the context switch function.,2004,0,
758,759,PSC-PWM in fault tolerant drive system for EMA operation,The introduction of EMA Systems requires the use of redundant inverters to drive the EMA and ensure reliability and safety. Redundant converters allow the implementation of fault tolerant control and high quality operation. Fault control has been implemented by means of redundant converter and fault detection system.,2010,0,
759,760,A fault-tolerant real-time supervisory scheme for an interconnected four-tank system,"In this paper, the implementation of a Command Governor (CG) strategy on a real-time computing system is described for the supervision of a laboratory four-tank test-bed. In particular, the real-time architecture has been developed on the RTAI/Linux operating system kernel and the CG module has been implemented in C++ on a general purpose off-the-shelf computing unit. An accurate model of the the four-tank process has been derived from both physical and experimental data and the applicability of the proposed method has been proved by means of real-time tests, which testified on the CG strategy ability to enforce the prescribed operative constraints even under unexpected adverse conditions, e.g. water pumps failures.",2010,0,
760,761,Evaluation of fault tolerance latency from real-time application's perspectives,"Information on Fault Tolerance Latency (FTL), which is defined as the total time required by all sequential steps taken to recover from an error, is important to the design and evaluation of fault-tolerant computers used in safety-critical real-time control systems with deadline information. In this paper, we evaluate FTL in terms of several random and deterministic variables accounting for fault behaviors and/or the capability and performance of error-handling mechanisms, while considering various fault tolerance mechanisms based on the trade-off between temporal and spatial redundancy, and use the evaluated FTL to check if an error-handling policy can meet the Control System Deadline (CSD) for a given real-time application",2000,0,
761,762,Research of Remote Fault Diagnosis System Based on Internet,"The technology of intelligent multi-agents is applied to design remote fault diagnosis system based on Internet. The system owns the kernel of the remote fault diagnosis platform, (RFDP) and the members of manufacturers and enterprise client. It is a distributed, remote monitoring and on-line diagnosis system. The system has overcome the function limitations of 2 kinds of traditional client/server architecture, equipment client-end remote diagnostic mode and manufacturer-end remote diagnostic mode. The platform has a rapid diagnosis response and makes it easy to realize information transmitting timely between platform and diagnosis members. For this reason, RFDP which holds a great ability for enabling on-line monitoring, general fault diagnosis, repairing service and updating knowledge base rapidly, can give a good service for remote distributed multi- equipments and multi-manufactures. As a common diagnosis platform, the system can be applied to various remote mechanical and electronic equipment diagnosis areas such as CNC and press machine diagnosis.",2007,0,
762,763,Application of compensation method in calculating symmetrical short circuit fault,"Based on compensation method, Symmetrical short-circuit fault current formula and nodal voltage formula are deduced in this paper. In the deduction process the time of calculating matrix inversion is eliminated for the node-admittance matrix being not modified when the fault occurs, however triangular matrix method is applied to calculate nodal impedance matrix at the program entrance in the process based on the original network nodal admittance matrix, thus the solution of the electrical network state variables is speed up with preparing data for fault calculation in advance. After further assumptions and simplification, short-circuit current formula is more efficient to estimate the size of short-circuit current, and it is very suitable for real-time online applications. At the same time, current coefficient power contributing which is different from power current distribution coefficient is put forward. A optimal node is found for new power supply to limit short-circuit current by judging the size of current coefficient power contributing.",2010,0,
763,764,On the design of error detection and correction cryptography schemes,"The paper introduces the method of modifying cryptography encryption and decryption units, which includes circuitry of checks that operations have been performed without errors. This technique is based on addition to storage devices, error correction codes, and module check of arithmetic and logic units operations",2000,0,
764,765,A platooning controller robust to vehicular faults,This paper presents a platooning controller for a four-wheel-driving four-wheel-steering vehicle to follow another. The controller is based on the full-state tracking theory and utilizes a vehicular model that makes it able to continue to operate when faults are detected at its steering systems or driving motors which are disabled accordingly. The unified controller is also able to track and follow the target either moving forward in front or moving backward in the back of the vehicle making the real-time implementation of different tracking modes simple. Tracking stability is secured by the proper selection of design parameters. Simulations show the proposed control scheme works properly even in the presence of faults at several different parts.,2004,0,
765,766,A new automated instrumentation for emulation-based fault injection,"Soft errors are an increasing threat in up-to-date technologies, so robustness evaluation has become an important part of digital circuit design. Emulation-based fault injection techniques have proved to be an efficient approach to perform such evaluations. In this paper, we propose new optimizations further improving the experimental duration and the instrumentation cost while maintaining the maximum flexibility for the dependability evaluation process.",2010,0,
766,767,Automatic Generation of Detection Algorithms for Design Defects,"Maintenance is recognised as the most difficult and expansive activity of the software development process. Numerous techniques and processes have been proposed to ease the maintenance of software. In particular, several authors published design defects formalising ""bad"" solutions to recurring design problems (e.g., anti-patterns, code smells). We propose a language and a framework to express design defects synthetically and to generate detection algorithms automatically. We show that this language is sufficient to describe some design defects and to generate detection algorithms, which have a good precision. We validate the generated algorithms on several programs",2006,0,
767,768,Analysis of fault-tolerant five-phase IPM synchronous motor,"The choice of a multi-phase motor is a potentially fault-tolerant solution and gives rise to many advantages, respect to the traditional three-phase motor drives. In this paper an high torque density five-phase IPM synchronous motor has been studied, and the motor performance have been evaluated in the case of healthy-mode and faulty-mode operation.",2008,0,
768,769,Space shuttle fault tolerance: Analog and digital teamwork,"The Space Shuttle control system (including the avionics suite) was developed during the 1970s to meet stringent survivability requirements that were then extraordinary but today may serve as a standard against which modern avionics can be measured. In 30 years of service, only two major malfunctions have occurred, both due to failures far beyond the reach of fault tolerance technology: the explosion of an external fuel tank, and the destruction of a launch-damaged wing by re-entry friction. The Space Shuttle is among the earliest systems (if not the earliest) designed to a ldquoFO-FO-FSrdquo criterion, meaning that it had to Fail (fully) Operational after any one failure, then Fail Operational after any second failure (even of the same kind of unit), then Fail Safe after most kinds of third failure. The computer system had to meet this criterion using a Redundant Set of 4 computers plus a backup of the same type, which was (ostensibly!) a COTS type. Quadruple redundancy was also employed in the hydraulic actuators for elevons and rudder. Sensors were installed with quadruple, triple, or dual redundancy. For still greater fault tolerance, these three redundancies (sensors, computers, actuators) were made independent of each other so that the reliability criterion applies to each category separately. The mission rule for Shuttle flights, as distinct from the design criterion, became ldquoFO-FS,rdquo so that a mission continues intact after any one failure, but is terminated with a safe return after any second failure of the same type. To avoid an unrecoverable flat spin during the most dynamic flight phases, the overall system had to continue safe operation within 400 msec of any failure, but the decision to shut down a computer had to be made by the crew. Among the interesting problems to be solved were ldquocontrol sliveringrdquo and ldquosync holes.rdquo The first flight test (Approach and Landing only) was the proof of the pudding: when a key wire harness solder - joint was jarred loose by the Shuttle's being popped off the back of its 747 mother ship, one of the computers ldquowent bananasrdquo (actual quote from an IBM expert).",2009,0,
769,770,Assessing the impact of active guidance for defect detection: a replicated experiment,"Scenario-based reading (SBR) techniques have been proposed as an alternative to checklists to support the inspectors throughout the reading process in the form of operational scenarios. Many studies have been performed to compare these techniques regarding their impact on the inspector performance. However, most of the existing studies have compared generic checklists to a set of specific reading scenarios, thus confounding the effects of two SBR key factors: separation of concerns and active guidance. In a previous work we have preliminarily conducted a repeated case study at the University of Kaiserslautern to evaluate the impact of active guidance on inspection performance. Specifically, we compared reading scenarios and focused checklists, which were both characterized as being perspective-based. The only difference between the reading techniques was the active guidance provided by the reading scenarios. We now have replicated the initial study with a controlled experiment using as subjects 43 graduate students in computer science at University of Bari. We did not find evidence that active guidance in reading techniques affects the effectiveness or the efficiency of defect detection. However, inspectors showed a better acceptance of focused checklists than reading scenarios.",2004,0,
770,771,A Reducing Transmission-Line Fault Current Method,"In this paper, a reducing transmission-line fault current method with capacitor compensators is proposed to limit the transmission-line fault current in power systems. In the normal mode of operation, the shunt capacitors banks as reactive power compensators that delivers reactive power to increase the power factor and used on medium-length and long transmission lines to increase line loadability and to maintain voltages near rated values. Their important effect is to reduce line-voltage drops and to increase the power factor and the steady-state stability limit. When faults states occurs, the capacitor another effect is to reduce transmission-line fault current peak value. Simulations performed in MATLAB/Simulink environment indicate that the proposed performance for capacitor compensators performs well to limit the fault currents of transmission lines and line-voltage drops.",2010,0,
771,772,Recovery of fault-tolerant real-time scheduling algorithm for tolerating multiple transient faults,"The consequences of missing deadline of hard real time system tasks may be catastrophic. Moreover, in case of faults, a deadline can be missed if the time taken for recovery is not taken into account during the phase when tasks are submitted or accepted to the system. However, when faults occur tasks may miss deadline even if fault tolerance is employed. Because when an erroneous task with larger execution time executes up to end of its total execution time even if the error is detected early, this unnecessary execution of the erroneous task provides no additional slack time in the schedule to mitigate the effect of error by running additional copy of the same task without missing deadline. In this paper, a recovery mechanism is proposed to augment the fault-tolerant real-time scheduling algorithm RM-FT that achieves node level fault tolerance (NLFT) using temporal error masking (TEM) technique based on rate monotonic (RM) scheduling algorithm. Several hardware and software error detection mechanisms (EDM), i.e. watchdog processor or executable assertions, can detect an error before an erroneous task finishes its full execution, and can immediately stops execution. In this paper, using the advantage of such early detection by EDM, a recovery algorithm RM-FT-RECOVERY is proposed to find an upper bound, denoted by Edm Bound, on the execution time of the tasks, and mechanism is developed to provide additional slack time to a fault-tolerant real-time schedule so that additional task copies can be scheduled when error occurs.",2007,0,
772,773,A Method to Evaluate Voltages to Earth During an Earth Fault in an HV Network in a System of Interconnected Earth Electrodes of MV/LV Substations,"An easy and swift method to evaluate, in a system of interconnected earth electrodes, earth potentials on earthing systems of medium-voltage/low-voltage (MV/LV) substations, in an event of single-line-to-earth fault inside a high-voltage/medium- voltage (HV/MV) station, is presented. The advantage of the method is the simplicity of the mathematical model for solving complex systems of any size with a sufficient accuracy for practical purposes. This paper shows the results of simulations, performed on networks with different extensions and characteristics, organized in easy-to-read graphs and tables. A comparison of these results with the values obtained according to the procedure explained in the IEC-Standard 60909-3, and a study on the accuracy of the method has been made. Moreover, some considerations on the inclusion of earth electrodes of HV/MV stations within global earthing systems are done.",2008,0,
773,774,Characterization of Upset-induced Degradation of Error-mitigated Highspeed I/O's Using Fault Injection,"Fault-injection experiments on Virtex-II FPGAs quantify failure and degradation modes in I/O channels incorporating triple modular redundancy (TMR). With increasing frequency (to 100 MHz), full TMR under both I/O standards investigated shows more configuration bits have a measurable performance effect.",2005,0,
774,775,Fault detection and protection system for the power converters with high-voltage IGBTs,"This paper addresses problems related to the design and implementation of a fault detection and protection system for high-voltage (HV) NPT IGBT-based converters. An isolated half-bridge power converter topology is investigated, which seems to be very attractive for the high-power electronic converters due to its overall simplicity, small component count and low realization costs. This converter is to be applied in rolling stock with its demanding reliability and safety requirements. Clearly, the robust control and protection system is essential.",2008,0,
775,776,All Bits Are Not Equal - A Study of IEEE 802.11 Communication Bit Errors,"In IEEE 802.11 Wireless LAN (WLAN) systems, techniques such as acknowledgement, retransmission, and transmission rate adaptation, are frame-level mechanisms designed for combating transmission errors. Recently sub-frame level mechanisms such as frame combining have been proposed by the research community. In this paper, we present results obtained from our bit error study for identifying sub-frame error patterns because we believe that identifiable bit error patterns can potentially introduce new opportunities in channel coding, network coding, forward error correction (FEC), and frame combining mechanisms. We have constructed a number of IEEE 802.11 wireless LAN testbeds and conducted extensive experiments to study the characteristics of bit errors and their location distribution. Conventional wisdom dictates that bit error probability is the result of channel condition and ought to follow corresponding distribution. However our measurement results identify three repeatable bit error patterns that are not induced by channel conditions. We have verified that such error patterns are present in WLAN transmissions in different physical environments and across different wireless LAN hardware platforms. We also discuss our current hypotheses for the reasons behind these bit error probability patterns and how identifying these patterns may help improving WLAN transmission robustness.",2009,0,
776,777,Evaluation of replication and fault detection in P2P-MPI,"We present in this paper an evaluation of fault management in the grid middleware P2P-MPI. One of P2P-MPI's objective is to support environments using commodity hardware. Hence, running programs is failure prone and a particular attention must be paid to fault management. The fault management covers two issues: fault-tolerance and fault detection. P2P-MPI provides a transparent fault tolerance facility based on replication of computations. Fault detection concerns the monitoring of the program execution by the system. The monitoring is done through a distributed set of modules called failure detectors. In this paper, we report results from several experiments which show the overhead of replication, and the cost of fault detection.",2009,0,
777,778,Testing of LUT delay aliasing faults in SRAM based FPGAs using half-frequencies,"In this paper, we present a technique for testing the delay aliasing faults associated with LUTs in SRAM based FPGAs. We compare the outputs of two identical LUTs when one is operated at half the frequency of the other. A Built in Self Test (BIST) circuitry consisting of a Test Pattern Generator, a Comparator, and the Circuit Under Test (CUT) is mapped on the FPGA. Application of input sequence vectors at half frequencies to the LUTs enable the detection of delay and aliasing faults which may go undetected by other techniques. The technique is verified using VHDL based simulations. The results are also experimentally verified using a Virtex II FPGA board.",2007,0,
778,779,Experimental studies on faults detection using residual generator,"In this paper one will develop the faults detection and localization method using residual vectors, in order to emphasize the noises, disturbances and faults on the outputs L<inf>1</inf> and L<inf>2</inf> of the control level plant with two coupled tanks Quanser Water Level Control Two Tank Module. The proposed method was theoretically developed and experimentally verified in this plant and allowed detection and localization of two faults created in a real plant. The experiments presented were realized using Matlab Simulink program.",2010,0,
779,780,Efficient Memory Error Coding for Space Computer Applications,"For the secure transaction of data between the central processing unit (CPU) of a satellite on board-computer and its local random access memory (RAM), the program memory has been usually designed with triple modular redundancy (TMR), which is a hardware implementation that includes replicated memory circuits and voting logic to detect and correct a faulty value. TMR error correction technique allows single correction of one error bit per stored word. For computers on board a satellite, there is however a definite risk of two error bits occurring within one byte of stored data. In this paper, the application of the quasi-cyclic codes to the routine error protection of SRAM program memory for satellites in low Earth orbit is described and implemented in field programmable gate array (FPGA) technology. The proposed device is transparent to the routine transfer of data between CPU and its local RAM",2006,0,
780,781,Immune Systems Inspired Approach to Anomaly Detection and Fault Diagnosis for Engines,"As more electronic devices are integrated into automobiles to improve the reliability, drivability and maintainability, automotive diagnosis becomes increasingly difficult to deal with. Unavoidable design defects, quality variations in the production process as well as different usage patterns make it is infeasible to foresee all possible faults that may occur to the vehicle. As a result, many systems rely on limited diagnostic coverage provided by a diagnostic strategy which tests only for a priori known or anticipated failures, and presumes the system is operating normally if the full set of tests is passed. To circumvent these difficulties and provide a more complete coverage for detection of any fault, a new paradigm for design of automotive diagnostic systems is needed. An approach inspired by the functionalities and characteristics of natural immune system is presented and discussed in the paper. The feasibility of the newly proposed paradigm is also partially demonstrated through application examples.",2007,0,
781,782,Research on Optimal Placement of Travelling Wave Fault Locators in Power Grid,"Taking the full network observability of power system operation state, maximum state measurement redundancy and minimum number of travelling wave fault location device (TFD) as objectives, an TFD optimal placement scheme for power grid fault location with travelling wave is presented in the paper. The scheme contains two steps: static processing and dynamic configuration. Terminal substations should install TFD. Then taking the terminal substations as starting point, the whole network is separated into several unattached branches. The branch which includes the most number of substations, via the longest line, and has not any loop, can install TFD at its both terminals. And then combining with the practical length of each line and coverage range of substations, the optimal disposition of TFD can successfully accomplish. A novel network-based fault location algorithm is also designed with travelling wave velocity on-line measuring and every TFD recorded travelling arrival times fusing. EMTP simulation results show that the TFD optimal placement scheme can use less TFDs to locate all faults in the whole power grid with economy and high reliability. The location error is no more than 100 m.",2008,0,
782,783,A Unity Power Factor Correction Preregulator with Fast Dynamic Response Based on a Low-Cost Microcontroller,"Low cost passive Power Factor Correction (PFC) and Single-Stage PFC converters cannot draw a sinusoidal input current and are only suitable solutions to supply low power levels. PFC preregulators based on the use of a multiplier solve such drawbacks, but a second stage DC/DC converter is needed to obtain fast output voltage dynamics. The output voltage response of PFC preregulators can be improved by increasing the corner frequency of the output voltage feedback loop. The main drawback to obtaining a faster converter output response is the distortion of the input current. This paper describes a simple control strategy to obtain a sinusoidal input current. Based on the static analysis of output voltage ripple, a modified sinusoidal reference is created using a low cost microcontroller in order to obtain a input sinusoidal current. This reference replaces the traditional rectified sinusoidal input voltage reference in PFC preregulators with multiplier control. Using this circuitry, PFC preregulator topologies with galvanic isolation are suitable solutions to design a power supply with fast output voltage response (10 ms or 8.33 ms) and low line current distortion. Finally, theoretical and simulated results are validated using a 500 W prototype.",2007,0,
783,784,Adaptive and Fault Tolerant Simulation of Relativistic Particle Transport with Data-Level Checkpointing,"Many scientific applications exhibit high demands on memory storage and computing capability. Improvements in commodity processors and networks have provided an opportunity to support such scientific applications within an everyday computing infrastructure. Good applications need the ability to work in constantly changing environments. Adaptability and fault tolerance are essential. Based on simulation of relativistic particle transport, this paper proposes a data-level checkpointing scheme for common scientific applications. This scheme takes advantage of the regular program layout, dominant computing loops, and fine-grained iterations. Without handling stack and heap segments directly, only application data is saved and restored as the computation state. Checkpointing interval can be dynamically adjusted to satisfy sensitivity and efficiency requirements for feasible fault tolerance. With this periodic but fixed-location checkpointing scheme, the MPI- based simulation system can be reconfigured by being shut down first and then restarted on same or different computer clusters. Application data can be redistributed for the new configuration. Experimental results have demonstrated this scheme's efficiency and effectiveness.",2008,0,
784,785,An intelligent FFT-analyzer with harmonic interference effect correction and uncertainty evaluation,"In the paper, the problem of the correction of harmonic interference effects on FFT results is discussed. A procedure for the effect evaluation and correction is proposed and implemented in an intelligent FFT-analyzer able also to provide the results with their uncertainty.",2003,0,
785,786,Forward error correction strategies for media streaming over wireless networks,"The success of next-generation mobile communication systems depends on the ability of service providers to engineer new added-value multimedia-rich services, which impose stringent constraints on the underlying delivery/transport architecture. The reliability of real-time services is essential for the viability of any such service offering. The sporadic packet loss typical of wireless channels can be addressed using appropriate techniques such as the widely used packet-level forward error correction. In designing channel-aware media streaming applications, two interrelated and challenging issues should be tackled: accuracy of characterizing channel fluctuations and effectiveness of application-level adaptation. The first challenge requires thorough insight into channel fluctuations and their manifestations at the application level, while the second concerns the way those fluctuations are interpreted and dealt with by adaptive mechanisms such as FEC. In this article we review the major issues that arise when designing a reliable media streaming system for wireless networks.",2008,0,
786,787,Tight exponential upper bounds on the ML decoding error probability of block codes over fully interleaved fading channels,"We derive tight exponential upper bounds on the decoding error probability of block codes which are operating over fully interleaved Rician fading channels, coherently detected and maximum-likelihood decoded. It is assumed that the fading samples are statistically independent and that perfect estimates of these samples are provided to the decoder. These upper bounds on the bit and block error probabilities are based on certain variations of the Gallager bounds. These bounds do not require integration in their final version and they are reasonably tight in a certain portion of the rate region exceeding the cutoff rate of the channel. By inserting interconnections between these bounds, we show that they are generalized versions of some reported bounds for the binary-input additive white Gaussian noise channel.",2003,0,
787,788,Fault location using traveling wave for power networks,"Fault location using traveling wave has been applied in extra-high voltage power grids successfully. Due to its complication and high cost, it is not easy for this technique to be accepted for use in distribution system. A new traveling wave fault location system is developed simply in a cost-effective way for power networks (especially for distribution system) in this paper. Two traveling wave sensors are developed to capture the current traveling wave flowing from the capacitive equipment to earth and the voltage traveling waves in all three phases. The outputs of the sensors are then applied to the trigger and time tagging by using Global Position System (GPS) receiver. The fault position is calculated by the traveling wave arrival times in every power station where only one fault locator is installed. The fault location system is tested in the power system. Testing results show that the fault locator has high precision and robustness.",2004,0,
788,789,Novel method for selective detection of earth faults in high impedance grounded distribution networks,"An elementary and reliable detection of earth faults in impedance grounded networks results in considerable benefits for the utility both in terms of outage duration and personal safety. This report describes an entirely new, only current measuring method; a method, which fulfils the standards of cost efficiency and reliability. Despite the seeming simplicity of the approach it is also demonstrated that it is an excellent method to detect arcing cable earth faults.",2005,0,
789,790,Fault Tolerance of Tornado Codes for Archival Storage,"This paper examines a class of low density parity check (LDPC) erasure codes called Tornado codes for applications in archival storage systems. The fault tolerance of Tornado code graphs is analyzed and it is shown that it is possible to identify and mitigate worst-case failure scenarios in small (96 node) graphs through use of simulations to find and eliminate critical node sets that can cause Tornado codes to fail even when almost all blocks are present. The graph construction procedure resulting from the preceding analysis is then used to construct a 96-device Tornado code storage system with capacity overhead equivalent to RAID 10 that tolerates any 4 device failures. This system is demonstrated to be superior to other parity-based RAID systems. Finally, it is described how a geographically distributed data stewarding system can be enhanced by using cooperatively selected Tornado code graphs to obtain fault tolerance exceeding that of its constituent storage sites or site replication strategies",2006,0,
790,791,Floating-point error analysis based on affine arithmetic,"During the development of floating-point signal processing systems, an efficient error analysis method is needed to guarantee the output quality. We present a novel approach to floating-point error bound analysis based on affine arithmetic. The proposed method not only provides a tighter bound than the conventional approach, but also is applicable to any arithmetic operation. The error estimation accuracy is evaluated across several different applications which cover linear operations, nonlinear operations, and feedback systems. The accuracy decreases with the depth of computation path and also is affected by the linearity of the floating-point operations.",2003,0,
791,792,A novel approach to faulted-phase selection using current traveling waves and wavelet analysis,"The early traveling wave faulted-phase selectors, due to lack of effective tool to process transient signals, had to directly used instantaneous values of signals so that they cannot overcome such bad influence as noise disturbance. Fortunately, wavelet analysis, with its time-frequency localization ability and the wavelet transform modulus maxima (WTMM) concept, is well suited to treat with singularity of fault-generated traveling waves in EHV/UHV transmission lines. This paper presents a novel approach to fast and accurate phase-selection, which used the WTMM of initial model current traveling waves according to the fault characteristic relations deduced from the boundary conditions of various types of faults. The criterion is explicit in characteristics and physical concepts, and is apt to be realized. A large number of EMTP simulations demonstrated the new faulted-phase selection algorithm.",2002,0,
792,793,A Case Study of Bias in Bug-Fix Datasets,"Software quality researchers build software quality models by recovering traceability links between bug reports in issue tracking repositories and source code files. However, all too often the data stored in issue tracking repositories is not explicitly tagged or linked to source code. Researchers have to resort to heuristics to tag the data (e.g., to determine if an issue is a bug report or a work item), or to link a piece of code to a particular issue or bug. Recent studies by Bird et al. and by Antoniol et al. suggest that software models based on imperfect datasets with missing links to the code and incorrect tagging of issues, exhibit biases that compromise the validity and generality of the quality models built on top of the datasets. In this study, we verify the effects of such biases for a commercial project that enforces strict development guidelines and rules on the quality of the data in its issue tracking repository. Our results show that even in such a perfect setting, with a near-ideal dataset, biases do exist - leading us to conjecture that biases are more likely a symptom of the underlying software development process instead of being due to the used heuristics.",2010,0,
793,794,Concurrent and simple digital controller of an AC/DC converter with power factor correction based on an FPGA,"Nowadays, most digital controls for power converters are based on DSPs. This paper presents a field programmable gate array (FPGA) based digital control for a power factor correction (PFC) flyback AC/DC converter. The main difference from DSP-based solutions is that FPGAs allow concurrent operation (simultaneous execution of all control procedures), enabling high performance and novel control methods. The control algorithm has been developed using a hardware description language (VHDL), which provides great flexibility and technology independence. The controller has been designed as simple as possible while maintaining good accuracy and dynamic response. Simulations and experimental results show the feasibility of the method, opening interesting possibilities in power converters control.",2003,0,
794,795,Feature set evaluation and fusion for motor fault diagnosis,"This paper proposes a novel approach to the feature fusion in motor fault diagnosis with the main aim of improving the performance and reliability of clustering and identification of the fault patterns. In addition, the significance of individual feature sets in specific fault scenarios, which is normally gained by engineers through experience, is investigated by using flexible Non-Gaussian modeling of the historical data. Furthermore the comparison is made by applying individual and fusion of feature sets to the probabilistic distributions of trained models using a Maximum a Posteriori (MAP) approach. To carry out the task, current waveforms are collected non-invasively from three-phase DC motors. Waveforms are then compressed into time, frequency and wavelet feature sets to form the input to the clustering algorithm. The result demonstrates the suitability of specific feature sets in different motor modes and the efficiency of fusion which is carried out with a Winner Takes All (WTA) approach.",2010,0,
795,796,High-speed serial communication with error correction using 0.25 m CMOS technology,"In this paper we propose a novel design for an autonomous high-speed serial off and on-chip communication system which incorporates impedance tuning, error correction with a packet transfer and a parallel asynchronous interface. The constructed transmitter-receiver pair has throughput of 5 Gbit/s. With error correction and packet transfer overhead accounted for this construct has bandwidth of 500 <bytes/s. The circuit has been simulated using HSpice with 0.25 m TMSC CMOS technology",2001,0,
796,797,"Attacking ""bad actor"" and ""no fault found"" electronic boxes","A the percentage of what are termed ""bad actor"" and no fault found (NFF) electronic box in military weapon systems is steadily growing. These are boxes that fail during operation, but test NFF during back shop testing, or, that fail during back shop testing and then test NFF at the depot repair facility. During operation, an electronic box is stressed by various environmental conditions which are normally absent on a test bench. If there are cold or cracked solder joints, corroded or dirty connector contacts, loose crimp joints, hairline cracks in a ribbon cable trace, or other intermittent conditions, the intermittency can occur while the box is under stress conditions, yet seldom occur while the box is on a test bench at room temperature. Very little concerted effort is currently focused on detecting, isolating and repairing these intermittent problems. Virtually all testing activity simply tests the unit for normal operation, one function, one circuit, or one set of circuits at a time. If an intermittent circuit is not displaying its intermittent nature at the instant it is being tested, the intermittency remains undetected. A three-pronged effort is currently underway to attack and repair bad actor and NFF electronic boxes. The first is to collect detailed repair data to identify which boxes are bad actor and NFF units. The second is to collect test data to determine which units yield inconsistent test results between back shop testing and depot testing, and why. The third is to employ a system that detects and isolates electronic box intermittent circuits. This paper describes the success realized to date by employing each of the three techniques described above, and how they are now effectively being employed together to reduce maintenance costs and improve avionics reliability for the F-16 weapon system.",2005,0,
797,798,Software-Implemented Fault Injection at Firmware Level,"Software-implemented fault injection is an established method to emulate hardware faults in computer systems. Existing approaches typically extend the operating system by special drivers or change the application under test. We propose a novel approach where fault injection capabilities are added to the computer firmware. This approach can work without any modification to operating system and / or applications, and can support a larger variety of fault locations. We discuss four different strategies in X86/X64 and Itanium systems. Our analysis shows that such an approach can increase portability, the non-intrusiveness of the injector implementation, and the number of supported fault locations. Firmware-level fault injection paves the way for new research directions, such as virtual machine monitor fault injection or the investigation of certified operating systems.",2010,0,
798,799,Application of neural networks and filtered back projection to wafer defect cluster identification,"During an electrical testing stage, each die on a wafer must be tested to determine whether it functions as it was originally designed. In the case of a clustered defect on the wafer, such as scratches, stains, or localized failed patterns, the tester may not detect all of the defective dies in the flawed area. To avoid the defective dies proceeding to final assembly, an existing tool is currently used by a testing factory to detect the defect cluster and mark all the defective dies in the flawed region or close to the flawed region; otherwise, the testing factory must assign five to ten workers to check the wafers and hand mark the defective dies. This paper proposes two new wafer-scale defect cluster identifiers to detect the defect clusters, and compares them with the existing tool used in the industry. The experimental results verify that one of the proposed algorithms is very effective in defect identification and achieves better performance than the existing tool.",2002,0,
799,800,Proactive fault management based on risk-augmented routing,"Carrier networks need to provide their customers with high availability of communication services. Unfortunately, failures are managed by recovery mechanisms getting involved only after the failure occurrence to limit the impact on traffic flows. However, there are often forewarning signs that a network device will stop working properly. We propose to take into account this risk exposure in order to improve the performance of the existing restoration mechanisms, in particular for IP networks. Based on an embedded and real-time risk-level assessment, we can perform a proactive fault-management and isolate the failing routers out of the routed topology, and thus totally avoid service unavailability. Our novel approach enables routers to preventively steer traffic away from risky paths by temporally tuning OSPF link cost.",2010,0,
800,801,Optimized resource allocation in grid networks using genetic algorithm with error rate factor,"Grid computing is an emerging computing paradigm that will have significant impact on the next generation information infrastructure. Due to the largeness and complexity of grid system, its quality of service, performance and reliability are difficult to model, analyze and evaluate. In real time evaluation, various noises will influence the model and which in turn accounts for increase in packet loss and Bit Error Rate (BER). Therefore, a novel optimization model for maximizing the expected grid service profit is mandatory. In our work, to achieve the improvement in the end to end grid network performance, an optimizer, which is based on Genetic Algorithm (GA) with Fitness Evaluation parameters considers BER and Service Execution Time, is designed in the RMS. This paper presents the novel tree structured model, is better than other existing models for grid computing performance and reliability analysis by not only considering data dependence and failure correlations, but also takes link failure, packet loss & BER real time parameters in account. The algorithm based on the Graph theory and Probability theory.",2009,0,
801,802,Improving SNR for DSM Linear Systems Using Probabilistic Error Correction and State Restoration: A Comparative Study,"Smaller feature sizes and lower supply voltages make DSM devices more susceptible to soft errors generated by alpha particles and neutrons as well as other sources of environmental noise. In this scenario, soft-error/noise tolerant techniques are necessary for maintaining the SNR of critical DSP applications. This paper studies linear DSP circuits and discusses two low cost techniques for improving the SNR of DSP filters. Both techniques use a single checksum variable for error detection. This gives a distance two code that is traditionally good for error detection but not correction. In this paper, such a code is used to improve SNR rather than perfectly remove the error. The first technique, 'checksum-based probabilistic error correction', uses the value indicated by the checksum variable to probabilistically correct the error and achieves up to 5 dB improvement in the SNR value. The second technique, 'state restoration', works well when the length of burst errors is small and the error magnitude is large. A general error statistics has been defined as a random process and the distribution of SNR is compared for the two proposed techniques",2006,0,
802,803,Simulation of Elman Neural network Extension Strategy Generator to Pattern Deformation Error in Flexibility Material Treating Field,"After analyzing flexibility material processing (such as quilting processing) influencing factor of pattern deformation, the edges of the original image and the deformation image are extracted. Then they are changed into coordinate. On top of it, the data are put into the Elman neural networks to train which has been built and the original image is used to as the teacher signal to tutoring. At last, the matter extenics model is built qua the database's data of the extension decision strategy generator by analysis the simulation result.",2008,0,
803,804,The dual parameterization approach to optimal least square FIR filter design subject to maximum error constraints,"This paper is concerned with the design of linear-phase finite impulse response (FIR) digital filters for which the weighted least square error is minimized, subject to maximum error constraints. The design problem is formulated as a semi-infinite quadratic optimization problem. Using a newly developed dual parameterization method in conjunction with the Caratheodory's dimensional theorem, an equivalent dual finite dimensional optimization problem is obtained. The connection between the primal and the dual problems is established. A computational procedure is devised for solving the dual finite dimensional optimization problem. The optimal solution to the primal problem can then be readily obtained from the dual optimal solution. For illustration, examples are solved using the proposed computational procedure",2000,0,
804,805,Model of Reliability of the Software with Coxian Distribution of Length of Intervals between the Moments of Detection of Errors,"The generalized software reliability model on the basis of nonstationary Markovian system of service is proposed. Approximation by distribution of Cox allows investigating growth of software reliability for any kinds of distribution of time between the moments of detection of errors and exponential distributions of time of their correction. The model allows receiving the forecast of important characteristics: the number of the corrected and not corrected errors, required time of debugging, etc. The diagram of transitions between states of the generalized model and system of the differential equations are presented. The example of calculation with use of the offered model is considered, research of influence of variation coefficient of Cox distribution of duration of intervals between the error detection moments on values of look-ahead characteristics is executed.",2010,0,
805,806,Managing Faults in the Service Delivery Process of Service Provider Coalitions,"In recent years, IT Service Management (ITSM) has become one of the most researched areas of IT. Incident Management and Problem Management form the basis of the tooling provided by an Incident Ticket System (ITS). As more compound or interdependent services are collaboratively offered by providers, the delivery of a service therefore becomes a responsibility of more than one provider's organization. In the ITS systems of various providers seemingly unrelated tickets are created and the connection between them is not realized automatically. The introduction of automation will reduce human involvement and time required for incident resolution.In this paper we consider a collaborative service delivery model that supports both per-request services and continuous high-availability services. In the case of high availability service the information stored in the ITS of the provider often includes information on the outage of a particular service rather than on the failure of a particular request. In this paper we offer an information model that consolidates and supports inter-organizational incident management and probabilistic model for fault discovery.",2009,0,
806,807,Adaptive partition size temporal error concealment for H.264,"Existing temporal error concealment methods for H.264 often decide the partition size of the lost macroblock (MB) before recovering the motion information, without actual quality comparison between different partition modes. In this paper, we propose to select the best partition mode by minimizing the Weighted Double-Sided External Boundary Matching Error (WDS-EBME), which jointly measures the inter-MB boundary discontinuity, inter-partition boundary discontinuity and intrapartition block artifacts in the recovered MB. The proposed method estimates the best motion vectors for each of the candidate partition modes, calculates the overall WDS-EBME values for them, and selects the partition mode with the smallest overall WDS-EBME to recover the lost MB. We also propose a progressive concealment order for the 4times4 partition mode. Test results show that the adaptive partition size method always outperforms the fixed partition size methods. Both the adaptive and fixed partition size methods are much superior to the temporal error concealment (TEC) method in the H.264 reference software.",2008,0,
807,808,A fault-tolerance mechanism in grid,"Grid appears as an effective technology coupling geographically distributed resources for solving large-scale problems in the wide area network. Fault tolerance in grid system is a significant and complex issue to secure a stable and reliable performance. Until now, various techniques exist for detecting and correcting faults in distributed computing systems. Unfortunately, few energy focus on fault-tolerance in grid environment, especially with the emergence of OGSA. A new fault-tolerant mechanism is needed to detect and recover service faults and nodes crash. Based on our previous work on Java threads state capturing and existing mobile agent techniques, we put forward a fault-tolerant mechanism providing effective fault-handling and recovering methods.",2003,0,
808,809,Building a Transformer Defects Database for UHF Partial Discharge Diagnostics,"In the case of a defective transformer, when a partial discharge is detected and recorded, critical information can be deduced from its pattern, such as the type of defect, its criticality or even information on the level of degradation of the insulation. This information can help to determine the remaining life of the transformer and thus provide criteria for its maintenance and operation. In this paper different artificial PD patterns will be recorded in the laboratory, representative of specific transformer defects, in order to build a database for comparison purposes when measuring on-line. This can greatly improve the recognition and identification of the defect and thus help take some important life assessment conclusions on the transformer.",2007,0,
809,810,RMS bounds and sample size considerations for error estimation in linear discriminant analysis,"The validity of a classifier depends on the precision of the error estimator used to estimate its true error. This paper considers the necessary sample size to achieve a given validity measure, namely RMS, for resubstitution and leave-one-out error estimators in the context of LDA. It provides bounds for the RMS between the true error and both the resubstitution and leave-one-out error estimators in terms of sample size and dimensionality. These bounds can be used to determine the minimum sample size in order to obtain a desired estimation accuracy, relative to RMS. To show how these results can be used in practice, a microarray classification problem is presented.",2010,0,
810,811,Icon based error concealment for JPEG and JPEG 2000 images,"This paper describes methods to recover the useful data in JPEG and JPEG 2000 compressed images and to estimate data for those portions of the image where correct data cannot be recovered. These techniques are designed to handle the loss of hundreds of bytes in the file. No use is made of restart markers or other optional error detection features of JPEG and JPEG 2000, but an uncorrupted low resolution version of the image, such as an icon, is assumed to be available. These icons are typically present in Exif or JFIF format JPEG files.",2003,0,
811,812,An efficient spatial domain error concealment method for H.264 video,"This paper presents an efficient spatial domain error concealment method for the forthcoming video coding standard H.264. In H.264, a frame is divided into 44 blocks during the encoding procedure. For natural image signal, the blocks are smoothly connected with each other. Based on this property, a linear smoothness constraint equation that describes the connection of the lost block and its neighboring blocks can be constructed. By solving this equation, the coefficients of lost block can be recovered. Because the reconstructed high frequency coefficients may be affected by noise, the recovered center pixel may have obvious error. To eliminate the error, we use the recovered pixels that are on the boundaries of the lost block and average pixel difference to interpolate the center pixels. The implementation is simple and is suitable for real-time video application. Experimental results show our method has better recovery result than conventional approach.",2003,0,
812,813,Fault-tolerant voltage-fed PWM inverter AC motor drive systems,This paper shows how to integrate fault compensation strategies into two different types of configurations of induction motor drive systems. The proposed strategies provide compensation for open-circuit and short-circuit failures occurring in the converter power devices. The fault compensation is achieved by reconfiguring the power converter topology with the help of isolating and connecting devices. These devices are used to redefine the post-fault converter topology. This allows for continuous free operation of the drive after isolation of the faulty power switches in the converter. Experimental results demonstrate the validity of the proposed systems.,2004,0,
813,814,"""That one's gotta work"" Mars Odyssey's use of a fault tree driven risk assessment process","The Odyssey project was the first mission to Mars after the failures of Mars Climate Orbiter and Mars Polar Lander. In addition to incorporating the results of those failure review boards and responding to external ""Red Team"" reviews, the Odyssey project itself implemented a risk assessment process. This paper describes that process and its use of fault trees as an enabling tool. These trees were used to break the mission down into the functional elements needed to make it a success. By determining how each function could be prevented from executing, a list of failure modes was created. Each fault was individually assessed as to what mitigations could prevent the fault from occurring, as well as what methods should be used to explicitly verify that mitigation. Fault trees turned out to be an extremely useful tool in both identifying risks as well as structuring the development of mitigations.",2002,0,
814,815,3D shape from multi-camera views by error projection minimization,"Traditional shape from silhouette methods compute the 3D shape as the intersection of the back-projected silhouettes in the 3D space, the so called visual hull. However, silhouettes that have been obtained with background subtraction techniques often present miss-detection errors (produced by false negatives or occlusions) which produce incomplete 3D shapes. Our approach deals with miss-detections and noise in the silhouettes. We recover the voxel occupancy which describes the 3D shape by minimizing an energy based on an approximation of the error between the shape 2D projections and the silhouettes. The energy also includes regularization and takes into account the visibility of the voxels in each view in order to handle self-occlusions.",2009,0,
815,816,Investigating no fault found in the aerospace industry,This paper describes a package of work to investigate the root cause of no fault found (NFF) events within the aerospace industry. The project focus is to develop practical guidance for designers and project managers to facilitate a reduction in NFF removal events for both current products and new designs. This investigation forms part of the second phase of the REMM (Reliability Enhancement Methodology and Modelling) project and comprises three diverse investigation activities: (i) examination of NFF issues at a system level that can highlight common areas of concern for all partner companies and across the Aerospace industry; (ii) classification and root cause analysis of service-data collected by partner companies; (iii) system modelling the 'softer' NFF issues to determine the effects of intervention. This paper describes the formulation of the work package strategy and details the progress made during the first year of this three-year project.,2003,0,
816,817,Fault Diagnosis of Mine Hoist Braking System Based on Wavelet Packet and Support Vector Machine,"This paper concerns mine hoist braking system fault diagnosis with the combination of wavelet packet and support vector machine. It is motivated by the scarce of fault samples in mine hoist such requiring very high security system. A novel approach is presented in order to diagnose blockage piston in cylinder, a typical fault of mine hoist braking system. This method mainly consists of three steps: (1) apply 3 levels wavelet package to construct and reconstruct signal of brake distance-time , extract fault feature vectors (2) set up training samples (3) establish a SVM fault classifier to complete fault diagnosis. Experimental results show that SVM method can effectively accomplish the blockage piston in cylinder fault diagnosis of braking system and has a high adaptability for fault diagnosis in the case of smaller number of samples.",2006,0,
817,818,Fault contribution trees for product families,"Software fault tree analysis (SFTA) provides a structured way to reason about the safety or reliability of a software system. As such, SFTA is widely used in mission-critical applications to investigate contributing causes to possible hazards or failures. In this paper we propose an approach similar to SFTA for product families. The contribution of the paper is to define a top-down, tree-based analysis technique, the fault contribution tree analysis (FCTA), that operates on the results of a product-family domain analysis and to describe a method by which the FCTA of a product family can serve as a reusable asset in the building of new members of the family. Specifically, we describe both the construction of the fault contribution tree for a product family (domain engineering) and the reuse of the appropriately pruned fault contribution tree for the analysis of a new member of the product family (application engineering). The paper describes several challenges to this approach, including evolution of the product family, handling of subfamilies, and distinguishing the limits of safe reuse of the FCTA, and suggests partial solutions to these issues as well as directions for future work. The paper illustrates the techniques with examples from applications to two product families.",2002,0,
818,819,Application of Fault Tolerant Control Using Sliding Modes With On-line Control Allocation on a Large Civil Aircraft,This paper describes an on-line sliding mode control allocation scheme for fault tolerant control of the lateral and longitudinal axes of the non-linear B747 aircraft. The effectiveness level of the actuators is used by the control allocation scheme to redistribute the control signals to the functioning actuators when a fault or failure occurs. The simulation results on the non-linear B747 model show good performance when tested on different fault and even certain total actuator failure scenarios without reconfiguring the controller.,2007,0,
819,820,Analysis of the effects of real and injected software faults: Linux as a case study,The application of fault injection in the context of dependability benchmarking is far from being straightforward. One decisive issue to be addressed is to what extent injected faults are representative of actual faults. This paper proposes an approach to analyze the effects of real and injected faults.,2002,0,
820,821,Anomaly Detection Support Vector Machine and Its Application to Fault Diagnosis,"We address the issue of classification problems in the following situation: test data include data belonging to unlearned classes. To address this issue, most previous works have taken two-stage strategies where unclear data are detected using an anomaly detection algorithm in the first stage while the rest of data are classified into learned classes using a classification algorithm in the second stage. In this study, we propose anomaly detection support vector machine (ADSVM) which unifies classification and anomaly detection. ADSVM is unique in comparison with the previous work in that it addresses the two problems simultaneously. We also propose a multiclass extension of ADSVM that uses a pairwise voting strategy. We empirically present that ADSVM outperforms two-stage algorithms in application to an real automobile fault dataset, as well as to UCI benchmark datasets.",2008,0,
821,822,Study on Fault Diagnosis Expert System for Power Supply Circuit Board on Vxi Bus,A high-tech information electronic equipment of some given type is designed in order to proceed automatically fault detection and improve the efficiency and accuracy of diagnosis. This thesis which is a part of the program introduces the research of algorithm of fault diagnose expert system of a power supply circuit board of an electronic device and algorithm realization and example proving on the hardware platform. It's quicker and more convenient to locate fault on the circuit boards with this equipment. It's proved that this expert system can solve the problems of high cost and long intervals of maintenance and keep the equipment in a stable status,2006,0,
822,823,"Software-based, low-cost fault detection for microprocessors","The PSW-NOP is a low-cost solution to the error detection problem because multiple versions of code or hardware redundancy are not needed. The approach is useful in creating dependable software, especially in deep-submicron ICs. It also supplements to existing approaches of control-flow error detection.",2008,0,
823,824,The Diagnosis System of Mechanical Fault Based on LabVIEW Platform and Its Application,"This paper introduced the content and the status quo of study of the mechanical fault diagnosis, detailed the construction method and the overall structure of mechanical fault diagnosis system based on LabVIEW, and carried out applied research of rotating mechanical fault diagnosis by the use of virtual instrument technology and the use of mechanical fault diagnosis system based on the LabVIEW platform according to rotating machinery and its characteristics.",2009,0,
824,825,Error Vector Magnitude Measurement On Cascaded Butler Matrices System,"This paper describes error vector magnitude (EVM) measurement on vehicle communication system that employs low noise amplifiers (LNAs) and cascading Butler Matrices in producing broad beam high linearity and high gain narrow beam system. The output signals from the first Butler Matrix that have high gain and narrow beam width can be used for long distance communication while the outputs from the second Butler Matrix, which have high linearity, and broad beam width can be used for short range communications.",2007,0,
825,826,Fault Tolerant Control for Nonlinear Systems: Sum-of-Squares Optimization Approach,"In this paper, the fault tolerant control problem of nonlinear systems against actuator failures is considered. By representing the open-loop nonlinear systems in a state dependent linear-like polynomial form and implementing a special class of Lyapunov functions, the above problem can be formulated in terms of state dependent linear polynomial inequalities. Semidefinite programming relaxations based on the sum of squares decomposition are then used to efficiently solve such inequalities.",2007,0,
826,827,Increasing data TLB resilience to transient errors,"This paper first demonstrates that a large fraction of data TLB entries are dead (i.e., not used again before being replaced) for many applications at any given time during execution. Based on this observation, it then proposes two alternate schemes that replicate actively accessed data TLB entries in these dead entries to increase the resilience of the TLB against transient errors.",2005,0,
827,828,An approach for analysing the propagation of data errors in software,"We present a novel approach for analysing the propagation of data errors in software. The concept of error permeability is introduced as a basic measure upon which we define a set of related measures. These measures guide us in the process of analysing the vulnerability of software to find the modules that are most likely exposed to propagating errors. Based on the analysis performed with error permeability and its related measures, we describe how to select suitable locations for error detection mechanisms (EDMs) and error recovery mechanisms (ERMs). A method for experimental estimation of error permeability, based on fault injection, is described and the software of a real embedded control system analysed to show the type of results obtainable by the analysis framework. The results show that the developed framework is very useful for analysing error propagation and software vulnerability and for deciding where to place EDMs and ERMs.",2001,0,
828,829,MPEG-2 error concealment based on block-matching principles,"The MPEG-2 compression algorithm is very sensitive to channel disturbances due to the use of variable-length coding. A single bit error during transmission leads to noticeable degradation of the decoded sequence quality, in that part or an entire slice information is lost until the next resynchronization point is reached. Error concealment (EC) methods, implemented at the decoder side, present one way of dealing with this problem. An error-concealment scheme that is based on block-matching principles and spatio-temporal video redundancy is presented in this paper. Spatial information (for the first frame of the sequence or the next scene) or temporal information (for the other frames) is used to reconstruct the corrupted regions. The concealment strategy is embedded in the MPEG-2 decoder model in such a way that error concealment is applied after entire frame decoding. Its performance proves to be satisfactory for packet error rates (PER) ranging from 1% to 10% and for video sequences with different content and motion and surpasses that of other EC methods under study",2000,0,
829,830,Accelerated functional modeling of aircraft electrical power systems including fault scenarios,"The more-electric aircraft concept is a fast-developing trend in modern aircraft power systems and will result in an increase in electrical loads fed by power electronic converters. Finalizing the architectural bus paradigm for the next generation of more-electric aircraft involves extensive simulations ensuring power system integrity. Since the possible number of loads in an on-board power system can be very large, the development of accurate, effective and computational time-saving models is of great importance. This paper focuses on development of a modeling approach based-on functional representation of individual power system units. This provides for possibility of fast simulation of a full generator-load power system under both normal and fault conditions. The paper describes the modeling principle, illustrates the acceleration attainable and shows how the functional representation can handle fault scenarios.",2009,0,
830,831,A Flexible Fault-Tolerance Mechanism for the Integrade Grid Middleware,"Computer grids have attracted great attention of both academic and enterprise communities, becoming an attractive alternative for the execution of applications that demand huge computational power, allowing the integration of computational resources spread through different administrative domains. The dynamic nature of the grid infrastructure, its high scalability, and great heterogeneity exacerbates the likelihood of errors occurrence, imposing fault tolerance as a major requirement for grid middlewares. This paper describes a flexible fault-tolerance mechanism implemented on integrate grid middleware that allows the customization of several fault tolerance parameters and the combination of different fault tolerance techniques. This paper also presents several experiments that measure the benefits of our approach, considering several different execution environments scenarios.",2007,0,
831,832,PMSM Bearing Fault Detection by means of Fourier and Wavelet transform,This paper presents a study of permanent magnet synchronous machines (PMSM) with bearing fault using a two-dimensional (2-D) finite element analysis (FEA). Fourier fast and wavelet transform were used to fault detection of bearing damage under stationary and non stationary working conditions. Simulation were carried out and compared with experimental results.,2007,0,
832,833,Testing for missing-gate faults in reversible circuits,"Logical reversibility occurs in low-power applications and is an essential feature of quantum circuits. Of special interest are reversible circuits constructed from a class of reversible elements called k-CNOT (controllable NOT) gates. We review the characteristics of k-CNOT circuits and observe that traditional fault models like the stuck-at model may not accurately represent their faulty behavior or test requirements. A new fault model, the missing gate fault (MGF) model, is proposed to better represent the physical failure modes of quantum technologies. It is shown that MGFs are highly testable, and that all MGFs in an N-gate k-CNOT circuit can be detected with from one to [N/2] test vectors. A design-for-test (DFT) method to make an arbitrary circuit fully testable for MGFs using a single test vector is described. Finally, we present simulation results to determine (near) optimal test sets and DFT configurations for some benchmark circuits.",2004,0,
833,834,Analysis of arcing fault models,"The objective of this paper is to present, discuss and compare, in some detail, the arc models used to represent an arcing fault, in order to determine which of them is the most precise for this purpose. At the beginning of the paper a brief explanation of arcing faults is given, bringing out the importance of a realistic simulation of them. A theoretical description of the black box equations to model arc is explained. The paper concludes with a comparison of the most representative arc models. Results will be useful not only in arcing fault detection but also in the design of autoreclosure schemes on transmission lines. Implementation and simulation method are based on ATP/EMTP.",2008,0,
834,835,A Fault-Tolerant Attitude Determination System Based on COTS Devices,"In this paper we present a low cost fault-tolerant attitude determination system to a scientific satellite using COTS devices. We related our experience in developing the attitude determination system, where we combine proven fault tolerance techniques to protect the whole system composed only by COTS from the effects produced by transient faults. We detailed the failure cases and the detection, reconfiguration and recovery schemes that assure the fault-tolerant condition. A testbed system was used to inject faults, evaluate the recovery capability of the fault-tolerant system and validate the solution proposed.",2008,0,
835,836,Secure and fault-tolerant voting in distributed systems,"Concerns about both security and fault-tolerance have had an important impact on the design and use of distributed information systems in the past. As such systems become more prevalent, as well as more pervasive, these concerns will become even more immediately relevant. We focus on integrating security and fault-tolerance into one, general-purpose protocol for secure distributed voting. Distributed voting is a well-known fault-tolerance technique. For the most part, however, security had not been a concern in systems that used voting. More recently, several protocols have been proposed to shore up this lack. These protocols, however, have limitations which make them particularly unsuitable for many aerospace applications, because those applications require very flexible voting schemes (e.g., voting among real-world sensor data). We present a new, more general voting protocol that reduces the vulnerability of the voting process to both attacks and faults. The algorithm is contrasted with the traditional 2-phase commit protocols typically used in distributed voting and with other proposed secure voting schemes. Our algorithm is applicable to exact and inexact voting in networks where atomic broadcast and predetermined message delays are present, such as local area networks. For wide area networks without these properties, we describe yet another approach that satisfies our goals of obtaining security and fault tolerance for a broad range of aerospace information systems",2001,0,
836,837,Fault-based attack of RSA authentication,"For any computing system to be secure, both hardware and software have to be trusted. If the hardware layer in a secure system is compromised, not only it would be possible to extract secret information about the software, but it would also be extremely hard for the software to detect that an attack is underway. In this work we detail a complete end-to-end fault-attack on a microprocessor system and practically demonstrate how hardware vulnerabilities can be exploited to target secure systems. We developed a theoretical attack to the RSA signature algorithm, and we realized it in practice against an FPGA implementation of the system under attack. To perpetrate the attack, we inject transient faults in the target machine by regulating the voltage supply of the system. Thus, our attack does not require access to the victim system's internal components, but simply proximity to it. The paper makes three important contributions: first, we develop a systematic fault-based attack on the modular exponentiation algorithm for RSA. Second, we expose and exploit a severe flaw on the implementation of the RSA signature algorithm on OpenSSL, a widely used package for SSL encryption and authentication. Third, we report on the first physical demonstration of a fault-based security attack of a complete microprocessor system running unmodified production software: we attack the original OpenSSL authentication library running on a SPARC Linux system implemented on FPGA, and extract the system's 1024-bit RSA private key in approximately 100 hours.",2010,0,
837,838,Novel method for fault section identification,"From stability point of view fault section identification is an important task for a transmission systems. In this paper, a novel method for section identification using extended Kalman filter (EKF) has been proposed. Subsynchronous frequency as indicator of fault section identification is estimated by EKF algorithm. Several tests as different fault locations have been performed to show performance of the method. Simulation results reveal high performance of the method. In all tests, the proposed algorithm detects accurately presence of subsynchronous frequency.",2009,0,
838,839,A Dynamic Temporal Error Concealment Algorithm for H.264,"Packet losses or errors of high compressed video stream during transmission over error-prone channel may cause serious decline in video quality. Error concealment (EC) at decoder side is an effective technology to reduce this video degradation. This paper proposes a Dynamic Temporal Error Concealment (DTEC) algorithm for H.264, which chooses different error concealment approach according to the variance of motion vectors of available macro-blocks (MBs) around the lost MB. Furthermore, a recovery method based Directional Temporal Boundary Match Algorithm (DTBMA) is proposed. Experimental results show that the proposed algorithm not only increases PSNR but also improves subjective video quality compared with conventional temporal error concealment algorithms in the case of the same packet loss rate.",2010,0,
839,840,Steward: Scaling Byzantine Fault-Tolerant Replication to Wide Area Networks,"This paper presents the first hierarchical byzantine fault-tolerant replication architecture suitable to systems that span multiple wide-area sites. The architecture confines the effects of any malicious replica to its local site, reduces message complexity of wide-area communication, and allows read-only queries to be performed locally within a site for the price of additional standard hardware. We present proofs that our algorithm provides safety and liveness properties. A prototype implementation is evaluated over several network topologies and is compared with a flat byzantine fault-tolerant approach. The experimental results show considerable improvement over flat byzantine replication algorithms, bringing the performance of byzantine replication closer to existing benign fault-tolerant replication techniques over wide area networks.",2010,0,
840,841,Modeling of hydraulic systems tailored to diagnostic fault detection systems,"The consistent and reliable operation of hydraulic componentry is paramount for many systems. From spacecraft to the most basic automotive bottle jack an undetected failure can have significant consequences if not noticed in time. Many hydraulic systems have diagnostics capabilities but these are normally very limited in scope. They can detect events only related to specifically monitored components or general system failures. Typically these diagnostic systems are designed after the fact and tuned to meet goals. When hydraulic systems are designed, simulation models are frequently used to gain some idea of the finished systems performance. Rarely is the simulation model designed to accommodated and optimize a diagnostic capability. The number and placement of diagnostic sensors can have a significant effect on the ability of a diagnostic system to resolve faults early in their evolution cycle. This paper describes a technique developed at the Penn State Applied Research Laboratories Systems Operations and Automation Department for the design of hydraulic simulation models that pre-incorporate fault diagnostic advanced design features. This technique is being applied to a US Army M1120 heavy expanded mobility tactical truck (HEMTT) load handling system (LHS) supply vehicle that utilizes a palletized hydraulic loading system. The test vehicle has a hydraulic system that was fully instrumented with sensors for this work. This paper addresses a piece in the diagnostic puzzle that has until now been looked at. Namely what additional features in a simulation model allow for optimal placement and monitoring of the hydraulic system. The researchers have found that this typically leads the readdressing or removing certain engineering assumptions that are typically made when designing simulation models. When this is done the models are more flexible when it comes to diagnostic implementation",2006,0,
841,842,Magnets faults characterization for Permanent Magnet Synchronous Motors,"Nowadays Permanent Magnet Synchronous Motor (PMSM) are an attractive alternative to induction machines for a variety of applications due to their higher efficiency, power density and wide constant power speed range. In this context the condition monitoring of magnets status is receiving more and more attention since is critical for industrial applications. This paper presents a characterization of rotor faults for such a motor due to local and uniform demagnetization by means of two dimensional (2-D) Finite Element Analysis (FEA) and proposes a new non-invasive method for their detection by means of a Fourier transform of the back-EMF. The proposed approach is then validated for three permanent magnet synchronous motors with different winding configurations.",2009,0,
842,843,Evaluation of Respiratory Motion Effect on Defect Detection in Myocardial Perfusion SPECT: A Simulation Study,"The objective of this study is to investigate the effects of respiratory motion (RM) on defect detection in Tc-99m sestamibi myocardial perfusion SPECT (MPS) using a phantom population that includes patient variability. Three RM patterns are included, namely breath-hold, slightly enhanced normal breathing, and deep breathing. For each RM pattern, six 4-D NCAT phantoms were generated, each with anatomical variations. Anterior, lateral and inferior myocardial defects with different sizes and contrasts were inserted. Noise-free SPECT projections were simulated using an analytical projector. Poisson noise was then added to generate noisy realizations. The projection data were reconstructed using the OS-EM algorithm with 1 and 4 subsets/iteration and at 1, 2, 3, 5, 7, and 10 iterations. Short-axis images centered at the centroid of the myocardial defect were extracted, and the channelized Hotelling observer (CHO) was applied for the detection of the defect. The CHO results show that the value of the area under the receiver operating characteristics (ROC) curve (AUC) is affected by the RM amplitude. For all the defect sizes and contrasts studied, the highest or optimal AUC values indicate maximum detectability decrease with the increase of the RM amplitude. With no respiration, the ranking of the optimal AUC value in decreasing order is anterior then lateral, and finally inferior defects. The AUC value of the lateral defect drops more severely as the RM amplitude increases compared to other defect locations. Furthermore, as the RM amplitude increases, the AUC values of the smaller defects drop more quickly than the larger ones. We demonstrated that RM affects defect detectability of MPS imaging. The results indicate that developments of optimal data acquisition methods and RM correction methods are needed to improve the defect detectability in MPS.",2009,0,
843,844,A Simulation Environment for the On-Line Monitoring of a Fault Tolerant Flight Control Computer,"An approach of designing a simulation environment for the on-line monitoring of a fault tolerant flight control computer is presented in this paper. The simulation environment is designed to evaluate an improved on-line monitoring technique for processors with a built-in cache. This technique assumes that a monitor checks on-line whether the execution of a program is in accordance with the control flow graph created for the program off-line by a preprocessor. The simulation environment consists of the target processor and the monitor, but also includes carefully chosen benchmark programs, fault injection modules and the preprocessor.",2009,0,
844,845,A New Topology of Fault-current Limiter and its control strategy,"In this paper a new type of fault current limiter based on DC reactor with using superconductor are presented. In normal operation condition the limiter has no obvious effect on loads. When fault happens, the bypass AC reactor and series resistor will insert the fault line automatically to limit the short circuit current, when the control circuit detects a short circuit fault, the solid state bridge in fault line works as an inverter and is closed as soon as possible. Subsequently the fault current is fully limited by the bypass AC reactor and series resistor. The magnitude of L<sub>ac</sub> and r<sub>ac</sub> must be equal with protected load. By using the electro-magnetic transients in DC systems which are the simulator of electric networks (EMTDC) software we carried out analysis of the voltage and current waveforms for fault conditions. Waveforms are considered in calculating the voltage drop at substation during the fault. The analysis used in selecting an appropriate inductance value for designing",2006,0,
845,846,Fault correction profiles,"In general, software reliability models have focused on modeling and predicting the failure detection process and have not given equal priority to modeling the fault correction process. However, it is important to address the fault correction process in order to identify the need for process improvements. Process improvements, in turn, will contribute to achieving software reliability goals. We introduce the concept of a fault correction profile "" a set of functions that predict fault correction events as a function of failure detection events. The fault correction profile identifies the need for process improvements and provides information for developing fault correction strategies. Related to the fault correction profile is the goal fault correction profile. This profile represents the fault correction goal against which the achieved fault correction profile can be compared. This comparison motivates the concept of fault correction process instability, and the attributes of instability. Applying these concepts to the NASA Goddard Space Flight Center fault correction process and its data, we demonstrate that the need for process improvement can be identified, and that improvements in process would contribute to meeting product reliability goals.",2003,0,
846,847,Cross layer error-control scheme for video quality support over 802.11b wireless LAN,"Mitigating the impact of errors on video quality over wireless network has been a major issue of concern which requires highly efficient and effective scheme. The dynamic and heterogeneous nature of the wireless network requires highly sophisticated approach to mitigate the impact of transmission error on video quality. The trade-off between delay and video quality should be considered while designing such applications to reasonably maintain video quality in wireless channel. In order to significantly reduce the impact of high error bit and error burst on transmitted video, more efficient error correction scheme are needed. In this paper, we paper presents an approach using forward error correction and cross layer mechanism which dynamically adapts with the channel condition to recover the loss packets in order to enhance the perceived video quality. The scenario has been simulated using NS-2 and it shows more dramatic improvement in video quality.",2009,0,
847,848,Cycle error correction in asynchronous clock modeling for cycle-based simulation,"As the complexity of SoCs is increasing, hardware/software co-verification becomes an important part of system verification. C-level cycle-based simulation could be an efficient methodology for system verification because of its fast simulation speed. The cycle-based simulation has a limitation in using asynchronous clocks that causes inherent cycle errors. In order to reuse the output of a C-level cycle-based simulation for the verification of a lower level model, the C-level model should be cycle-accurate with respect to the lower level model. In this paper, a cycle error correction technique is presented for two asynchronous clock models. An example design is devised to show the effectiveness of the proposed method. Our experimental results show that the fast speed of cycle-based simulation can be fully exploited without sacrificing the cycle accuracy",2006,0,
848,849,The same is not the same - postcorrection of alphabet confusion errors in mixed-alphabet OCR recognition,"Character sets for Eastern European languages typically contain symbols that are optically almost or fully identical to Latin letters. When scanning documents with mixed Cyrillic-Latin or Greek-Latin alphabets, even high-quality OCR-software is often not able to correctly separate between Cyrillic (Greek) and Latin symbols. This effect leads to an error rate that is far beyond the usual error rates observed when recognizing single-alphabet documents. In this paper we first survey similarities between Latin and Cyrillic (Greek) letters and words for distinct languages and fonts. After briefly introducing a new and public corpus collected by our groups for evaluating OCR-technology over mixed-alphabet documents, we describe how to adapt general algorithms and tools for postcorrection of OCR results to the new context of mixed-alphabet recognition. Experimental results on Bulgarian documents from the corpus and from other sources demonstrate that a drastic reduction of error rates can be achieved.",2005,0,
849,850,Single-phase power-factor-correction AC/DC converters with three PWM control schemes,"Three pulse-width modulation (PWM) control schemes for a single-phase power-factor-correction (PFC) AC/DC converter are presented to improve the power quality. A diode bridge with two power switches is employed as a PFC circuit to achieve a high power factor and low line current harmonic distortion. The control schemes are based on look-up tables with hysteresis current controller (HCC) to generate two-level or three-level PWM on the DC side of diode rectifier. Based on the proposed three control schemes, the line current is driven to follow the sinusoidal current command which is in phase with the supply voltage, and two capacitor voltages on the DC bus are controlled to be balanced. The simulation and experimental results of a 1 kW converter with load as well as line voltage variation and shown to verify the proposed control schemes. It is shown that unity PFC is achieved using a simple control circuit and the measured line current harmonics satisfy the IEC 1000-3-2 requirements",2000,0,
850,851,New EMTP-RV Equivalent Circuit Model of Core-Shielding Superconducting Fault Current Limiter Taking Into Account the Flux Diffusion Phenomenon,"In order to successfully integrate superconducting fault current limiters (SFCL) into electric power system networks, accurate and fast simulation models are needed. This led us to develop a generic electric circuit model of an inductive SFCL, which we implemented in the EMTP-RV software. The selected SFCL is of shielded-core type, i.e. a HTS hollow cylinder surrounds the central leg of a magnetic core, and is located inside a primary copper winding, generating an AC magnetic field proportional to the line current. The model accounts for the highly nonlinear flux diffusion phenomenon across the superconducting cylinder, governed by the Maxwell equations and the non-linear E-J relationship of HTS materials. The computational efficiency and simplicity of this model resides in a judicious 1-D approximation of the geometry, together with the use of an equivalent electric circuit that reproduces accurately the actual magnetic behavior for the flux density (B) inside the walls of the HTS cylinder. The HTS properties are not restricted to the simple power law model, but instead, any resistivity function depending on J, B and T can be used and inserted directly in the model through a non-linear resistance appearing in the equivalent circuit.",2009,0,
851,852,Safety assessment for safety-critical systems including physical faults and design faults,"Two types of faults, design faults and physical faults, are discussed in this paper. Since they are two mutually exclusive and complete fault types on the fault space, the safety assessment of safety-critical computer systems in this paper considers the hazard contribution from both types. A three-state Markov model is introduced to model safety-critical systems. Steady state safety and mean time to unsafe failure (MTTUF) are the two most important metrics for safety assessment. Two homogenous Markov models are derived from the three-state Markov model to estimate the steady state safety and the MTTUF. The estimation results are generalized given the fault space is divided by M mutually exclusive and complete types of faults",2006,0,
852,853,Fault Detection System Based on Embedded Platform,"This paper introduced a gear-box fault detection system that based on PC/104 embedded platform. Gear-box is almost as important as engine in a vehicle, so it is necessary to make sure that it is not out of order when driving. By collecting the vibrant signal from the gear, axes and bearing in gear-box, then processing and analysis the data, we can get acquirement of the working condition of the gear-box. And we can store the information in PC/104 embedded module for a future analysis or send the data to control center or upper PC by network or serial port, then achieving remote monitoring in real-time. This system can help safely driving, and it can reduce the risk of accident that caused by the fault of gear-box.",2008,0,
853,854,The Design and Implementation of Checkpoint/Restart Process Fault Tolerance for Open MPI,"To be able to fully exploit ever larger computing platforms, modern HPC applications and system software must be able to tolerate inevitable faults. Historically, MPI implementations that incorporated fault tolerance capabilities have been limited by lack of modularity, scalability and usability. This paper presents the design and implementation of an infrastructure to support checkpoint/restart fault tolerance in the Open MPI project. We identify the general capabilities required for distributed checkpoint/restart and realize these capabilities as extensible frameworks within Open MPI's modular component architecture. Our design features an abstract interface for providing and accessing fault tolerance services without sacrificing performance, robustness, or flexibility. Although our implementation includes support for some initial checkpoint/restart mechanisms, the framework is meant to be extensible and to encourage experimentation of alternative techniques within a production quality MPI implementation.",2007,0,
854,855,Early error detection in systems-on-chip for fault-tolerance and at-speed debugging,"In this paper we propose a new method for the design of duplex fault-tolerant systems with early error detection and high availability. All the scannable memory elements (flip-flops) of the duplicated system are implemented as multimode memory elements according to Singh et al. (1999), thus allowing during normal operation the accumulation of a signature of its states in its scan-paths. By continuously comparing a 1-bit sequence of the compacted scan-out outputs of the accumulated signatures of the duplicated systems an error can be already detected and a recovery procedure started before an erroneous result appears at the system outputs when a computations is completed. The accumulation of a signature during normal operation can also be used for debugging at-speed. For this application the system need not be duplicated",2001,0,
855,856,"Intelligent, Fault Tolerant Control for Autonomous Systems","We present a methodology for intelligent control of an autonomous and resource constrained embedded system. Geared towards mastering permanent and transient faults by dynamic reconfiguration, our approach uses rules for describing device functionality, valid environmental interactions, and goals the system has to reach. Besides rules, we use functions that characterize a goal's target activity profile. The target activity profile controls the frequency our system uses to reach the corresponding goal. In the paper we discuss a first implementation of the given methodology, and introduce useful extensions. In order to underline the feasibility and effectiveness of the presented control system, we present a case study that has been carried out on a prototype system.",2007,0,
856,857,Low Cost Differential GPS Receivers (LCD-GPS): The Differential Correction Function,"Wireless Sensor Networks (WSN) are used in many applications such as environmental data collection, smart home, smart care and intelligent transportation system. Sensor nodes composing the WSN cooperate together in order to monitor physical entities such as temperature, humidity, sound, atmospheric pressure, motion or pollutants at different locations. To have location information, it is possible to configure nodes with their locations, in small deployments, but in large-scale deployments or when the nodes are mobile, the use of GPS is very interesting. However, the current accuracy of standard civil GPS is not sufficient for all WSN applications. Indeed, GPS measurements suffer from many errors especially in city. To improve GPS accuracy the differential mode (DGPS) has been introduced. In this paper, we present a WSN used to provide a DGPS solution. It's consisting of a set of low cost standard civil GPS communicating receivers. We present the design, implementation and some experimental results of this solution.",2008,0,
857,858,Ground distance relaying algorithm for high resistance fault,"This study proposes a new fault impedance estimation algorithm of phase-ground fault for ground distance relaying based on the negative-, zero- and comprehensive negative-zero-sequence current component. The principle is based on the assumption that fault path is purely resistive, and the phase angle of fault point voltage and fault path current is equal to construct the fault impedance estimating equations for ground distance relay, which can eliminate the effect of fault path resistance, load current and power swing. PSCAD software simulations show the accuracy of proposed algorithm.",2010,0,
858,859,A PIN-Based Dynamic Software Fault Injection System,"Fault injection plays a critical role in the verification of fault-tolerant mechanism, software testing and dependability benchmarking for computer systems. In this paper, according to the characteristics of software faults, we propose a new fault injection design pattern based on the PIN framework provided by Intel company, and develop a PIN-based dynamic software fault injection system (PDSFIS). Faults can be injected by PDSFIS without the source code of target applications under assessment, nor does the injection process involve interruption or software traps. Experimental assessment results of an Apache Web server obtained by the dependability benchmarking are presented to demonstrate the potentials of PDSFIS.",2008,0,
859,860,Evaluating the Fault Tolerance of Stateful TMR,"Module redundancy is often used in the construction of reliable systems. Triple Module Redundancy (TMR) is a method for improving reliability through module redundancy, although it does not give the correct results when two out of three modules fail. We, therefore, proposed a new voting architecture known as Stateful TMR, which uses both the results of TMR and the history of states to select the most reliable module. Through simulations, we evaluate the reliability of a module using both TMR and Stateful TMR, and show that for both transient and permanent failures, Stateful TMR achieves higher reliability than TMR.",2010,0,
860,861,A design of the low-pass filter using the novel microstrip defected ground structure,"A new defected ground structure (DGS) for the microstrip line is proposed in this paper. The proposed DGS unit structure can provide the bandgap characteristic in some frequency bands with only one or more unit lattices. The equivalent circuit for the proposed defected ground unit structure is derived by means of three-dimensional field analysis methods. The equivalent-circuit parameters are extracted by using a simple circuit analysis method. By employing the extracted parameters and circuit analysis theory, the bandgap effect for the provided defected ground unit structure can be explained. By using the derived and extracted equivalent circuit and parameters, the low-pass filters are designed and implemented. The experimental results show excellent agreement with theoretical results and the validity of the modeling method for the proposed defected ground unit structure",2001,0,
861,862,Notice of Retraction<BR>Material inner defect detection by a vibration spectrum analysis,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In this paper is described possibility of non-destructive diagnostics of solid objects by software analysis of vibration spectrum. With using platform MATLAB, we can process and evaluate information from accelerometer, which is placed on the measured object. The analog signal is digitized by special I/O device dSAPCE CP-1104 and then processed offline with FFT (Fast Fourier Transform). The power spectrum is then examined by developed evaluating procedures and individual results are displayed in bar graph. When we take a look on results, there is an evidently correlation between spectrum and examining object (inner defects).",2010,0,
862,863,Use of DGPS corrections with low power GPS receivers in a post SA environment,"With the removal of the dithering effects of Selective Availability (SA), use of Differential GPS (DGPS) corrections can now be applied for extended periods of time allowing enhanced performance for low power configurations of a Si RF based GPS receiver. The software selectable low power settings, implemented by Si RF, employ three states; track, navigate and trickle. During track and trickle states there is no UART communication making reception of DGPS correction unavailable. During the NAV state (when the navigation calculation is performed), corrections may be received. Previously, SA induced error shortened the viable extrapolation time to less than 30 seconds; else significant navigation error would build up between measurements. Additionally, the need to return to a full power state every 30 seconds significantly increased the overall average power dissipation over standard TricklePower<sup>TM</sup> operation. Now that SA (the dominate error source of the DGPS correction) has been removed, the time limit that a DGPS correction can be applied has been extended from 30 seconds to several minutes without significant degradation in navigation performance. This opens up opportunity for low power GPS receiver operation to make use of the DGPS correction to improve navigation without severely impacting the average power requirements. Si RF's implementations of low power operation, leverages off its unique architecture that allows 100 ms signal reacquisition allowing a pseudorange measurement to as little 200 ms. The chipset is then shut down for 800 ms, significantly reducing the power consumption, while still maintaining 1 Hz navigation updates",2001,0,
863,864,A Labview based rotor fault diagnostics tool for inverter fed induction machines by means of the Vienna monitoring method at variable speed,"The Vienna monitoring method (VMM) is a fault detection technique for squirrel cage induction machines. It is based on the comparison of the calculated torque values of two machine models with different model structure. Till now, steady state operation has been investigated only. This contribution deals with an exploitation for variable speed drives under dynamic conditions. The introduced configuration is due to a Labview application, based on a portable personal computer system",2000,0,
864,865,High performance error concealment algorithm by motion vector refinement for MPEG-4 video,A new error concealment algorithm is proposed using recursive motion vector refinement. The proposed method utilizes the top/bottom motion vectors of lost macroblocks in current and reference frames and refines motion vectors recursively. Simulation results based on the MPEG-4 codec present a superior subjective and objective performance of the proposed technique compared with conventional temporal concealment techniques.,2005,0,
865,866,Temperature Correction of PSP Measurement for Low-Speed Flow Using Infrared Camera,"Pressure-Sensitive Paint (PSP) system combined with an infrared (IR) camera has been developed at 2 m x 2 m low-speed wind tunnel at WINTEC/JAXA. The temperature correction of PSP was conducted using both temperature image acquired by the IR camera and wind-off images immediately after the wind tunnel shutdown. As a verification test, the pressure distribution on a supersonic transfer (SST) model was measured by the PSP/IR combined system. The measurement accuracy was fairly improved compared to the previous method, i.e., the temperature correction of PSP using only wind-off PSP images immediately after wind tunnel shutdown.",2005,0,
866,867,Research of the Middleware Based Fault Tolerance for the Complex Distributed Simulation Applications,"With the rapid development of computer simulation technology, the Radar simulation applications scale up increasingly. More and more Radar simulation applications adopt distributed structure to improve system performance and availability. Hence, how to enhance the robustness and efficiency of these complex distributed simulation systems is a hot point. At the same time, fault tolerance middleware makes the applications more robust, available and reliable. Therefore, we strengthen the functionalities of existing fault tolerant middleware and integrate our middleware with the complex distributed simulation systems to provide efficient fault tolerance with balanced workload allocation among different replicas for the distributed simulation applications.",2009,0,
867,868,Joint evaluation of performance and robustness of a COTS DBMS through fault-injection,"Presents and discusses observed failure modes of a commercial off-the-shelf (COTS) database management system (DBMS) under the presence of transient operational faults induced by SWIFI (software-implemented fault injection). The Transaction Processing Performance Council (TPC) standard TPC-C benchmark and its associated environment is used, together with fault-injection technology, building a framework that discloses both dependability and performance figures. Over 1600 faults were injected in the database server of a client/server computing environment built on the Oracle 8.1.5 database engine and Windows NT running on COTS machines with Intel Pentium processors. A macroscopic view on the impact of faults revealed that: (1) a large majority of the faults caused no observable abnormal impact in the database server (in 96% of hardware faults and 80% of software faults, the database server behaved normally); (2) software faults are more prone to letting the database server hang or to causing abnormal terminations; (3) up to 51% of software faults lead to observable failures in the client processes",2000,0,
868,869,Finding liveness errors with ACO,"Model checking is a well-known and fully automatic technique for checking software properties, usually given as temporal logic formulae on the program variables. Most of model checkers found in the literature use exact deterministic algorithms to check the properties. These algorithms usually require huge amounts of memory if the checked model is large. We propose here the use of an algorithm based on ACOhg, a new kind of ant colony optimization model, to search for liveness property violations in concurrent systems. This algorithm has been previously applied to the search for safety errors with very good results and we apply it here for the first time to liveness errors. The results state that our algorithmic proposal, called ACOhg-live, is able to obtain very short error trails in faulty concurrent systems using a low amount of resources, outperforming by far the results of nested-DFS, the traditional algorithm used for this task in the model checking community and implemented in most of the explicit state model checkers. This fact makes ACOhg-live a very suitable algorithm for finding liveness errors in large faulty concurrent systems, in which traditional techniques fail because of the model size.",2008,0,
869,870,A 5 GHz class-AB power amplifier in 90 nm CMOS with digitally-assisted AM-PM correction,This paper presents a technique for correcting AM-PM distortion in power amplifiers. The technique uses a varactor as part of a tuned circuit to introduce a phase shift that counteracts the AM-PM distortion of the PA. The varactor is controlled by the amplitude of the IQ baseband data in a feedforward fashion. The technique has been demonstrated in a class-AB CMOS power amplifier designed for WEAN applications and implemented in a 90 nm CMOS process. The PA delivers 10.5 dBm of average power while transmitting at 54 Mbps (64 QAM). The proposed technique is shown to improve the efficiency of the PA by a factor of 2,2005,0,
870,871,Enhanced detection of electrode placement/connection errors,Lead connection and electrode positional errors are a common problem in ECG recording. This study set out to review the sensitivity and specificity of existing criteria in the Glasgow program using an older (1997) version of the software and to produce enhancements where required for incorporation into the current version of the program still in development. 50 volunteers were recruited to the study. Arm and leg lead connection errors were introduced as were V1/V2 and V2/V3 connection reversals. It was shown that detection of arm lead connection errors could be enhanced from 64% to 88% at 100% specificity. Chest lead misconnections were detected with improved sensitivity. V1 and V2 reversal was much more easily detected than V2 and V3 reversal while maintaining high specificity.,2008,0,
871,872,Distance errors correction for the time of flight (ToF) cameras,One of the most important distance measurement errors is produced by light reflections. These errors can't be avoided and black are more affected than white objects. The measured distance by the ToF camera to an object in the scene changes if surrounding objects are moved. The distance error can be greater than 50% and camera calibration is useless if objects are moved. The calibration method we propose can be performed in any conditions not only in the laboratory. The distance errors for all objects in the scene can be corrected if on the objects are attached white or black tags/ labels. The ToF cameras can be improved using an active illumination with structured light. The improvement will eliminate the distance errors produced by light reflections.,2008,0,
872,873,Lessons learned in building a fault-tolerant CORBA system,"The Eternal system pioneered the interception approach to providing transparent fault tolerance for CORBA, which allows it to make a CORBA application reliable with little or no modification to the application or the ORB. The design and implementation of the Eternal system has influenced industrial practices by providing the basis for the specifications of the fault-tolerant CORBA standard that the Object Management Group adopted. We discuss our experience in developing the Eternal system, with particular emphasis on the challenges that we encountered and the lessons that we learned.",2002,0,
873,874,A Practical Framework of Realizing Actuators for Autonomous Fault Management in SOA,"Due to the key features of service-oriented architecture (SOA); blackbox-nature of services, heterogeneity, service dynamism, and service evolvability, fault management in SOA is known to be more challenging than conventional system management. An efficient way of managing faults in SOA is to apply principles of autonomic computing (AC), of which process is specified in MAPE. The first two phases of MAPE are to monitor target systems and diagnose faults to determine underlying cause. The other two phases are to plan healing/actuation methods and to execute them. Devising methods to remedy service faults which can run in autonomous manner is a hard problem, mainly due to the remoteness and the limited visibility and controllability. In this paper, we present a practical framework to design actuators which can be invoked autonomously. By considering the relationships among fault, cause, and actuator, we derive the abstract and concrete actuators. For some essential concrete actuators, we present their algorithms which can be implemented in practice. We believe our proposed service actuation framework makes the realization of autonomous service management more feasible.",2009,0,
874,875,Fault-tolerant data delivery for multicast overlay networks,"Overlay networks represent an emerging technology for rapid deployment of novel network services and applications. However, since public overlay networks are built out of loosely coupled end-hosts, individual nodes are less trustworthy than Internet routers in carrying out the data forwarding function. Here we describe a set of mechanisms designed to detect and repair errors in the data stream. Utilizing the highly redundant connectivity in overlay networks, our design splits each data stream to multiple sub-streams which are delivered over disjoint paths. Each sub-stream carries additional information that enables receivers to detect damaged or lost packets. Furthermore, each node can verify the validity of data by periodically exchanging Bloom filters, the digests of recently received packets, with other nodes in the overlay. We have evaluated our design through both simulations and experiments over a network testbed. The results show that most nodes can effectively detect corrupted data streams even in the presence of multiple tampering nodes.",2004,0,
875,876,Behavioral analysis of a fault-tolerant software system with rejuvenation,"In recent years, considerable attention has been devoted to continuously running software systems whose performance characteristics are smoothly degrading in time. Software aging often affects the performance of a software system and eventually causes it to fail. A novel approach to handle transient software failures due to software aging is called software rejuvenation, which can be regarded as a preventive and proactive solution that is particularly useful for counteracting the aging phenomenon. In this paper, we focus on a high assurance software system with fault-tolerance and preventive rejuvenation, and analyze the stochastic behavior of such a highly critical software system. More precisely, we consider a fault-tolerant software system with two-version redundant structure and random rejuvenation schedule, and evaluate quantitatively a dependability measure like the steady-state system availability based on the familiar Markovian analysis. In numerical examples, we examine the dependence of two system diversity techniques; design and environment diversity techniques, on the system dependability measure.",2005,0,
876,877,FISCADE - A Fault Injection Tool for SCADE Models,"This paper presents the FISCADE fault injection tool which has been developed as a plug-in to SCADE (Safety-Critical Application Development Environment). The tool automatically replaces original operators with fault injection nodes (FINs). A FIN is a node that encapsulates the original operator so the operator can be replaced or the operator output can be manipulated. During execution of the generated source code, FISCADE controls the SCADE simulator to execute the model, inject the fault, and log the results. The tool allows the user to inject errors (activated faults) in all signals in the model. Furthermore FISCADE can simulate specification of design errors by automatically replacing operators with fault injection nodes, as well as simulating transient, intermittent or permanent faults affecting memories and CPU registers. The tool automatically performs a pre-injection analysis to reduce the number of fault injection experiments needed and supports the work of configuring and carrying out automated fault injection campaigns.",2007,0,
877,878,Perceptually Unequal Packet Loss Protection by Weighting Saliency and Error Propagation,"We describe a method for achieving perceptually minimal video distortion over packet-erasure networks using perceptually unequal loss protection (PULP). There are two main ingredients in the algorithm. First, a perceptual weighting scheme is employed wherein the compressed video is weighted as a function of the nonuniform distribution of retinal photoreceptors. Secondly, packets are assigned temporal importance within each group of pictures (GOP), recognizing that the severity of error propagation increases with elapsed time within a GOP. Using both frame-level perceptual importance and GOP-level hierarchical importance, the PULP algorithm seeks efficient forward error correction assignment that balances efficiency and fairness by controlling the size of identified salient region(s) relative to the channel state. PULP demonstrates robust performance and significantly improved subjective and objective visual quality in the face of burst packet losses.",2010,0,
878,879,Detection of Rotor Faults in Brushless DC Motors Operating Under Nonstationary Conditions,"There are several applications where the motor is operating in continuous nonstationary operating conditions. Actuators and servo motors in the aerospace and transportation industries are examples of this kind of operation. Detection of faults in such applications is, however, challenging because of the need for complex signal processing techniques. Two novel methods using windowed Fourier ridges and Wigner-Ville-based distributions are proposed for the detection of rotor faults in brushless dc motors operating under continuous nonstationarity. Experimental results are presented to validate the concepts and illustrate the ability of the proposed algorithms to track and identify rotor faults. The proposed algorithms are also implemented on a digital signal processor to study their usefulness for commercial implementation",2006,0,
879,880,The check-pointed and error-recoverable MPI JAVA library of agent teamwork grid computing middleware,"We are implementing a fault-tolerant mpiJava API on top of the AgentTeamwork grid-computing middleware system. Our mpiJava implementation consists of the mpiJava API, the GridTcp socket library, and the user program wrapper, each providing a user with the standard mpiJava functions, facilitating message-recording/error-recovering socket connections, and monitoring a user process. This paper presents the application framework, mpiJava implementation, and communication performance in AgentTeamwork.",2005,0,
880,881,"ACE: an aggressive classifier ensemble with error detection, correction and cleansing","Learning from noisy data is a challenging and reality issue for real-world data mining applications. Common practices include data cleansing, error detection and classifier ensembling. The essential goal is to reduce noise impacts and enhance the learners built from the noise corrupted data, so as to benefit further data mining procedures. In this paper, we present a novel framework that unifies error detection, correction and data cleansing to build an aggressive classifier ensemble for effective learning from noisy data. Being aggressive, the classifier ensemble is built from the data that has been preprocessed by the data cleansing and correcting techniques. Experimental comparisons will demonstrate that such an aggressive classifier ensemble is superior to the model built from the original noisy data, and is more reliable in enhancing the learning theory extracted from noisy data sources, in comparison with simple data correction or cleansing efforts",2005,0,
881,882,Hybrid intelligent fault diagnosis based on granular computing,"To solve the problem of lacking hybrid modes and common algorithms in hybrid intelligent diagnosis, this paper presents a new approach to hybrid intelligent fault diagnosis of the mechanical equipment based on granular computing. The hybrid intelligent diagnosis model based on neighborhood rough set is constructed in different granular levels, and the results of support vector machines (SVMS) and artificial neural network (ANN) in granular levels are combined by criterion matrix algorithm as output of hybrid intelligent diagnosis. Finally, the proposed model is applied to fault diagnosis in roller bearings of high-speed locomotive. The applied results show that the classification accuracy of hybrid model reaches to 97.96%, which is 8.49% and 39.12% higher than the classification accuracy of SVMS and ANN respectively. It shows that the proposed model as a new common algorithm can reliably recognize different fault categories and effectively enhance robustness of the hybrid intelligent diagnosis model.",2009,0,
882,883,Fault Detection by Means of HilbertHuang Transform of the Stator Current in a PMSM With Demagnetization,"This paper presents a novel method to diagnose demagnetization in permanent-magnet synchronous motor (PMSM). Simulations have been performed by 2-D finite-element analysis in order to determine the current spectrum and the magnetic flux distribution due to this failure. The diagnostic just based on motor current signature analysis can be confused by eccentricity failure because the harmonic content is the same. Moreover, it can only be applied under stationary conditions. In order to overcome these drawbacks, a novel method is used based upon the Hilbert-Huang transform. It represents time-dependent series in a 2-D time-frequency domain by extracting instantaneous frequency components through an empirical-mode decomposition process. This tool is applied by running the motor under nonstationary conditions of velocity. The experimental results show the reliability and feasibility of the methodology in order to diagnose the demagnetization of a PMSM.",2010,0,
883,884,Localizing Software Faults Simultaneously,"Current automatic diagnosis techniques are predominantly of a statistical nature and, despite typical defect densities, do not explicitly consider multiple faults, as also demonstrated by the popularity of the single-fault Siemens set. We present a logic reasoning approach, called Zoltar-M(ultiple fault), that yields multiple-fault diagnoses, ranked in order of their probability. Although application of Zoltar-M to programs with many faults requires further research into heuristics to reduce computational complexity, theory as well as experiments on synthetic program models and two multiple-fault program versions from the Siemens set show that for multiple-fault programs this approach can outperform statistical techniques, notably spectrum-based fault localization (SFL). As a side-effect of this research, we present a new SFL variant, called Zoltar-S(ingle fault), that is provably optimal for single-fault programs, outperforming all other variants known to date.",2009,0,
884,885,Interface faults injection for component-based integration testing,"This paper presents a simple and improved technique of interface fault insertion for conducting component integration testing through the use of aspect-oriented software development (AOSD). Taking the advantage of aspect's cross-cutting features, this technique only requires additional codes written in AspectJ rather than having a separate tool to perform this operation. These aspect codes act as wrappers around interface services and perform operations such as disabling the implementation of the interface services, raising exceptions or corrupting the inputs and outputs of interface services. Interface faults are inserted into the system under test to evaluate the quality of the test cases by ensuring not only that they detect errors due to the interactions between components, but they are also able to handle exceptions raised when interface faults are triggered.",2006,0,
885,886,Combinational fault diagnosis in a monitored environment by a wireless sensor network,"This paper studies a combinational algorithm of a limit-trend checking, plausibility test and model-based method to attain a secure fault diagnosis in a wireless sensor network. It has been implemented based on a new theoretical identification method. The sensor nodes of the network have been distributed inside an intelligent container to monitor environmental parameters (temperature and relative humidity). It employs measured parameters, residuals and a developed model of the environment to introduce a topology, applicable in several applications of fault diagnosis area.",2009,0,
886,887,Modeling a fault-tolerant distributed system,A C-based simulation model of the time-triggered protocol (TTP/C) has been designed and implemented as a tool for verifying the properties of a system designed on the basis of it. The model has been provided with a user-friendly interface to allow easy visualization and evaluation of the results. The functionality of this general-purpose model is demonstrated on a simple TTP/C cluster application running under the influence of fault injection. The first round of experiments shows that the system is tolerant toward some typical transient faults like memory data distortion.,2001,0,
887,888,Induction machines fault simulation based on FEM modelling,Induction machines operated by inverter use to present efficiency decrease and sometimes present additional rotor and stator fault. FEM has been used for faulty motor simulation and shows results of motor fault effects. Experimental results corroborate the simulation and theoretical effects.,2007,0,
888,889,"Corrections to On the Suitability of a High-<formula formulatype=""inline""> <img src=""/images/tex/18651.gif"" alt=""k""> </formula> Gate Dielectric in Nanoscale FinFET CMOS Technology","In the above titled paper (ibid., vol. 55, no. 7, pp. 1714-1719), the propagation delays in Table III are incorrect, being too long by a factor of two. Furthermore, there was a typo in the table title. The corrected table and title are presented here.",2009,0,
889,890,Techniques for fast transient fault grading based on autonomous emulation [IC fault tolerance evaluation],"Very deep submicron and nanometer technologies have increased notably integrated circuit (IC) sensitivity to radiation. So errors are currently appearing in ICs working at the Earth's surface. Hardened circuits are currently required in many applications where fault tolerance (FT) was not a requirement in the very near past. The use of platform FPGAs for the emulation of single-event upset effects (SEU) is gaining attention in order to speed up the FT evaluation. In this work, a new emulation system for FT evaluation with respect to SEU effects is proposed, providing shorter evaluation times by performing all the evaluation process in the FPGA and avoiding emulator-host communication bottlenecks.",2005,0,
890,891,The utility of hybrid error-erasure LDPC (HEEL) codes for wireless multimedia,"Traditional wireless communication protocols do not relay corrupted packets towards the application layer and neither do they forward such packets over multiple hops. Such an approach can lead to a significant number of packet drops and thus a severe deterioration in performance of high bandwidth applications. Cross-layer protocols which do relay and forward corrupted packets have exhibited substantial promise to mitigate the above problem and thus their utility for wireless multimedia needs to be explored further. Moreover, there is a need to identify efficient channel coding methods for the cross-layer channel. Unlike the traditional schemes, where the channel observed at the application layer is a pure erasure channel, in the cross-layer schemes the application layer channel exhibits hybrid erasure-error impairments. Thus in this paper, we use a rather abstract link-layer model on the basis of which we compare the performance of cross-layer and conventional schemes. We identify the modifications required to be made to RS and LDPC based FEC schemes in order to use them over hybrid erasure-error channels. Finally we compare the considered schemes in terms of video quality using the emerging H.264 video standard. Our video analysis is based on employing a hybrid error-erasure channel coding FEC for the cross-layer schemes versus employing erasure recovery FEC for the traditional protocols. We show that cross-layer schemes can lead to a significant improvement in video quality.",2005,0,
891,892,Digital processing of touch signal - error probability,"A new algorithm for digital processing of a touch signal is proposed. We consider the calculation of the error probability in the process of deciding on the existence of the optical infrared beam between the infrared transmitter and receiver. The error probability is calculated in the function of: (a) the ambient illumination simulated by the reflector placed a certain distance in front of the center of the touch interface; (b) window functions implemented in the algorithm for processing of the touch signal. The error probability is calculated for some classical, time-symmetrical as well as for some original, time-asymmetrical window functions.",2001,0,
892,893,Evaluation of a Monte Carlo scatter correction in clinical 3D PET,"Phantom and patient data were used to compare performance of a one-iteration Monte Carlo scatter correction (MC-SC-1i) for 3D PET, a vendor-supplied one-iteration single scatter model-based correction (SSS-1i) for 3D PET, unscatter-corrected 3D PET (No-SC), a SSS-1i followed by Monte Carlo scatter correction as a second iteration (MC-SSS) for 3D PET, and a convolution-subtraction scatter correction for 2D PET in terms of quantitative accuracy and lesion detectability. ROI analysis showed 2D PET images were more accurate than 3D, particularly for large phantoms, and MC-SSS corrected 3D PET images were more accurate than SSS-1i corrected 3D PET images for this data set. 2D and 3D PET images were reconstructed from 59 patient data sets. Bias of 3D PET images with respect to 2D images was determined using Corresponding Intensity Variance. 3D PET uncorrected images overestimated activity by 50% (smallest patients) to 150% (largest patients). The average absolute bias of SSS-1i corrected images (16%) was twice that of MC-SSS (8%) and more dependent on patient size. Lesion detection sensitivity in these patient images was evaluated using a Channelized Hotelling Observer. Scatter corrected 3D PET images performed 10% better than uncorrected 3D PET images for smaller patients. Slightly better lesion sensitivity was seen for large patients in images reconstructed using SSS-1i (CHO-SNR=2.230.29) compared to MC-SSS (2.080.27) and uncorrected images (2.020.23).",2003,0,
893,894,Harmonic resistance emulator technique for three-phase unity power factor correction,"In this paper, a new technique for three-phase power factor correction, using the typical three-phase line side active front-end converter, is proposed. The proposed technique is capable of simplifying the three-phase power factor correction algorithms to a greater extent. As a consequence, the sampling time will reduce considerably and switching frequency of the converter can be pushed further. The proposed scheme is suitable for the sine-triangle PWM (pulse width modulation) implementation but it completely eliminates the need of frame-synchronization. It also avoids the forward and backward d-q reference-frame transformations. Moreover, presetting of the two orthogonal references is also not required. Simulation results are presented.",2005,0,
894,895,On the value of static analysis for fault detection in software,"No single software fault-detection technique is capable of addressing all fault-detection concerns. Similarly to software reviews and testing, static analysis tools (or automated static analysis) can be used to remove defects prior to release of a software product. To determine to what extent automated static analysis can help in the economic production of a high-quality product, we have analyzed static analysis faults and test and customer-reported failures for three large-scale industrial software systems developed at Nortel Networks. The data indicate that automated static analysis is an affordable means of software fault detection. Using the orthogonal defect classification scheme, we found that automated static analysis is effective at identifying assignment and checking faults, allowing the later software production phases to focus on more complex, functional, and algorithmic faults. A majority of the defects found by automated static analysis appear to be produced by a few key types of programmer errors and some of these types have the potential to cause security vulnerabilities. Statistical analysis results indicate the number of automated static analysis faults can be effective for identifying problem modules. Our results indicate static analysis tools are complementary to other fault-detection techniques for the economic production of a high-quality software product.",2006,0,
895,896,Adaptive noise canceller using LMS algorithm with codified error in a DSP,"In this paper we present an implementation of a digital adaptive filter on the digital signal processor TMS320C6713, using a variant of the LMS algorithm, which consists in error codification, thus the speed of convergence is increased and the complexity of design for its implementation in digital adaptive filters is reduced, because the resulting codified error is composed of integer values. The LMS Algorithm with codified error (ECLMS), was tested in an environmental noise canceller and the results demonstrate an increase in the convergence speed, and a reduction of processing time.",2009,0,
896,897,Polish N-Grams and Their Correction Process,"Word n-gram statistics collected from over 1 300 000 000 words are presented. Eventhough they were collected from various good sources, they contain several types of errors. The paper focuses on the process of partly supervised correction of the n- grams. Types of errors are described as well as our software allowing efficient and fast corrections.",2010,0,
897,898,Manga University: web-based correction system for artistic design education,"In artistic design education, the teacher instructs each student individually face to face. As a result, it is difficult to share coaching with a third party or to teach distantly. To solve these problems, we have developed Manga University, which is a web-based application to aid artistic design education. It enables distant teaching or shared teaching and provides a learning portfolio for collaboration. It is applicable to several other types of artistic design education, such as fashion design or GUI design for software or the web.",2002,0,
898,899,"A Case Study of an Electrolytic Tinning Line, with an Analysis of Faults in the Power Rectifiers",A model is represented in this paper to simulate the tin plating coating process. The control of the output current provided by high current rectifiers (HCR) is the best way to assure the quality of the final product. The model includes the interaction of power rectifiers and thickness of the final coating. ARMAX model has been used for this purpose. An application of the model is presented to identify faults in the set of power rectifiers,2005,0,
899,900,Wavelet analysis based protection for high impedance ground fault in supply systems,"Many high impedance ground faults (HIGF) that happen in a low voltage (LV) system often cause loss of customer supply, fire and human safety hazards. Traditional ground fault protection is provided by residual current circuit breaker (RCCB). The RCCB often causes nuisance tripping and it is difficult to detect HIGF. Wavelet analysis based HIGF protection is developed in the paper. The wavelet transform is applied to filter out some frequency bands of harmonics from residual current and line current. The root mean square (RMS) value of harmonics are calculated using their wavelet coefficients directly. HIGF is identified from disturbance by the RMS difference between the residual current and the line current. The digital protection scheme is designed. EMTP simulation results show that the new protection is able to detect HIGF and prevent electric shock with high sensitivity and robustness.",2002,0,
900,901,A fault tolerance infrastructure for high-performance COTS-based computing in dependable space systems,"A fundamental solution that allows the use of high-performance, but poorly checked processors in dependable space systems is the use of a generic, hierarchical, fault-tolerant hardware infrastructure (FTI). This FTI is a software-independent innermost defense for an autonomous, fault-tolerant long-life system that may also employ other, especially software-based , fault tolerance techniques. The entire FTI is fault-tolerant and contains no software, thus being immune to malicious software intrusions.",2004,0,
901,902,A fault-tolerant structure for reliable multi-core systems based on hardware-software co-design,"To cope with the soft errors and make full use of the multi-core system, this paper gives an efficient fault-tolerant hardware and software co-designed architecture for multi-core systems. And with a not large number of test patterns, it will use less than 33% hardware resources compared with the traditional hardware redundancy (TMR) and it will take less than 50% time compared with the traditional software redundancy (time redundant).Therefore, it will be a good choice for the fault-tolerant architecture for the future high-reliable multi-core systems.",2010,0,
902,903,Error sensitivity data structures and retransmission strategies for robust JPEG 2000 wireless imaging,"In this paper we address the problem of JPEG 2000 imaging in a wireless environment. We first define a flexible and efficient data structure for the description of the error sensitivity of different parts of a JPEG 2000 codestream or file format; the data structure is designed in such a way that it can seamlessly integrated as payload of a JPEG 2000 marker segment or file format box. Moreover, we investigate ARQ policies for robust packet-based JPEG 2000 image transmission over 3G mobile communication systems, and highlight how the proposed data structure can be exploited to improve the end-to-end performance.",2003,0,
903,904,The framework of a web-enabled defect tracking system,"This paper presents an evaluation and investigation of issues to implement a defect management system; a tool used to understand and predict software product quality and software process efficiency. The scope is to simplify the process of defect tracking through a web-enabled application. The system will enable project management, development, quality assurance and software engineer to track and manage problem specifically defects in the context of software project. A collaborative function is essential as this will enable users to communicate in real time mode. This system makes key defect tracking coordination and information available disregards the geographical and time factor.",2004,0,
904,905,"Operation, Design and Testing of Generator 100% Stator Earth Fault Protection Using Low Frequency Injection","This paper describes the development of a new 100% generator stator earth fault protection scheme, based on low frequency injection principle. The design of a new analogue input module and the digital filtering technique are presented. Results of the simulation and site testing are also discussed.",2008,0,
905,906,Toward fault-tolerant and reconfigurable digital microfluidic biochips,"Microfluidics-based biochips are revolutionizing high-throughput sequencing, parallel immunoassays, blood chemistry for clinical diagnostics, and drug discovery. These devices enable the precise control of nanoliter volumes of biochemical samples and regents. They combine electronics with biology, and they integrate various bioassay operations, such as sample preparation, analysis, separation, and detection. This survey paper provides an overview of droplet-based digital microfluidic biochips. It describes emerging techniques for designing fault-tolerant and reconfigurable digital microfluidic biochips. Recent advances in fault modeling, testing, diagnosis and reconfiguration techniques are presented. These quality-driven techniques ensure that biochips can be used reliably during liquid-based biochemical assays.",2010,0,
906,907,A simulation technique for the evaluation of random error effects in time-domain measurement systems,"While many papers deal with time-domain network analyzer calibration procedures for the correction of systematic errors, little work has been published about the treatment of random errors. This paper is focused on the evaluation of random error effects in time-domain measurement systems. As a first step, an experimental identification of the measurement system random errors is achieved. Random errors addressed are jitter, vertical noise, and fast time drifts. Based on this identification, mathematical models are developed to simulate random errors. At a second step, time-domain measurements are simulated with these random errors. These simulations are used to predict measurement system repeatability and dynamic range. Then, as an application example, simulations of the measurement of the complex propagation coefficient and S parameters of a lossy mismatched microstrip line are achieved. By comparison with real measurements, it is shown that random error effects can be accurately predicted by Monte Carlo simulations",2001,0,
907,908,Bayesian Calibration of a Lookup Table for ADC Error Correction,"This paper presents a new method for the correction of nonlinearity errors in analog-to-digital converters (ADCs). The method has been designed to allow a self-calibration in systems where an internal signal can be generated, such as base stations for mobile communications. The method has been implemented and tested in simulation on the behavioral model of commercial ADCs and on a hardware setup composed by a data acquisition board and a distorting circuit",2007,0,
908,909,A Novel Fault Observer Design and Application in Flight Control System,"For estimating the fault signals of the discrete flight control system with unknown senor faults and actuator faults simultaneously, a discrete proportional integral observer (PIO) is presented. The proposed PIO uses the augment states to estimate the sensor faults of the flight control system. Moreover, this observer uses an additionally introduced integral term of the output error to obtain the estimation of actuator faults. The convergence of the PIO is proved. The proposed fault observer is applied in flight control system. Simulation results are given to demonstrate the effectiveness of the proposed fault observer.",2009,0,
909,910,Fault detection of open-switch damage in voltage-fed PWM motor drive systems,This paper investigates the use of different techniques for fault detection in voltage-fed asynchronous machine drive systems. With the proposed techniques it is possible to detect and identify the power switch in which the fault has occurred. Such detection requires the measurement of some voltages and is based on the analytical model of the voltage source inverter. Simulation and experimental results are presented to demonstrate the correctness of the proposed techniques. The results obtained so far indicate that it is possible to embed some fault-tolerant properties for the voltage-fed asynchronous machine drive system.,2003,0,
910,911,Fault Tolerant Control in NCS Medium Access Constraints,"This paper deals with the problem of fault-tolerant control of a Network Control System (NCS) for the case in which the sensors, actuators and controller are inter-connected via various Medium Access Control protocols which define the access scheduling and collision arbitration policies in the network and employing the so-called periodic communication sequence. A new procedure for controlling a system over a network using the concept of an NCS-Information-Packet is described which comprises an augmented vector consisting of control moves and fault flags. The size of this packet is used to define a <i>Completely Fault Tolerant NCS.</i> The fault-tolerant behaviour and control performance of this scheme is illustrated through the use of a process model and controller. The plant is controlled over a network using Model-based Predictive Control and implemented via MATLABcopy and LABVIEWcopy software.",2007,0,
911,912,Modeling and simulation of inner defect in impulse storage capacitor,"Because of big capability and small volume, impulse storage capacitor was found that the fast impulse would do much damage to capacitor insulation. Based on the electrical discharge mechanism, several classical defects of capacitor were put forward in this paper. To estimate the status of insulation, the electrical field distribution of defects should be analyzed carefully. As the most popular defect in storage capacitor, inner defect models had been designed for FEA (finite element analysis). Through simulation and analysis, the result proved that the different size and location of inner defect in insulation would result in dissimilar partial concentration and aberrance of electrical field distribution.",2005,0,
912,913,Six-phase brushless DC motor for fault tolerant electric power steering systems,"This paper is focused on the development of a multiphase fault tolerant BLDC machine, oriented towards the stator windings, number of phases and commutation sequences. As a first step, a six-phase BLDC machine will be modeled and simulated, using JMAG-studio software, for no-load regime. The induced emfs, cogging torque, magnetic field density map and distribution will be processed. The results of simulation, in terms of induced emfs, will give the possibility to develop, in the second step, the commutation sequences in order to get optimal torque quality.",2007,0,
913,914,Segmented attenuation correction using <sup>137</sup>Cs single photon transmission,"Using a <sup>137</sup>Cs single photon transmission source for transmission scanning allows a higher photon flux and thus, better transmission statistics compared to coincidence transmission scanning. However, <sup>137</sup>Cs suffers from a high scatter fraction as well as emission contamination, both leading to an underestimation of the attenuation values. On our NaI- and GSO-systems this is currently compensated by subtracting emission contamination, scatter-scaling and re-mapping. Histogram based segmentation, widely used to shorten the scan time on <sup>68</sup>Ge devices, inherently is capable to compensate for a potential bias in the attenuation values. We have investigated segmented attenuation correction for <sup>137</sup>Cs transmission scans with NaI(Tl) PET scanners in previous work, and came to the conclusion that our current processing was superior to the formerly used segmentation routine. In this paper we re-investigate segmentation, however, using a more sophisticated algorithm. Our focus was mainly to improve the accuracy of our transmission scans rather than shorten the scan times. However, the potential to reduce the scan duration was investigated as well",2001,0,
914,915,Fault tolerant switched reluctance machine for fuel pump drive in aircraft,"As switched reluctance motors (SRM) generally offer a simple and robust design, they are very suitable for an aircraft main engine fuel pump drive which needs to be actuated by fault tolerant drives. Based on analytical comparison the merits and demerits of some different machine topologies including redundancies, a six phase 12/8 fault-tolerant SRM is proposed and designed for fuel pump drive application. The finite element model based on field-circuit coupling is established, results indicate that this machine meets the needs of demanding fuel pump drive system in aerospace environments.",2009,0,
915,916,A Mac-error-warning method for SCTP congestion control over high BER wireless network,"The problem of high BER (bit error rate) usually plagues the wireless connection, especially for some real time applications such as VoIP (voice over IP) and some military uses. The newly developed transport layer protocol SCTP (stream control transmission protocol) also has to face this problem. Though equipped with many new features, SCTP congestion control mechanism fails to distinguish wireless loss from congestion loss, thus its performance over high BER wireless network suffers from unnecessary congestion window decreasing. To improve the performance of SCTP in such a scenario, a Mac-error-warning method is proposed in this paper. Simulation experiments conducted through extended ns-2 validated that the proposed method could achieve higher throughput. The throughput improvement arrives at 946.22% when BER is 0.0005.",2005,0,
916,917,Comparison of Euclidean distance based neural networks for analog Integrated Circuits fault recognition- LVQS &SOM,"The advent of integrated circuits (ICs) and hence the subsequent miniaturization of electronic circuitry has brought out considerable difficulties encountered during identification of faults in integrated circuits during the testing phase of manufacturing and subsequent mass production. Artificial neural network (ANN) augurs well in handling such complex tasks in such systems as it generalizes well without the need to explicitly define the relationship between variables. There has been resurgence in interest among researchers in utilizing ANN for recognizing faults in analog circuits. This work aims at analyzing the role played by the various training parts of both the Euclidean distance based ANNs namely, the self organizing feature maps(SOM) and various versions of learning vector quantization neural network (LVQNN)i.e., LVQ1, LVQ 2, LVQ 2.1 and LVQ. Extensive studies have been conducted to ascertain the role played by learning rate and other unique parameters such as the role played by normalization as a part of preprocessing technique and the number of iterations for convergence. Moreover the results have been compared with the generalized multilayer feedforward network with back propagation algorithm. The best combination of network parameters was also determined. For this purpose an analog filter circuit with 1 fault free and 10 single hard faults was simulated using SPICE simulation software. Experimental results demonstrate the high classification accuracy and the adaptability of both the Euclidean classifiers and its suitability for fault recognition in analog circuits.",2007,0,
917,918,Fault Tolerant Control of a Civil Aircraft Using a Sliding Mode Based Scheme,"This paper presents a sliding mode control scheme for reconfigurable control of a civil aircraft. The controller is based around a state-feedback sliding mode scheme where the nonlinear unit vector term is allowed to adaptively increase when the onset of a fault is detected. Compared to other fault tolerant controllers which have been implemented on this model, the controller proposed here is relatively simple and yet is shown to work across the entire `up and away' fight envelope. Unexpected deviation of the switching variables from their nominal condition triggers the adaptation mechanism.",2005,0,
918,919,Fault detection based on H<inf></inf> states observer for networked control systems,"The influence of random short time-delay to networked control systems (NCS) is changed into an unknown bounded uncertain part. Without changing the structure of the system, an H<inf></inf> states observer is designed for NCS with short time-delay. Based on the designed states observer, a robust fault detection approach is proposed for NCS. In addition, an optimization method for the selection of the detection threshold is introduced for better tradeoff between the robustness and the sensitivity. Finally, some simulation results demonstrate that the presented states observer is robust and the fault detection for NCS is effective.",2009,0,
919,920,User-Centered Interface Reconfiguration for Error Reduction in Human-Computer Interaction,"Human-computer interaction (HCI) is greatly influenced by findings in psychological research concerning interaction with complex technical processes. Psychological concepts like perception and comprehension of information, as well as projection of future system states can be summarized as situation awareness. Poor situation awareness can increase the probability of human error during interaction, caused by erroneous recall and interpretation of percept information through an interface that does not match the user's understanding and mental capabilities. Therefore, a new approach to human-centric reconfiguration of user interfaces will be presented to support situation awareness and, in this way, reduce human errors that occur during interaction.",2010,0,
920,921,Demagnetization Analysis of Permanent Magnet Synchronous Machines under Short Circuit Fault,"Sudden symmetrical short circuit is a serious fault when the entire demagnetization or partial demagnetization of permanent magnet can occur. The aim of this paper is to analyze the demagnetization phenomenon of permanent magnet synchronous machines (PMSM) based on FEM and analytic method. Firstly, the transient FEM is utilized to analyze the demagnetization operating point of permanent magnet when symmetrical short circuit occurs. The computed time evolution of the maximum partial demagnetization operating point is derived. Secondly, the synthesized magnetomotive force (MMF) of the short circuit current is analyzed by analytical method. Through the analysis some characters are obtained. At last several factors that affect the demagnetization operating point are summarized and several measures are put forward to improve the max demagnetization operating point of permanent magnet.",2010,0,
921,922,FuSE - a hardware accelerated HDL fault injection tool,"The ongoing miniaturization of digital circuits makes them more and more susceptible to faults which also complicates the design of fault tolerant systems. In this context fault injection plays an important role in the process of fault tolerance validation. As a result many fault injection tools have emerged during the last decade. However these tools only operate on specific domains and can therefore be referred to as hardware- or software-, simulation- or emulation based techniques. In this paper we present FuSE, a single fault injection tool which covers multiple domains as well as different fault injection purposes. FuSE has been designed for usage with the SEmulator<sup>reg</sup>-an FPGA-based hardware accelerator. The created tool set has been fully automated for the fault injection process and only requires a VHDL description and a test bench of the circuit under test. FuSE can then perform fault injection experiments with a diagnostic resolution that is known from simulation-based approaches, but at a speed that even handles long running experiments with ease.",2009,0,
922,923,Dealing with dormant faults in an embedded fault-tolerant computer system,"Accumulation of dormant faults is a potential threat in a fault tolerant system, especially because most often fault tolerance is based on the single-fault assumption. We investigate this threat by the example of an automotive steer-by-wire application based on the Time-Triggered Architecture (TTA). By means of a Markov model we illustrate that the effect of fault dormancy can degrade the MTTF of a system by several orders of magnitude. We study potential remedies, of which transparent online testing proves to be the most powerful one, while taking a hot spare offline temporarily to test it provides a more feasible solution, though with tight constraints regarding the test duration.",2003,0,
923,924,"Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems","Biological structures and organizations in nature, from gene, molecular, immune systems, and biological populations, to ecological communities, are built to stand against perturbations and biological robustness is therefore ubiquitous. Furthermore, it is intuitively obvious that the counterpart of bio-robustness in engineered systems is fault tolerance. With the objective to stimulate inspiration for building reliable and survivable computer networks, this paper reviews the state-of-the-art research on bio-robustness at different biological scales (level) including gene, molecular networks, immune systems, population, and community. Besides identifying the biological/ecological principles and mechanisms relevant to biological robustness, we also review major theories related to the origins of bio-robustness, such as evolutionary game theory, self-organization and emergent behaviors. Evolutionary game theory, which we present in a relative comprehensive introduction, provides an ideal framework to model the reliability and survivability of computer networks, especially the wireless sensor networks. We also present our perspectives on the reliability and survivability of computer networks, particularly wireless sensor and ad hoc networks, based on the principles and mechanisms of bio-robustness reviewed in the paper. Finally, we propose four open questions including three in engineering and one in DNA code robustness to demonstrate the bidirectional nature of the interactions between bio-robustness and engineering fault tolerance.",2008,0,
924,925,A Systematic Robot Fault-Tolerance Approach,"This paper introduces a systematic approach to suggesting proper fault-tolerance techniques for robots. We classified general robot faults into six types, and developed a UML profile to organize and model a dependancy structure of existing fault-tolerance techniques. In future, we will organize fault-tolerance techniques into the form of the UML profile and determine the relationships between each fault and techniques.",2009,0,
925,926,Fault Diagnosis Way Based on RELAX Algorithms in Frequency Domain for the Squirrel Cage Induction Motors,"The traditional method of spectrum analysis on the current signal via FFT is hard to diagnose the fault of the broken rotor bars. This paper presents a fault diagnosis method using the RELAX algorithm in frequency domain. It can estimate the amplitude and phase values of various frequency components using coarse and fine estimation according to the criterion of minimum energy. Finally the fundamental component can be eliminated in frequency domain after the expressions of the above frequency components are constructed. Compared with the method of eliminating the fundamental component in time domain, this method has the advantage of computing faster although less accurate. However, it has been proved that the algorithm can highlight the fault characteristic and value a lot early in the motor fault diagnosis.",2010,0,
926,927,Ionospheric corrections from a prototype operational assimilation and forecast system,"This paper describes an operational system, sponsored by the US Air Force, for generating and distributing near real-time three-dimensional ionospheric electron densities and corresponding GPS propagation delays. The core ionospheric model solves plasma dynamics and composition equations governing evolution of density, velocity and temperature for ion species on a fixed grid in magnetic coordinates. It uses a realistic model of the Earth's magnetic field and solar indices obtained in real time from NOAA's Space Environment Center. At the present time the model computes real-time ion and electron densities at a grid of more than one million points. Higher resolutions are anticipated in the future. While the core model is capable of delivering realistic results, its accuracy can be significantly improved by employing a special set of numerical techniques known as data assimilation. These techniques originated and are currently used for numerical weather forecasting. The core ionospheric model is constantly fed real-time observational data from a network of reference GPS ground stations. This improves both the nowcast and the forecast of electron densities. Web-based access to the system is provided to early users for validation and exploration purposes at: http://fusionnumerics.com/ionosphere.",2004,0,
927,928,Efficiently utilization of redundancy backup server by forming dynamic clustering in Distributed Systems for tolerating faults,"This paper proposes a novel REDENDENCY THROUGH BACKUP PROCESS with CLUSTERING scheme which aims to address the following concerns: reliable, effective with high maintenance backup can be achieved by constructing a multi-node clustering backbone with a small number of backup cluster-heads for redundancy filling system through backup process for FAULT TOLRANCE DISTRIBUTED SYSTEMS. We can successfully reduce the overhead of backup servers and enhance the speed of backup delivering in an allowable time span compared to other redundancy fault tolerance distributed operating systems for both WAN and LAN networks. Next, we utilize the CLUSTER STRUCHING MECHANISM which determines the cluster size according to the leaving frequency of cluster-members. As the number of leaving events is reduced, the cluster topology is more stable and the BACKUP may also be available in a short and manageable time span before all the distributed system becomes failed. We have done our simulation using MATLAB to show cluster formation with a back-up server election and have investigated the performance of our system for different scenarios.",2010,0,
928,929,McC++/Java: Enabling Multi-core Based Monitoring and Fault Tolerance in C++/Java,"Monitoring and fault tolerance are important approaches to give high confidence that long-running online software systems run correctly. But these approaches will certainly cause high overhead cost, i.e. the loss of efficiency. Multi-core platforms can make such cost acceptable because of the advantage of the parallel performance. For allowing ordinary software developers without any knowledge of multi-core platforms to handle such programming tasks more efficiently, we propose an approach to enable multi-core based monitoring and fault tolerance in C++/Java.",2010,0,
929,930,A New Weak Fault Component Reactance Distance Relay Based on Voltage Amplitude Comparison,"A new weak fault component reactance distance relay is proposed in this paper. By adaptive setting of the compensated voltage, the scheme synthesizes the performance of the impedance distance relay and the reactance distance relay. The distance protection relay on the receiving end will misoperate when the fault resistance is larger than the critical resistance. So a new switching criterion is applied to eliminate this disadvantage. Based on that, the proposed scheme can detect the fault with the high fault resistance in the setting coverage, regardless of whether the relay is located at the receiving end or the sending end. Test results from the simulation and experimental conditions show that the new scheme is successful in detecting the internal fault. It has higher sensitivity and selectivity during different conditions than the traditional fault component protection schemes.",2008,0,
930,931,Differential busbar protection current circuits transients features during nonsimultaneous short faults,The paper deals with special features of transients in current transformer groups of differential busbar and busway protection. The unfaulted phases currents nature is described. The methods of determining of current transformer deep saturation condition and of these protections input currents extreme distortion condition are proposed. The methods described allow initial information obtaining necessary for stable protections algorithms development.,2003,0,
931,932,"An outlook on the dynamic error ""blind"" correction for the time-varying measurement channel","The paper presents the measuring system, which allows for correction of dynamic error caused by the analogue signal transducers, whose dynamic characteristics are changing with a rate approximate to the rate of change of the measured signal. Three methods for self-identification of the coefficients of the transducers' dynamics model, using exclusively the measured signal at the transducers' operating locations, are proposed. Analytical justification for the correctness of the proposed methods is presented for both: the special case of measuring periodic signals and for the general case when the measured signals are nonperiodic. The self-identification and correction procedures are performed as the algorithms processing the data collected from transducers.",2004,0,
932,933,Software Defect Content Estimation: A Bayesian Approach,"Software inspection is a method to detect errors in software artefacts early in the development cycle. At the end of the inspection process the inspectors need to make a decision whether the inspected artefact is of sufficient quality or not. Several methods have been proposed to assist in making this decision like capture recapture methods and Bayesian approach. In this study these methods have been analyzed and compared and a new Bayesian approach for software inspection is proposed. All of the estimation models rely on an underlying assumption that the inspectors are independent. However, this assumption of independence is not necessarily true in practical sense, as most of the inspection teams interact with each other and share their findings. We, therefore, studied a new Bayesian model where the inspectors share their findings, for defect estimate and compared it with Bayesian models in the literature, where inspectors examine the artefact independently. The simulations were carried out under realistic software conditions with a small number of difficult defects and a few inspectors. The models were evaluated on the basis of decision accuracy and median relative error and our results suggest that the dependent inspector assumption improves the decision accuracy (DA) over the previous Bayesian model and CR models",2006,0,
933,934,Broadband measurements of nanofiber devices: Repeatability and random error analysis,"On-wafer, broadband measurements of two-port nanofiber devices were made in order to test the short-term repeatability of a widely used measurement approach that builds on established on-wafer calibration techniques. The test devices used in this study consist of Pt nanowire and Au microbridge structures incorporated into two-port coplanar waveguides. Based on repeated measurements of these test structures, we computed statistical (Type A) uncertainties. The standard deviation (k=1) of five repeated measurements of a Pt nanowire device was less than 50 S. The analysis suggests refinements to the measurement process depending on the desired output of the measurements, e.g. the broadband response itself or the extraction of circuit model parameters.",2010,0,
934,935,Prioritizing Tests for Software Fault Localization,"Test prioritization techniques select test cases that maximize the confidence on the correctness of the system when the resources for quality assurance (QA) are limited. In the event of a test failing, the fault at the root of the failure has to be localized, adding an extra debugging cost that has to be taken into account as well. However, test suites that are prioritized for failure detection can reduce the amount of useful information for fault localization. This deteriorates the quality of the diagnosis provided, making the subsequent debugging phase more expensive, and defeating the purpose of the test cost minimization. In this paper we introduce a new test case prioritization approach that maximizes the improvement of the diagnostic information per test. Our approach minimizes the loss of diagnostic quality in the prioritized test suite. When considering QA cost as the combination of testing cost and debugging cost, on the Siemens set, the results of our test case prioritization approach show up to a 53% reduction of the overall QA cost, compared with the next best technique.",2010,0,
935,936,Automatic detection of head refixation errors in fractionated stereotactic radiotherapy (FSR),Patient surface images are acquired using a novel 3D camera when the patient is at the CT-simulation position and after setup for fractionated stereotactic treatment. The simulation and treatment images are aligned through an initial registration using several feature points followed by a refined automatic matching process using an iterative-closest-point mapping-align algorithm. All of the video-surface images could be automatically transformed to the machine coordinate according to the calibration file obtained from a template image. Phantom tests have demonstrated that we can capture surface images of patients in a second with spatial resolution of submillimeter. A millimeter shift and one-degree rotation relative to the treatment machine can be accurately detected. The entire process takes about two minutes. Our primary result on patients involved in a clinical trial is very promising. This research is partially supported by INH Grant 1R43CA91690-01 and NIH CA88843.,2004,0,
936,937,Error Rate Expression for Perpendicular Magnetic Recording,"An expression for estimating error rate in a perpendicular magnetic recording system is developed. The probability of error is estimated for a dominant dibit error event. Noise includes stationary white Gaussian head/electronics noise and nonstationary colored medium transition noise. Error rate is determined for a variety of parameter changes. In particular, it is shown how two systems with the same total signal-to-noise ratio can have different error rates. Expansion of the model to include additional signal and noise effects as well as the evaluation of different error events is discussed.",2008,0,
937,938,A New Method for Dynamic Fault Diagnosis of Electric Appliance,"A new method of fault diagnosis by dynamic modeling is put forward to obtain the diagnosis parameters of intelligent appliance in different running stages. The model identification approach using support vector regression (SVR) and immune clone selection algorithm (ICSA) is presented in this paper. The relation between process status and the temperature change rate is analyzed in the paper. For appliance fault with uncertainty, the way of fuzzy inference is applied for actualizing inference engine of fault diagnosis. Experimental results prove that the fault diagnosis method for intelligent appliance is credible in the accuracy.",2009,0,
938,939,"State-of-the-art, single-phase, active power-factor-correction techniques for high-power applications - an overview","A review of high-performance, state-of-the-art, active power-factor-correction (PFC) techniques for high-power, single-phase applications is presented. The merits and limitations of several PFC techniques that are used in today's network-server and telecom power supplies to maximize their conversion efficiencies are discussed. These techniques include various zero-voltage-switching and zero-current-switching, active-snubber approaches employed to reduce reverse-recovery-related switching losses, as well as techniques for the minimization of the conduction losses. Finally, the effect of recent advancements in semiconductor technology, primarily silicon-carbide technology, on the performance and design considerations of PFC converters is discussed.",2005,0,
939,940,Towards fault-tolerant software architectures,"""Software engineering has produced no effective methods to eradicate latent software faults. "" This sentence is, of course, a stereotype, but it is as true as a stereotype can get. And yet, it begs some questions. If it is not possible to construct a large software system without residual faults, is it at least possible to construct it to degrade gracefully if and when a latent fault is encountered? This paper presents the approach adopted on CAATS (Canadian Automated Air Traffic System), and argues that OO design and certain architectural properties are the enabling elements towards a true fault-tolerant software architecture",2001,0,
940,941,Techniques and experience in on-line transformer condition monitoring and fault diagnosis in ElectraNet SA,"With evolving maintenance strategies in the electricity industry internationally, there has been increasing pressure to develop improved techniques for condition monitoring. Specifically there has been a trade off between the speed and accuracy of testing. Traditionally, transformer condition monitoring involved high accuracy tests, which due to their duration, could only be performed on a discrete periodic basis. ElectraNet SA has experienced many limitations associated with this form of condition monitoring, and there has been a trend towards high speed on-line monitoring techniques for power transformers. Though these new techniques do not provide the level of accuracy found in traditional forms of testing, they overcome many of their limitations. This paper, describes ElectraNet SA's techniques and experience with power transformer monitoring",2000,0,
941,942,Assessment of fault location algorithms in transmission grids,"The increased accuracy into the fault's detection and location make an easier task for maintenance, this being the reason to develop new possibilities to a precise estimation of the fault location. The paper presents the results of the implementation of two fault location algorithms in ATP-EMTP program. Some ATP-MODELS modules were associated to the ATP model of different transmission grids, these modules being developed on basis of Takagi algorithm applied in two-machine systems and on basis of one algorithm processing synchronized positive-sequence phasor quantities on both transmission lines' terminals. DFT and A3 type filters were used to calculate the fundamental frequency phasors of the transient voltages and currents. There are presented some simulations', the considered parameters of the presented analysis being: line's load, fault's type and resistance and fault position along the overseen line.",2009,0,
942,943,On the Automation of Software Fault Prediction,"This paper discusses the issues involved in building a practical automated tool to predict the incidence of software faults in future releases of a large software system. The possibility of creating such a tool is based on the authors' experience in analyzing the fault history of several large industrial software projects, and constructing statistical models that are capable of accurately predicting the most fault-prone software entities in an industrial environment. The emphasis of this paper is on the issues involved in the tool design and construction and an assessment of the extent to which the entire process can be automated so that it can be widely deployed and used by practitioners who do not necessarily have any particular statistical or modeling expertise",2006,0,
943,944,"Modelling, calibration and correction of nonlinear illumination dependent fixed pattern noise in logarithmic CMOS image sensors","At present, most CMOS image sensors use an array of pixels with a linear response. However, logarithmic CMOS sensors are also possible, which are capable of imaging high dynamic range scenes without saturating. Unfortunately, logarithmic sensors suffer from fixed pattern noise (FPN). Work reported in the literature generally assumes the FPN is independent of illumination. This paper develops a nonlinear model y=a+bln(c+x) of the pixel response y to an illuminance z showing that FPN arises from variation of the offset a, gain b and bias c. Equations are derived which can be used to extract these parameters by calibration against a uniform illuminance of varying intensity. Experimental results, demonstrating parameter calibration and FPN correction, show that the nonlinear model outperforms outputs previous models that assume either only offset or offset and gain variation",2001,0,
944,945,Stator Windings Fault Diagnostics of Induction Machines Operated From Inverters and Soft-Starters Using High-Frequency Negative-Sequence Currents,"This paper studies the application of high-frequency voltage excitation-based stator winding diagnostic methods to three-phase ac machines operated from power converters that create the necessary high-frequency excitation as part of their normal operation. This paper focuses on two specific operating modes: 1) machines operated from inverters in the overmodulation region and 2) machines operated from soft-starters during startup. In both cases, high-frequency (in the range of the hundred hertz) voltage components at well-defined frequencies are created. The negative-sequence currents induced from these high-frequency voltages are shown to contain accurate information on the level of asymmetry (fault) in the machine. This information is significantly richer than exists in other modes of operation, i.e., inverters working in the linear modulation region or soft-starters in the steady state, and provides interesting opportunities to complement other diagnostic methods.",2009,0,
945,946,High throughput Byzantine fault tolerance,"This paper argues for a simple change to Byzantine fault tolerant (BFT) state machine replication libraries. Traditional BFT state machine replication techniques provide high availability and security but fail to provide high throughput. This limitation stems from the fundamental assumption of generalized state machine replication techniques that all replicas execute requests sequentially in the same total order to ensure consistency across replicas. We propose a high throughput Byzantine fault tolerant architecture that uses application-specific information to identify and concurrently execute independent requests. Our architecture thus provides a general way to exploit application parallelism in order to provide high throughput without compromising correctness. Although this approach is extremely simple, it yields dramatic practical benefits. When sufficient application concurrency and hardware resources exist, CBASE, our system prototype, provides orders of magnitude improvements in throughput over BASE, a traditional BFT architecture. CBASE-FS, a Byzantine fault tolerant file system that uses CBASE, achieves twice the throughput of BASE-FS for the IOZone micro-benchmarks even in a configuration with modest available hardware parallelism.",2004,0,
946,947,Intelligent fault-tolerant CORBA service on real-time CORBA,"Distributed object applications can be made fault tolerant by replicating their constituent objects, and by distributing these replicas across the different computers in the network. The idea behind object replication is that the failure of one replica of an object can be masked from a client of the object because the other replicas can continue to perform any operation that the client requires. We propose IFTS (Intelligent Fault Tolerant CORBA Service) for handling faults of server object replica using a replication concept to support fault tolerance. It can choose the fastest primary replica using the multicast mechanism. It also introduces passive replication for secure fault tolerance. Furthermore, we propose the design and implementation of IFTS service to provide reliability and faster service using multicast technology by extending existing CORBA ORB",2001,0,
947,948,Application of BPNN and CBR on Fault Diagnosis for Missile Electronic Command System,"Based on the complexity of the mobile missile electronic command system (MMECS), applying the single method in system fault diagnosis can hardly achieve satisfactory results. The fault diagnosis system combining the BP neural network (BPNN) method and the case-based reasoning (CBR) method was presented. The framework of the mixed neural network and the case presentation was put forward. The question of redundancy reasoning was solved, moreover, it can interpret the diagnoses by providing the successful case. Finally, with the example of voice interrupt, the system's correctness and validity was proved. It is shown that the system is suitable for both the operators training and online decision making for the army",2006,0,
948,949,Harmonic suppression with photonic bandgap and defected ground structure for a microstrip patch antenna,"This letter presents a microstrip patch antenna integrated with two-dimensional photonic bandgap (PBG) and one-dimensional defected ground structure (DGS) jointly in ground plane. It is demonstrated that application of both PBG and DGS eliminates the second and third harmonics and improves the return loss level. Moreover, the combination use of PBG and DGS decreases the occupied area by 70% compared to the conventional PBG patch antenna.",2005,0,
949,950,A newly developed web-based fault locating technology for transmission lines and its experience in the field,"This paper describes a newly developed fault locating (FL) system using Internet technology. The method using current and voltage data at one terminal for FL has been applied, and good results have been obtained. But this method has theoretical error in multi-terminal transmission lines because of the influence of the fault current from the terminal without FL equipment. It is necessary for precise fault location to gather the data from all of the terminals connected to the faulted fine. Internet technology has progressed remarkably and enabled us to gather the data easily from distributed multi-terminals. The proposed FL system uses current, voltage data and status information of power apparatus of multi-terminals via Internet communication. By using these data, fault-locating accuracy of proposed FL system improves remarkably. The network for transmitting information is constituted by general network devices. To perform fault location, a general PC is used. Moreover, for the acquisition of real-time information from power system, compact terminal devices are used. These devices have micro web servers for realizing the data transmission over HTTP. This combination of Internet technology and real time processing technology realized low-cost and high-performance FL systems. The proposed system has been in practical use in Chubu Electric Power Co. Inc.. and excellent results have been obtained.",2002,0,
950,951,Fault-tolerant output tracking control for a flexible air-breathing hypersonic vehicle,"This paper addresses the problem of guaranteed cost fault-tolerant output tracking control against actuator faults for a flexible air-breathing hypersonic vehicle. Firstly, using the parameters of the trim condition, a linearized model is established around the trim point for a nonlinear, dynamically coupled simulation model. Secondly, the control objective and models of actuator faults are presented. Thirdly, the performance analysis condition is proposed in the frame of convex optimization problems via Lyapunov functional approach. Then, the stand controller and fault-tolerant controller are designed such that the resulting closed-loop system is asymptotically stable and satisfies a prescribed performance cost respectively. Finally, the simulation results are given to show the effectiveness of the proposed control method, which is verified by excellent reference altitude and velocity tracking performance.",2010,0,
951,952,Estimating Error-probability and its Application for Optimizing Roll-back Recovery with Checkpointing,"The probability for errors to occur in electronic systems is not known in advance, but depends on many factors including influence from the environment where the system operates. In this paper, it is demonstrated that inaccurate estimates of the error probability lead to loss of performance in a well known fault tolerance technique, Roll-back Recovery with checkpointing (RRC). To regain the lost performance, a method for estimating the error probability along with an adjustment technique are proposed. Using a simulator tool that has been developed to enable experimentation, the proposed method is evaluated and the results show that the proposed method provides useful estimates of the error probability leading to near-optimal performance of the RRC fault-tolerant technique.",2010,0,
952,953,Support Vector Machine for Mechanical Faults Diagnosis,"Aiming at the difficulty that Support Vector Machine (SVM) model selection of classification algorithm affect classification accuracy, it research relevant factors that influence the precision of fault classifiers based on the typical fault data samples obtained by experimental setup of rotor-bearing systems. The results show that different SVM classifiers, in which different kernel functions and different kernel functions parameters are adopted, will influence the precision of fault classifiers in conditions that fault data samples is small. It can be conveniently applied to choose appropriate kernel functions and kernel functions parameters in engineering application.",2010,0,
953,954,Frequency error measurement in GMSK signals in a multipath propagation environment,"The paper presents an efficient method for evaluating the carrier frequency in GMSK communication systems. This method operates in a nonintrusive way. It utilizes the learning vector quantisation neural network based demodulator for reconstructing the transmitted phases. From these and the expected phases is estimated the carrier frequency error. The method is able to operate both in static and multipath propagation cases and it does not require a high frequency sampling rate because the base-band signal is processed. In order to apply the method two procedures, PSP (Procedure for Static Propagation) and PMP (Procedure for Multipath Propagation), are set-up. Tests performed on GMSK signals show that the method is quite attractive, fast and more accurate if compared with other approaches",2001,0,
954,955,A Novel Red-Eye Correction Method Based on AdaBoost Algorithm for Mobile Phone and Digital Camera,"Caused by light reflected off the subject's retina, red-eye is a troublesome problem in consumer photography. Correction of red eyes without any human intervention is an important task. There are some algorithms existing for red-eye detection, but almost all of them have less accuracy, in addition, they cannot support both high pixel and single red-eye. In this paper, a novel approach is proposed to eliminate red-eyes in the digital images automatically with a satisfactory result. This method gets the face region first by AdaBoost algorithm and then detects the red-eye on the top part of the face region, corrects the red-eye in the eye region for recovering the image's original color at last. Experiments in the platform of mobile phone and digital camera show that this method can eliminate the red-eye with high accuracy of 87%, which is higher than the best known technology of face detection base on complexion by 7%, and it can also support 8 million pixels image, moreover, the method has advantages of robustness and real-time computability.",2009,0,
955,956,Bug reports retrieval using Self-organizing Map,"An important process when implementing complex software systems consist of documenting the bugs found in that software. However, since many developers are working at the same time on the project, a bug may easily be reported multiple times, resulting in duplicated bug reports. Therefore, developers responsible for fixing bugs may spend time and effort reading and trying to understand two bugs that actually are the same. This way, we propose in this paper an approach for identifying duplicated bug reports that combines document indexing and self-organizing maps (SOM). The results of our experiments show that at most 69% of duplicated bug reports were identified, representing saving of time and effort for the developers.",2008,0,
956,957,Fuzzy ART neural network algorithm for classifying the power system faults,"This paper introduces advanced pattern recognition algorithm for classifying the transmission line faults, based on combined use of neural network and fuzzy logic. The approach utilizes self-organized, supervised Adaptive Resonance Theory (ART) neural network with fuzzy decision rule applied on neural network outputs to improve algorithm selectivity for a variety of real events not necessarily anticipated during training. Tuning of input signal preprocessing steps and enhanced supervised learning are implemented, and their influence on the algorithm classification capability is investigated. Simulation results show improved algorithm recognition capabilities when compared to a previous version of ART algorithm for each of the implemented scenarios.",2005,0,
957,958,"Improving fault-tolerance in intelligent video surveillance by monitoring, diagnosis and dynamic reconfiguration","In this paper, we present an approach for improving fault-tolerance and service availability in intelligent video surveillance (IVS) systems. A typical IVS system consists of various intelligent video sensors that combine image sensing with video analysis and network streaming. System monitoring and fault diagnosis followed by appropriate dynamic system reconfiguration mitigate effects of faults and therefore enhance the system's fault-tolerance. The applied monitoring and diagnosis unit (MDU) allows the detection of both node- and system-level faults. Lacking redundant hardware such reconfigurations are established by graceful degradation of the overall application. An optimizer module that performs multi-criterion optimization is used to compute a new degraded system configuration by trading off quality of service (QoS), energy consumption, and service availability. We demonstrate the functionality of our approach by an illustrative example.",2005,0,
958,959,A Trustworthy Network Fault Diagnosis Approach,"Trustworthy network fault diagnosis approach is one critical management item to enhance network trustworthiness. Aiming at gaining highly trustworthy of fault diagnosis in Internet, we present a trustworthy fault diagnosis approach based on integration of Artificial Neural Network and Rule-based Reasoning. Supported by the topology of hierarchical and distributed in multi-domains, reasoning rule matrix and its operation are studied to acquire parallel reasoning capability. Moreover, the quantitative trustworthy degree is defined and information entropy is applied to define threshold function marked on arcs and nodes in the Artificial Neural Network. Our approach possesses higher parallel capability guaranteeing by matrix operation and trustworthiness by trustworthy degree definition and calculation using Artificial Neural Network. Further, it is general so that it can be transplanted into various application fields.",2009,0,
959,960,Data Mining Applied to the Electric Power Industry: Classification of Short-Circuit Faults in Transmission Lines,"Data mining can play a fundamental role in modern power systems. However, the companies in this area still face several difficulties to benefit from data mining. A major problem is to extract useful information from the currently available non-labeled digitized time series. This work focuses on automatic classification of faults in transmission lines. These faults are responsible for the majority of the disturbances and cascading blackouts. To circumvent the current lack of labeled data, the alternative transients program (ATP) simulator was used to create a public comprehensive labeled dataset. Results with different preprocessing (e.g., wavelets) and learning algorithms (e.g., decision trees and neural networks) are presented, which indicate that neural networks outperform the other methods.",2007,0,
960,961,Adding fault tolerance mechanisms to Interbus-S,"Field bus technology is now a reality in industrial environments. There are many field bus systems commercially available, and each is suitable for particular kinds of applications. In this scenario the Interbus-S system is playing a leading role, due to the efficiency of its protocol. However, a drawback of this communication system is the centralisation of the mono-master arbitration scheme. The presence of a single device to co-ordinate communication activities makes the Interbus-S protocol vulnerable to fault occurrences in the master. Maintaining full compatibility with the existing standard, the authors have defined a protocol extension which allows the whole communication system to continue working after the occurrence of a fault in the master node",2000,0,
961,962,Effects of carrier phase error on EGC receivers in correlated Nakagami-m fading,"The effects of incoherently combining on dual-branch equal-gain combining (EGC) receivers in the presence of correlated, but not necessarily identical, Nakagami-m fading and additive white Gaussian noise are studied. Novel closed-form expressions for the moments of the output signal-to-noise ratio (SNR) are derived. Based on these expressions, the average output SNR and the amount of fading are obtained in closed-form. Moreover, the outage and the average bit error probability for binary and quadrature phase-shift keying are also studied using the moments-based approach. Numerical and computer simulation results clearly depict the effect of the carrier phase error, correlation coefficient, and fading severity on the EGC performance. An interesting finding is that higher values of the correlation coefficient results to lower irreducible error floors.",2005,0,
962,963,The influence of fault distribution on stochastic prediction of voltage sags,"This paper analyzes the influence of modeling of fault distribution along transmission line on the assessment of number and characteristics of voltage sags. The generic distribution network was used in all simulations. Different types of transformer winding connections were modeled and different (symmetrical and asymmetrical) types of faults were simulated. A line was selected from the previously identified area of vulnerability for a given bus and different faults having different distributions along the line were simulated. It was shown that depending on the fault distribution (uniform, normal, exponential) along the line, different numbers and characteristics of voltage sags could be expected at the selected bus.",2005,0,
963,964,Time domain phase noise correction for OFDM signals,"We introduce an algorithm for compensating for carrier phase noise in an OFDM communication system. Through the creation of a linearized parametric model for phase noise, we generate a least squares (LS) estimate of the transmitted symbol. Using digitized DVB-T RF signals created in a laboratory and a DVB-T compliant receiver model, simulation results are presented to evaluate the effectiveness of the algorithm in practical environments.",2002,0,
964,965,Using Register Lifetime Predictions to Protect Register Files against Soft Errors,"To increase the resistance of register files to soft errors, this paper presents the ParShield architecture. ParShield is based on two observations: (i) the data in a register is only useful for a small fraction of the register's lifetime, and (ii) not all registers are equally vulnerable. ParShield selectively protects registers by generating, storing, and checking the ECCs of only the most vulnerable registers while they contain useful data. In addition, it stores a parity bit for all the registers, re-using the ECC circuitry for parity generation and checking. ParShield has no SDC AVF and a small average DUE AVF of 0.040 and 0.010 for the integer and floating-point register files, respectively. ParShield consumes on average only 81% and 78% of the power of a design with full ECC for the SPECint and SPECfp applications, respectively. Finally, ParShield has no performance impact and little area requirements.",2007,0,
965,966,Unequal error protected transmission with dynamic classification in H.264/AVC,"As an efficient error resilient tool in H.264, FMO (Flexible Macroblock Ordering) still has 2 disadvantages: (1) unacceptable bitrate overhead, and (2) unsuitable for widely used UEP (unequal error protected) transmission. In this paper, to overcome these 2 disadvantages, a dynamic FMO classification (DFMOC) is proposed. For disadvantage(l), in DFMOC since lots of MBs in the same slice are placed together, thus the bitrate overhead is smaller. For disadvantage^), DFMOC generates 2 slices and each of them takes unequal priority in transmission by the large and small motion area extraction. After employing LDPC coding for UEP transmission strategy, experiment shows DFMOC has a better error robustness while still keeps less bitrate overhead compared with traditional FMO mode: the PSNR has 1 to 2 db outperforming and the bitrate overhead keeps no more than 5% which is about a half of traditional FMO overhead.",2007,0,
966,967,Wafer Topography-Aware Optical Proximity Correction,"Depth of focus is the major contributor to lithographic process margin. One of the major causes of focus variation is imperfect planarization of fabrication layers. Presently, optical proximity correction (OPC) methods are oblivious to the predictable nature of focus variation arising from wafer topography. As a result, designers suffer from manufacturing yield loss as well as loss of design quality through unnecessary guardbanding. In this paper, the authors propose a novel flow and method to drive OPC with a topography map of the layout that is generated by chemical-mechanical polishing simulation. The wafer topography variations result in local defocus, which the authors explicitly model in the OPC insertion and verification flows. In addition, a novel topography-aware optical rule check to validate the quality of result of OPC for a given topography is presented. The experimental validation in this paper uses simulation-based experiments with 90-nm foundry libraries and industry-strength OPC and scattering bar recipes. It is found that the proposed topography-aware OPC (TOPC) can yield up to 67% reduction in edge placement errors. TOPC achieves up to 72% reduction in worst case printability with little increase in data volume and OPC runtime. The electrical impact of the proposed TOPC method is investigated. The results show that TOPC can significantly reduce timing uncertainty in addition to process variation",2006,0,
967,968,Design and Implementation of Inference Engine in Safety Risk Assessment Expert System in Petrochemical Industry Based on Fault Tree,"The project in petrochemical industry is complex and risky. For this feature, we established a safety risk assessment (SRA) expert system based on fault tree in petrochemical industry. In this paper, we studied the design and implementation of infer engine in SRA expert system. we adopted the method of fault tree analysis (FTA) to acquire expert knowledge and the fault tree established is to be the basis of inference. The knowledge in petrochemical industry was divided into shallow knowledge and deep knowledge and the method of KR(knowledge representation) adopted in this paper is production rule combined with the framework. On the basis of good representation and organization of knowledge, we adopted infer control strategy of forward reason combined with depth-first search which improve the validity and accuracy of the SRA expert system to a certain extent.",2010,0,
968,969,The sensor of traveling-wave for fault location in power systems,"The fault-generated high frequency signals contain much information which can be used to accurately locate the fault point in power systems. For capturing the high frequency signals, two specially designed traveling-wave sensors are developed in the paper. One is the current sensor installed on the earth line of capacitive equipment (such as: CVT, capacitive CT, transformer bushing, wall bushing) to capture the current traveling-waves flowing from the equipment to earth. The other is the voltage sensor installed at the zero sequence winding of a voltage transformer to capture the voltage traveling waves in three-phase generated by faults. The fault location system with the traveling wave sensors is also developed simply. The sensors and the fault location system have been tested on an 110 kV power system. Results show that the sensors have good performance to capture traveling-waves and the error of fault location is no more than 120 m.",2004,0,
969,970,Fault-Adaptive Control for Robust Performance Management of Computing Systems,"This paper introduces a fault-adaptive control approach for the robust and reliable performance management of computing systems. Fault adaptation involves the detection and isolation of faults, and then taking appropriate control actions to mitigate the fault effects and maintain control.",2007,0,
970,971,Error reduction in non-electric measurement by interpolation combined with loop transformation method,"In this paper, we used second order interpolation method via three succeeding data points in narrow area, combined with loop transformation method and used samples to build an algorithm for processing of measurement data to reduce non-linear errors and transformation errors of non-electric measuring devices. Simulation and experimental results shown the ability to reduce errors of measurement devices.",2010,0,
971,972,Virtual sensor for fault detection and isolation in flight control systems - fuzzy modeling approach,"A virtual sensor for normal acceleration has been developed and implemented in the flight control system of a small commercial aircraft. The inputs of the virtual sensor are the consolidated outputs of dissimilar sensor signals. The virtual sensor is a fuzzy model of the Takagi-Sugeno type and it has been identified from simulated data, using a detailed, realistic Matlab/Simulink<sup>TM</sup> model used by the aircraft manufacturer. This virtual sensor can be applied to identify a failed sensor in the case that only two real sensors are available and even to detect a failure of the last available sensor",2000,0,
972,973,A modified Dendritic Cell Algorithm for on-line error detection in robotic systems,"The immune system is a key component in the maintenance of host homeostasis. Key actors in this process are cells known as dendritic cells (DCs). An artificial immune system based on DCs (known as the Dendritic Cell Algorithm: DCA) is well established in the literature and has been applied in a number of applications. Work in this paper is concerned with the development of an integrated homeostatic system for small, autonomous robotic systems, implemented on a resource limited micro-controller. As a first step, we have modified the DCA to operate in both simulated robotic units, and a resource constrained micro-controller that can operate in an on-line manner. Errors can be introduced into the robotic unit during operation, and these can be detected and then circumvented by the modified DCA.",2009,0,
973,974,Evaluating the Post-Delivery Fault Reporting and Correction Process in Closed-Source and Open-Source Software,"Post-delivery fault reporting and correction are important activities in the software maintenance process. It is worthwhile to study these activities in order to understand the difference between open-source and closed-source software products from the maintenance perspective. This paper proposes three metrics to evaluate the post-delivery fault reporting and correction process, the average fault hidden time, the average fault pending time, and the average fault correction time. An empirical study is further performed to compare the fault correction processes of NASA Ames (closed-source) projects and three open-source projects: Apache Tomcat, Apache Ant, and Gnome Panel.",2007,0,
974,975,Quasi-active power factor correction using transformer-assisted driving voltage,"This paper presents a novel simple input current shaper based on a quasi-active power factor correction (PFC) scheme. In this method high power factor and low harmonic content are achieved by providing an auxiliary PFC circuit with a driving voltage derived from a third winding of the transformer of a DC-DC flyback converter. It eliminates the use of active switch and control circuit for PFC. Operating principles, analysis, and simulation results of the proposed method are presented.",2009,0,
975,976,An Energy Based Approach of Electromagnetism Applied to Adaptive Meshing and Error Criteria,"In order to improve the finite-element modeling of macroscopic eddy currents (associated with motion and/or a time-varying electrical excitation), an original error criterion for adaptive meshing, based on a local power conservation, is proposed. Then, the importance of the order element in the error computation is investigated. Finally, the criterion is coupled to a ldquobubblerdquo mesh generator, and an adaptive meshing of a 2D induction heating case is performed.",2008,0,
976,977,Fault Tolerance of Multiprocessor-Structured Control System by Hardware and Software Reconfiguration,"Since the traditional redundancy for fault tolerance of a control system is complex in structure and expensive, a novel method for fault tolerance of multiprocessor-structured control system by hardware and software reconfiguration is presented. Based on the fact that the control system is composed of several processors, this method performs fault detection by self-diagnosis implemented in each processor and validation of exchanged information between the processors, tolerates fault by hardware and software reconfiguration carried out by monitoring and configuring device. Security strategy and operation mode were presented. The principle of the monitoring and configuring device was discussed in detail. The method was proved by a control system of dc motor and got a satisfied result.",2007,0,
977,978,Joint write policy and fault-tolerance mechanism selection for caches in DSM technologies: Energy-reliability trade-off,"Write-through caches potentially have higher reliability than write-back caches. However, write-back caches are more energy efficient. This paper provides a comparison between the write-back and write-through policies based on the combination of reliability and energy consumption criteria. In the experiments, SIMPLESCALAR tool and CACTI model are used to evaluate the characteristics of the caches. The results show that a write-through cache with one parity bit per word is as reliable as a write-back cache with SEC-DED code per word. Furthermore, the results show that the energy saving of the write-through cache over the write-back cache increases if any of the following changes happens: i) a decrease in the feature size, ii) a decrease in the L2 cache size, and iii) an increase in the L1 cache size. The results also show that when feature size is bigger than 32 nm, the write-back cache is usually more energy efficient. However, for 32 nm and smaller feature sizes the write-through cache can be more energy efficient.",2009,0,
978,979,The study of single line to ground fault line selection in non-direct ground power system based on DSP device,"The middle-low voltage distribution networks mostly adopt neuter point not-valid grounding(be so called to the small current groundding system).The distribution wire fault particularly the fast and accurate location that the single grounding fault, not only to the repair wire and promise dependable power supply, and circulate to the safe stability and economy that promises the whole electric power system to all have a very important function. With reference of actual conditions for single phase earthing fault, the paper puts forward the principle on single phase earthing faulty line selection, and with the support of digital signal processing device(DSP)capable in floating point computation, to design a new type of low current single phase earthing faulty line selection device for power distribution system.",2010,0,
979,980,Fault-Tolerant Sensor Coverage for Achieving Wanted Coverage Lifetime with Minimum Cost,"We study how to select and arrange multiple types of wireless sensors to build a star network that meets the coverage, the lifetime, the fault-tolerance, and the minimum-cost requirements, where the network lifetime, the acceptable failure probability of the network, and the failure rate of each type of sensors are given as parameters. This problem is NP-hard. We model this problem as an integer linear programming minimization problem. We then present an efficient approximation algorithm to find a feasible solution to the problem, which provides a sensor arrangement and a scheduling. We show that, through numerical experiments, our approximation provides solutions with approximation ratios less than 1.4.",2007,0,
980,981,Just-in-time statistical process control for flexible fault management,"A new fault detection and identification method is proposed to solve the problems that obstruct industrial applications of multivariate statistical process control (MSPC) techniques. The proposed method, referred to as just-in-time statistical process control (JIT-SPC), can realize flexible, adaptive, high-performance process monitoring. In addition, fault identification can be done through contribution plot in the framework of JIT-SPC. The usefulness of JIT-SPC is demonstrated through a numerical example, which conventional methods cannot cope with, and a case study of the vinyl acetate monomer production plant.",2010,0,
981,982,Local/global fault diagnosis of event-driven controlled systems based on probabilistic inference,"This paper presents a new local/global fault diagnosis strategy for the event-driven controlled systems such as the programmable logic controller (PLC). First of all, the controlled plant is decomposed into some subsystems, and the global diagnosis is formulated using the Bayesian network (BN), which represents the causal relationship between the fault and observation in subsystems. Second, the local diagnoser is developed using the conventional timed Markov model (TMM), and the local diagnosis results are used to specify the conditional probability assigned to each arc in the BN. By exploiting the local/global diagnosis architecture, the computational burden for the diagnosis can be drastically reduced. As the result, large scale diagnosis problems in the practical situation can be solved. Finally, the usefulness of the proposed strategy is verified through some experimental results of an automatic transfer line.",2007,0,
982,983,The overview of fiber fault localization technology in TDM-PON network,"In this paper, we discussed the mechanism of optical fiber break in time division multiplexing passive optical network (TDM-PON) and the upwardly or downwardly monitoring issues with conventional fiber fault localization technique by using optical time domain reflectometer (OTDR). We also studied the previous fault localization technology that had been recommended. Finally, we proposed a centralized inline monitoring and network testing system named as centralized failure detection system (CFDS). CFDS will be installed with optical line terminal (OLT) at the central office (CO) or network operation center to centrally monitoring each optical fiber line's status and detecting the failure location that occurs in the multi-line drop region of fiber-to-the-home (FTTH) access network downwardly from CO towards the customer premises to improve the service reliability and reduce the restoration time and maintenance cost.",2008,0,
983,984,Design of quasi-cyclic Tanner codes with low error floors,"Tanner codes represent a broad class of graph-based coding schemes, including low-density parity-check (LDPC) and turbo codes. Whereas many different classes of LDPC and turbo codes have been proposed and studied in the past decade, very little work has been performed on the broader class of Tanner codes. In this paper we propose a design technique which leads to efficiently encodable quasi-cyclic Tanner codes based on both Hamming and single parity check (SPC) nodes. These codes are characterized by fast message-passing decoders and can be encoded using shift-register-based circuits. The resulting schemes exhibit excellent performance in both the error floor and waterfall regions on the additive white Gaussian noise channel.",2006,0,
984,985,Low Area Adaptive Fail-Data Compression Methodology for Defect Classification and Production Phase Prognosis,"With the shrinking technology and increasing statistical defects, multiple design respins are required based on yield learning. Hence, a solution is required to efficiently diagnose the failure types of memory during production in the shortest time frame possible. This paper introduces a novel method of fault classification through image based prognosis of predefined fail signature dictionary. In contrary to the existing Bitmap Diagnosis methodologies, this method predicts the compressed failure map without generating and transferring complete Bitmap to the tester. The proposed methodology supports testing through a very low cost ATE. This architecture is partitioned to achieve sharing among various memories and at-speed testing.",2007,0,
985,986,Error-injection-based failure characterization of the IEEE 1394 bus,This paper investigates the behavior of the IEEE 1394 bus in the presence of transient errors in the hardware layers of the protocol. Software-implemented error injection is used to introduce errors into the internals of the 1394 bus hardware chipset. Results from this study indicate that the IEEE 1394 bus protocol provides robust network communication in the presence of single-bit errors in the chipset.,2003,0,
986,987,Design and emulate on motor fault diagnosis system,"In the traditional motor fault diagnosis, only a certain type of motor fault diagnosis was diagnosed. The less amount of information leads to diagnostic conclusions unreliable. In this article, a new fault diagnosis method was put forward. Information fusion technology, stator current and rotor vibration signals as a diagnostic characteristics input signal were introduced into the motor fault diagnosis. Neural network method was applied to the fault identification. In order to improve the diagnostic precision, the input signs were divided into the stator current signal related and the rotor vibration signal related. They separately adopt a diagnosis sub-network to complete different aspects of fault diagnosis. Finally, each sub-network diagnostic results information fusion were carried out and the final diagnosis results were got. The simulation of the diagnostic method showed that it is feasible that the neural network data fusion applied to the motor fault diagnosis.",2010,0,
987,988,Deformation correction in ultrasound images using contact force measurements,"During an ultrasound scan, contact between the probe and the skin deforms the underlying tissue. This can be considered a feature (as in elastography), but is in general undesirable, particularly for 3D scanning. In this paper we propose a novel system to correct this deformation by measuring the contact force at the time of the ultrasound scan and then using an elastic model to predict the tissue deformation. The inverse of this deformation is then applied to the image, generating the image that would have been seen had there been no contact with the probe. A prototype system has been implemented using an a priori finite element model to predict the deformation. This has been tested on gelatine phantoms and shown to remove the contact deformation and so give improved 3D reconstructions",2001,0,
988,989,"An Effective Approach for the Diagnosis of Transition-Delay Faults in SoCs, based on SBST and Scan Chains","In this paper, a Software-Based Diagnosis (SBD) procedure suitable for SoCs is proposed to tackle the diagnosis of transition-delay faults. The illustrated methodology takes advantage of an initial Software-Based Self-Test (SBST) test set and of the scan-chains included in the final SoC design release. In principle, the proposed methodology consists in partitioning the considered SBST test set in several slices, and then proceeding to the evaluation of the diagnostic ability owned by each slice with the aim of discarding diagnosis-ineffective test programs portions. The proposed methodology is aimed to provide precise feedback to the failure analysis process focusing the systematic timing failures characteristic of new technologies. Experimental results show the effectiveness and feasibility of the proposed approach on a suitable SoC test vehicle including an 8-bit microcontroller, 4 SRAM memories and an arithmetic core, manufactured by STMicroelectronics, whose purpose is to provide precise information to the failure analysis process. The reached diagnostic resolution is up to the 99.75%, compared to the 93.14% guaranteed by the original SBST procedure.",2007,0,
989,990,Introduction to fault attacks on smartcard,We present what can be achieved by attacks through faults induction on smart cards. We first describe the different means to perform fault attacks on chips and explain how fault attacks on cryptographic algorithms are used to recover secret keys. We next study the impact of fault attacks when focused on the disruption of the functional software layer. We conclude with the overall impact of this type of attacks on the smartcard environment and the need for software countermeasures and their limits.,2005,0,
990,991,Efficient method for correction and interpolation signal of magnetic encoders,"Magnetic encoders are widely used for speed or position measurement. This research presents a suitable method to correct the quadratic signal from a magnetic sensor. A new quadrature all digital phased-locked loop (QADPLL) method is presented. The method minimizes the effect of amplitude imbalance, noise, phase shift, and signal offsets. It also can solve waveform distortion and time-lag problems. Moreover, this paper proposes an interpolation technique to improve the accuracy of position information. By deriving the high-order signal from a sinusoidal signal, a high-resolution position can be obtained from a low-resolution encoder. Simulation and experiment on a linear motor were conducted. The results verify the performance of the proposed methods.",2008,0,
991,992,Improving robustness of gene ranking by resampling and permutation based score correction and normalization,"Feature ranking, which ranks features via their individual importance, is one of the frequently used feature selection techniques. Traditional feature ranking criteria are apt to produce inconsistent ranking results even with light perturbations in training samples when applied to high dimensional and small-sized gene expression data. A widely used strategy for solving the inconsistencies is the multi-criterion combination. But one problem encountered in combining multiple criteria is the score normalization. In this paper, problems in existing methods are first analyzed, and a new gene importance transformation algorithm is then proposed. Experimental studies on three popular gene expression datasets show that the multi-criterion combination based on the proposed score correction and normalization produces gene rankings with improved robustness.",2010,0,
992,993,An immune inspired fault diagnosis system for analog circuits using wavelet signatures,This work focuses on fault diagnosis of electronic analog circuits. A fault diagnosis system for analog circuits based on wavelet decomposition and artificial immune systems is proposed. It is capable of detecting and identifying faulty components in analog circuits by analyzing its impulse response. The use of wavelet decomposition for preprocessing of the impulse response drastically reduces the size of the detector used by the Real-valued Negative Selection Algorithm (RNSA). Results have demonstrated that the proposed system is able to detect and identify faults in a Sallen-Key bandpass filter circuit.,2004,0,
993,994,Timing for the Loran-C signal by Beidou satellite and error correction for the transmission time,"The capability of positioning of Beidou/Loran-C integrated navigation system is restricted by the transmitting precision of Loran-C signal. In this paper, the wavelet shrinkage de-noising method of correction for the transmission time error of Loran-C signal, which is timed by the Beidou satellite, is discussed. Getting the random offset from the time difference, and then, adopting the soft-threshold function and the SUREShrink estimation in the de-noising. The results demonstrated that the transmission time error was decreased about 30 ns, and the performance of three dimension positioning was improved evidently through the method in the experiment.",2008,0,
994,995,Research on information engineering surveillance risk evaluation based on fault tree analysis,"Information security risk analysis method is now a hot issue of information security management field. Fault tree analysis method has proposed since the 1960s, obtain the widespread application in many large-scale complicated system's security fall-safe analyses. It's recognized as an effective method for the complex system reliability, security analysis. The basic principle, qualitative analysis and quantitative analysis of fault tree analysis method are introduced. And this article introduced briefly the information security risk analysis method, and to has carried on the exhaustive elaboration based on fault tree's risk analysis's modeling way and the analysis principle. Finally, an example is introduced to tome to the conclusion whether the project is feasible.",2010,0,
995,996,A software-implemented fault injection methodology for design and validation of system fault tolerance,"Presents our experience in developing a methodology and tool at the Jet Propulsion Laboratory (JPL) for software-implemented fault injection (SWIFI) into a parallel-processing supercomputer which is being designed for use in next-generation space exploration missions. The fault injector uses software-based strategies to emulate the effects of radiation-induced transients occurring in the system hardware components. JPL's SWIFI tool set, which is called JIFI (JPL's Implementation of a Fault Injector), is being used in conjunction with an appropriate system fault model to evaluate candidate hardware and software fault tolerance architectures, to determine the sensitivity of applications to faults, and to measure the effectiveness of fault detection, isolation and recovery strategies. JIFI has been validated to inject faults into user-specified CPU registers and memory regions with a uniform random distribution in location and time. Together with verifiers, classifiers and run scripts, JIFI enables massive fault injection campaigns and statistical data analysis.",2001,0,
996,997,Fault Testing And Diagnosis System Of Armored Vehicle Based On Information Fusion Technology,"This paper introduces a fault testing and diagnosis system of bearing in armored vehicle. It can realize real-time testing and precise fault diagnosis of bearing on vehicle chassis. In the hardware, it adopts PCL1800 data acquisition card to acquire sample data and send to the software. In the software, we compile fault testing and diagnosis system of bearing in armored vehicle on a portable computer, its core diagnostic method based on virtual instrument and multi-sensor information fusion technology. Virtual testing subsystem has various functions, including on-line data acquiring, signal displaying, different kinds of signals analyzing and data management. It only monitors the status of bearing and warning alarm. The further work to judge the type and the severity factor of the faults lies on fault diagnosis subsystem based on neural network information fusion technology.",2007,0,
997,998,Design and realization on the fault diagnostic flat based on virtual instrument for warship equipment,"Based on virtual instrument (VI) technology, Delphi and database, etc., the fault diagnostic flat for shipboard equipment is developed in order to avoid various abuses that conventional methods brought. The modularization and universalization are proposed in its database-based design concept, realized the design of software and hardware. It broke through conventional check diagnosis patterns for warships equipment, resolved difficult to overcome problems brought on adopting existing conventional fashions examined and repaired shipboard equipment, greatly shortened the cycle of maintenance for naval warships equipment. It was proved by experiments that the flat system has merits, such as operation simpleness, high testing precision, strong flexibility and reliability and extensibility, and economical practicability. Also it is of some values for developing the other fault diagnostic instrument.",2010,0,
998,999,Detection of Turn to Turn Faults in Stator Winding with Axial Magnetic Flux in Induction Motors,"Induction motors play a very important part in the safe and efficient running of any industrial plant. Early detection of abnormalities in the motor helps to avoid costly breakdown. Accordingly, this work presents a technique for the diagnosis of turn to turn faults in stator winding in induction motors, with use of axial magnetic flux. Axial leakage flux generates voltages in flux coil sensor and their time based waveforms and frequency presentations are subsequently analyzed. Turn to turn failures in stator windings are studied with load and unload effects. Experimental results prove the efficiency of employed method.",2007,0,
999,1000,On error detection and error synchronization of reversible variable-length codes,"Reversible variable-length codes (RVLCs) are not only prefix-free but also suffix-free codes. Due to the additional suffix-free condition, RVLCs are usually nonexhaustive codes. When a bit error occurs in a sentence from a nonexhaustive RVLC, it is possible that the corrupted sentence is not decodable. The error is said to be detected in this case. We present a model for analyzing the error detection and error synchronization characteristics of nonexhaustive VLCs. Six indices, the error detection probability, the mean and the variance of forward error detection delay length, the error synchronization probability, the mean and the variance of forward error synchronization delay length are formulated based on this model. When applying the proposed model to the case of nonexhaustive RVLCs, these formulations can be further simplified. Since RVLCs can be decoded in backward direction, the mean and the variance of backward error detection delay length, the mean and the variance of backward error synchronization delay length are also introduced as measures to examine the error detection and error synchronization characteristics of RVLCs. In addition, we found that error synchronization probabilities of RVLCs with minimum block distance greater than 1 are 0.",2005,0,
1000,1001,Biologically-Based Signal Processing Chips with Emphasis on Telecommunication Defect Tracking and Reliability Estimation,"This paper provides observations and motivations to mimic biological information processing. Alternative bio-inspired systems definitions, basics, approaches, algorithms, and chip implementations will be illustrated to offer a base of choice for bio-based Intelligent Information Processing (IIP) systems. Hybrid biological and bio-based IIP are briefly presented. Two specific applications follow with embedded bio-based systems: Bio-chemical sensing and detection E-nose; and Track improvements In the reliability of the software used in telecommunication network deployments. The biologically-based processing discoveries gleaned from observing the spikes in the brain activity of monkeys, introduced the concept of plasticity in synapses used in our embedded Spiking Neural Network (SNN) system for the E-Nose The mathematical construct of a defect tracking classifier is nonlinear, and the event to be recognized involves a sequentially varying or non-stationary phenomenon for telecommunication defect tracking and reliability estimation. Thus, Adaptive Recurrent Dynamic Neural Network (ARDNN) system using wavelet function as the basis improved the failure event estimation of software defect tracking in telecommunications and reduced the error from 88% to L25-8%.",2007,0,
1001,1002,Age-related Neural Changes during Memory Conjunction Errors,"Human behavioral studies demonstrate that healthy aging is often accompanied by increases in memory distortions or errors. Here we used event-related fMRI to examine the neural basis of age-related memory distortions. We used the memory conjunction error paradigm, a laboratory procedure known to elicit high levels of memory errors. For older adults, right parahippocampal gyrus showed significantly greater activity during false than during accurate retrieval. We observed no regions in which activity was greater during false than during accurate retrieval for young adults. Young adults, however, showed significantly greater activity than old adults during accurate retrieval in right hippocampus. By contrast, older adults demonstrated greater activity than young adults during accurate retrieval in right inferior and middle prefrontal cortex. These data are consistent with the notion that age-related memory conjunction errors arise from dysfunction of hippocampal system mechanisms, rather than impairments in frontally mediated monitoring processes.",2010,0,
1002,1003,Selective ground-fault protection using an adaptive algorithm model in neutral ungrounded power systems,"The selective ground-fault protection is greatly valued for the safe and reliable operation of power systems. In order to eliminate the effect of zero-sequence current transformer's phase character on selective ground-fault protection devices, this paper proposes a new adaptive principle of selective ground-fault protection, and gives an algorithm model of action criterion based on the half-wave Fourier algorithm. The simulation results show that this criterion will possess very good selectivity to ground faults",2000,0,
1003,1004,Enhanced DO-RE-ME based defect level prediction using defect site aggregation-MPG-D,"Predicting the final value of the defective part level after the application of a set of test vectors is not a simple problem. In order for the defective part level to decrease, both the excitation and observation of defects must occur. This research shows that the probability of exciting an as yet undetected defect does indeed decrease exponentially as the number of observations increases. In addition, a new defective part level model is proposed which accurately predicts the final defective part level (even at high fault coverages) for several benchmark circuits and which continues to provide good predictions even as changes are made an the set of test patterns applied",2000,0,
1004,1005,Delivering error detection capabilities into a field programmable device: the HORUS processor case study,"Designing a complete SoC or reuse SoC components to create a complete system is a common task nowadays. The flexibility offered by current design flows offers the designer an unprecedented capability to incorporate more and more demanded features like error detection and correction mechanisms to increase the system dependability. This is especially true for programmable devices, were rapid design and implementation methodologies are coupled with testing environments that are easily generated and used. This paper describes the design of the HORUS processor, a RISC processor augmented with a concurrent error mechanism, the architectural modifications needed on the original design to minimize the resulting performance penalty.",2002,0,
1005,1006,Fault Analysis of a MEMS Tuneable Bandpass Filter,"The availability of Micro-Electro-Mechanical Systems (MEMS) switches has enabled the design of a high Q-factor but low insertion loss tuneable bandpass filter. This paper investigates the potential faults that could occur during fabrication and long term operation of a tuneable bandpass filter using MEMS switches. The causes of the filter defects and the resulting filter response will be identified, simulated and co- related, with the final aim of being able to identify the defects by measuring the faulty responses in the future. The different defects are simulated using SONNET to obtain the response of the faulty filter. Parameters such as insertion loss and return loss of the tuneable filter vary for different faults. In the future study, the defects will be recreated and tested experimentally to corroborate simulation findings. Eventually, a relationship between defects and the filter response will be developed.",2007,0,
1006,1007,Bi-direction Motion Vector retrieval based error concealment scheme for H.264/AVC,"As the newest video coding standard, H.264/AVC adopts the high-efficiently predictive coding and variable length entropy coding to achieve high compression efficiency. On the other side, transmission errors become the major problem faced by video broadcasting service providers. Error concealment (EC) here is adopted to handle slices with huge conjunctive corrupted areas inside. Considering error propagation from corrupted slice to succeeding ones is the key factor affecting the video quality, this paper proposes a novel temporal EC scheme including the bi-direction motion vector (MV) retrieval method and an adaptive EC ordering basing on it. Background and motional steady shift part of slice will be given top and second priority, respectively. Combined with our proposed improved boundary matching algorithm (IBMA) which provides more accurate distortion function, experiments results show that our proposal achieves better performance under different error rate channel, compared with EC algorithm adopted in H.264 reference software.",2009,0,
1007,1008,Effects of Systematic and Stochastic Errors on Estimated Failure Probability of Anisotropic Conductive Film,"This paper analyzes the effects of systematic and stochastic errors on the failure probability of anisotropic-conductive-film (ACF) assemblies estimated using the V-shaped-curve method. It is shown that the effect of systematic errors varies as a function of the volume fraction and the volume-fraction bias. The effects of stochastic errors are investigated by using an in-house software program to generate random conductive-particle distributions in the pad and inter pad regions of the ACF package for the given volume fractions and package geometries. The dependences of the coefficient of variation (CV), essentially the degree of uniformity of the particle distribution, and the failure probability on the volume fraction are examined, and the corresponding results are used to derive the correlation between the stochastic error and the CV for a given volume fraction. In general, the current results indicate that the effects of systematic errors on the accuracy of the estimated failure probability can be controlled by improving the accuracy with which the resin and conductive-particle components of the ACF-compound material are weighed during the ACF fabrication process. However, the effects of stochastic errors cannot be controlled and vary as a function of the volume fraction and the degree of nonuniformity of the particle distribution. Nevertheless, the results indicate that the effects of both systematic and stochastic errors can be suppressed by specifying the volume fraction as the value corresponding to the tip of the V-shaped curve when designing the ACF compound.",2007,0,
1008,1009,A Robust High Speed Serial PHY Architecture With Feed-Forward Correction Clock and Data Recovery,"This paper describes a robust architecture for high speed serial links for embedded SoC applications, implemented to satisfy the 1.5 Gb/s and 3 Gb/s Serial-ATA PHY standards. To meet the primary design requirements of a sub-system that is very tolerant of device variability and is easy to port to smaller nanometre CMOS technologies, a minimum of precision analog functions are used. All digital functions are implemented in rail-to-rail CMOS with maximum use of synthesized library cells. A single fixed frequency low-jitter PLL serves the transmit and receive paths in both modes so that tracking and lock time issues are eliminated. A new oversampling CDR with a simple feed-forward error correction scheme is proposed which relaxes the requirements for the analog front-end as well as for the received signal quality. Measurements show that the error corrector can almost double the tolerance to incoming jitter and to DC offsets in the analog front-end. The design occupies less than 0.4 mm<sup>2</sup> in 90 nm CMOS and consumes 75 mW.",2009,0,
1009,1010,An approach to reliability growth planning based on failure mode discovery and correction using AMSAA projection methodology,"Exact expressions for the expected number of surfaced failure modes and system failure intensity as functions of test time are presented under the assumption that the surfaced modes are mitigated through corrective actions. These exact expressions depend on a large number of parameters. Functional forms are derived to approximate these quantities that depend on only a few parameters. Such parsimonious approximations are suitable for developing reliability growth plans and portraying the associated planned growth path. Simulation results indicate that the functional form of the derived parsimonious approximations can adequately represent the expected reliability growth associated with a variety of patterns for the failure mode initial rates of occurrence. A sequence of increasing MTBF target values can be constructed from the parsimonious MTBF projection approximation based on the following: (1) planning parameters that determine the parsimonious approximation; (2) corrective action mean lag time with respect to implementation and; (3) test schedule that gives the number of planned reliability, availability, and maintainability (RAM) test hours per month and specifies corrective action implementation periods",2006,0,
1010,1011,GPRS-based fault monitoring for distribution grid,"The GPRS is a useful mean to solve the communication problem in the automatic system of power distribution network system. This paper designs a GPRS-based real-time system to monitor the on-off states of the switching station in the power distribution system. The system is consisting of a low-power controller of MSP430F149, the module of GR64 from WAVECOM company and detection circuits of on-off states. The IEC60870-5-101 protocol is used in the system. Therefore, it can conveniently communicate with other SCADA systems. It is reliable and stable running on site.",2010,0,
1011,1012,Fixed point error analysis of CORDIC processor based on the variance propagation,"The effects of angle approximation and rounding in the CORDIC processor have been intensively studied for the determination of design parameters. However, the conventional analyses provide only the error bound which results in large discrepancy between the analysis and the actual implementation. Moreover, some of the signal processing architectures require the specification in terms of the mean squared error (MSE) as in the design specification of FFT processor for OFDM. This paper proposes a fixed point MSE analysis based on the variance propagation for more accurate error expression of the CORDIC processor. It is shown that the proposed analysis can also be applied to the modified CORDIC algorithms. As an example of application, an FFT processor for OFDM using the CORDIC processor is presented. The results show close match between the analysis and simulation.",2003,0,
1012,1013,Information Retrieval Based on OCR Errors in Scanned Documents,"An important proportion of documents are document images, i.e. scanned documents. For their retrieval, it is important to recognize their contents. Current technologies for optical character recognition (OCR) and document analysis do not handle such documents adequately because of the recognition errors. In this paper, we describe an approach that integrates the detection of errors in scanned texts without relying on a lexicon, and this detection is integrated in the research process. The proposed algorithm consists of two basic steps. In the first step, we apply editing operations on OCR words that generate a collection of error-grams and correction rules. The second step uses query terms, error-grams, and correction rules to create searchable keywords, identify appropriate matching terms, and determine the degree of relevance of retrieved document images. Algorithms has been tested on 979 document images provided by Media-team databases from Washington University, and the experimental results obtained show the effectiveness of our method and indicate improvement in comparison with the standard methods such as exact or partial matching, N-gram overlaps, and Q-gram distance.",2003,0,
1013,1014,A novel ray-space based color correction algorithm for multi-view video,"In multi-view video, color inconsistency among different views always exists because of imperfect camera calibration, CCD noise, etc. Since color inconsistency greatly reduces the coding efficiency and rendering quality of multi-view video, a novel ray-space based color correction algorithm is proposed in this paper. Firstly, for each epipolar plane image (EPI) in ray-space domain, feature points are extracted to form a corresponding feature EPI (FEPI). Secondly, radon transform is applied to each FEPI to detect corresponding points from different views and the average color is calculated from the detected corresponding points. Finally, for each viewpoint image, the optimal color correction matrix is calculated by minimizing the error energy between the color of the current view and the average color based on the least square error criteria. Experimental results show that the proposed algorithm greatly improves the color consistency among different views. Moreover, the coding efficiency of the corrected multi-view images is greatly improved compared to that of the original ones and the ones corrected by histogram matching method.",2009,0,
1014,1015,Automated red-eye detection and correction in digital photographs,"Caused by light reflected off the subject's retina, red-eye is a troublesome problem in consumer photography. Although most of the cameras have the red-eye reduction mode, the result reality is that no on-camera system is completely effective. In this paper, we propose a fully automatic approach to detecting and correcting red-eyes in digital images. In order to detect red-eyes in a picture, a heuristic yet efficient algorithm is first adopted to detect a group of candidate red regions and then an eye classifier is utilized to confirm whether each candidate region is a human eye. Thereafter, for each detected redeye, we can correct it by the correction algorithm. In case that a red-eye cannot be detected automatically, another algorithm is also provided to detect red-eyes manually with the user's interaction by clicking on an eye. Experimental results on about 300 images with various red-eye appearances demonstrate that the proposed solution is robust and effective.",2004,0,
1015,1016,Forward link capacity evaluation for W-CDMA with amplitude limiter and forward error correction,"In this paper, the influence of the amplitude limiter of combined multiuser signals in the base station of the wideband code division multiple access (W-CDMA) system is described. The transmission performance under Rayleigh fading and interference from an adjacent 6-cell environment are evaluated by computer simulation; the tool of its simulation is MATLAB software. The relationship between limiter level and capacity degradation is clarified. Furthermore, it is shown that the introduction of the limiter is effective in reducing the dynamic range of the power amplifier. The effect of forward error correction (FEC) is also described. The results show that it is effective to use FEC in compensating the degradation by the amplitude limiter.",2002,0,
1016,1017,Defect recognition algorithm based on curvelet moment and support vector machine,"In this paper, a new recognition algorithm based on curvelet moment and support vector machine(SVM) is proposed for chip defect recognition. The proposed recognition method is implemented through a reference comparison method. First the defect regions of chips are extracted through preprocessing, and then the curvelet moment feature of the defect region is computed as the input of SVM classifier, the output of the trained SVM classifier is the result of defect recognition. The algorithm combines the good properties of curvelet moment and SVM classifier, the former can provide multi-scale, local details and orientation information of the defect region, and the latter is suitable to solve the small samples, nonlinear and high dimensions pattern recognition problem. Experimental results show that the algorithm has higher recognition rate compared with PCA based method and can solve the complex defects recognition problem effectively.",2010,0,
1017,1018,A rate-distortion approach to wavelet-based encoding of predictive error frames,"We develop a framework for efficiently encoding predictive error frames (PEF) as part of a rate scalable, wavelet-based video compression algorithm. We investigate the use of rate-distortion analysis to determine the significance of coefficients in the wavelet decomposition. Based on this analysis, we allocate the bit budget assigned to a PEF to the coefficients that yield the largest reduction in distortion, while maintaining the embedded and rate scalable properties of our video compression algorithm",2000,0,
1018,1019,"A comparative analysis of network dependability, fault-tolerance, reliability, security, and survivability","A number of qualitative and quantitative terms are used to describe the performance of what has come to be known as information systems, networks or infrastructures. However, some of these terms either have overlapping meanings or contain ambiguities in their definitions presenting problems to those who attempt a rigorous evaluation of the performance of such systems. The phenomenon arises because the wide range of disciplines covered by the term information technology have developed their own distinct terminologies. This paper presents a systematic approach for determining common and complementary characteristics of five widely-used concepts, dependability, fault-tolerance, reliability, security, and survivability. The approach consists of comparing definitions, attributes, and evaluation measures for each of the five concepts and developing corresponding relations. Removing redundancies and clarifying ambiguities will help the mapping of broad user-specified requirements into objective performance parameters for analyzing and designing information infrastructures.",2009,0,
1019,1020,Study on the Neural Network Model for Shield Construction Faults Diagnosis,"In order to solve the problem of establishing the mathematic model for shield construction faults diagnosis, an approach to the mathematic model by using BP neural network is presented in this paper. The BP neural network model for diagnosing three familiar shield construction faults based on the data of shield excavation parameters was built. The inputs of the model are respectively nine shield excavation parameters which are correlative with shield construction faults. The outputs of the model are three shield construction faults which are respectively the spewing at screw conveyer, the wear of disc-cutters and the jamming of shield. The case study of a shield project validated that the structure of the established model is practical, the diagnostic results are right and the diagnosis method is effective. The conclusion provides the beneficial guidance for the design of the online diagnosis system of shield construction faults based on the data of shield excavation parameters.",2010,0,
1020,1021,A novel fault tolerant design and an algorithm for tolerating faults in digital circuits,"This paper proposes a novel fault tolerant algorithm for tolerating stuck-at-faults in digital circuits. We consider in this paper single stuck-at type faults, occurring either at a gate input or at a gate output. A stuck-at-fault may adversely affect on the functionality of the user implemented design. A novel fault tolerant design based on hardware redundancy (replication) is presented here for single fault model to tolerate transient as well as permanent faults. The design is also suitable to be used for highly dependable systems implemented by means of Field Programmable Gate Arrays (FPGAs) at RTL level. This approach offers the possibility of using larger and more cost effective devices that contain interconnect defects without compromising on performance or configurability. The algorithm presented here demonstrates the fault tolerance capability of the design and is implemented for a full adder circuit but can be generalized for any other digital circuit. Using exhaustive testing the functioning of all the three full adders can be easily verified. In case of occurrence of stuck-at-faults; the circuit will configure itself to select the other fault free outputs. We have evaluated our novel fault tolerant technique (NFT) in five different circuits: full adder, encoder, counter, shift register and microprocessor. The proposed design approach scales well to larger digital circuits also and does not require fault detection. We have also presented and compared the results of triple modular redundancy (TMR) method with our technique. All possible faults are tested by injecting the faults using a multiplexer.",2008,0,
1021,1022,Planar Microwave Bandpass Filters with Defected Ground Resonators,"In this paper a study of some planar microwave bandpass filters composed of defected ground resonators is presented. Different kinds of couplings between two defected ground resonators are investigated: electric coupling, magnetic coupling and two different mixed couplings. The values of the coupling coefficients are extracted from the simulation results, obtained by using full-wave EM-field simulation software. On the basis of this study, some second-order coupled planar microwave bandpass Chebyshev filters are designed. The simulated performances of these novel filter structures are very close to the filter requirements, validating this way the design method and its results",2006,0,
1022,1023,The construction of a novel agent fault-tolerant migration model,"In agent migration process, malicious hosts can compromise the agent. To solve the problem, the paper introduces the measure of agent integrity verification and constructs a novel agent fault-tolerant migration model. The model avoids much agent replicas in migration process. By simulation experiment, the results prove that the model provided by the paper is feasible and efficient, and can save network resource much than other relative works.",2004,0,
1023,1024,"Fault detection, diagnosis and control in a tactical aerospace vehicle",In this paper we propose a fault-tolerant control ( FTC ) scheme using multiple controller switching. The performance of this scheme is studied on a tactical aerospace vehicle. A parity space (PS) based residual generation approach is used to detect the fault. Once a fault is detected the diagnosis scheme identifies the faulty actuator. Using this information on-line reconfiguration of the controller is done based on the configuration of the existing healthy actuator. To implement this scheme no modification were done in hardware (H/W) configuration and only existing redundancies were utilised. Simulation with nonlinear 6-degree of freedom (6-DoF) model shows that the above fault tolerant control approach is able to reduce the probability of failure due to actuators.,2003,0,
1024,1025,A Multi-Perspective Taxonomy for Systematic Classification of Grid Faults,Classification turns chaotic knowledge into regularity by systematizing a domain and providing a common vocabulary. Currently there is a lack of systematic and comprehensive studies in organization and classification of Grid faults. We address this gap with a multi-perspective Grid fault taxonomy describing an incident using eight different characteristics. It is hard to define a taxonomy of broad validity and acceptance that satisfies the vast number of requirements of the many Grid user communities. Nevertheless we proof that our taxonomy can serve as a solid basis for defining project-specific custom classification schemes by giving a concrete example created for a state-of-the-art Grid middleware environment.,2008,0,
1025,1026,Three-phase synchronous PWM for flyback converter with power-factor correction using FPGA ASIC design,The design and development of a synchronous pulsewidth modulation (PWM) generator suitable for the three-phase flyback converter with transformer isolated and power-factor correction using a field-programmable gate array is proposed. The proposed three-phase synchronous PWM makes it possible for the converter to obtain the sinusoidal supply currents with a near-unity power factor. A high-frequency transformer is considered in the design to provide galvanic isolation and serves the dual role of inductor and transformer. Results are provided to demonstrate the effectiveness of the design.,2004,0,
1026,1027,Fault Diagnosis on Hermetic Compressors Based on Sound Measurements,"A fault identification study is made to identify five common faults in hermetic compressors manufactured in a large plant. Sound power level is used as raw data. Sound measurements were made in a room where microphones were located at different places of a virtual hemi-sphere, designed according to international standards. Obtained data is analyzed using the artificial neural networks method, where the multilayer perceptron model is used. Two different analysis approaches are carried out. In the first approach, only the summary data that emanated from the information coming from all microphones are used. In the second approach, all data coming from all microphones are used. The results indicate that the first approach is partially successful and the second is successful.",2007,0,
1027,1028,Reflective fault-tolerant systems: from experience to challenges,"This paper presents research work performed on the development and the verification of dependable reflective systems based on MetaObject Protocols (MOPS). We describe our experience, we draw the lessons learned from both a design and a validation viewpoint, and we discuss some possible future trends on this topic. The main originality of this work relies on the combination of both design and validation issues for the development of reflective systems, which has led to the definition of a reflective framework for the next generation of fault-tolerant systems. This framework includes: 1) the specification of a MetaObject Protocol suited to the implementation of fault-tolerant systems and 2) the definition of a general test strategy to guide its verification. The proposed approach is generic and solves many issues related to the use and evolution of system platforms with dependability requirements. Two different instances of the specified MOP have been implemented in order to study the impact of the MOP architecture in the development of a reflective fault-tolerant system. As far as the test strategy is concerned, a different testing level is associated with each reflective mechanism defined in the MOP. For each testing level, we characterize the test objectives and the required test environments. According to this experience, several new research challenges are finally identified.",2003,0,
1028,1029,Implementation and verification of the Amplitude Recovery Method algorithm with the faults diagnostic system on induction motors,"The implementation and verification of the amplitude recovery method algorithm (A.R.M.), which was first presented in ICEMS2008, are displayed in this paper. The mathematics deduction, application and test results show that the A.R.M. can extract directly the energy of the harmonics of other orders (including high orders and fractional orders) in the tested original signals of three phase stator currents of induction motors even though the harmonics elements are much smaller than the fundamental one.",2009,0,
1029,1030,Influence of window width selection in fault diagnosis of loudspeaker based on Short-time Fourier Transform,"It is important that selects window function and width to Short-time Fourier Transform(STFT). Especially, when diagnosing fault loudspeaker, the different analysis window function and width can influent the result of analysis on respond signal of loudspeaker. So, it proposed a selecting analysis window function and width method based on the energy correction factor, the maximum side lobe and main lobe peak value in the frequency domain. Through reasonable selecting analysis window function and width, it can reduce the influence of truncation of signal caused by the Gibbs phenomenon and resolve the frequency resolution problem on STFT. Therefore, it can be more accurately to extract the fault feature of loudspeaker, and improve the loudspeaker on-line automatic fault detection accuracy.",2010,0,
1030,1031,Bridge fault diagnosis using stuck-at fault simulation,A new diagnostic fault simulator is described that diagnoses both feedback and nonfeedback bridge faults in combinational circuits while using information from fault simulation of single stuck-at faults. A realistic fault model is used which considers the existence of the Byzantine Generals problem. Sets representing nodes possibly involved in a defect are partitioned based on logic and fault simulation of failing vectors. The approach has been demonstrated for two-line bridge faults on several large combinational benchmark circuits containing Boolean primitives and has achieved over 98% accuracy for nonfeedback bridge faults and over 85% accuracy for feedback bridge faults with good diagnostic resolution,2000,0,
1031,1032,"Correction Technique for Cascade Gammas in I-124 Imaging on a Fully-3D, Time-of-Flight PET Scanner","It has been shown that I-124 PET imaging can be used for accurate dose estimation in radio-immunotherapy techniques. However, I-124 is not a pure positron emitter, leading to two types of coincidence events not typically encountered: increased random coincidences due to non-annihilation cascade photons, and true coincidences between an annihilation photon and primarily a coincident 602 keV cascade gamma (true coincidence gamma-ray background). The increased random coincidences are accurately estimated by the delayed window technique. Here we evaluate the radial and time distributions of the true coincidence gamma-ray background in order to correct and accurately estimate lesion uptake for I-124 imaging in a time-of-flight (TOF) PET scanner. We performed measurements using a line source of activity placed in air and a water-filled cylinder, using F-18 and I-124 radio-isotopes. Our results show that the true coincidence gamma-ray backgrounds in I-124 have a uniform radial distribution, while the time distribution is similar to the scattered annihilation coincidences. As a result, we implemented a TOF-extended single scatter simulation algorithm with a uniform radial offset in the tail-fitting procedure for accurate correction of TOF data in I-124 imaging. Imaging results show that the contrast recovery for large spheres in a uniform activity background is similar in F-18 and I-124 imaging. There is some degradation in contrast recovery for small spheres in I-124, which is explained by the increased positron range, and reduced spatial resolution, of I-124 compared to F-18. Our results show that it is possible to perform accurate TOF based corrections for I-124 imaging.",2009,0,
1032,1033,Application of Bayesian belief networks to fault detection and diagnosis of industrial processes,"In industrial processes, to confide the success of planed operation, implementing early and accurate method for recognizing abnormal operating conditions, known as faults, is essential. Effective method for fault detection and diagnosis helps reducing impact of these faults, extols the safety of operation, minimizes down time and reduces manufacturing costs. In this paper, application of BBNs is studied for a benchmark chemical industrial process, known as, Tennessee Eastman in order to achieve early fault detection and accurate probable diagnosis of their causes. Application of Bayesian belief networks for fault detection and diagnosis of Tennessee Eastman process in the graphical context description has not been tested yet. Success of this feature confirms capability and ease use of it as a diagnostic system in actual industrial processes.",2010,0,
1033,1034,Detection of rotor faults in torque controlled induction motor drives,"In the supervision of electrical equipment, the task of diagnostic system is to detect an upcoming machine fault as soon as possible, in order to save expensive manufacturing processes or to replace fault parts. An important issue in such effort is the modelling of the induction machine (IM) including rotor bar and end-ring faults, with a minimum of computational complexity. In this paper, a simpler method is employed in the simulation of an induction motor with rotor asymmetries. Simulation of classical and dynamic space vector models, Finite Element Analysis and experimental results are presented to support the proposed model. The need for detection of rotor faults at an earlier stage has pushed the development of monitoring methods with increasing sensitivity and noise immunity. Addressing diagnostic techniques based on current signatures analysis (MCSA), the characteristic components introduced by specific faults in the current spectrum are investigated and a diagnosis procedure correlates the amplitudes of such components to the fault extent. The impact of feedback control on asymmetric rotor cage induction machine behavior is also analyzed. It is shown that the variables usually employed in diagnosis procedures assuming open-loop operation are no longer effective under closed-loop operation. Simulation results show that signals already present at the drive are suitable to effective diagnostic procedure. The utilization of the current regulator error signals in rotor failure detection are the aim of the present work. The use of a band-pass filter bank to detect the presence of sidebands is also proposed.",2007,0,
1034,1035,Analysis of Hyperion data with the FLAASH atmospheric correction algorithm,"A combination of good spatial and spectral resolution make visible to shortwave infrared spectral imaging from aircraft or spacecraft a highly valuable technology for remote sensing of the Earth's surface. Many applications require the elimination of atmospheric effects caused by molecular and particulate scattering; a process known as atmospheric correction, compensation, or removal. The Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes (FLAASH) atmospheric correction code derives its physics-based algorithm from the MODTRAN4 radiative transfer code. A new spectra; recalibration algorithm, which has been incorporated into FLAASH, is described. Results from processing Hyperion data with FLAASH are discussed.",2003,0,
1035,1036,Transient Fault Prediction Based on Anomalies in Processor Events,"Future microprocessors will be highly susceptible to transient errors as the sizes of transistors decrease due to CMOS scaling. Prior techniques advocated full scale structural or temporal redundancy to achieve fault tolerance. Though they can provide complete fault coverage, they incur significant hardware and/or performance cost. It is desirable to have mechanisms that can provide partial but sufficiently high fault coverage with negligible cost. To meet this goal, we propose leveraging speculative structures that already exist in modern processors. The proposed mechanism is based on the insight that when a fault occurs, it is likely that the incorrect execution would result in abnormally higher or lower number of mispredictions (branch mispredictions, L2 misses, store set mispredictions) than a correct execution. We design a simple transient fault predictor that detects the anomalous behavior in the outcomes of the speculative structures to predict transient faults",2007,0,
1036,1037,Comparison of accelerated DRAM soft error rates measured at component and system level,"Single event upsets from terrestrial cosmic rays (i.e. high-energy neutrons) are more important than alpha particle induced soft errors in modern DRAM devices. A high intensity broad spectrum neutron source from the Los Alamos Neutron Science Center (LANSCE) was used to characterize the nature of these upsets in DRAM technologies ranging from 180 nm down to 70 nm from several vendors at the DIMM component level using a portable memory tester. Another set of accelerated neutron beam tests were made with DRAM DIMMs mounted on motherboards. Soft errors were characterized using these two methods to determine the influence of neutron angle, frequency, data patterns and process technology. The purpose of this study is to analyze the effects of these differences on DRAM soft errors.",2008,0,
1037,1038,Error analysis of the moment method,"Because of the widespread use of the Method of Moments for simulation of radiation and scattering problems, analysis and control of solution error is a significant concern in computational electromagnetics. The physical problem to be solved, its mesh representation, and the numerical method all impact accuracy. Although empirical approaches such as benchmarking are used almost exclusively in practice for code validation and accuracy assessment, a number of significant theoretical results have been obtained in recent years, including proofs of convergence and solution-error estimates. This work reviews fundamental concepts such as types of error measures, properties of the problem and numerical method that affect error, the optimality principle, and basic approximation error estimates. Analyses are given for surface-current and scattering-amplitude errors for several scatterers, including the effects of edge and corner singularities and quadrature error. We also review results on ill-conditioning due to resonance effects and the convergence rates of iterative linear-system solutions.",2004,0,
1038,1039,Fault management in ECLIPSE,"ECLIPSE is a next-generation virtual telecommunication network that provides multimedia services that integrate voice, video, text and images. ECLIPSE facilitates the modular decomposition of new telecommunication services. In this paper, we sketch the challenges we face in making ECLIPSE highly available when running on top of a heterogenous and widely distributed system. We describe how we approach these challenges using the modular decomposition of services provided by ECLIPSE, together with its novel failure detection and recovery strategies",2001,0,
1039,1040,The state of documentation practice within corrective maintenance,"Consistent, correct and complete documentation is an important vehicle for the maintainer to gain understanding of a software system, to ease the learning and/or relearning processes, and to make the system more maintainable. Former studies have shown that documentation is one of the most neglected process issues within software engineering today. The authors check the current state of documentation practice within corrective maintenance in Sweden",2001,0,
1040,1041,What Types of Defects Are Really Discovered in Code Reviews?,"Research on code reviews has often focused on defect counts instead of defect types, which offers an imperfect view of code review benefits. In this paper, we classified the defects of nine industrial (C/C++) and 23 student (Java) code reviews, detecting 388 and 371 defects, respectively. First, we discovered that 75 percent of defects found during the review do not affect the visible functionality of the software. Instead, these defects improved software evolvability by making it easier to understand and modify. Second, we created a defect classification consisting of functional and evolvability defects. The evolvability defect classification is based on the defect types found in this study, but, for the functional defects, we studied and compared existing functional defect classifications. The classification can be useful for assigning code review roles, creating checklists, assessing software evolvability, and building software engineering tools. We conclude that, in addition to functional defects, code reviews find many evolvability defects and, thus, offer additional benefits over execution-based quality assurance methods that cannot detect evolvability defects. We suggest that code reviews may be most valuable for software products with long life cycles as the value of discovering evolvability defects in them is greater than for short life cycle systems.",2009,0,
1041,1042,Regularized B-spline deformable registration for respiratory motion correction in PET images,"A major challenge in respiratory motion correction of gated PET images is their low signal to noise ratios (SNR). This particularly affects the accuracy of image registration. This paper presents an approach to overcoming this problem using a deformable registration algorithm which is regularized using a Markov random field (MRF). The deformation field is represented using B-splines and is assumed to form a MRF. A regularizer is then derived and introduced to the registration, which penalizes noisy deformation fields. Gated PET images are aligned using this registration algorithm and summed. Experiments with simulated data show that the regularizer effectively suppresses the noise in PET images, yielding satisfactory deformation fields. After motion correction, the PET images have significantly better image quality.",2008,0,
1042,1043,Refined Spatial Error Concealment with Directional Entropy,"A refined error concealment method for intra frame in H.264 is proposed in this work. Directional entropy of neighboring edges is used to classify the content of the lost block. Some techniques aimed to shorten the computation time without degrading quality of reconstructed video are presented to enforce multi-directional interpolation. As for block with high texture, either bilinear or multi-directional interpolation alone is not so well to recover, we integrate the two methods to get the final result. Experimental results demonstrate the efficiency and better performance of proposed method as compared to other interpolation based methods.",2009,0,
1043,1044,Adaptive error control scheme for multimedia applications in integrated terrestrial-satellite wireless networks,"This paper presents an adaptive error control (AEC) scheme for multimedia applications in integrated terrestrial-satellite wireless networks. The AEC protocol supports both real-time and non-real-time applications. In the AEC protocol, we propose new adaptive FEC (AFEC) and hybrid ARQ (HARQ) schemes for real-time and non-real-time traffic, respectively. Throughput performance for non-real-time application shows that the proposed AEC protocol outperforms hybrid ARQ (HARQ) protocols with the same code used. Under real-time application, the AEC protocol outperforms the static FEC (SFEC) protocols with respect to packet miss probability",2000,0,
1044,1045,Analysis of the ABS Wheel Speed Signal Error and Method of Equal Period Sampling,"The measuring error of wheel speed signal of anti-lock braking system (ABS) has been analyzed in this paper. It has been concluded that trigger error is the main factor which influences the wheel speed measurement limitation, and the correlation between trigger error and the wheel speed signal are surveyed by experiments. It is necessary in ABS to transform the wheel speed signal sampled in the form of equiangular to the form of equal period. In the low speed measurement situation of lacking wheel speed signal, a prediction algorithm based on polynomial order two fit has been utilized to estimate wheel speed. Simulation results verified the algorithm.",2006,0,
1045,1046,Evaluation of several Efron bootstrap methods to estimate error measures for software metrics,"A narrow confidence interval of a sample statistic or a model parameter implies low variability of that statistic, and permits a strong conclusion to be made about the underlying population. Conversely, the analysis should be considered inconclusive if the confidence interval is wide. Efron's (1992) bootstrap statistical analysis appears to address the fact that many statistics used in software metrics analysis do not come with theoretical formulas to allow accuracy assessment. In this paper we will present preliminary results on an empirical analysis of the reliability of several Efron nonparametric bootstrap methods in assessing the accuracy of sample statistics in the context of software metrics. In particular, we focus on the standard errors and 90% confidence intervals of five basic statistics as a tool to evaluate the Bootstrap. It was found confidence intervals for mean and median were accurately estimated, those for variance grossly under-estimated with skewness and kurtosis grossly over-estimated.",2002,0,
1046,1047,Fault-Tolerant CORBA: From Specification to Reality,"Based on 10 years' personal experience implementing academic prototypes of FT-CORBA, helping to establish the FT-CORBA standard, and subsequently developing and selling its commercial implementations, this critique provides an overview of the FT-CORBA standard's specifications from the viewpoint of realizing its concrete implementation for real-world applications",2007,0,
1047,1048,Interleaving for combating bursts of errors,"To ensure data fidelity, a number of random error correction codes (ECCs) have been developed. ECC is, however, not efficient in combating bursts of errors, i.e., a group of consecutive (in one-dimensional (1-D) case) or connected (in two- and three- dimensional (2-D and 3-D) case) erroneous code symbols owing to the bursty nature of errors. Interleaving is a process to rearrange code symbols so as to spread bursts of errors over multiple code-words that can be corrected by ECCs. By converting bursts of errors into random-like errors, interleaving thus becomes an effective means to combat error bursts. In this article, we first illustrate the philosophy of interleaving by introducing a 1-D block interleaving technique. Then multi-dimensional (M-D) bursts of errors and optimality of interleaving are defined. The fundamentals and algorithms of the state of the art of M-D interleaving - the t-interleaved array approach by Blaum, Bruck and Vardy and the successive packing approach by Shi and Zhang-are presented and analyzed. In essence, a t-interleaved array is constructed by closely tiling a building block, which is solely determined by the burst size t. Therefore, the algorithm needs to be implemented each time for a different burst size in order to maintain either the error burst correction capability or optimality. Since the size of error bursts is usually not known in advance, the application of the technique is somewhat limited. The successive packing algorithm, based on the concept of 2  2 basis array, only needs to be implemented once for a given square 2-D array, and yet it remains optimal for a set of bursts of errors having different sizes. The performance comparison between different approaches is made. Future research on the successive packing approach is discussed. Finally, applications of 2-D/3-D successive packing interleaving in enhancing the robustness of image/video data hiding are presented as examples of practical utilization of interleaving.",2004,0,
1048,1049,Research on domestic PV module structure based on fault detection,"As to detecting the location of fault in photovoltaic (PV) module structure, this paper presents a new type of PV arrays connection: CTCT structure (complex-total-cross-tied array). In the array of CTCT-type PV cells, by adding a certain number of current sensors and comparing the detected current, the Hot Spot cells can be located to avoid their damage to PV panels. This paper derives a formula to demonstrate the relationship between the total number of PV panels in parallel, the resolution and the total number of current sensors in the PV systems. And the algorithm has been verified correctly in a 3*9 board of PV panel. With the certain number of PV panels, selecting the reasonable value of resolution for determining the number of sensors needed, we can detect the precise location of PV cells that have Hot Spot by using current sensors as few as possible.",2010,0,
1049,1050,On the Threat of Metastability in an Asynchronous Fault-Tolerant Clock Generation Scheme,"Due to their handshake-based flow control, asynchronous circuits generally do not suffer from metastability issues as much as synchronous circuits do. We will show, however, that fault effects like single-event transients can force (sequential) asynchronous building blocks such as Muller C-Elements into a metastable state. At the example of a fault-tolerant clock generation scheme, we will illustrate that metastability could overcome conventional error containment boundaries, and that, ultimately, a single metastable upset could cause even a multiple Byzantine fault-tolerant system to fail. In order to quantify this threat, we performed analytic modeling and simulation of the elastic pipelines, which are at the heart of our physical implementation of the fault-tolerant clocks. Our analysis results reveal that only transient pulses of some very specific width can trigger metastable behavior. So even without consideration of other masking effects the probability of a metastable upset to propagate through a pipeline is fairly small. Still, however, a thorough metastability analysis is mandatory for circuits employed in high-dependability applications.",2009,0,
1050,1051,"Using program analysis to identify and compensate for nondeterminism in fault-tolerant, replicated systems","Fault-tolerant replicated applications are typically assumed to be deterministic, in order to ensure reproducible, consistent behavior and state across a distributed system. Real applications often contain nondeterministic features that cannot be eliminated. Through the novel application of program analysis to distributed CORBA applications, we decompose an application into its constituent structures, and discover the kinds of nondeterminism present within the application. We target the instances of nondeterminism that can be compensated for automatically, and highlight to the application programmer those instances of nondeterminism that need to be manually rectified. We demonstrate our approach by compensating for specific forms of nondeterminism and by quantifying the associated performance overheads. The resulting code growth is typically limited to one extra line for every instance of nondeterminism, and the runtime overhead is minimal, compared to a fault-tolerant application with no compensation for nondeterminism.",2004,0,
1051,1052,Neural network approach to diagnose faults in linear antenna array,"A novel approach using artificial neural network (ANN) is proposed to identify the faulty elements present in a non uniform linear array. The input to the neural network is amplitude of radiation pattern and output of neural network is the location of faulty elements. In this work, ANN is implemented with two algorithms; radial basis function neural network (RBF) and probabilistic neural network and their performance is compared. The network is trained with some of the possible faulty radiation patterns and tested with various measurement errors. It is proved that the method gives a high success rate.",2008,0,
1052,1053,Stator winding turn-fault detection for closed-loop induction motor drives,"Sensorless diagnostics for line-connected machines is based on extracting fault signatures from the spectrum of the line currents. However, for closed-loop drives, the power supply is a regulated current source and, hence, the motor voltages must also be monitored for fault information. In this paper, a previously proposed neural network scheme for turn-fault detection in line-connected induction machines is extended to inverter-fed machines, with special emphasis on closed-loop drives. Experimental results are provided to illustrate that the method is impervious to machine and instrumentation nonidealities, and that it requires lesser data memory and computation requirements than existing schemes, which are based on data lookup tables.",2003,0,
1053,1054,Fault injection approach based on dependence analysis,"Fault injection is used to validate a system in the presence of faults. Jaca, a software injection tool developed in previous work, is used to inject faults at interfaces between classes of a system written in Java. We present a strategy for fault injection validation based on dependence analysis. The dependence analysis approach is used to help in reducing the number of experiments necessary to cover the system's interfaces. For the experiments we used a system that consists of two integrated components, an ODBMS performance test benchmark, Wisconsin 007 and an ODBMS, Ozone. The results of some experiments and their analysis are presented.",2005,0,
1054,1055,Analysis of the practical capacity of multi-valued hetero-associator considering fault tolerance,"Presents a method of pattern recognition using the multi-valued polynomial bidirectional hetero-associator (PBHA). This network can be used for the industrial application of optical character recognition. According to detailed simulations, the PBHA has a higher capacity for pattern pair storage than that of the conventional bidirectional associative memories and fuzzy memories. Meanwhile, the practical capacity of a PBHA considering fault tolerance is discussed. The fault tolerance requirement leads to the discovery of the attraction radius of the basin for each stored pattern pair. The PBHA takes advantage of multi-valued characteristics in evolution equations such that the signal-noise-ratio is significantly increased. We apply the result of this research to pattern recognition problems. The practical capacity of the multi-valued data recognition using the PBHA considering fault tolerance in the worst case is also estimated. Simulation results are presented to verify the derived theory",2001,0,
1055,1056,A DSP based controller for power factor correction (PFC) in a rectifier circuit,In this paper a digital signal processor (DSP) based power factor correction (PFC) scheme is presented. A dual-loop controller is designed to control the average input AC current as well as DC bus voltage. The DSP controller is implemented and tested. Design methodologies and trade-offs such as discrete-time implementation methods are also presented,2001,0,
1056,1057,Total Sensitivity Index Calculation via Error Propagation Equation,"This paper presents a new and convenient method to calculate the total sensitivity indices defined by variance-based sensitivity analysis. By decomposing the output variance using error propagation equations, this method can transform the ""double-loop"" sampling procedure into ""single-loop"" one and obviously reduce the computation cost of analysis. In contrast with Sobol and Fourier amplitude sensitivity test (FAST) method, which is limited in non-correlated variables, new approach is suitable for correlated input variables. An application in semiconductor assemble and test manufacturing (ATM) factory indicates that this approach has a good performance in additive model and simple non-additive mathematical model.",2007,0,
1057,1058,Accurate Bit-Error-Rate Analysis of Bandlimited Cooperative OSTBC Networks Under Timing Synchronization Errors,"The distributed multiple-input-multiple-output (MIMO) system (e.g., intercluster communication via cooperating nodes in a wireless sensor network) is a topic of emerging interest. Many previous studies have assumed perfect synchronization among cooperating nodes and identically distributed communication links. Such assumptions are rarely valid in practical operating scenarios. This paper develops an analytical framework for computing the average bit error rate (ABER) of a distributed multiple-input-single-output (MISO) space-time-coded system with binary phase shift keying (BPSK) modulation affected by timing synchronization errors. The cooperating nodes use data pulse-shaping filters for transmission over generalized frequency-nonselective fading channels. As an illustrative example, the performance evaluation of a 2 times 1 MISO system that uses distributed orthogonal space-time block coding (OSTBC) is presented, although this approach can be readily extended to analyze distributed transmit diversity with a larger number of cooperating nodes. We show that under certain conditions, a distributed MISO system with time synchronization errors can still outperform a perfectly synchronized single-input-single-output (SISO) system.",2009,0,
1058,1059,Software defect detection and process improvement using personal software process data,"Statistical evidence has shown that programmers perform better when following a defined, repeatable process such as the Personal Software Process (PSP).Anecdotal and qualitative evidence from industry indicates that two programmers working side-by-side at one computer, collaborating on the same design, algorithm, code, or test, perform substantially better than the two working alone i.e. pair programming. Bringing these two ideas together, a new Software Process has been formulated. The Hybrid Personal Software Process (HPSP) is a defined, repeatable process for two programmers working collaboratively. The development time and cost of the product are reduced in HPSP when compared with PSP programming.",2010,0,
1059,1060,Fuzzy Expert System for Defect Classification for Non-Destructive Evaluation of Petroleum Pipes,"In this paper, an expert system has been outlined to classify the defects in metallic petroleum pipelines using acoustic techniques with non-destructive evaluation (NDE) protocols, the proposed system maps the quantitative defect data through a novel perception-based kernel that has its roots in multidimensional fuzzy set theory to map the relative weights given to various features; mathematical or statistical, to the decision surface to deduce the type of the defect. The system has a centralized database which holds the defect information in the form of known and calculated features. The known features and their quantitative representations are used to initialize the database. Then experiments are conducted on known defects and the collected experimental data is then modeled into autoregressive process models using state of the art l<sub>tinfin</sub> deconvolution algorithm. With each feature set, a classifier tag is associated that assigns a class number to that defect. The classifier tag is then used to classify any new data using the fuzzy classifier.",2007,0,
1060,1061,"A Model Driven Architecture approach to fault tolerance in Service Oriented Architectures, a performance study","In modern service oriented architectures (SoA) identifying the occurrences of failure is a crucial task, which can be carried out by the creation of diagnosers to monitor the behavior of the system. Model driven architecture (MDA) can be used to automatically create diagnosers and to integrate them into the system to identify if a failure has occurred. There are different methods of incorporating a diagnoser into a group of interacting services. One option is to modify the BPEL file representing services to incorporate the diagnoser. Another option is to implement the diagnoser as a separate service which interacts with the existing services. Moreover, the interaction between the diagnoser and the services can be either orchestration or choreography. As result, there are four options for the implementation of the diagnoser into the SoA via MDA. This paper reports on an Oracle JDeveloper plugin tool developed which applies MDA to create these four possible implementations and compares the performance of them with the help of a case study.",2008,0,
1061,1062,Rigorous development of an embedded fault-tolerant system based on coordinated atomic actions,"Describes our experience using coordinated atomic (CA) actions as a system structuring tool to design and validate a sophisticated and embedded control system for a complex industrial application that has high reliability and safety requirements. Our study is based on an extended production cell model, the specification and simulator for which were defined and developed by FZI (Forschungszentrum Informatik, Germany). This ""fault-tolerant production cell"" represents a manufacturing process involving redundant mechanical devices (provided in order to enable continued production in the presence of machine faults). The challenge posed by the model specification is to design a control system that maintains specified safety and liveness properties even in the presence of a large number and variety of device and sensor failures. Based on an analysis of such failures, we provide details of: (1) a design for a control program that uses CA actions to deal with both safety-related and fault tolerance concerns and (2) the formal verification of this design based on the use of model checking. We found that CA action structuring facilitated both the design and verification tasks by enabling the various safety problems (involving possible clashes of moving machinery) to be treated independently. Even complex situations involving the concurrent occurrence of any pairs of the many possible mechanical and sensor failures can be handled simply yet appropriately. The formal verification activity was performed in parallel with the design activity, and the interaction between them resulted in a combined exercise in ""design for validation""; formal verification was very valuable in identifying some very subtle residual bugs in early versions of our design which would have been difficult to detect otherwise",2002,0,
1062,1063,An Error Resilient Coding Scheme for Video Transmission Based on Pixel Line Decimation,"The loss of packets is unavoidable when compressed video data is transmitted over error prone channels. In this study, an error resilient coding scheme based on pixel lines decimation is proposed to enhance performance of error concealment for both intra and inter frames. At the encoder, an input picture is decimated by pixel lines into two similar sub-pictures and then they are merged together before encoding. At the decoder, the high correlation of two sub-pictures is employed to facilitate error concealment. For an intra frame, the lost pixel lines in a sub-picture can be spatial concealed by interpolation between spatial neighbor correct pixel lines in the other sub-picture in a short distance. For an inter frame, a corrupt macroblock can be temporal concealed by utilizing motion vectors of its co-located macroblock in the other sub-picture. Experimental results demonstrate that the proposed scheme can significantly improve both subjective and objective quality of the reconstructed picture.",2008,0,
1063,1064,Diagnosing arbitrary defects in logic designs using single location at a time (SLAT),"A new form of logic diagnosis is described that is suitable for diagnosing fails in combinational logic. It can diagnose defects that can affect arbitrarily many elements in the integrated circuit. It operates by first identifying patterns during which only one element is affected by the defect, and then diagnosing the fails observed during the application of such patterns, one pattern at a time. Single stuck-at faults are used for this purpose, and the aggregate of stuck-at fault locations thus identified is then further analyzed to obtain the most accurate estimate of the identities of those elements that can be affected by the defect. This approach to logic diagnosis is as effective as that of classical stuck-at fault-based diagnosis, when the latter applies, but is far more general. In particular, it can diagnose fails caused by bridges and opens as well as fails caused by regular stuck-at faults.",2004,0,
1064,1065,Minimizing temperature drift errors of conditioning circuits using artificial neural networks,"Temperature drift errors are a problem that affect the accuracy of measurement systems. When small amplitude signals from transducers are considered and environmental conditions of conditioning circuits exhibit a large temperature range, the temperature drift errors have a real impact in systems accuracy. In this paper, a solution to overcome the problem of temperature drift errors of conditioning circuits is proposed. As an example, a thermocouple-based temperature measurement system is considered, and the stability of its conditioning circuit (AD595) is analyzed in two cases: with and without temperature drift error compensation. An Artificial Neural Network (ANN) is used for data optimization and a Virtual Instrument, using GPIB instrumentation, is used to collect experimental data. Final results show a significant improvement in the accuracy of the system when the proposed temperature drift error compensation technique is applied to compensate errors caused by temperature variations",2000,0,
1065,1066,"A fault tolerant, peer-to-peer replication network","We propose a fault tolerant, peer-to-peer replication network for synchronizing files across multiple hosts. The proposed topology is constructed by applying existing technologies and tools to ensure that files are kept synchronized even after subsequent modifications. One of its main advantages lies in the fact that there is no central authority to coordinate the process, hosts are connected in a peer-to-peer fashion, thus avoiding a single point of failure. Our proposal is intended for use in networks of personal computers where a small number of hosts have to be synchronized.",2010,0,
1066,1067,A formal specification of fault-tolerance in prospecting asteroid mission with Reactive Autonomie Systems Framework,"The NASA's Autonomous Nano Technology Swarm (ANTS) is a generic mission architecture consisting of miniaturized, autonomous, self-similar, reconfigurable, and addressable components forming structures. The Prospecting Asteroid Mission (PAM) is one of ANTS applications for survey of large dynamic populations. In this paper, we propose a formal approach based on Category Theory to specify the fault-tolerance property in PAM by Reactive Autonomie Systems Framework.",2010,0,
1067,1068,Analytical redundancy for sensor fault isolation and accommodation in public transportation vehicles,"The paper discusses an instrument fault detection, isolation, and accommodation procedure for public transportation vehicles. After a brief introduction to the topic, the rule set implementing the procedure with reference to the kinds of sensors usually installed on public transportation vehicles is widely discussed. Particular attention is paid to the description of the rules aimed at allowing the vehicle to continue working regularly even after a sensor fault develops. Finally, both the estimated diagnostic and dynamic performances in the off-line processing of the data acquired in several drive tests are then analyzed and commented upon.",2004,0,
1068,1069,An error tolerance scheme for 3D CMOS imagers,"A three-dimensional (3D) CMOS imager constructed by stacking a pixel array of backside illuminated sensors, an analog-to-digital converter (ADC) array, and an image signal processor (ISP) array using micro-bumps (ubumps) and through-silicon vias (TSVs) is promising for high throughput applications. However, due to the direct mapping from pixels to ISPs, the overall yield relies heavily on the correctness of the ubumps, ADCs and TSVs - a single defect leads to the information loss of a tile of pixels. This paper presents an error tolerance scheme for the 3D CMOS imager that can still deliver high quality images in the presence of bump, ADC, and/or TSV failures. The error tolerance is achieved by properly interleaving the connections from pixels to ADCs so that the corrupted data, if any, can be recovered in the ISPs. A key design parameter, the interleaving stride, is decided by analyzing the employed error correction algorithm. Architectural simulation results demonstrate that the error tolerance scheme enhances the effective yield of an exemplar 3D imager from 46% to 99%.",2010,0,
1069,1070,Fault Tolerant Active Rings for Structured Peer-to-Peer Overlays,"Algorithms by which peers join and leave structured overlay networks can be classified as passive or active. Passive topology maintenance relies on periodic background repair of neighbor pointers. When a node passively leaves the overlay, subsequent lookups may fail silently. Active maintenance has been proven only for fault-free networks. We develop an active topology maintenance algorithm for practical, fault-prone networks. Unlike prior work, it a) maintains ring continuity during normal topology changes and b) guarantees consistency and progress in the presence of faults. The latter property is inherited by novel extension of the Paxos commit algorithm. The topology maintenance algorithm is formally developed using the B method and its event-driven extensions for dynamic systems. Messaging and storage overheads are quantified",2005,0,
1070,1071,On the error distribution for randomly-shifted lattice rules,"Randomized quasi-Monte Carlo (RQMC) methods estimate the expectation of a random variable by the average of n dependent realizations of it. In general, due to the strong dependence, the estimation error may not obey a central limit theorem. Analysis of RQMC methods have so far focused mostly on the convergence rates of asymptotic worst-case error bounds and variance bounds, when n  , but little is known about the limiting distribution of the error. Here we examine this limiting distribution for the special case of a randomly-shifted lattice rule, when the integrand is smooth. We start with simple one-dimensional functions, where we show that the limiting distribution is uniform over a bounded interval if the integrand is non-periodic, and has a square root form over a bounded interval if the integrand is periodic. In higher dimensions, for linear functions, the distribution function of the properly standardized error converges to a spline of degree equal to the dimension.",2009,0,
1071,1072,Consistent detection of global predicates under a weak fault assumption,"We study the problem of detecting general global predicates in distributed systems where all application processes and at most t<m monitor processes may be subject to crash faults, where m is the total number of monitor processes in the system. We introduce two new observation modalities called negotiably and discernibly (which correspond to possibly and definitely in fault-free systems) and present detection algorithms for them which work under increasingly weak fault assumptions",2000,0,
1072,1073,A fault-tolerant P-Q decoupled control scheme for static synchronous series compensator,"Control of nonlinear devices in power systems relies on the availability and the quality of sensor measurements. Measurements can be corrupted or interrupted due to sensor failure, broken or bad connections, bad communication, or malfunction of some hardware or software (referred to as missing sensor measurements in this paper). This paper proposes a fault-tolerant control scheme (FTCS) for a static synchronous series compensator (SSSC). This FTCS consists of a sensor evaluation and (missing sensor) restoration scheme (SERS) cascaded with a P-Q decoupled control scheme (PQDC). It is able to provide effective control to the SSSC when single or multiple crucial sensor measurements are unavailable. Simulation studies are carried out to examine the validity of the proposed FTCS. During the simulations, single and multiple phase current sensors are assumed to be missing, respectively. Results show that the SERS restores the missing data correctly during steady and transient states, including small and large disturbances, and unbalanced three-phase operation. Thus, the FTCS continuously provides effective control to the SSSC with and without missing sensor measurements",2006,0,
1073,1074,BER Performance of FSO Links over Strong Atmospheric Turbulence Channels with Pointing Errors,"In this letter, we investigate the error rate performance of free-space optical (FSO) links over strong turbulence fading channels together with misalignment (pointing error) effects. First, we present a novel closed-form expression for the distribution of a stochastic FSO channel model which takes into account both atmospheric turbulence-induced fading and misalignment-induced fading. Then, we evaluate the average bit-error rate in closed form of a FSO system operating in this channel environment, assuming intensity modulation/direct detection with on-off keying. Numerical examples are further provided to collaborate on the derived analytical expressions.",2008,0,
1074,1075,Deterministic high-speed simulation of complex systems including fault-injection,"FAUmachine is a virtual machine for the highly detailed simulation of standard PC hardware together with an environment. FAUmachine comes with fault injection capabilities and an automatic experiment controller facility. Due to its use of just-in-time compiler techniques, it offers good performance. This tool description introduces the new feature of FAUmachine to simulate systems deterministically. This will enable developers to design and test complex systems for fault tolerance by running identically reproducible automated tests in reasonable time and thus even allow testing for real time constraints.",2009,0,
1075,1076,Fault Diagnosis Using a Timed Discrete-Event Approach Based on Interval Observers: Application to Sewer Networks,This paper proposes a fault diagnosis method using a timed discrete-event approach based on interval observers that improves the integration of fault detection and isolation tasks. The interface between fault detection and fault isolation considers the activation degree and the occurrence time instant of the diagnostic signals using a combination of several theoretical fault signature matrices that store the knowledge of the relationship between diagnostic signals and faults. The fault isolation module is implemented using a timed discrete-event approach that recognizes the occurrence of a fault by identifying a unique sequence of observable events (fault signals). The states and transitions that characterize such a system can directly be inferred from the relation between fault signals and faults. The proposed fault diagnosis approach has been motivated by the problem of detecting and isolating faults of the Barcelona's urban sewer system limnimeters (level meter sensors). The results obtained in this case study illustrate the benefits of using the proposed approach in comparison with the standard fault detection and isolation approach.,2010,0,
1076,1077,A Zero Module Current Obtaining Approach Based on Magnetic Induction for Single Phase Grounding Fault,"A new approach based on the magnetic field induction is presented to obtain transient zero module current for single phase grounding gault of overhead lines. The paper analyses the characteristics of magnetic field around the overhead lines and presents the magnetic field under the lines is proportional to the zero module current, and the zero module current can be measured by inducting the magnetic field. The paper proposes a zero-module current obtaining approach using a hall sensor to induct magnetic field, and elaborates the solution to the key issues in practical applications, at last simulation and experiment results demonstrate the feasibility of the approach.",2010,0,
1077,1078,Similarity-Based Bayesian Learning from Semi-structured Log Files for Fault Diagnosis of Web Services,"With the rapid development of XML language which has good flexibility and interoperability, more and more log files of software running information are represented in XML format, especially for Web services. Fault diagnosis by analyzing semi-structured and XML like log files is becoming an important issue in this area. For most related learning methods, there is a basic assumption that training data should be in identical structure, which does not hold in many situations in practice. In order to learn from training data in different structures, we propose a similarity-based Bayesian learning approach for fault diagnosis in this paper. Our method is to first estimate similarity degrees of structural elements from different log files. Then the basic structure of combined Bayesian network (CBN) is constructed, and the similarity-based learning algorithm is used to compute probabilities in CBN. Finally, test log data can be classified into possible fault categories based on the generated CBN. Experimental results show our approach outperforms other learning approaches on those training datasets which have different structures.",2010,0,
1078,1079,Path Splicing with Guaranteed Fault Tolerance,"This paper addresses the problem of exploring the fault tolerance potential of the routing primitive called path splicing. This routing mechanism has been recently introduced in order to improve the reliability level of networks. The idea is to provide for each destination node in a network several different routing trees, called slices, by running different routing protocols simultaneously. The possibility for the traffic to switch between different slices at any hop on the way to the destination makes it possible to achieve a level of reliability that is close to the ideal level achieved by the underlying network. In this work we show that there is a method for computing just two slices that achieves fault tolerance against all single-link failures that do not disconnect the underlying network. We present an experimental evaluation of our approach, showing that for a number of realistic topologies our method of computing the slices achieves the same level of fault tolerance that is achieved by a much larger number of slices using the previously proposed method.",2009,0,
1079,1080,Simulated Fault Injection for Quantum Circuits Based on Simulator Commands,"This paper addresses the problem of evaluating the fault tolerance algorithms and methodologies (FTAMS) designed for quantum circuits, by making use of fault injection techniques. These techniques are inspired from their rich classical counterparts, and were adapted to the quantum computation specific features, including the available error models. Because of their wide spectrum of application, including quantum circuit simulation, and their flexibility in circuit representation (i.e. both behavioral and structural descriptions of a circuit are possible), the hardware description languages (HDLs) appear as the best choice for our simulation experiments. The techniques employed for fault injection are based on simulator commands. The simulation results for the fault-affected circuit were compared to the outputs of a Gold circuit (a faultless circuit) in order to compute the quantum failure rate.",2007,0,
1080,1081,Speeding up Fault Injection for Asynchronous Logic by FPGA-Based Emulation,"While stability and robustness of synchronous circuits becomes increasingly problematic due to shrinking feature sizes, delay-insensitive asynchronous circuits are supposed to provide inherent protection against various fault types. However, results on experimental evaluation and analysis of these fault tolerance properties are scarce, mainly due to the lack of suitable prototyping platforms. Using a soft-core processor as an example, this paper shows how an off-the-shelf FPGA can be used for asynchronous four state logic designs, on which future fault injection experiments will be conducted.",2009,0,
1081,1082,The effect of the specification model on the complexity of adding masking fault tolerance,"In this paper, we investigate the effect of the representation of safety specification on the complexity of adding masking fault tolerance to programs - where, in the presence of faults, the program 1) recovers to states from where it satisfies its (safety and liveness) specification and 2) preserves its safety specification during recovery. Specifically, we concentrate on two approaches for modeling the safety specifications: 1) the bad transition (BT) model, where safety is modeled as a set of bad transitions that should not be executed by the program, and 2) the bad pair (BP) model, where safety is modeled as a set of finite sequences consisting of at most two successive transitions. If the safety specification is specified in the BT model, then it is known that the complexity of automatic addition of masking fault tolerance to high atomicity programs - where processes can read/write all program variables in an atomic step) - is polynomial in the state space of the program. However, for the case where one uses the BP model to specify safety specification, we show that the problem of adding masking fault tolerance to high atomicity programs is NP-complete. Therefore, we argue that automated synthesis of fault-tolerant programs is likely to be more successful if one focuses on problems where safety can be represented in the BT model.",2005,0,
1082,1083,Fault detection and isolation based on system feedback,"This paper present a method to detect the transducers fault in the close loop control systems. The necessities imposed for the fault detection algorithm are: rapid answer in the case of fault, which comes out; the diminution of the risk to come out false alarms; lower effort calculation. In the paper are presented the equations of fault detection structure that suggest the software algorithms. In last part of the paper, the algorithm was verified on the steam overhead equations, developed in this paper.",2008,0,
1083,1084,Analysis and management system for power system fault information based on intranet network,"Using Internet network technique, database technique and object-oriented design method, a comprehensive analysis and management system for power system fault information is developed and a feasible network connection project for fault information connection is put forward. The fault analysis functions, e.g., fault diagnosis and location, supervisory and judgment to equipment operation, harmonic analysis and waveform treatment, etc. are developed. Fault information management system is also set up, and easy inquiry and browse based on the Web is developed. The practical application shows that the proposed system makes the information be wide share and promotes operation automation.",2002,0,
1084,1085,Using simulation for assessing the real impact of test-coverage on defect-coverage,"The use of test-coverage measures (e.g., block-coverage) to control the software test process has become an increasingly common practice. This is justified by the assumption that higher test-coverage helps achieve higher defect-coverage and therefore improves software quality. In practice, data often show that defect-coverage and test-coverage grow over time, as additional testing is performed. However, it is unclear whether this phenomenon of concurrent growth can be attributed to a causal dependency, or if it is coincidental, simply due to the cumulative nature of both measures. Answering such a question is important as it determines whether a given test-coverage measure should be monitored for quality control and used to drive testing. Although it is no general answer to this problem, a procedure is proposed to investigate whether any test-coverage criterion has a genuine additional impact on defect-coverage when compared to the impact of just running additional test cases. This procedure applies in typical testing conditions where the software is tested once, according to a given strategy, coverage measures are collected as well as defect data. This procedure is tested on published data, and the results are compared with the original findings. The study outcomes do not support the assumption of a causal dependency between test-coverage and defect-coverage, a result for which several plausible explanations are provided",2000,0,
1085,1086,Performance analysis of three-phase induction motor drives under inverter fault conditions,"This paper presents a comparative analysis involving several fault tolerant operating strategies applied to three-phase induction motor drives. The paper exploits the advantages and the inconveniences of using remedial operating strategies under different control techniques, such as the field oriented control and the direct torque control. Global results are presented concerning the analysis of some key parameters like efficiency, motor line currents harmonic distortion, among others.",2003,0,
1086,1087,Tool Support for Fault Localization Using Architectural Models,"Locating software faults is a problematic activity in many systems. Existing tool approaches usually work close to the system implementation, requiring the developer to perform tedious code analyses in which the amount of information she must manage is usually overwhelming. This problem calls for approaches able to work at higher abstraction levels than code. In this context, we present a tool approach, called FLABot, to assist fault-localization tasks. A novelty of FLABot is that it reasons about faults using software architecture information. Based on Use-case-maps and system logs, FLABot performs a heuristic search for possible faulty functions in the architecture, and then maps these functions to code sections. This allows the developer to quickly navigate large systems and spot code regions that may contain faults, which can be further debugged using conventional techniques. Our preliminary experiments have shown that FLABot is practical and reduces the efforts for discovering faults.",2009,0,
1087,1088,Performance Improvement of the IPMSM Position Sensor-less Vector Control System by the On-line Motor Parameter Error Compensation and the Practical Dead-time Compensation,"This paper proposes the performance improvement methods for the IPMSM position sensor-less vector control system. The stability of the position sensor-less control is influenced by the motor parameter error and dead time compensation error. Especially, it becomes problem in case of the extreme temperature variation and at low speed. The influence on the position estimation is analyzed for the proposed method which using an armature current flux adaptive observer. For performance improvement and simplification of control system, the offline parameter measurement of a stator resistance, permanent magnet flux, d,q-axis inductances, and dead-time compensation are proposed. And the on-line parameter error compensation method is applied to reduce the position error, which uses only the observer signal without any additional sensor or signal injection. In addition, the practical dead-time compensation method is proposed based on the experimental measurement. Experimental results of the test system using the proposed methods showed good performance.",2007,0,
1088,1089,Path vs. subpath vs. link restoration for fault management in IP-over-WDM networks: performance comparisons using GMPLS control signaling,"We investigate three restoration techniques (path, subpath, and link restoration) for fault management in an IP-over-WDM network. We have implemented all of these techniques on the ns-2 simulation platform using generalized multiprotocol label switching (GMPLS) control signaling. These techniques can handle practical situations such as simultaneous multiple fiber failures, which are difficult to design for and recover from by nonrestoration techniques. We then present performance measurement results for the three restoration techniques by applying them to a typical nationwide mesh network running IP over WDM. We investigate interesting trade-offs in the performance of the restoration techniques on restoration success rate, average restoration time, availability, and blocking probability.",2002,0,
1089,1090,Error Models for the Transport Stream Packet Channel in the DVB-H Link Layer,"Digital video broadcasting for hand held terminals (DVB-H) is a broadcast system designed for high-speed data transmission in highly dispersive mobile channel conditions. In this paper, methods of reproducing the statistical properties of measured DVB-H packet error traces are presented. Statistical and finite-state modeling approaches are found to be suitable for simulating the error performance of a DVB-H system operating in typical urban channel conditions. Evaluation of these models focuses on the accuracy of the models in replicating the high-order statistical properties of measured DVB-H transport stream error traces. Also, the effect of these error statistics on the DVB-H link layer frame error rate is considered.",2006,0,
1090,1091,Operating system supports to enhance fault tolerance of real-time systems,"The virtual memory functions in real-time operating systems have been used in real-time systems. The virtual memory functions of real-time operating systems enhance the fault-tolerance of real-time systems because their memory protection mechanism isolates faulty real-time tasks. Recent RISC processors provide virtual memory support through software-managed translation lookaside buffer (TLB) in software. In real-time systems, managing TLB entries is the most important issue because overhead at TLB miss time greatly affects overall performance of the system. In this paper, we propose several virtual memory management algorithms by comparing overheads at task switching times and TLB miss times.",2003,0,
1091,1092,Effect of errors in the system matrix on iterative image reconstruction,"Statistically based iterative image reconstruction is widely used in emission tomography. One important component in iterative image reconstruction is the system matrix, which defines the mapping from the image space to the data space. Several groups have demonstrated that an accurate system matrix can improve image quality in both SPECT and PET. While iterative methods are amenable to arbitrary and complicated system models, the true system response is never known exactly. In practice, one also has to sacrifice the accuracy of the system model because of limited computing and imaging resources. This paper analyzes the effect of errors in the system matrix on iterative image reconstruction. We derived a theoretical expression for calculating artifacts in a reconstructed image that are caused by errors in the system matrix. Using this theoretical expression, we can address the question of how accurate the system matrix needs to be. Computer simulations were conducted to validate theoretical results.",2004,0,
1092,1093,Design and Verification of Internet Service Automatic Fault-Heal System,"Based on the current situation of Internet service management, we put forward an Internet service automatic fault-heal system. Combined with autonomic computing and services probes components, we put forward organization model and autonomic computing model of the system. Simultaneously, with the use of timed automata and UPPAAL - a model checking tool for timed automatons, we modeled the system and simulated it. Then we made a detailed description and analysis of each component of this model. Finally, combined with the verifier of UPPAAL and the TCTL formula, we verified the deadlock, safety and feasibility of the system.",2009,0,
1093,1094,Rolling element bearing fault classification using soft computing techniques,"This paper presents a method, based on classification techniques, for automatically detecting and diagnosing various types of defects which may occur on a rolling element bearing. In the experiments we have used vibration signals coming from a mechanical device including more than ten rolling element bearings monitored by means of four accelerometers: the signals have been collected both with all faultless bearings and substituting one faultless bearing with an artificially damaged one: four different defects have been taken into account. The proposed technique considers all the aspects of classification: feature selection, different base classifiers (two statistical classifiers, namely LDC and QDC, and MLP neural networks) and classifier fusion. Experiments, performed on the vibration signals represented in the frequency domain, have shown that the proposed classification method is highly sensitive to different types of defects and to different severity degrees of the defects.",2009,0,
1094,1095,Study of solid state fault interruption device for medium voltage distribution systems with distributed generators,"Solid state fault interruption devices (FID) can interrupt fault currents much faster than the presently available circuit breakers. Due to their current inability to block system level voltages, presently available semiconductor devices are connected in series to increase blocking capability of the fault interrupting device. To verify the ability of the interruption device for use in a medium voltage distribution system, a FID model is subjected to simulated tests for continuous current carrying capability, rated fault current interruption, and lightning impulse withstand tests. A simulation to demonstrate the ability of the FID to interrupt fault currents in a 7.2kV distribution system and to study the effect on system voltages is shown.",2010,0,
1095,1096,Error analysis of Chinese text segmentation using statistical approach,"The Chinese text segmentation is important for the indexing of Chinese documents, which has significant impact on the performance of Chinese information retrieval. The statistical approach overcomes the limitations of the dictionary based approach. The statistical approach is developed by utilizing the statistical information about the association of adjacent characters in Chinese text collected from the Chinese corpus. Both known words and unknown words can be segmented by the statistical approach. However, errors may occur due to the limitation of the corpus. In this work, we have conducted the error analysis of two Chinese text segmentation techniques using statistical approach, namely, boundary detection and heuristic method. Such error analysis is useful for the future development of the automatic text segmentation of Chinese text or other text in oriental languages. It is also helpful to understand the impact of these errors on the information retrieval system in digital libraries.",2004,0,
1096,1097,Synthesis of Flexible Fault-Tolerant Schedules with Preemption for Mixed Soft and Hard Real-Time Systems,"In this paper we present an approach for scheduling with preemption for fault-tolerant embedded systems composed of soft and hard real-time processes. We are interested to maximize the overall utility for average, most likely to happen, scenarios and to guarantee the deadlines for the hard processes in the worst case scenarios. In many applications, the worst-case execution times of processes can be much longer than their average execution times. Thus, designs for the worst-case can be overly pessimistic, i.e., result in low overall utility. We propose preemption of process executions as a method to generate flexible schedules that maximize the overall utility for the average case while guarantee timing constraints in the worst case. Our scheduling algorithms determine off-line when to preempt and when to resurrect processes. The experimental results show the superiority of our new scheduling approach compared to approaches without preemption.",2008,0,
1097,1098,"Assessing and Estimating Corrective, Enhancive, and Reductive Maintenance Tasks: A Controlled Experiment","This paper describes a controlled experiment of student programmers performing maintenance tasks on a C++ program. The goal of the study is to assess the maintenance size, effort, and effort distribution of three different maintenance types and to describe estimation models to predict the programmer's effort on maintenance tasks. The results of our study suggest that corrective maintenance is much less productive than enhancive and reductive maintenance. Our study also confirms the previous results which conclude that corrective and reductive maintenance requires large proportions of effort on program comprehension activity. Moreover, the best effort model we obtained from fitting the experiment data can estimate the time of 79% of the programmers with the error of 30% or less.",2009,0,
1098,1099,A New Algorithm for the Detection of Inter-Turn Stator Faults in Doubly-Fed Wind Generators,Steady state analysis techniques (i.e. Motor Current Signature Analysis) cannot be applied to variable speed wind generators since their operation is predominately in the transient. A new non-stationary fault detection technique is proposed to detect inter-turn stator faults in doubly-fed wind generators. The technique is a combination of the extended Park's vector approach and a new adaptive algorithm. It will be shown that the new technique can unambiguously detect inter-turn stator faults under transient conditions while providing insight into the degree of severity of the fault,2006,0,
1099,1100,Automatic Recognition of Defect Signatures and Notification of Tool Malfunctions - IECON'06,"We have developed an automatic method that efficiently detects the defect signatures of substrates and identifies possible problems pertaining to LSI/TFT-LCD manufacturing processes and tools. This system, which has no built-in libraries, can be applied to the mass production line of thin film devices. This method is useful to quickly detect problems that have been overlooked thus far",2006,0,
1100,1101,antenna-pattern correction for near-field-to-far field RCS transformation of 1D linear SAR measurements,"In a previous AMTA paper (B. E. Fischer, et al.), we presented a first-principles algorithm, called wavenumber migration (WM), for estimating a target's far-field RCS and/or far-field images from extreme near-field linear (one-dimensional) or planar (two-dimensional) SAR measurements, such as those collected for flight-line diagnostics of aircraft signatures. However, the algorithm assumes the radar antenna has a uniform, isotropic pattern for both transmitting and receiving. In this paper, we describe a modification to the (one-dimensional) linear SAR wavenumber migration algorithm that compensates for nonuniform antenna-pattern effects. We also introduce two variants to the algorithm that eliminate certain computational steps and lead to more efficient implementations. The effectiveness of the pattern compensation is demonstrated for all three versions of the algorithm in both the RCS and the image domains using simulated data from arrays of simple point scatterers.",2004,0,
1101,1102,Error resilience of EZW coder for image transmission in lossy networks,We investigate the effect of network errors on Embedded Zerotree Wavelet (EZW) encoded images and propose modifications to the EZW coder to increase error resilience in bursty packet loss conditions. A hybrid-encoding scheme that uses data interleaving to spread correlated information into independently processed groups and layered encoding to protect significant information within each group is presented. Simulation results for various packet loss percentages show the improved error resiliency of our scheme in random and bursty packet loss environments.,2002,0,
1102,1103,Optimum Machine Performance in Fault-Tolerant Networked Control Systems,"This paper investigates the effect of failures on the productivity of fault-tolerant networked control systems under varying loads. Higher speeds of operation are sometimes used to increase production and compensate for down time due to component failures. Improved Markov models are developed and used to calculate system probabilities. When these probabilities are combined with the maximum speed of operation in each system state, the average speed of operation is obtained. If machines cannot be run at maximum speed all the time, the Markov models are used again to find the best speed mix that would yield maximum output capacity",2005,0,
1103,1104,A novel algorithm of wide area backup protection based on fault component comparison,"This paper proposes a novel wide area back-up protection algorithm that measures synchronized information from different buses in region. The amplitude of voltage fault component from different buses is compared, and the bus with maximum magnitude will be selected. Hence, suspected fault line set could be established according to the sub-graph and complete incidence matrix of selected buses. Then, voltage fault component amplitude at two sides of each suspected line could be calculated from another side, and the amplitude comparison between computed and measured value could be implemented. The ratio would be 1 when external fault occurs, and it would be greater than 1 when internal fault occurs. Thus, the fault line could be identified finally. The technique doesn't need high precision synchronization of wide area information, and could response to different faults. The simulation of 10-unit and 39-bus New England system using PSCAD/EMTDC illustrates the effectiveness of this method.",2010,0,
1104,1105,Fault tolerant mechanism in grid based on Backup Node,"Grid is a very efficient technology to performing heavy processing with distributed resources. These resources at geographically distributed widely and without central control unit. One of the important challenges in grid is fault tolerant, because the grid resources in this environment are not reliable. And to avoid duplication with processing done by a resource, the Check Pointing mechanism has been proposed. The state of application at particular point of time can be store and in case of failure the status information can restore on new node and computation can continue. Check Pointe storage location is a major challenge in the fault tolerant techniques. In this paper we presented a new method by using a backup node. Adding a component to GRAM to improve efficiency and fault tolerant. With clear Backup Node the Check point information can save on it. This method increases the efficiency and system reliability.",2010,0,
1105,1106,Embedded Implementation of a SIP Server Gateway with Forward Error Correction to a Mobile Network,"The emergence of the Voice over Internet Protocol (VoIP) services requires interconnection between different technologies. In this paper we present an embedded Session Initiation Protocol (SIP) Proxy Gateway (GW) to a Mobile Network with Forward Error Corrections (FEC). Our server registers, locates and forwards the calls of an end user providing intelligent routing for user tracking. In addition, FEC along with a Markov-Chain loss model has been integrated into the server to perform a range of tests. Results prove that under high packet loss rate the embedded SIP server GW improves the speech quality.",2010,0,
1106,1107,Information theoretic fault detection,In this paper we propose a novel method of fault detection based on a clustering algorithm developed in the information theoretic framework. A mathematical formulation for a multi-input multi-output (MIMO) system is developed to identify the most informative signals for the fault detection using mutual information (MI) as the measure of correlation among various measurements on the system. This is a model-independent approach for the fault detection. The effectiveness of the proposed method is successfully demonstrated by employing MI-based algorithm to isolate various faults in 16-cylinder diesel engine in the form of distinct clusters.,2005,0,
1107,1108,The Fault Diagnosis of a Class of Nonlinear Stochastic Time-delay systems,"This paper presents a new fault detection algorithm for a class of nonlinear stochastic time-delay systems. Different from the classical fault detection design, a fault detection filter with an output observer and a consensus filter is constructed for fault detecting. Simulations are provided to show the efficiency of the proposed approach.",2006,0,
1108,1109,Three-Stage Error Concealment for Distributed Speech Recognition (DSR) with Histogram-Based Quantization (HQ) Under Noisy Environment,"In this paper, a three-stage error concealment (EC) framework based on the recently proposed histogram-based quantization (HQ) for distributed speech recognition (DSR) is proposed, in which noisy input speech is assumed and both the transmission errors and environmental noise are considered jointly. The first stage detects the erroneous feature parameters at both the frame and subvector levels. The second stage then reconstructs the detected erroneous subvectors by MAP estimation, considering the prior speech source statistics, the channel transition probability, and the reliability of the received subvectors. The third stage then considers the uncertainty of the estimated vectors during Viterbi decoding. At each stage, the error concealment (EC) techniques properly exploit the inherent robust nature of histogram-based quantization (HQ). Extensive experiments with AURORA 2.0 testing environment and GPRS simulation indicated the proposed framework is able to offer significantly improved performance against a wide variety of environmental noise and transmission error conditions.",2007,0,
1109,1110,Error-free arithmetic for discrete wavelet transforms using algebraic integers,"A novel encoding scheme is introduced with applications to error-free computation of discrete wavelet transforms (DWT) based on Daubechies wavelets. The encoding scheme is based on an algebraic integer decomposition of the wavelet coefficients. This work is a continuation of our research into error-free computation of DCTs and IDCTs, and this extension is timely since the DWT is part of the new standard for JPEG2000. This encoding technique eliminates the requirements to approximate the transformation matrix elements by obtaining their exact representations. As a result, we achieve error-free calculations up to the final reconstruction step where we are free to choose an approximate substitution precision based on a hardware/accuracy trade-off.",2003,0,
1110,1111,Identifying efficient combinations of error detection mechanisms based on results of fault injection experiments,"We introduce novel performance ratings for error detection mechanisms. Given a proper setup of the fault injection experiments, these ratings can be directly computed from raw readout data. They allow the evaluation of the overall performance of arbitrary combinations of mechanisms without the need for further experiments. With this means we can determine a minimal subset of mechanisms that still provides the required performance",2002,0,
1111,1112,Immune-Inspired Adaptable Error Detection for Automated Teller Machines,"This paper presents an immune-inspired adaptable error detection (AED) framework for automated teller machines (ATMs). This framework has two levels: one is local to a single ATM, while the other is network-wide. The framework employs vaccination and adaptability analogies of the immune system. For discriminating between normal and erroneous states, an immune-inspired one-class supervised algorithm was employed, which supports continual learning and adaptation. The effectiveness of the proposed approach was confirmed in terms of classification performance and impact on availability. The overall results are encouraging as the downtime of ATMs can de reduced by anticipating the occurrence of failures before they actually occur.",2007,0,
1112,1113,Fault Tolerance and Recovery in Grid Workflow Management Systems,"Complex scientific workflows are now commonly executed on global grids. With the increasing scale complexity, heterogeneity and dynamism of grid environments the challenges of managing and scheduling these workflows are augmented by dependability issues due to the inherent unreliable nature of large-scale grid infrastructure. In addition to the traditional fault tolerance techniques, specific checkpoint-recovery schemes are needed in current grid workflow management systems to address these reliability challenges. Our research aims to design and develop mechanisms for building an autonomic workflow management system that will exhibit the ability to detect, diagnose, notify, react and recover automatically from failures of workflow execution. In this paper we present the development of a Fault Tolerance and Recovery component that extends the ActiveBPEL workflow engine. The detection mechanism relies on inspecting the messages exchanged between the workflow and the orchestrated Web Services in search of faults. The recovery of a process from a faulted state has been achieved by modifying the default behavior of ActiveBPEL and it basically represents a non-intrusive checkpointing mechanism. We present the results of several scenarios that demonstrate the functionality of the Fault Tolerance and Recovery component, outlining an increase in performance of about 50% in comparison to the traditional method of resubmitting the workflow.",2010,0,
1113,1114,Quantization errors in digital motor control systems,"When implementing a motor control drive scheme digitally, the quantization errors always exist in the system. Two major sources of quantization errors are analog-to-digital (A-D) process and numerical calculation in the fixed-point computing device. Typically, the effects of quantization errors due to A-D conversion contribute less than one produced by numerical calculation. This paper studies the quantization errors in a sensorless direct vector control system of induction motor system using a 32-bit fixed-point digital signal processor (DSP) from Texas Instruments (TMS320x28xx series). The investigation of quantization errors produced by numerical computation in such DSP is focused. Both simulation and experiment are carried out within DSP itself in three kinds of data formats; 16-bit fixed-point, 32-bit fixed-point, and floating-point. By comparing the results between floating-point and fixed-point implementation on one machine, numerical issues related to quantization errors can be verified and resolved. As a result, the system performance and behavior can be affected by quantization errors in 16-bit word length while it is not significant in the 32-bit word length.",2004,0,
1114,1115,Fault containment and error detection in the time-triggered architecture,"This paper investigates the fault-containment and error-detection mechanisms of distributed safety-critical time-triggered systems. The following critical failure modes of a fault-containment region are introduced and analyzed in detail: babbling idiot failures, masquerading failures, slightly-off-specification (SOS) failures, crash/omission (CO) failures, and massive transient disturbances. After a short description of the two time-triggered protocols TTP/C and FlexRay this paper tries to show how these two protocols handle the listed failure modes at the architecture level.",2003,0,
1115,1116,How a cyber-physical system can efficiently obtain a snapshot of physical information even in the presence of sensor faults,We present a distributed algorithm for cyber-physical systems to obtain a snapshot of sensor data. The snapshot is an approximate representation of sensor data; it is an interpolation as a function of space coordinates. The new algorithm exploits a prioritized medium access control (MAC) protocol to efficiently transmit information of the sensor data. It scales to a very large number of sensors and it is able to operate in the presence of sensor faults.,2008,0,
1116,1117,Bit-Error-Rate (BER) for modulation technique using Software defined Radio,"Software-defined radio technologies are attractive for future mobile communication systems because of reconfigurable and multimode operation capabilities. The reconfigurable feature is useful for enhancing functions of equipment with out replacing hardware. Multimode operation is essential for future wireless terminals because a number of wireless communication standards will still coexist. The transceiver is modeled in Matlab and consists of a BPSK transmitter, an additive white Gaussian noise (AWGN) channel, and a BPSK receiver. In this paper, we have considered basics modulation techniques used in mobile and wireless systems. Based on this analysis a PSK modulation scheme for SDR is proposed to pick the constellation size that offers the best reconstructed signal quality for each average SNR. Simulation results of signal transmissions confirm the effectiveness of our proposed PSK modulation scheme. The performance of the modulation technique is evaluated when the system is subjected to noise and interference in the channel. Computer simulation tool, MATLAB, is used to evaluate Bit-Error-Rate (BER) for Software defined Radio.",2009,0,
1117,1118,Modeling Depth Estimation Errors for Side Looking Stereo Video Systems,"In the EU funded Integrated Project APROSYS, a side pre-crash sensing system will be set up consisting of a stereo video rig and a radar network. A second goal of APROSYS is to provide tools for the efficient development of future products based on such a sensing system. If a stereo rig points to the side of a moving road vehicle, then maximum angular velocities in azimuth are typically very large. Synchronous operation of the stereo video cameras therefore becomes highly important for correct depth estimation, and hence crucial for pre-crash sensing. This paper proposes a tool for automated verification of the synchronicity of a stereo rig. It consists of a running light clock and automatic image analysis. An error model relates synchronization errors to depth estimation errors. For a given pair of cameras, the tool is applied to give an upper bound to the resulting depth estimation error for the APROSYS application scenario. The tool can be used as a standard for quality control of future product developments",2006,0,
1118,1119,Fault-Tolerant Real-Time Scheduling Algorithm for Tolerating Multiple Transient Faults,"The influence of computer systems in human life is increasing and thereby increases the need for having reliable, robust and real-time services of computer systems. Avoidance of any catastrophic consequences due to faults in such systems is one of the main objectives. This paper presents a fault-tolerant realtime scheduling algorithm, RM-FT, by extending the rate monotonic (RM) scheduling for real-time systems. The main approach used is employing temporal error masking (TEM) technique to achieve node level fault tolerance (NLFT) within the least common multiple of periods of a set of pre-emptively scheduled periodic tasks with at most f transient faults.",2006,0,
1119,1120,Hybrid Fault Diagnosis Scheme Implementation for Power Distribution Systems Automation,"Power distribution automation and control are important tools in the current restructured electricity markets. Unfortunately, due to its stochastic nature, distribution systems faults are hardly avoidable. This paper proposes a novel fault diagnosis scheme for power distribution systems, composed by three different processes: fault detection and classification, fault location, and fault section determination. The fault detection and classification technique is wavelet based. The fault-location technique is impedance based and uses local voltage and current fundamental phasors. The fault section determination method is artificial neural network based and uses the local current and voltage signals to estimate the faulted section. The proposed hybrid scheme was validated through Alternate Transient Program/Electromagnetic Transients Program simulations and was implemented as embedded software. It is currently used as a fault diagnosis tool in a Southern Brazilian power distribution company.",2008,0,
1120,1121,Emulation of software faults by educated mutations at machine-code level,"This paper proposes a new technique to emulate software faults by educated mutations introduced at the machine-code level and presents an experimental study on the accuracy of the injected faults. The proposed method consists of finding key programming structures at the machine code-level where high-level software faults can be emulated. The main advantage of emulating software faults at the machine-code level is that software faults can be injected even when the source code of the target application is not available, which is very important for the evaluation of COTS components or for the validation of software fault tolerance techniques in COTS based systems. The technique was evaluated using several real programs and different types of faults and, additionally, it includes our study on the key aspects that may impact on the technique accuracy. The portability of the technique is also addressed. The results show that classes of faults such as assignment, checking, interface, and simple algorithm faults can be directly emulated using this technique.",2002,0,
1121,1122,Understanding earthquake fault systems using QuakeSim analysis and data assimilation tools,"We are using the QuakeSim environment to model interacting fault systems. One goal of QuakeSim is to prepare for the large volumes of data that spaceborne missions such as DESDynI will produce. QuakeSim has the ability to ingest distributed heterogenous data in the form of InSAR, GPS, seismicity, and fault data into various earthquake modeling applications, automating the analysis when possible. Virtual California simulates interacting faults in California. We can compare output from long time-history Virtual California runs with the current state of strain and the strain history in California. In addition to spaceborne data we will begin assimilating data from UAVSAR airborne flights over the San Francisco Bay Area, the Transverse Ranges, and the Salton Trough. Results of the models are important for understanding future earthquake risk and for providing decision support following earthquakes. Improved models require this sensor web of different data sources, and a modeling environment for understanding the combined data.",2009,0,
1122,1123,Commercial fault tolerance: a tale of two systems,"This paper compares and contrasts the design philosophies and implementations of two computer system families: the IBM S/360 and its evolution to the current zSeries line, and the Tandem (now HP) NonStop Server. Both systems have a long history; the initial IBM S/360 machines were shipped in 1964, and the Tandem NonStop System was first shipped in 1976. They were aimed at similar markets, what would today be called enterprise-class applications. The requirement for the original S/360 line was for very high availability; the requirement for the NonStop platform was for single fault tolerance against unplanned outages. Since their initial shipments, availability expectations for both platforms have continued to rise and the system designers and developers have been challenged to keep up. There were and still are many similarities in the design philosophies of the two lines, including the use of redundant components and extensive error checking. The primary difference is that the S/360-zSeries focus has been on localized retry and restore to keep processors functioning as long as possible, while the NonStop developers have based systems on a loosely coupled multiprocessor design that supports a ""fail-fast"" philosophy implemented through a combination of hardware and software, with workload being actively taken over by another resource when one fails.",2004,0,
1123,1124,Application of immune-based optimization method for fault-section estimation in a distribution system,"In this paper, an immune algorithm-based (IA-based) optimization approach for the fault-section estimation of a distribution system is proposed. To apply the method to solve this estimation problem, each section of a power system model can be considered as an antibody. Through the immunology evolution, an antibody that most fits the antigen of concern becomes the solution. An affinity calculation has been employed in this computation process to measure the combination intensity. As this method can operate the population of antibodies simultaneously, the process stagnation can be better prevented. The proposed approach has been tested on Taiwan Power System (Taipower) through the utility data. Test results demonstrated the feasibility and effectiveness of the method for the applications.",2002,0,
1124,1125,Intrinsic Error Sources of Neural Networks,"The nature of radial basis function (RBF) networks necessitates some types of errors which can never be removed by traditional training algorithms. This paper is an attempt to introduce the natural error sources of neural networks such as bias error, iteration-restricted error, and Gibbs' error. Moreover, a new method is introduced, called post-training, to reduce these errors as far as desired",2006,0,
1125,1126,ISOMAP Algorithm-Based Feature Extraction for Electromechanical Equipment Fault Prediction,"As for the difficult problem of sensitive feature extraction during fault prediction for nonlinear electromechanical equipment, nonlinear dimensionality reduction ISOMAP (isometric feature mapping) algorithm is introduced based on the comprehensive analysis of the operation state data to reduce the dimensionality of high dimensional operation data and acquire sensitive fault features, furthermore, the ISOMAP dimensionality reduction result is verified by compiling algorithm programs based on MATLAB platform. The suitability of applying ISOMAP algorithm to dimensionality reduction of high dimensional data is discussed in terms of algorithm principle in this paper, and calculation steps of the algorithm are provided. According to MATLAB tests, ISOMAP algorithm is able to reduce dimensionality greatly and find out the essential data structure so as to provide a new method for extracting sensitive features of electromechanical equipment fault from another point of view.",2009,0,
1126,1127,Error Estimation Models Integrating Previous Models and Using Artificial Neural Networks for Embedded Software Development Projects,"In an earlier paper, we established 9 models for estimating errors in a new project. In this paper, we integrate the 9 models into 5 by investigating similarities among the models. In addition, we establish a new model using an artificial neural network (ANN). It is becoming increasingly important for software-development corporations to ascertain how to develop software efficiently, whilst guaranteeing delivery time and quality, and keeping development costs low. Estimating the manpower required by new projects and guaranteeing the quality of software are particularly important, because the estimation relates directly to costs while the quality reflects on the reliability of the corporations. In the field of embedded software, development techniques, management techniques, tools, testing techniques, reuse techniques, real-time operating systems and so on, have already been studied. However, there is little research on the relationship between the scale of the development and the number of errors using data accumulated from past projects. Hence, we integrate the previous models and establish a new model using an artificial neural network (ANN). We also compare the accuracy of the ANN model and the regression analysis models. The results of these comparisons indicate that the ANN model is more accurate than any of the 5 integrated models.",2008,0,
1127,1128,Multiple-antenna-aided OFDM employing genetic-algorithm-assisted minimum bit error rate multiuser detection,"The family of minimum bit error rate (MBER) multiuser detectors (MUD) is capable of outperforming the classic minimum mean-squared error (MMSE) MUD in terms of the achievable bit-error rate (BER) owing to directly minimizing the BER cost function. In this paper, we will invoke genetic algorithms (GAs) for finding the optimum weight vectors of the MBER MUD in the context of multiple-antenna-aided multiuser orthogonal frequency division multiplexing (OFDM) . We will also show that the MBER MUD is capable of supporting more users than the number of receiver antennas available, while outperforming the MMSE MUD.",2005,0,
1128,1129,Development of an expert system to fault diagnosis of three phase induction motor drive system,"Power electronic systems are considered as one of the most important components in many applications, such as nuclear reactors, aerospace, military applications and life saving machines. In such applications the system should be high reliable, a knowledge-based expert system was developed to diagnose faults in a three phase induction motor system . A software tool called KAPPA PC 2.1 was used to develop the expert system, the system is modeled in MATLAB SIMULINK and the simulation results at normal and fault conditions was rewritten as a set of if-then rules by which the expert system can discriminate the fault.",2008,0,
1129,1130,System independent and distributed fault management system,"This paper will outline a distributed and dynamic fault management system and practice of it. This work shows that proposed platform-independent, distributed and reusable fault management system architecture can be an integral part of the next generation of network management systems. Another feature of the proposed fault management system is being an extensible fault management computing framework for researchers. By the proposed infrastructure , researcher can carry out their original work on this framework by the help of the event, correlator and alarm programming interfaces. Proposed architecture is applicable not only to network management system but also to intrusion detection systems, business systems. We present these architecture and the use of advanced technologies such as RMI and Java. Finally, we demonstrate how these technological solutions have been implemented in the distributed fault management system called JADFAME - Java distributed fault management engine.",2003,0,
1130,1131,Automated antenna detection and correction methodology in VLSI designs,"As more and more devices are packed on a single chip and as the complexities of VLSI designs are increasing, antenna detection and correction is becoming an increasingly challenging task. The paper presents a methodology, which employs a combination of prevention and correction of antennae at various stages of ASIC (Application specific Integrated Circuits) design flow such as cell library development, block design flow and chip design flow. The methodology advocates adding protection diodes only in a certain number of cells in the library. We have implemented this methodology in our ASIC design flow and are able to solve antenna issues in designs with negligible impact on die size (24% increase in die-size in less than 5% of the designs) and performance (0.3%-0.6% worst case impact to delay). By employing this methodology, we found that the number of antennae in the final layout reduced to very small number and even to zero in some cases, and we were able to save the time involved in correcting antennae.",2003,0,
1131,1132,A validation fault model for timing-induced functional errors,"The violation of timing constraints on signals within a complex system can create timing-induced functional errors which alter the value of output signals. These errors are not detected by traditional functional validation approaches because functional validation does not consider signal timing. Timing-induced functional errors are also not detected by traditional timing analysis approaches because the errors may affect output data values without affecting output signal timing. A timing fault model, the Mis-Timed Event (MTE) fault model, is proposed to model timing-induced functional errors. The MTE fault model formulates timing errors in terms of their effects on the lifespans of the signal values associated with the fault. We use several examples to evaluate the MTE fault model. MTE fault coverage results shows that it efficiently captures an important class of errors which are not targeted by other metrics",2001,0,
1132,1133,JULIET: a distributed fault tolerant load balancer for .NET Web services,"The execution time of computationally-intensive applications such as protein folding and fractal generation can be reduced by implementing these applications as Web services that run in parallel. Additionally, some of these Web services may save state periodically to resume execution later on. However, currently, there is no solution to load balance this class of Web services, and to replicate the saved state for the purposes of resumption. This paper describes the architecture of JULIET, a system that load balances .NET Web services across a Windows cluster in a distributed fashion. The system is also fault tolerant since it supports failovers and replication of data generated by the Web services at the application level. The system is designed to be minimally-visible to the Web service and the client that consumes it.",2004,0,
1133,1134,A binary spelling interface with random errors,"An algorithm for design of a spelling interface based on a modified Huffman's algorithm is presented. This algorithm builds a full binary tree that allows to maximize an average probability to reach a leaf where a required character is located when a choice at each node is made with possible errors. A means to correct errors (a delete-function) and an optimization method to build this delete-function into the binary tree are also discussed. Such a spelling interface could be successfully applied to any menu-orientated alternative communication system when a user (typically, a patient with devastating neuromuscular handicap) is not able to express an intended single binary response, either through motor responses or by using of brain-computer interfaces, with an absolute reliability",2000,0,
1134,1135,Integrating reliability into the design of fault-tolerant power electronics systems,"This paper presents a methodology for integrating reliability considerations into the performance analysis carried out during the design of fault-tolerant power converters. The methodology relies on using a state-space representation of the power converter, based on averaging, similar to the ones used when analyzing linear time-invariant systems, and assumes an unknown-but-bounded uncertainty model for the converter uncontrolled inputs, such as load or variations in input voltage. The converter must be designed such that, for any uncontrolled input, the state variables remain within a region of the state space defined by performance requirements, e.g., output voltage tolerance or switch ratings. In the presence of component faults, and depending on the uncontrolled inputs, the converter may or may not meet performance requirements. Given the uncertain nature of these uncontrolled inputs, and for each particular fault, we introduce an analytical method to compute the probability that the performance requirements are met, which will define the reliability of the converter for each particular fault. By including these probabilities in a Markov reliability model, it is possible to obtain the overall converter reliability. The application of the methodology is illustrated with a case study of a fault-tolerant interleaved buck converter.",2008,0,
1135,1136,Automated duplicate detection for bug tracking systems,"Bug tracking systems are important tools that guide the maintenance activities of software developers. The utility of these systems is hampered by an excessive number of duplicate bug reports-in some projects as many as a quarter of all reports are duplicates. Developers must manually identify duplicate bug reports, but this identification process is time-consuming and exacerbates the already high cost of software maintenance. We propose a system that automatically classifies duplicate bug reports as they arrive to save developer time. This system uses surface features, textual semantics, and graph clustering to predict duplicate status. Using a dataset of 29,000 bug reports from the Mozilla project, we perform experiments that include a simulation of a real-time bug reporting environment. Our system is able to reduce development cost by filtering out 8% of duplicate bug reports while allowing at least one report for each real defect to reach developers.",2008,0,
1136,1137,Designing equally fault-tolerant configurations for kinematically redundant manipulators,"In this article, the authors examine the problem of designing nominal manipulator Jacobians that are optimally fault-tolerant to multiple joint failures. In this work, optimality is defined in terms of the worst case relative manipulability index. Building on previous work, it is shown that for a robot manipulator working in three-dimensional workspace to be equally fault-tolerant to any two simultaneous joint failures, the manipulator must have precisely six degrees of freedom. A corresponding family of Jacobians with this property is identified. It is also shown that the two-dimensional workspace problem has no such solution.",2009,0,
1137,1138,A partition-based approach for identifying failing scan cells in scan-BIST with applications to system-on-chip fault diagnosis,"We present a new partition-based fault diagnosis technique for identifying failing scan cells in a scan-BIST environment. This approach relies on a two-step scan chain partitioning scheme. In the first step, an interval-based partitioning scheme is used to generate a small number of partitions, where each element of a partition consists of a set of scan cells. In the second step, additional partitions are created using an earlier-proposed random-selection partitioning method. Two-step partitioning leads to higher diagnostic resolution than a scheme that relies only on random-selection partitioning, with only a small amount of additional hardware. The proposed scheme is especially suitable for a system-on-chip (SOC) composed of multiple embedded cores, where test access is provided by means of a TestRail that is threaded through the internal scan chains of the embedded cores. We present experimental results for the six largest ISCAS-89 benchmark circuits and for two SOCs crafted from some of the ISCAS-89 circuits.",2003,0,
1138,1139,A simple fault detection of the open-switch damage in BLDC motor drive systems,This paper proposes a novel fault detection algorithm for a brushless DC (BLDC) motor drive system. This proposed method is configured without the additional sensor for fault detection and identification. The fault detection and identification are achieved by a simple algorithm using the operating characteristic of the BLDC motor. The drive system after the fault identification is reconfigured by four-switch topology connecting a faulty leg to the middle point of DC-link using bidirectional switches. This proposed method can also be embedded into existing BLDC motor drive systems as a subroutine without excessive computational effort. The feasibility of a novel fault detection algorithm is validated in simulation.,2007,0,
1139,1140,"Design, Simulation, and Fault Analysis of a 6.5-MV LTD for Flash X-Ray Radiography","The design of a 6.5-MV linear transformer driver (LTD) for flash-radiography experiments is presented. The design is based on a previously tested 1-MV LTD and is predicted to be capable of producing diode voltages of 6.5 MV for a 50-Omega radiographic-diode load. Several fault modes are identified, and circuit simulations are used to determine their effect on the output pulse and other components. For all the identified fault modes, the peak load voltage is reduced by less than 5%",2006,0,
1140,1141,Large-scale fault isolation,"Of the many distributed applications designed for the Internet, the successful ones are those that have paid careful attention to scale and robustness. These applications share several design principles. In this paper, we illustrate the application of these principles to common network monitoring tasks. Specifically, we describe and evaluate 1) a robust distributed topology discovery mechanism and 2) a mechanism for scalable fault isolation in multicast distribution trees. Our mechanisms reveal a different design methodology for network monitoring-one that carefully trades off monitoring fidelity (where necessary) for more graceful degradation in the presence of different kinds of network dynamics.",2000,0,
1141,1142,"A router for improved fault isolation, scalability and diagnosis in CAN","Controller Area Network (CAN) provides an inexpensive and robust network technology in many application domains. However, the use of CAN is constrained by limitations with respect to fault isolation, bandwidth, wire length, namespaces and diagnosis. This paper presents a solution to overcome these limitations by replacing the CAN bus with a star topology. We introduce a CAN router that detects and isolates node failures in the value and time domain. The CAN router ensures that minimum message interarrival times are satisfied and reserves CAN identifiers for individual CAN nodes. In addition, the CAN router exploits knowledge about communication relationships for a more efficient use of communication bandwidth through multicast messaging. An implementation of the CAN router based on a Multi-Processor System-on-a-Chip (MPSoC) shows the feasibility of the proposed solution.",2010,0,
1142,1143,Linux Highly Available (HA) Fault-Tolerant Servers,"High availability is becoming increasingly important as our business depend more and more on computers. Unfortunately, many off-the-shelf solutions for high availability (HA) are expensive and require expertise. This paper explains the design and implementation of an inexpensive high-availability solution for our business-critical needs without requiring the use of expensive additional hardware or software. Along with discussion on high availability, this paper also discusses the data integrity of files and database, of the services which are to be made highly available. Using HTTP as the service example and MySQL as the database to be replicated for data integrity, a two node cluster has been configured to implement the concept.",2007,0,
1143,1144,An Online Model Checking Tool for Safety and Liveness Bugs,"Modern software model checkers are usually used to find safety violations. However, checking liveness properties can offer a more natural and effective way to detect errors, particularly in complex concurrent and distributed e-business systems. Specifying global liveness properties which should always eventually be true proves to be more desirable, but it is hard for existing software model checkers to verify liveness in real codes because doing so requires finding an infinite execution. For solving such a challenge, this paper proposes an online checking tool to verify the safety and liveness properties of complex systems. We adopt the linear temporal logic to describe the semantics of the finite model checking, use binary instrumentation to obtain the distribute states and apply a checking engine to dynamically verify the finite trace linear temporal logic properties. At last, we demonstrate the method in a distributed system using distributed protocol Paxos and achieve good results by experiments.",2008,0,
1144,1145,Failure diagnosis of discrete event systems: the case of intermittent faults,"The diagnosis of ""intermittent"" faults in dynamic systems modeled as discrete event systems is considered. In many systems, faulty behavior often occurs intermittently, with fault events followed by corresponding ""reset"" events for these faults, followed by new occurrences of fault events, and so forth. Since these events are usually unobservable, it is necessary to develop diagnostic methodologies for intermittent faults. This paper addresses this issue by: (1) proposing a modeling methodology for discrete event systems with intermittent faults; (2) introducing new notions of diagnosability associated with fault and reset events; and (3) developing necessary and sufficient conditions, in terms of the system model and the set of observable events, for these notions of diagnosability. The associated necessary and sufficient conditions are based upon the technique of ""diagnosis"" introduced in earlier work, albeit the structure of the diagnosis needs to be enhanced to capture the dynamic nature of faults in the system model. The diagnosability conditions are verifiable in polynomial time in the number of states of the diagnosis.",2002,0,
1145,1146,Starting Synchrophasor measurements in Egypt: A pilot project using fault recorders,"The analysis of the large systems blackouts during the last years have pointed out the need for the function of real-time wide area monitoring, protection and control (RT WAM PAC). Phaser measurements (PM) has proved to be the vital source of data necessary for many applications in power system RT WAM PAC. The paper explains this new technology, the reasons behind its use and points out the role it plays in the overall view of RT WAM PAC. The recent applications of the synchrophasors projects allover the world is highlighted. Generic and specific technical requirements for synchrophasors measurements are explained. The paper also illustrates, using actual recorded case from the fault recording system in the Egyptian Power Network, how the synchronized voltage phasors at different locations within the system, could be presented to the system operator during different operating states, using a software program designed specially for this purpose. A pilot project for PM in Egypt using the existing disturbance recorders is presented showing the benefits which could be gained from such a project.",2008,0,
1146,1147,Maximizing the Fault Tolerance Capability of Fixed Priority Schedules,"Real-time systems typically have to satisfy complex requirements, mapped to the task attributes, eventually guaranteed by the underlying scheduler. These systems consist of a mix of hard and soft tasks with varying criticality, as well as associated fault tolerance requirements. Additionally, the relative criticality of tasks could undergo changes during the system evolution. Time redundancy techniques are often preferred in embedded applications and, hence, it is extremely important to devise appropriate methodologies for scheduling real-time tasks under failure assumptions.In this paper, we propose a methodology to provide a priori guarantees in fixed priority scheduling (FPS) such that the system will be able to tolerate one error per every critical task instance. We do so by using integer linear programming (ILP) to derive task attributes that guarantee re-execution of every critical task instance before its deadline, while keeping the associated costs minimized. We illustrate the effectiveness of our approach, in comparison with fault tolerant (FT) adaptations of the well-known rate monotonic (RM) scheduling, by simulations.",2008,0,
1147,1148,Application of Neuro-fuzzy Network for Fault Diagnosis in an Industrial Process,"The purpose of this paper is to present results that were obtained in fault diagnosis of an industrial process. The diagnosis algorithm combines an artificial neural network (ANN) based supplement of a fuzzy system in a block-oriented configuration. A methodology for designing the system is described. As a motivating example, a chemical plant with a recycle stream is considered. Faults in the supply of raw materials and in controllers are simulated. The performance of the system in handling simultaneous faults is also analysed. The running test results show that the strategy appears to be better suited to diagnose faults of such an industrial process.",2007,0,
1148,1149,Reliability analysis of AUV based on fuzzy fault tree,"Traditional fault tree analysis method need obtain exact value of the event occurrence's probability, un-completeness and fuzziness of the data is ignored. Fuzzy fault tree analysis method is proposed in study on the system reliability. The fuzzy fault tree of the AUV is established. Using operational rule calculated the fuzzy probability of the top event which is AUV can't work normally and analysis the results. The results showed that this method can resolve the problem of fuzzy data and have part events failure criterion in fault tree analysis. It provided valuable reference for the design of the AUV, fault diagnosis and maintenance.",2009,0,
1149,1150,Remote Synchronization of Onboard Crystal Oscillator for QZSS Using L1/L2/L5 Signals for Error Adjustment,"A new error adjustment method for remote synchronization of the onboard crystal oscillator for the quasi-zenith satellite system (QZSS) using three different frequency positioning signals (L1/L2/L5) is proposed. The error adjustment method that uses L1/L2 positioning signals was demonstrated in the past. In both methods, the frequency-dependent part and the frequency-independent part were considered separately, and the total time information delay was estimated. By adopting L1/L2/L5, synchronization was improved by approximately 15% compared with that using L1/L2 and approximately 10% compared with that using L1/L5.",2007,0,
1150,1151,An Efficient Fault Simulator for QDI Asynchronous Circuits,"Testing asynchronous circuits is a difficult task because of two main reasons; first, the absence of a global clock does not allow the use of traditional test generation techniques used for synchronous circuits. Second, correct (i.e., hazard- free) operations of asynchronous circuits are usually obtained by introducing redundancies, that is, sacrificing the testability. So test frameworks such as fault simulator for synchronous circuits are not applicable for asynchronous circuits. In this paper we present an efficient fault simulator for template-based asynchronous circuits which is based on checking sequence of signals in templates. Our proposed fault simulator provides higher fault coverage by taking into consideration the detection of a special class of faults called premature firing faults without introducing any hardware redundancy in the designed circuit. Experimental results on a set of circuits have shown the effectiveness of the fault simulator.",2008,0,
1151,1152,Spatio-temporal boundary matching algorithm for temporal error concealment,"In this paper, a novel temporal error concealment algorithm, called spatio-temporal boundary matching algorithm (STBMA), is proposed to recover the information lost in the video transmission. Different from the classical boundary matching algorithm (BMA), which just considers the spatial smoothness property, the proposed algorithm introduces a new distortion function to exploit both the spatial and temporal smoothness properties to recover the lost motion vector (MV) from candidates. The new distortion function involves two terms: spatial distortion term and temporal distortion term. Since both the spatial and temporal smoothness properties are involved, the proposed method can better minimize the distortion of the recovered block and recover more accurate MV. The proposed algorithm has been tested on H.264 reference software JM 9.0. The experimental results demonstrate the proposed algorithm can obtain better PSNR performance and visual quality, compared with BMA which is adopted in H.264",2006,0,
1152,1153,Formal development of software for tolerating transient faults,"Transient faults constitute a wide-spread class of faults typical in control systems. These are faults that appear for some time during system operation and might disappear and reappear later. However, even by appearing for a short time, they might cause dangerous system errors. Hence designing mechanisms for tolerating transient faults is an acute issue, especially in the development of safety-critical control systems. In this paper we propose a formal approach to specifying software-based mechanisms for tolerating transient faults in the B method. We focus on deriving a general specification and development pattern which can be applied in the development of various control systems. We illustrate an application of the proposed patterns by an example from avionics software product line.",2005,0,
1153,1154,Multi-level fault injection experiments based on VHDL descriptions: a case study,"The probability of transient faults increases with the evolution of technologies. There is a corresponding increased demand for an early analysis of erroneous behaviors. This paper reports on results obtained with SEU-like fault injections in VHDL descriptions of digital circuits. Several circuit description levels are considered, as well as several fault modeling levels. These results show that an analysis performed at a very early stage in the design process can actually give a helpful insight into the response of a circuit when a fault occurs.",2002,0,
1154,1155,An innovative fault injection method in embedded systems via background debug mode,"The embedded systems usage in different applications is prevalent in recent years. These systems include a wide range of equipments from cell phones to medical instruments, which consist of hardware and software. In many examples of embedded systems, fault occurrence can lead to serious dangers in system behavior (for example in satellites). Therefore, we try to increase the fault tolerance feature in these systems. Therefore, we need some mechanisms that increase the robustness and reliability of such systems. These objects cause the on-line test to be a great concern. It is not important that these mechanisms work in which level (Hardware level, Software level or Firmware). The major concern is that how well these systems can provide debugging, test and verification features for the user regardless of their implementation levels. Background Debug Module is a real time tool for these features. In this paper we apply an innovative way to use the BDM tool for fault injection in an embedded system.",2009,0,
1155,1156,Using a Fault Hierarchy to Improve the Efficiency of DNF Logic Mutation Testing,"Mutation testing is a technique for generating high quality test data. However, logic mutation testing is currently inefficient for three reasons. One, the same mutant is generated more than once. Two, mutants are generated that are guaranteed to be killed by a test that kills some other generated mutant. Three, mutants that when killed are guaranteed to kill many other mutants are not generated as valuable mutation operators are missing. This paper improves logic mutation testing by 1) extending a logic fault hierarchy to include existing logic mutation operators, 2) introducing new logic mutation operators based on existing faults in the hierarchy, 3) introducing new logic mutation operators having no corresponding faults in the hierarchy and extending the hierarchy to include them, and 4) addressing the precise effects of equivalent mutants on the fault hierarchy. An empirical study using minimal DNF predicates in avionics software showed that a new logic mutation testing approach generates fewer mutants, detects more faults, and outperforms an existing logic criterion.",2009,0,
1156,1157,FAME: a fault-pattern based memory failure analysis framework,"A memory failure analysis framework is developed-the Failure Analyzer for MEmories (FAME). The FAME integrates the Memory Error Catch and Analysis (MECA) system and the Memory Defect Diagnostics (MDD) system. The fault-type based diagnostics approach used by MECA can improve the efficiency of the test and diagnostic algorithms. The fault-pattern based diagnostics approach used by MDD further improves the defect identification capability. The FAME also comes with a powerful viewer for inspecting the failure patterns and fault patterns. It provides an easy way to narrow down the potential cause of failures and identify possible defects more accurately during the memory product development and yield ramp-up stage. An experiment has been done on an industrial case, demonstrating very accurate results in a much shorter time as compared with the conventional way.",2003,0,
1157,1158,Genome-Wide Search for Splicing Defects Associated with Amyotrophic Lateral Sclerosis (ALS),"Amyotrophic lateral sclerosis (ALS) is a progressive neurodegenerative disease caused by the degeneration of motor neurons. Although the cause of ALS is unknown, mutations in the gene that produces the SOD1 enzyme are associated with some cases of familial ALS. SOD1 is a powerful antioxidant that protects the body from damage caused by superoxide, a toxic free radical. It has been proposed that defects in splicing of some mRNAs, induced by oxidative stress, can play a role in ALS pathogenesis. Alterations of splicing patterns have also been observed in ALS patients and in ALS murine models, suggesting that alterations in the splicing events can contribute to ALS progression. Using Exon 1.0 ST GeneChips, which allow the definition of alternative splicing events (ASEs) , the SH-SY5Y neuroblastoma cell line has been profiled after treatment with paraquat, which by inducing oxidative stress alters the patterns of alternative splicing. Furthermore, the same cell line stably transfected with wt and ALS mutant SOD has also been profiled. The integration of the two ALS models efficiently moderates ASE false discovery rate, one of the most critical issues in high-throughput ASEs detection. This approach allowed the identification of a total of 14 splicing events affecting respectively both internal coding exons and 5' UTR of known gene isoforms.",2009,0,
1158,1159,QoS-aware connection resilience for network-aware grid computing fault tolerance,"Current grid computing fault tolerance leverages IP dynamic rerouting and schemes implemented in the application or in the middleware to overcome both software and hardware failures. Despite the flexibility of current grid computing fault tolerant schemes in recovering inter-service connectivity from an almost comprehensive set of failures, they might not be able to repristinate also connection QoS guarantees, such as minimum bandwidth and maximum delay. This phenomenon is exacerbated when, as in global grid computing, the grid computing sites are not connected by dedicated network resources but share the same network infrastructure with other Internet services. This paper aims at showing the advantages of integrating grid computing fault tolerance schemes with next generation networks (NGNs) resilient schemes. Indeed, by combining the utilization of generalized multi-protocol label switching (GMPLS) resilient schemes, such as path restoration, and application or middleware layer fault tolerant schemes, such as service migration or replication, it is possible to guarantee the necessary QoS to the connections between grid computing sites while limiting the required network and computational resources.",2005,0,
1159,1160,A Fast Fault Location Algorithm Based on Pre-computed for Optical Burst Switching Network,"Aiming at the disadvantages of existing location algorithms, this paper proposed and experienced an effective method of fault location algorithm based on pre-computed for optical burst switching network. In order to minimize the monitoring cost, we introduced monitoring-cycle by which the network is divided into a number of monitoring domain. Each monitoring domain has a monitor, when faults occurred, fault codes are generated. According to the fault codes, we can search the binary tree algorithm to achieve the pre-computed of faults in the OBS network. Examples prove that the algorithm can not only realize single fault location but also multi-fault location.",2009,0,
1160,1161,MV generator low-resistance grounding and stator ground fault damage,"Most of the in-plant medium-voltage generators are grounded through resistors ranging from 200 to 400 A. In some unusual situations, the resistors may even be as high as 1200 A. Low-resistance grounding is a preferred choice for a medium-voltage power distribution system. However, extensive generator stator ground fault damages had been reported due to the prolonging generator neutral ground current, which would continue to flow even after the main and field breaker opened. This ground fault current could continue to flow for as long as 5 s depending on the generator open-circuit time constant T'<sub>do</sub>. As a result, the higher the generator neutral current, would lead to the higher stator ground fault damages. An IEEE Working Group has recently completed its study and made recommendations for medium-voltage generator grounding in a multiple source industrial environment. Based on the considerations of transient overvoltage, ground fault damages, and ground fault protection, the working group suggests a few variations of grounding systems, which basically are low-resistance grounding systems during normal operating conditions, and the generator would be switched to a high-resistance grounded system from a normal low-resistance grounded system when a ground fault occurs in the generator stator. The purposes of this paper are: 1) to examine the generator transient over-voltage and currents under the low-resistance ground fault conditions, and also to evaluate their corresponding stator ground fault damages, and 2) to establish an acceptable maximum system ground fault level. For comparison purposes, three versions of low-resistance grounding systems have been considered and they are: a low-resistance grounding system with a neutral breaker at the generator (hereinafter called a ""neutral breaker system""); a low-resistance grounding system with the generator neutral low resistor being switched to a high-resistor after a stator ground fault (hereinafter called a ""hybrid system""); and a low-resistance grounding system similar to the current practice (hereinafter called a ""traditional system""). The simulation study is conducted with the aid of the Electro Magnetic Transient Program. An experimental analog generator model is also used to verify the - simulation results.",2004,0,
1161,1162,Fault-tolerant adaptive and minimal routing in mesh-connected multicomputers using extended safety levels,"The minimal routing problem in mesh-connected multicomputers with faulty blocks is studied. Two-dimensional meshes are used to illustrate the approach. A sufficient condition for minimal routing in 2D meshes with faulty blocks is proposed. Unlike many traditional models that assume all the nodes know global fault distribution, our approach is based on the concept of an extended safety level, which is a special form of limited fault information. The extended safety level information is captured by a vector associated with each node. When the safety level of a node reaches a certain level (or meets certain conditions), a minimal path exists from this node to any nonfaulty nodes in 2D meshes. Specifically, we study the existence of minimal paths at a given source node, limited distribution of fault information, and minimal routing itself. We propose three fault-tolerant minimal routing algorithms which are adaptive to allow all messages to use any minimal path. We also provide some general ideas to extend our approaches to other low-dimensional mesh-connected multicomputers such as 2D tori and 3D meshes. Our approach is the first attempt to address adaptive and minimal routing in 2D meshes with faulty blocks using limited fault information",2000,0,
1162,1163,Study and Realizing of Method of AC Locating Fault in Distribution System,"The idea of AC locating fault method in distribution system is that inject an AC signal into the fault phase after a single line to ground fault happened and then diagnose the fault along the transmission line with the handled AC signal detector utilizing dichotomy method until the fault is determined. The frequency of the injected AC signal used in this study is 60 Hz. Compared with the injected S signal technique, this method is called low frequency AC signal injection method. In this paper, the hardware of the signal source and the software design are introduced. The SCM managing pulse is used in the control section of the hardware, and the application of PWM control technique in this hardware is discussed in this reference; as the software design, the PWM signal is generated by coding based on the relation between the injected signal and PWM waveforms. The high frequency PWM signal excited a couple of breakers in the inversion source and then the output terminal will get high stable injected signals by filtering and generate AC signal with invariable frequency and adjustable voltage, based on which the signal detector could detect the required signal easily. The proposed signal source device reduces the difficulty of high impedance to ground detecting and improves the accuracy and reliability of locating fault. This technique, which allows the ground detecting and is convenient for engineers' operation, reduces the locating fault time, improves its efficiency and is proved by simulation and analysis on its validity.",2010,0,
1163,1164,Notice of Retraction<BR>A novel weighted voting algorithm based on neural networks for fault-tolerant systems,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Voting algorithms are used in a wide area of control systems from real-time and safety-critical control systems to pattern recognition, image processing and human organization systems in order to arbitrating among redundant results of processing in redundant hardware modules or software versions. From a point of view, voting algorithms can be categorized to agreement-based voters like plurality and majority or some voters which produce output regardless to agreement existence among the results of redundant variants. In some applications it is necessary to use second type voters including median and weighted average. Although both of median and weighted average voters are the choicest voters for highly available application, weighted average voting is often more trustable than median. Meanwhile median voter simply selects the mid-value of results; weighted average voter assigns weight to each input, based on their pre-determined priority or their differences, so that the share of more trustable inputs will increase rather than the inputs with low probable correctness. This paper introduces a novel weighted average voting algorithm based on neural networks that is capable of improving the rate of system reliability. Our experimental results showed that the neural weighted average voter has increases the reliability 116.63% in general and 309.82%, 130.27% and 9.37% respectively for large, medium and small errors in comparison with weighted average, and 73.87% in general and 160.44%, 83.59% and 7.52% respectively for- large, medium and small errors in comparison with median voter.",2010,0,
1164,1165,Fault Diagnosis of Variable Frequency Speed Regulation System Based on Information Fusion,"The multi-sensor information fusion theory has been widely used in the fault diagnosis domain. In order to improve the reliability of fault diagnosis of the variable frequency speed regulation system (VFSRS), this paper presents a new fault diagnosis method of the VFSRS based on the multi-sensor information fusion. A calculating method of the basic probability assignment (BPA) for the VFSRS was given. Taking the invisible electric fault of the VFSRS as an example, this paper presents the implementing process of this fault diagnosis method in detail. The diagnosis result indicates that the multi-sensor information fusion has the stability and the ability of fault tolerance, and can improve the accuracy and reliability of fault diagnosis of the VFSRS effectively.",2007,0,
1165,1166,Error probability analysis of TAS/MRC-based scheme for wireless networks [point-to-point link example],"We develop the framework to analyze the symbol-error probability (SEP) for the scheme integrating transmit antenna selection (TAS) with maximal-ratio combining (MRC) used in wireless networks. Applying this scheme, the transmitter always selects an optimal antenna out of all possible antennas based on channel state information (CSI) feedback. Over a flat-fading Rayleigh channel, we develop the closed-form SEP expressions for a set of commonly used constellations when assuming perfect and delayed CSI feedback, respectively. We also derive the Chernoff-bounds of the SEP's for both perfect and delayed feedback. Our analyses show that while the antenna diversity improves the system performance, the feedback delay can significantly impact the SEP of TAS/MRC schemes.",2005,0,
1166,1167,Design and control of an intelligent dual-arm manipulator for fault-recovery in a production scenario,"This paper describes the design and control methodology used for the development of a dual-arm manipulator as well as its deployment in a production scenario. Multi-modal and sensor-based manipulation strategies are used to guide the robot on its task to supervise and, when necessary, solve faulty situations in a production line. For that task the robot is equipped with two arms, aimed at providing the robot with total independence from the production line. In other words, no extra mechanical stoppers are mounted on the line to halt targeted objects, but the robot will employ both arms to (a) stop with one arm a carrier that holds an object to be inserted/replaced, and (b) use the second arm to handle such object. Besides, visual information from head and wrist-mounted cameras provide the robot with information such as the state of the production line, the unequivocal detection/recognition of the targeted objects, and the location of the target in order to guide the grasp.",2009,0,
1167,1168,Error analysis in Croatian morphosyntactic tagging,"In this paper, we provide detailed insight on properties of errors generated by a stochastic morphosyntactic tagger assigning multext-East morphosyntactic descriptions to Croatian texts. Tagging the Croatia Weekly newspaper corpus by the CroTag tagger in stochastic mode revealed that approximately 85 percent of all tagging errors occur on nouns, adjectives, pronouns and verbs. Moreover, approximately 50 percent of these are shown to be incorrect assignments of case values. We provide various other distributional properties of errors in assigning morphosyntactic descriptions for these and other parts of speech. On the basis of these properties, we propose rule-based and stochastic strategies which could be integrated in the tagging module, creating a hybrid procedure in order to raise overall tagging accuracy for Croatian.",2009,0,
1168,1169,A Survey of Mobile Agent-Based Fault-Tolerant Technology,This paper surveys the state of the art of agentbased fault tolerance techniques. Existing mobile agent-based fault-tolerant techniques are identified on prevent mobile agents from being blocked by a failure.,2005,0,
1169,1170,On the outphasing power amplifier nonlinearity analysis and correction using digital predistortion technique,"This paper proposes a comprehensive theoretical and experimental analysis of the source of nonlinearity exhibited by outphasing based power amplifiers The important load-pulling effect engendered by the outphasing decomposition and the Chireix combiner is first investigated. Its effect on the two power amplifiers behavior is then identified as the dominant source of nonlinearity in the LINC system. In addition, this paper suggests the application of baseband predistortion technique to mitigate this nonlinear behavior.",2008,0,
1170,1171,Temperature correction to chemoresistive sensors in an e-NOSE-ANN system,"The influence of the temperature coefficient of resistance in the chemoresistive response of inherently conductive polymer (ICP) sensors in the performance of an artificial neural network (ANN) e-natural olfactory sensor emulator (e-NOSE) system is evaluated. Temperature was found to strongly influence the response of the chemoresistors, even over modest ranges (ca. 2/spl deg/C). An e-NOSE array of eight ICP sensor elements, a relative humidity (RH/spl plusmn/0.1%) sensor, and a resistance temperature device (RTD/spl plusmn/0.1/spl deg/C) was tested at five different RH levels while the temperature was allowed to vary with the ambient. A temperature correction algorithm based on the temperature coefficient of resistance /spl beta/ for each material was independently and empirically determined then applied to the raw sensor data prior to input to the ANN. Conversely, uncorrected data was also passed to the ANN. The performance of the ANN was evaluated by determining the error found between the actual humidity versus the calculated humidity. The error obtained using raw input sensor data was found to be 10.5% and using temperature corrected data, 9.3%. This negligible difference demonstrates that the ANN was capable of adequately addressing the temperature dependence of the chemoresistive sensors once temperature was inclusively passed to the ANN.",2003,0,
1171,1172,Application of a matched filter approach for finite aperture transducers for the synthetic aperture imaging of defects,"The suitability of the synthetic aperture imaging of defects using a matched filter approach on finite aperture transducers was investigated. The first part of the study involved the use a finite-difference time-domain (FDTD) algorithm to simulate the phased array ultrasonic wave propagation in an aluminum block and its interaction with side-drilled hole-like defects. B-scans were generated using the FDTD method for three active aperture transducer configurations of the phased array (a) single element and (b) 16-element linear scan mode, and (c) 16-element steering mode. A matched filter algorithm (MFA) was developed using the delay laws and the spatial impulse response of a finite size rectangular phased array transducer. The conventional synthetic aperture focusing technique (SAFT) algorithm and the MFA were independently applied on the FDTD signals simulated with the probe operating at a center frequency of 5 MHz and the processed B-scans were compared. The second part of the study investigated the capability of the MFA approach to improve the SNR. Gaussian white noise was added to the FDTD generated defect signals. The noisy B-scans were then processed using the SAFT and the MFA and the improvements in the SNR were estimated. The third part of the study investigated the application of the MFA to image and size surface-crack-like defects in pipe specimens obtained using a 45 steered beam from a phased array probe. These studies confirm that MFA is an alternative to SAFT with little additional computational burden. It can also be applied blindly, like SAFT, to effect synthetic focusing with distinct advantages in treating finite transducer effects, and in handling steered beam inspections. Finally, limitations of the MFA in dealing with larger-sized transducers are discussed.",2010,0,
1172,1173,Lazy verification in fault-tolerant distributed storage systems,"Verification of write operations is a crucial component of Byzantine fault-tolerant consistency protocols for storage. Lazy verification shifts this work out of the critical path of client operations. This shift enables the system to amortize verification effort over multiple operations, to perform verification during otherwise idle time, and to have only a subset of storage-nodes perform verification. This paper introduces lazy verification and describes implementation techniques for exploiting its potential. Measurements of lazy verification in a Byzantine fault-tolerant distributed storage system show that the cost of verification can be hidden from both the client read and write operation in workloads with idle periods. Furthermore, in workloads without idle periods, lazy verification amortizes the cost of verification over many versions and so provides a factor of four higher write bandwidth when compared to performing verification during each write operation.",2005,0,
1173,1174,An autonomous FPGA-based emulation system for fast fault tolerant evaluation,"Platform FPGAs provide a high degree of reconfigurability and a high density of integration. These features make these devices very suitable for hardware emulation and in particular for fault tolerance evaluation. There are several FPGA-based approaches that enhance notably the fault tolerance evaluation process achieving an important speed up. However, such methods are limited by the communication between the FPGA and the host computer, which manages the emulation process. In order to minimize this communication and therefore accelerate the overall process, an autonomous emulation system is proposed in this paper. This solution profits from additional hardware resources available in current platform FPGAs such as embedded RAM. In the proposed system, a complete emulation campaign and its management is embedded in the FPGA, accelerating emulation process up to two orders of magnitude without losing flexibility with respect to other hardware solutions.",2005,0,
1174,1175,A fault tolerance mechanism for network intrusion detection system based on intelligent agents (NIDIA),"The intrusion detection system (IDS) has as objective to identify individuals that try to use a system in way not authorized or those that have authorization to use but they abuse of their privileges. The IDS to accomplish its function must, in some way, to guarantee reliability and availability to its own application, so that it gets to give continuity to the services even in case of faults, mainly faults caused by malicious agents. This paper proposes an adaptive fault tolerance mechanism for network intrusion detection system based on intelligent agents. We propose the creation of a society of agents that monitors a system to collect information related to agents and hosts. Using the information which is collected, it is possible: to detect which agents are still active; which agents should be replicated and which strategy should be used. The process of replication depends on each type of agent, and its importance to the system at different moments of processing. We use some agents as sentinels for monitoring and thus allowing us to accomplish some important tasks such load balancing, migration, and detection of malicious agents, to guarantee the security of the proper IDS",2006,0,
1175,1176,A novel method for transmission network fault location using genetic algorithms and sparse field recordings,"The paper presents an approach to locate a fault in a transmission network based on waveform matching. Matching during-fault recorded phasor with the during fault simulated phasor is used to determine the fault location. The search process to find the best waveform match is actually an optimization problem. The genetic algorithm (GA) is introduced to find the optimal solution. The proposed approach is suitable for the situations where only the data recorded sparsely is available. Under such circumstances, it can offer more accurate results than other known techniques.",2002,0,
1176,1177,Lowering Error Floor of LDPC Codes Using a Joint Row-Column Decoding Algorithm,"Low-density parity-check codes using the belief-propagation decoding algorithm tend to exhibit a high error floor in the bit error rate curves, when some problematic graphical structures, such as the so-called trapping sets, exist in the corresponding Tanner graph. This paper presents a joint row-column decoding algorithm to lower the error floor, in which the column processing is combined with the processing of each row. By gradually updating the pseudo-posterior probabilities of all bit nodes, the proposed algorithm minimizes the propagation of erroneous information from trapping sets into the whole graph. The simulation indicates that the proposed joint decoding algorithm improves the performance in the waterfall region and lowers the error floor. Implementation results into field programmable gate array (FPGA) devices indicate that the proposed joint decoder increases the decoding speed by a factor of eight, compared to the traditional decoder.",2007,0,
1177,1178,Error-robust Scalable Extension of H.264/AVC Ubiquitous Streaming Using the Adaptive Packet Interleaving Mechanism,"In this paper, we realize error-robust scalable extension of H.264/AVC ubiquitous video streaming using an adaptive packet interleaving mechanism among heterogeneous networks and devices. The state-of-the-art SVC is used to provide combined scalability to adjust spatiotemporal resolutions and SNR quality. In an error-prone channel, a method of bandwidth estimation and a three-state network transition chain are proposed to adjust the transmission bit-rate and the interleaving window size, respectively. Unlike past packet interleaving methods, the interleaving window size can be adjusted dynamically to compromise pre-processing delay, e.g., packet re-arrangement time for interleaving, and quality improvement. Additionally, with the understanding of the global SVC bitstream structure, a SVC coder is designed to increase the decoding efficiency and real-timely extract the proper SVC-quality bitstream based on the network condition. In our experiments, performances of the adaptive packet interleaving mechanism w.r.t. distinct interleaving window sizes and network conditions are demonstrated for distinct kinds of videos.",2009,0,
1178,1179,Test mode method and strategy for RF-based fault injection analysis for on-chip relaxation oscillators under EMC standard tests or RFI susceptibility characterization,"Nowadays some microcontroller clock circuits have been implemented using relaxation oscillators instead of quartz type approach to attend cost effective designs. The oscillator is compensated over temperature and power supply and trimming during device test phase adjusts the oscillation frequency on target to overcome process variations. In that way, the relaxation oscillator becomes competitive with regard to ceramic resonator options. However, robust applications as industrial, automotive and aero spatial, requires aggressive EMC tests reproducing the behavior in these environments. High levels of RF interference introduce frequency deviation, jitter or clock corruption causing severe faults on the application. This work discusses the impact of RF interference in relaxation oscillators proposing a strategy to implement test mode in microcontrollers and other complex SOCs, allowing yet characterization and fault debug. Theoretical analysis and experimental results with a silicon implementation are presented and discussed.",2010,0,
1179,1180,Intelligent agent-based system using dissolved gas analysis to detect incipient faults in power transformers,Condition monitoring and software-based diagnosis tools are central to the implementation of efficient maintenance management strategies for many engineering applications including power transformers.,2010,0,
1180,1181,Algorithm of fault diagnosis for satellite network,"Automated fault diagnosis becomes increasingly important to satellite work. In an early paper, we introduced system level diagnosis theory into satellite iietwork firstly and presented two-level-node graph, a novel modeling method,. Based on these work, a new test invalidation model under certain fault pattern is presented and a diagnosis algorithni is proposed in this paper. Diagnosis can be divided into two steps, local diagnosis and centralized diagnosis. The former is distributed, which can reduce the diagnosis delay by collecting test results in satellites parallelly. The latter is centralized, which can make use of the regularity of satellite network. The procedure of local diagnosis is described by activity cycle diagram. During diagnosing, little professional knowledge about satellite needs to be involved. An example illustrates the diagnosis algorithm and its effect.",2004,0,
1181,1182,Grid-connected PV system with power-factor correction capability,"In this paper a grid-connected PV system is presented. Its main feature is that, besides injecting energy into the grid, it behaves as an active filter. The reference signal used to modulate the output current is obtained through an adaptive filter. The main advantages of the technique are that the system becomes almost independent of the parameter variations, and that it lends itself to a very simple hardware implementation. A 1 kW prototype was built and tested. The results obtained show good compensation of the current drawn by nonlinear loads.",2002,0,
1182,1183,Analysis on Fault of Yun-Guang hybrid AC/DC Power Transmission System,"This paper addresses the transient response of instant ground fault in Yun-Guang hybrid UHVDC/AC power transmission system. A detailed bipolar UHVDC electromagnetic transient model is established by PSCAD/EMTDC software. The control strategies for UHVDC system are investigated and the main factors of commutation failure are analyzed. A lot of simulation results demonstrate that Yunnan rectifier AC system fault can not cause UHVDC commutation failure. Metallic ground fault of Guangdong inverter AC system will result in UHVDC commutation failure but enough transition resistance can reduce the chance of it. When one pole ground fault of UHVDC line occurs, DC current of the fault pole has an obvious overshoot and non-fault pole is influenced a little. UHVDC can come back to normal operation once the instant ground fault is cleared.",2008,0,
1183,1184,2D Frequency Selective Extrapolation for Spatial Error Concealment in H.264/AVC Video Coding,"The frequency selective extrapolation extends an image signal beyond a limited number of known samples. This problem arises in image and video communication in error prone environments where transmission errors may lead to data losses. In order to estimate the lost image areas, the missing pixels are extrapolated from the available correctly received surrounding area which is approximated by a weighted linear combination of basis functions. In this contribution, we integrate the frequency selective extrapolation into the H.264/AVC coder as spatial concealment method. The decoder reference software uses spatial concealment only for I frames. Therefore, we investigate the performance of our concealment scheme for I frames and its impact on following P frames caused by error propagation due to predictive coding. Further, we compare the performance for coded video sequences in TV quality against the non-normative concealment feature of the decoder reference software. The investigations are done for slice patterns causing chequerboard and raster scan losses enabled by flexible macroblock ordering (FMO).",2006,0,
1184,1185,TMS320 DSP based neural networks on fault diagnostic system of turbo-generator,"Artificial neural networks (ANN) are massive parallel interconnections of simple neurons that function as a collective system. More and more people are paying attention to ANN on fault diagnostic of turbo-generator for its association of thought, recollection and study function. However, the disadvantage of ANN lies in its huge data computation and the low speed of convergence. If we realize ANN with common CPU, it needs so much time on computing the huge data, so that real-time fault diagnosis become impossible. In fact, most of the computation in ANN is multiplication and addition. While digital signal processors (DSP) has altitude advantage on multiplication and addition computation, it can perform parallel multiplication and addition in a single cycle clock. Consequently we design a master/slave system to solve the problem. The slave system was mainly made up of DSP, which perform high speed ANN calculation. The master system was made up of PC, which performs data communication and real-time fault diagnosis. In this paper we bring forward a practical system design and present particular design method of hardware and software by using back-propagation (BP) network often used in fault diagnosis.",2003,0,
1185,1186,A Diagnostic Tree Approach for Fault Cause Identification in the Attitude Control Subsystem of Satellites,"Space and Earth observation programs demand stringent guarantees ensuring smooth and reliable operations of space vehicles and satellites. Due to unforeseen circumstances and naturally occurring faults, it is desired that a fault-diagnosis system be capable of detecting, isolating, identifying, or classifying faults in the system. Unfortunately, none of the existing fault-diagnosis methodologies alone can meet all the requirements of an ideal fault- diagnosis system due to the variety of fault types, their severity, and handling mechanisms. However, it is possible to overcome these shortcomings through the integration of different existing fault-diagnosis methodologies. In this paper, a novel learning-based, diagnostic-tree approach is proposed which complements and strengthens existing efficient fault detection mechanisms with an additional ability to classify different types of faults to effectively determine potential fault causes in a subsystem of a satellite. This extra capability serves as a semiautomatic diagnostic decision support aid to expert human operators at ground stations and enables them to determine fault causes and to take quick and efficient recovery/reconfiguration actions. The developed diagnosis/analysis procedure exploits a qualitative technique denoted as diagnostic tree (DX-tree) analysis as a diagnostic tool for fault cause analysis in the attitude control subsystem (ACS) of a satellite. DX-trees constructed by our proposed machine-learning-based automatic tree synthesis algorithm are demonstrated to be able to determine both known and unforeseen combinations of events leading to different fault scenarios generated through synthetic attitude control subsystem data of a satellite. Though the immediate application of our proposed approach would be at ground stations, the proposed technique has potential for being integrated with causal model-based diagnosis and recovery techniques for future autonomous space vehicle missions.",2009,0,
1186,1187,Assessing the Dependability of SOAP RPC-Based Web Services by Fault Injection,"This paper presents our research on devising a dependability assessment method for SOAP-based Web Services using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing SOAP RPC-based applications and derive a new method and fault model for testing web services. From this we have implemented an extendable fault injector framework and undertaken some proof of concept experiments with a system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy within our system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard web service to the stateful environment of an OGSA service.",2003,0,
1187,1188,Fault-Tolerant Encryption for Space Applications,"This paper is concerned with the use of commercial security algorithms like the Advanced Encryption Standard (AES) in Earth observation small satellites. The demand to protect the sensitive and valuable data transmitted from satellites to ground has increased and hence the need to use encryption on board. AES, which is a very popular choice in terrestrial communications, is slowly emerging as the preferred option in the aerospace industry including satellites. This paper first addresses the encryption of satellite imaging data using five AES modes - ECB, CBC, CFB, OFB and CTR. A detailed analysis of the effect of single even upsets (SEUs) on imaging data during on-board encryption using different modes of AES is carried out. The impact of faults in the data occurring during transmission to ground due to noisy channels is also discussed and compared for all the five modes of AES. In order to avoid data corruption due to SEUs, a novel fault-tolerant model of AES is presented, which is based on the Hamming error correction code. A field programmable gate array (FPGA) implementation of the proposed model is carried out and measurements of the power and throughput overhead are presented.",2009,0,
1188,1189,A Fault-Tolerant Routing Algorithm of P2P Network Based on Hierarchical Structure,"Fault-tolerant routing in P2P network has been a hot point. To raise the performance of fault-tolerant routing can highly enhance the stability and efficiency of P2P network. Through research, we find most routing errors is caused by highly dynamic characteristic of P2P network, such as peers frequently join, leave and fail, which is the main factor that induce routing errors. We proposed an improved algorithm to raise the performance of fault-tolerant routing of P2P network based on hierarchical structure theory, which is named as FTARH (Fault-tolerant Algorithm of Routing with Hiberarchy). The new algorithm makes full use of the superfluous capability of super peers to enhance the performance of fault-tolerant routing of entire P2P network.",2010,0,
1189,1190,Towards Interactive Fault Localization Using Test Information,"Finding the location of a fault is a central task of debugging. Typically, a developer employs an interactive process for fault localization. To accelerate this task, several approaches have been proposed to automate fault localization. In practice, testing-based fault localization (TBFL), which uses test information to locate faults, has become a research focus. However, experimental results reported in the literature showed that current automation of fault localization can only serve as a means to confirming the search space and prioritizing search sequences, not a substitute of the interactive fault localization process. In this paper, we propose an approach based on test information to support the entire interactive fault localization process. During this process, the information gathered from previous interaction steps can be used to provide the ranking of suspicious statements for the current interaction step. As a feasibility study of our approach, we performed an experiment on applying our approach together with some other TBFL approaches on the Siemens programs, which have been used in the literature. Our experimental results show the effectiveness of our approach.",2006,0,
1190,1191,Correction of bias field in MR images using singularity function analysis,"A new approach for correcting bias field in magnetic resonance (MR) images is proposed using the mathematical model of singularity function analysis (SFA), which represents a discrete signal or its spectrum as a weighted sum of singularity functions. Through this model, an MR image's low spatial frequency components corrupted by a smoothly varying bias field are first removed, and then reconstructed from its higher spatial frequency components not polluted by bias field. The thus reconstructed image is then used to estimate bias field for final image correction. The approach does not rely on the assumption that anatomical information in MR images occurs at higher spatial frequencies than bias field. The performance of this approach is evaluated using both simulated and real clinical MR images.",2005,0,
1191,1192,Categorizing and Analysis of Activated Faults in the FlexRay Communication Controller Registers,"FlexRay communication protocol is expected becoming the de-facto standard for distributed safety-critical systems. In this paper, transient single bit-flip faults were injected into the FlexRay communication controller to categorize and analyze the activated faults. In this protocol, an activated fault results in one or more error types which are boundary violation, conflict, content, freeze, synchronization, and syntax. To study the activated faults, a FlexRay bus network, composed of four nodes, was modeled by verilog HDL; and a total of 135,600 transient faults were injected in only one node, where 9,342 (6.9%) of the faults were activated. The results show that the synchronization error is the widespread error with the occurrence ratio of about 70.1%. The Boundary violation and the syntax errors have the occurrence ratios of 32.4% and 24.6%, respectively. The results also show that the Freeze error which more frequent resulted system failures has the occurrence ratio of about 17.3%.",2009,0,
1192,1193,Fault-tolerant scheduling using primary-backup approach for optical grid applications,"Fault-tolerant scheduling is an important issue for optical gird applications because of a wide range of grid resource failures. To improve the availability of the DAGs (directed acyclic graphs), a primary-backup approach is considered when making DAG scheduling decision. Experiments demonstrate the effectiveness and the practicability of the proposed scheme.",2009,0,
1193,1194,A new audio skew detection and correction algorithm,"The lack of synchronisation between a sender clock and a receiver audio clock in an audio application results in an undesirable effect known as ""audio skew"". This paper proposes and implements a new approach to detecting and correcting audio skew, focusing on the accuracy of measurements and on the algorithm's effect on the audio experience of the listener. The algorithms presented are shown to remove audio skew successfully, thus reducing delay and loss and hence improving audio quality.",2002,0,
1194,1195,Algebraic expression for extra degree added to hypercube interconnection network in case of multiple edge-faults,"Researchers have used more than one method for implementing algorithms on faulty hypercube interconnection networks. Some modified the structures of the hypercube whilst others didn't, implementing their operations on a faulty hypercube. Modifying the structure of a hypercube entails adding extra degrees to each node, called reconfiguration of hypercubes. I improved an algebraic expression for added extra degrees to a hypercube to make it non-faulty",2001,0,
1195,1196,A Fault-tolerance Framework for Distributed Component Systems,"The requirement for higher reliability and availability of systems is continuously increasing even in domains not traditionally strongly concerned by such issues. Required solutions are expected to be efficient, flexible, reusable on rapidly evolving hardware and of course at low cost. Combining both model and component seems to be a very promising cocktail for building solutions to this problem. Hence, we will present in this paper an approach using a model as its first structural citizen all along the development process. Our proposal will be illustrated with an application modeled with UML (extended with some of its dedicated profiles). Our approach includes an underlying execution infrastructure/middleware, providing fault-tolerance services. For the component aspect, our framework promotes firstly an infrastructure based on the Component/Container/Connectorparadigm to provide run-time facilities enabling transparent management of fault-tolerance (mainly fault-detection and redundancy mechanisms). For the model-driven point of view, our framework provides tool support for assisting the users to model their applications and to deploy and configure them on computing platforms. In this paper we focus on the run-time support offered by the component framework, specially the replication-aw are interaction mechanism enabling a transparent replication management mechanisms and some additional system components dedicated to fault-detection and replicas management.",2008,0,
1196,1197,Segment based X-Filling for low power and high defect coverage,"Many X-Filling strategies are proposed to reduce test power during scan based testing. Because their main motivation is to reduce the switching activities of test patterns in the test process, some of them are prone to reduce the test ability of test patterns, which may lead to low defect coverage. In this paper, we propose a segment based X-filling(SBF) technique to reduce test power using multiple scan chains, with minimal impact on defect coverage. Different from the previous filling methods, our X-filling technique is segment based and defect coverage aware. The method can be easily incorporated into traditional ATPG flow to keep capture power below a certain limit and keep the defect coverage at a high level.",2009,0,
1197,1198,"An automated industrial fish cutting machine: Control, fault diagnosis and remote monitoring","In this paper, an automated industrial fish cutting machine, which was developed and tested in the Industrial Automation Laboratory (IAL) of the University of British Columbia, is presented including its hardware structure, control sub-system, fault diagnosis sub-system and the remote monitoring sub-system. First, the hardware of the machine including the mechanical conveyer system, pneumatic system and the hydraulic system, and the associated sensors are introduced. Next, a fuzzy position control system is designed for the control of the cutting table moving along the horizontal (x) direction and its performance is compared with that with traditional proportional-integral-derivative (PID) control. A multi-sensor neuro-fuzzy fault diagnosis system is developed as well for the purpose of providing accurate and reliable diagnosis of the machine states in an automated factory environment. Finally, a web-based remote monitoring system is discussed, which allows engineers and researches to remotely monitor the health of the machine from any remote geographic location through the Internet.",2008,0,
1198,1199,A Fault Tolerant Wired/Wireless Sensor Network Architecture for Monitoring Pipeline Infrastructures,"This paper proposes a new fault-tolerant sensor network architecture for monitoring pipeline infrastructures. This architecture is an integrated wired and wireless network. The wired part of the network is considered the primary network while the wireless part is used as a backup among sensor nodes when there is any failure in the wired network. This architecture solves the current reliability issues of wired networks for pipelines monitoring and control. This includes the problem of disabling the network by disconnecting the network cables due to artificial or natural reasons. In addition, it solves the issues raised in recently proposed network architectures using wireless sensor networks for pipeline monitoring. These issues include the issues of power management and efficient routing for wireless sensor nodes to extend the life of the network. Detailed advantages of the proposed integrated network architecture are discussed under different application and fault scenarios.",2008,0,
1199,1200,Fault simulation and modelling of microelectromechanical systems,High-reliability and safety-critical markets for microelectromechanical systems are driving new proposals for the integration of efficient built-in test and monitoring functions. The realisation of this technology will require support tools and validation methodologies including fault simulation and testability analysis and full closed-loop simulation techniques to ensure cost and quality targets. This article proposes methods to extend the capabilities of mixed signal and analogue integrated circuit fault simulation techniques to MEMS by including failure mode and effect analysis data and using behavioural modelling techniques compatible with electrical simulators.,2000,0,
1200,1201,A power transformer protection with recurrent ANN saturation correction,"Current transformers (CTs) are present in electric power systems for protection and measurement purposes and they are susceptible to the saturation phenomenon. This paper presents an alternative approach to the correction of distorted waveforms caused by CT saturation. The method uses recurrent artificial neural networks (ANN) algorithms. As an example of an application, a complete protection system for a power transformer based on the deferential logic has been utilized. The EMTP-ATP software has been chosen as the computational tool to simulate the electrical system in order to generate data to train and test the ANNs. Many ANN architectures were trained and tested. Encouraging results related to the application of the new method are presented.",2005,0,
1201,1202,Fault tolerance for communication-based multirobot formation,"This paper investigates the ability of fault tolerance for multirobot formation, which is important for practical formation in complex environment. Our model enables mobile robots group to continue to complete given tasks by reorganizing their formation, when some members are in failure. First, to build such model, a multi-agent architecture is presented, which is implemented through communication. Second, we introduce the hierarchy graph of multirobot formation to be the theoretical foundation of the fault tolerance system. The graph analysis is suitable for general leader-follower formation format. And then, the failure detection mechanism for formation is discussed. Finally, integrated fault tolerance algorithm is investigated, including supplement for faulty robots and formation reconfiguration. The improved agent architecture adding the fault tolerance module is also presented. The experiments on real multiple mobile robots demonstrate our design is feasible.",2004,0,
1202,1203,A self-optimization of the fault management strategy for device software,"With the growth of network technologies, abundance of network resources, and increase of various services, mobile devices have gained much functionality and intelligence. At the same time, mobile devices are becoming complicated and many software related problems appear. The traditional remote repair method needs the software providers to supply fault information with corresponding repair strategy. It is inconvenient for users when the sold mobile devices have software faults. However, it is impossible for the manufacturers to supply all the fault information and repair-strategy before selling them. So far, no method has been given to collect repair-strategy from the sold mobile device and optimize the self-repair strategy. In this paper, we propose a self-optimization method to learn the software repair strategy from the sold mobile devices and to optimize self-repair strategy based on the Open Mobile Alliance (OMA) Device Management (DM) standard. The managed objects (MOs) are defined for collecting the strategy data and the self-optimization algorithm is proposed and implemented at the central server.",2009,0,
1203,1204,Guidelines for 2D/3D FE transient modeling of inductive saturable-core Fault Current Limiters,"Fault Current Limiters (FCLs) are expected to play an important role in protection of future power systems. FCLs can be classified in three groups: passive, solid-state and hybrid FCLs. Passive FCLs have merit to inherently react on a fault, requiring no fault detection and triggering circuit. Inductive FCLs based on core saturation belong to this group. Analytical models, used for design of inductive FCLs, are not accurate enough; BH curve cannot be expressed as an explicit function. Numerical models offer better approximations, but they often do not include effects such as leakage and fringing fluxes, which can have considerable influence on the result. Verification of such models is of utmost importance. Finite element modeling (FEM) tools offer possibility to model any inductive FCL topology, while all the effects, e.g. non-linear BH curve, fringing effects etc., are taken into account. However, the modeling of these devices in FEM softwares is difficult. This paper introduces the guidelines for development of 2D/3D transient FE models of inductive FCLs in Ansys. The guidelines are developed with respect to the single-core inductive FCL topology. The model can be applied to any inductive FCL and presents a valuable tool for design, verification and optimization of these devices. Signals' waveforms, obtained through the transient analysis, provide precise depiction of FCL operation during both normal and fault regimes. The model is validate by means of lab experiment. Simulation and experimental results show very good matching. In addition, modeling results are used to prove that single-core FCL topology operates properly during both nominal and fault regimes.",2009,0,
1204,1205,Model-based fault-tolerant control reconfiguration for general network topologies,This article describes a fault-tolerant approach to systems with arbitrary network topologies that uses a model-based diagnosis and control reconfiguration mechanism. The authors illustrate this technique using a wireless sensor network as an example,2001,0,
1205,1206,Harmonic distortion and measurement principles based on digital fault recorder (DFR) analysis,"Each type of device causing harmonics has a particular shape of harmonic current and voltage (amplitude and phase displacement). This work provides a methodology for analyzing the distortion from the data record of digital fault recorders in order to quantify the distortion in current and voltage. This can be done by decomposing the signal into its constituent components in the frequency domain, because of the fact that it is not practical to obtain and represent all the system detail for analysis. It can lead to inaccurate estimation of distortion in voltages and currents. A simple but realistic approach for resonance analysis is presented.",2009,0,
1206,1207,Predicting the severity of a reported bug,"The severity of a reported bug is a critical factor in deciding how soon it needs to be fixed. Unfortunately, while clear guidelines exist on how to assign the severity of a bug, it remains an inherent manual process left to the person reporting the bug. In this paper we investigate whether we can accurately predict the severity of a reported bug by analyzing its textual description using text mining algorithms. Based on three cases drawn from the open-source community (Mozilla, Eclipse and GNOME), we conclude that given a training set of sufficient size (approximately 500 reports per severity), it is possible to predict the severity with a reasonable accuracy (both precision and recall vary between 0.65-0.75 with Mozilla and Eclipse; 0.70-0.85 in the case of GNOME).",2010,0,
1207,1208,Software solution for fault record analysis in power transmission and distribution,"Digital protection relays provide the functionality of recording network disturbances during faults. Meanwhile the digital share in the installed relay number is a substantial one, so the utilities can gather valuable information with a large coverage of their grid and can start to enjoy the additional benefits of modem technology beyond the functions built into the devices. After a fault the operating personnel wants to obtain a most precise fault location to narrow the search for possible damage on the line. The fault locator precision of a single relay is limited by physics and by the grid conditions of mixed lines, load taps etc. But an easy-to-use software system for relay fault records can provide the desired precision to the utility personnel. The system is open to fault records of any relay, which is accomplished via the Comtrade data format. It also contains the parameters of segmented or untransposed lines. Furthermore it uses sophisticated self-adapting algorithms for analysis beyond those used at the protection relays.",2004,0,
1208,1209,REST-Based SOA Application in the Cloud: A Text Correction Service Case Study,"In this paper, we present a REST-based SOA system, Set It Right (SIR), where people can get feedback on and help with short texts. The rapid development of the SIR system, enabled by designing it as a set of services, and also leveraging commercially offered services, illustrates the strength of the SOA paradigm. Finally, we evaluate the Cloud Computing techniques and infrastructures used to deploy the system and how cloud technology can help shorten the time to market and lower the initial costs.",2010,0,
1209,1210,An approach to detecting duplicate bug reports using natural language and execution information,"An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as duplicate and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. In this paper we present a new approach that further involves execution information. In our approach, when a new bug report arrives, its natural language information and execution information are compared with those of the existing bug reports. Then, a small number of existing bug reports are suggested to the triager as the most similar bug reports to the new bug report. Finally, the triager examines the suggested bug reports to determine whether the new bug report duplicates an existing bug report. We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67%-93% of duplicate bug reports in the Firefox bug repository, compared to 43%-72% using natural language information alone.",2008,0,
1210,1211,Intelligent Agents for Fault Tolerance: From Multi-agent Simulation to Cluster-Based Implementation,"Recent research in multi-agent systems incorporate fault tolerance concepts, but does not explore the extension and implementation of such ideas for large scale parallel computing systems. The work reported in this paper investigates a swarm array computing approach, namely 'Intelligent Agents'. A task to be executed on a parallel computing system is decomposed to sub-tasks and mapped onto agents that traverse an abstracted hardware layer. The agents intercommunicate across processors to share information during the event of a predicted core/processor failure and for successfully completing the task. The feasibility of the approach is validated by simulations on an FPGA using a multi-agent simulator, and implementation of a parallel reduction algorithm on a computer cluster using the Message Passing Interface.",2010,0,
1211,1212,Research on RTOS-Integrated TMR for Fault Tolerant Systems,"Safety and availability are issues of major importance in many critical systems. A RTOS (realtime operating system)-integrated fault-tolerant system using TMR technology is presented in this paper. The system incorporates three homogeneous microcomputers and provides the fault-tolerant function through system-APIs to applications. As it is integrated with RTOS, the system is more general-purpose, and programmers need not pay too much attention to the fault tolerance technology. This system works in normal and degraded (duple or even single modular) modes, and can tolerate transient or permanent faults. The system also provides MultiTask-support fault-tolerant function, and reconfiguration after a fault occurs is transparent to applications. Meanwhile, a novel seamless software upgrade method through intelligent state-transition-control is brought forward.",2007,0,
1212,1213,A system architecture assisting user trial-and-error process in in-silico drug design,"Screening on computers, or in-silico screening, has the potential to dramatically reduce the total cost and time required for the whole drug design process. Recently, a variety of in-silico screening systems has been reported on literatures. Nonetheless, scientists still have much difficulty in benefiting from such systems. The reason can be explained from the fact that scientists' trial-and-error consideration, whose purpose is to precisely describe molecules on a computer, in prior to in-silico screening stage takes a long time. We present a flexible user-support environment that assists in scientists' trial-and-error consideration with a ""trial set"" concept. On this environment, scientists utilize the trial set to easily complete a sequence of tasks accompanied with parameter changes. The experiment in this paper shows that the first prototype system featured by this trial set effectively works. Furthermore, we describe the future plan of the system.",2004,0,
1213,1214,Flight technical error analysis of the SATS higher volume operations simulation and flight experiments,"This paper provides an analysis of flight technical error (FTE) from recent SATS experiments, called the higher volume operations (HVO) simulation and flight experiments, which NASA conducted to determine pilot acceptability of the HVO concept for normal operating conditions. Reported are FTE results from simulation and flight experiment data indicating the SATS HVO concept is viable and acceptable to low-time instrument rated pilots when compared with today's system (Baseline). Described is the comparative FTE analysis of lateral, vertical, and airspeed deviations from the Baseline and SATS HVO experimental flight procedures. Based on FTE analysis, all evaluation subjects, low-time instrument-rated pilots, flew the HVO procedures safely and proficiently in comparison to today's system (Baseline). In all cases, the results of the flight experiment validated the results of the simulation experiment and confirm the utility of the simulation platform for comparative human in the loop (HITL) studies of SATS HVO and Baseline operations.",2005,0,
1214,1215,Cancer-radiotherapy equipment as a cause of soft errors in electronic equipment,"The undesirable production of secondary neutrons by cancer-radiotherapy linear accelerators (linac) has been demonstrated to cause soft errors in nearby electronics through the <sup>10</sup>B(n,a)<sup>7</sup>Li reaction. <sup>10</sup>B is a component in the BPSG used as a dielectric material in some integrated-circuit (IC) fabrication processes.",2005,0,
1215,1216,Diagnostics using airborne survey and fault location systems as the means to increase OHTL reliability,"Airborne survey application for diagnostics of overhead transmission lines (OHTL) is quite relevant for power utilities of industrialized counties where power network counts thousands or millions km, of which considerable part has reached 30-50 year lifetime and older. Airborne survey based on aerial scanning as a method of OHTL condition monitoring, is efficient instrument of detection of the line elements deviation off regular condition, serves as a convenient facility of network utility inventory. Advantage of aerial scanning is a combination of high survey accuracy with high work productivity. Processing of digital survey data allows to get essential data required for OHTL reliability analysis: precise span lengths, sag and tension values, conductor clearance to ground, crossed and adjacent objects, clearance to vegetation, distance to nearby trees that may damage OHTL if fallen. For analysis of OHTL reliability, existing software packages allow to carry out modeling condition of separate elements and entire line under extreme ice and wind loads, check safety of conductor clearance to ground and crossed lines under condition of significant conductor overheating determined by necessity to ensure transmission under long-term or short-term (but considerable) load increase. Collection, storage, systemizing, practical use of survey data for development and implementing management decisions and rational usage of network resources is reasonable to accomplish with a specialized information system. Information system helps to provide integrate OHTL monitoring data, modules of record and analysis of technical condition of separate components and entire line, 2D and 3D representation of objects with high georeference accuracy. One of negative examples of insufficient OHTL reliability is fault current caused by lightning, conductor or insulator mechanical damage, etc. Duration of OHTL malfunction, timing and success of emergency elimination depends greatly on accuracy of fa- ult location (FL) on line. Advanced FL system allows to locate fault with accuracy of 5 to 150 m. Combined with aerial scanning data and visualizing line section detected by FL system essentially improves efficiency of service technology, emergency recovery of electric network by maintenance crew, and hence increases system reliability of power objects.",2005,0,
1216,1217,Range Non-linearities Correction in FMCW SAR,"The limiting factor to the use of Frequency Modulated Continuous Wave (FMCW) technology with Synthetic Aperture Radar (SAR) techniques to produce lightweight, cost effective, low power consuming imaging sensors with high resolution, is the well known presence of non-linearities in the transmitted signal. This results in contrast and range resolution degradation, especially when the system use is intended for long range applications, as it is the case for SAR. The paper presents a novel processing solution, which completely solves the non- linearity problem. It corrects the non-linearity effects for the whole range profile at once, differently from the algorithms described in literature so far, which work only for very short range intervals. The proposed method operates directly on the deramped data and it is very computationally efficient.",2006,0,
1217,1218,Remote sensing of power system arcing faults,Electromagnetic radiation in the form of atmospheric radiowaves (or sferics) originate from power system apparatus when transient fault currents are present. A system to monitor these events via the detection of the induced very high frequency (VHF) sferic radiation has been in operation since November 1998. This system is part of an ongoing research program to develop overhead line fault detection and location equipment. This paper details the implementation of the sferic monitoring system and the latest developments that aim to improve event detection and triggering efficiency. Example transient sferic radiations records taken from the extensive data archive are presented. Fourier time frequency domain analysis is employed to extract features from the sferic signal data. Finally the future application of such monitoring technologies to power distribution networks is discussed.,2000,0,
1218,1219,Extended transfer bound error analysis in the presence of channel random nuisance parameter,"In this paper, we present the extended transfer bound analysis for the error performance of a general trellis code in the channel with the overall correlated continuous valued nuisance parameters. We introduce proper parameter model and include it into a new extended form of the transfer function. In this way, both, the new additional parameter space and the original error space are incorporated into system error analysis. An example application with simple trellis code and Rayleigh fading channel is investigated in order to demonstrate the functionality of the principle. Computer simulation results are presented for two different codes and various fading scenarios, and comparisons are made among analytical and measured system error performances. It was shown that for fading amplitude our approach was able to predict correct error asymptotes",2005,0,
1219,1220,Sub-picture: ROI coding and unequal error protection,Region-of-interest coding and unequal error protection are two important tools in video communication systems to improve the received visual quality. One common property of the two techniques is that unequal coding or transmission is applied to improve the quality of the most important parts of images. The proposed sub-picture coding technique facilitates both region-of-interest coding and unequal error protection by partitioning images to regions of interest and separating the corresponding coded data units from each other. Simulation results show that the overall subjective quality is considerably improved compared to the conventional coding schemes.,2002,0,
1220,1221,Fault detection and isolation in the NT-OMS/RCS,"In this paper, we consider the problem of test design for real-time fault detection and diagnosis in the Space Shuttle's non-toxic orbital maneuvering system and reaction control system (NT-OMS/RCS). For demonstration purposes, we restrict our attention to the shaft section of the NT-OMS/RCS, which consists of 160 faults (each fault being either a leakage, blockage, igniter fault, or regulator fault) and 128 sensors. Using the proposed tests, we are able to uniquely isolate a large number of the faults of interest in the NT-OMS/RCS. Those that cannot be uniquely isolated can generally be resolved into small ambiguity groups and then uniquely isolated via manual/automated commands. Simulation of the NT-OMS/RCS under various fault conditions was conducted using the TRICK<sup></sup> modeling software.",2004,0,
1221,1222,A fault-driven lightweight process improvement approach,"Process improvement is of high importance and with crucial impact on business and prosperity for software developing companies. The requirements on software are that it needs to be produced faster, cheaper and with higher quality. A recent trend in software development is the use of agile methods. The general idea of more lightweight approaches can also be applied to process improvement. The authors describe a fault-driven lightweight process improvement approach to be used between projects. The objective is to decrease the number of faults and hence shorten the project lead-time. The fault-driven process improvement approach sets focus on business requirements and relevance for the company associated. We discuss the need for a lightweight approach and introduce a lightweight process improvement method. It also reports on some findings from an industrial study and presents some conclusions.",2003,0,
1222,1223,Online Defects Inspection Method for Velcro Based on Image Processing,"Quality control is a crucial issue in producing velcro, and defects existing in velcro can dramatically downgrade velcro level. Manual inspection can not meet the requirements of production efficiency, so an online feasible inspection method is referred to control the surface quality of the velcro. The original algorithm for the edge detection has been improved, and the flaws are extracted according to the first-order characteristic value. And these defects are classified according to the spectrum. Finally, the experiments have indicated that the various defects can be detected by proposed algorithm accurately, and the defects inspection method has been efficient and of great significance.",2010,0,
1223,1224,An Automated System for Analyzing Impact of Faults in IP Telephony Networks,"The widespread use of IP telephony (IPT) has introduced corresponding management issues related to the diagnosis and impact analysis of faults in the IPT network. There are complex logical relationships between the various elements present in the IPT network ranging from call processing engines, PSTN voice gateways, and conferencing and voice mail servers, to endpoints like IP-based handsets. Without an understanding of these complex relationships, traditional element-oriented network fault-management applications are inadequate to assess IPT service impact in the presence of a network or system fault. In this paper, we present a proposal for determining the root cause and assessing the impact of a fault in an IPT network on voice services and end users by automatically executing a series of diagnostic tests from various probe points in the network, and correlating the resulting information with the faults reported from the network. We use a CIM-based model of the IPT network to aid the diagnostic and impact assessment process",2006,0,
1224,1225,Holistic schedulability analysis of a fault-tolerant real-time distributed run-time support,"The feasibility test of a hard real time system must not only take into account the temporal behavior of the application tasks but also the behavior of the run-time support in charge of executing applications. The paper is devoted to the schedulability analysis of a run-time support for distributed dependable hard real time applications. In contrast to previous works that consider rather simple run-time supports (e.g. a real time kernel made of a simple tick scheduler and an unreliable communication protocol), our work deals with a complex run-time support with fault tolerance capabilities and made of multiple tasks that invoke each other",2000,0,
1225,1226,The Fault-Tolerant Design in Space Information Processing System Based on COTS,"In order to make Space Information Processing System (SIPS) based on Commercial-Off-The-Shelf (COTS) have a much stronger ability of radiation resistance in space, this paper presents multilevel fault-tolerant technique based on FPGA and Single Event Latch-up (SEL) resistance protection circuit. The multilevel fault-tolerant technique includes the dual fault-tolerant design on system level, the redundancy design of memory on module level and the fault-tolerant design of FPGA on chip level. By reliability analysis and experimentation, it can be concluded that the reliability of SIPS has been greatly increased by making use of fault-tolerant design. Moreover, this fault-tolerant design has been implemented successfully and run well.",2009,0,
1226,1227,Fault diagnosis in optical access network using Bayesian network,"Due to ever rising need for higher bandwidth dictated by the increasing amounts and quality of services (Asymmetric Digital Subscriber Line (ADSL), Internet Protocol television (IPTV) and Voice over Internet Protocol (VOIP)), and the global development of technology, Fiber To The Home (FTTH) is becoming an affordable answer for the service provider. Before fully launching the service for the mass market the most important prerequisite is being able to provide customers with a high quality experience. To be able to achieve that, one of the main factors that service providers have to ensure is accurate and precise diagnosis of customer reported faults. Low quality of diagnostics prolongs the period of service degradation and increases the likelihood of repeated faults which results in dissatisfaction amongst customers. That is why our objective is to improve the accuracy of diagnostics performed by network technicians to at least 80%. In this paper we are introducing a solution based on Bayesian network and presenting the results of the applied method.",2010,0,
1227,1228,Misalignment between emission scan and X-ray CT derived attenuation maps causes artificial defects in attenuation corrected myocardial perfusion SPECT,"Attenuation correction increases the specificity of myocardial perfusion SPECT. However, we noticed an unusually high number of scans with defects only visible in the attenuation-corrected images. We suspected these to be false positive readings. Visual inspection using the supplied software suggested mismatch in the ventrodorsal (Y-) direction between SPECT images and transmission maps as an explanation. As the fusion tool only allows for a coarse grading, we wrote software to quantify the mismatch. A phantom study was done to verify that the observed mismatch can cause the defect patterns visible in the attenuation-corrected images. 25 patients who showed the most pronounced artifact were chosen for re-alignment and re-evaluation. Overall, the defects in the attenuation-corrected images got less intense (15/25) or even vanished (6/25) after re-aligning emissions images and transmission maps. In response to our complaints, the vendor replaced the support rollers which should prevent the bed from deflecting with a re-engineered, more robust version. No more clinically relevant artefacts were observed after this modification. Evaluation of another 28 probably-normal patients showed the mean mismatch between emission and transmission scan to be significantly reduced. We conclude that great care has to be taken to ensure correct alignment of the scans even in a dual-modality imaging device. Bed deflection can be a major source for misalignment and artifacts.",2004,0,
1228,1229,BDD based analysis of parametric fault trees,"Several extensions of the fault tree (FT) formalism have been proposed in the literature. One of them is called parametric fault tree (PFT) and is oriented to the modeling of redundant systems, and provides a compact form to model the redundant parts of the system. Using PFTs instead of FTs to model systems with replicated parts, the model design is simplified since the analyst can fold subtrees with the same structure in a single parametric subtree, reducing the number of elements in the model. The method based on binary decision diagrams (BDD) for the quantitative analysis of FTs, is adapted in this paper to cope with the parametric form of PFTs: an extension of BDDs called parametric BDD (pBDD) is used to analyze PFTs. The solution process is simplified by using pBDDs: comparing the pBDD obtained from a PFT, with the ordinary BDD obtained from the unfolded FT, we can observe a reduction of the number of nodes inside the pBDD. Such reduction is proportional to the level of redundancy inside the PFT and leads to a consequent reduction of the number of steps necessary to perform the analysis. Concerning the qualitative analysis, we can observe that several minimal cut sets (MCS) obtained from the FT model of a redundant system, involve basic events relative to similar components. A parametric MCS (pMCS) allows to group such MCSs in an equivalence class, and consequently, to evidence only the failure pattern, regardless the identity of replicated components. A method to derive pMCSs from a PFT is provided in the paper",2006,0,
1229,1230,An efficient error concealment implementation for MPEG-4 video streams,"This paper presents an efficient error concealment implementation for damaged MPEG-4 video bitstreams. The chosen spatial and temporal concealment algorithms are designed to fit in real-time decoders and are advantageously combined in a hybrid spatial/temporal approach to provide visually more plausible pictures than basic concealment techniques. In addition, the encoder impact on the visual quality of the reconstruction in presence of channel errors is highlighted",2001,0,
1230,1231,Adaptive Checkpoint Replication for Supporting the Fault Tolerance of Applications in the Grid,"A major challenge in a dynamic Grid with thousands of machines connected to each other is fault tolerance. The more resources and components involved, themore complicated and error-prone becomes the system. Migol is an adaptive Grid middleware, which addresses the fault tolerance of Grid applications and services by providing the capability to recover applications from checkpoint files automatically. A critical aspect for an automatic recovery is the availability of checkpoint files: If a resource becomes unavailable, it is very likely that the associated storage is also unreachable, e. g. due to a network partition. A strategy to increase the availability of checkpoints isreplication.In this paper, we present the Checkpoint Replication Service. A key feature of this service is the ability to automatically replicate and monitor checkpoints in the Grid.",2008,0,
1231,1232,Analyses and correction of the dynamic properties of the VALYDYNE <sup>TM</sup> differential pressure sensor,"The paper presents the results of the modeling and measurement study of the differential pressure transducer MP45 series made by the Validyne<sup>TM</sup> Corp. This research is necessary during the construction of the device for identification of the human airducts model using the time method. The transducer parameters in the manufacturer's model are significantly different from the parameters calculated after the measurement study. This paper presents verification of models and frequency range of transducer. A new, more accurate model is proposed. The results obtained show that it is necessary to make a correction of the dynamic properties of this pressure sensor. The effects of this correction using first- and second-order correctors are presented and analyzed",2001,0,
1232,1233,Scheduling algorithms for ultra-reliable execution of tasks under both hardware and software faults,"Summary form only given, as follows. We study the development of integrated fault-tolerant scheduling algorithms. The proposed algorithms ensure ultra-reliable execution of tasks where both hardware and software failures are considered, and system performance improvement. Also, the proposed algorithms have the capability for on-line system-level fault diagnosis.",2003,0,
1233,1234,Real-Time Implementation of Fault Detection in Wireless Sensor Networks Using Neural Networks,"This paper presents the real-time implementation of a neural network-based fault detection for wireless sensor networks (WSNs). The method is implemented on TinyOS operating system. A collection tree network is formed and multi-hoping data is sent to the base station root. Nodes take environmental measurements every N seconds while neighboring nodes overhear the measurement as it is being forwarded to the base station and record it. After nodes complete M and receive/store M measurements from each neighboring node, recurrent neural networks (RNNs) are used to model the sensor node, the node's dynamics, and interconnections with neighboring nodes. The physical measurement is compared against the predicted value and a given threshold of error to determine sensor fault. By simply overhearing network traffic, this implementation uses no extra bandwidth or radio broadcast power. The only cost of the approach is battery power required to power the receiver to overhear packets and MCU processor time to train the RNN.",2008,0,
1234,1235,Design Diverse-Multiple Version Connector: A Fault Tolerant Component Based Architecture,"Component based software engineering (CBSE) is a new archetype to construct the systems by using reusable components ldquoas it isrdquo. To achieve high dependability in such systems, there must be appropriate fault tolerance mechanism in them at the architectural level. This paper presents a fault tolerant component based architecture that relies on the C2 architectural style and is based on design diverse and exception handling fault tolerance strategies. The proposed fault tolerant component architecture employs special-purpose connectors called design diverse-multiple version connectors (DD-MVC). These connectors allow design diverse n-versions of components to run in parallel. The proposed architecture has a fault tolerant connector (FTC), which detects and tolerates different kinds of errors. The proposed architecture adjusts the tradeoff between dependability and efficiency at run time and exhibits the ability to tolerate the anticipated and unanticipated faults effectively. The applicability of proposed architecture is demonstrated with a case study.",2008,0,
1235,1236,Adaptive unequal error protection for subband image coding,"An adaptive subband image coding system is proposed to investigate the performance offered by implementing unequal error protection among the subbands and within the subbands. The proposed system uses DPCM and PCM codecs for source encoding the individual subbands, and a family of variable rate channel codes for forward error correction. A low resolution family of trellis coded modulation codes and a high resolution family of punctured convolutional codes are considered. Under the constraints of a fixed information rate, and a fixed transmission bandwidth, for any given image, the proposed system adaptively selects the best combination of channel source coding rates according to the current channel condition. Simulations are performed on the AWGN channel, and comparisons are made with corresponding systems where the source coder is optimized for a noiseless transmission (classical optimization) and a single channel code is selected. Our proposed joint source-channel systems greatly outperform any of the nonadaptive conventional nonjoint systems that use only a single channel code at all channel SNRs, extending the useful channel SNR range by an amount that depends on the code family. A nonjoint adaptive equal error protection system is considered which uses the classically optimized source codec, but chooses the best single channel code for the whole transmission according to the channel SNR. Our systems outperform the corresponding adaptive equal error protection system by at most 2 dB in PSNR; and more importantly, show a greater robustness to channel mismatch. It is found that most of the performance gain of the proposed systems is obtained from implementation of unequal error protection among the subbands, with at most 0.7 dB in PSNR additional gain achieved by also applying unequal error protection within the subbands. We use and improve a known modeling technique which enables the system to configure itself optimally for the transmission of an arbitrary image, by only measuring the mean of lowest frequency subband and variances of all the subbands",2000,0,
1236,1237,Fault diagnosis based on granular matrix-SDG and its application,"The hierarchical fault diagnosis based on granular matrix and Signed Directed Graph (SDG) is presented in the paper. Granular Computing (GrC) theory can be introduced into SDG-based fault diagnosis to optimize the decision table. The rules of fault diagnosis are reasoned out through searching the associated path of the SDG model. The redundant nodes of the failure diagnosis rules are reduced by the attribute reduction algorithm based on granular matrix, which can simplify the solution of failure diagnosis, avoid the setting of the redundant sensor, and decrease the complexity of collocating sensor network. Compared with the traditional failure diagnosis based on SDG, the designed scheme and an experimental example of a hot nitric acid cooling failure diagnosis system show that the hierarchical fault diagnosis based on granular matrix and SDG in the paper is not only feasibly and effectively, but also valuable in practice.",2009,0,
1237,1238,Fault-Tolerant Bit-Parallel Multiplier for Polynomial Basis of GF(2m),"In this paper, we present novel fault-tolerant architecture for bit-parallel polynomial basis multiplier over GF(2m) which can correct the erroneous outputs using linear code. We have designed a parity prediction circuit based on the code generator polynomial that leads lower space overhead. For bit-parallel architectures, the space overhead is about 11%. Moreover, there is only marginal time overhead due to incorporation of error-correction capability that amounts to 3.5% in case of the bit-parallel multiplier. Unlike the existing concurrent error correction (CEC) multipliers or triple modular redundancy (TMR) techniques for single error correction, the proposed architectures have multiple error-correcting capabilities.",2009,0,
1238,1239,Practical Methods for Geometric and Photometric Correction of Tiled Projector,"We describe a novel, practical method to create largescale, immersive displays by tiling multiple projectors on curved screens. Calibration is performed automatically with imagery from a single uncalibrated camera, without requiring knowledge of the 3D screen shape. Composition of 2D-mesh-based coordinate mappings, from screen-tocamera and from camera-to-projectors, allows image distortions imposed by the screen curvature and camera and projector lenses to be geometrically corrected together in a single non-parametric framework. For screens that are developable surfaces, we show that the screen-to-camera mapping can be determined without some of the complication of prior methods, resulting in a display on which imagery is undistorted, as if physically attached like wallpaper. We also develop a method of photometric calibration that unifies the geometric blending, brightness scaling, and black level offset maps of prior approaches. The functional form of the geometric blending is novel in itself. The resulting method is more tolerant of geometric correction imprecision, so that visual artifacts are significantly reduced at projector edges and overlap regions. Our efficient GPUbased implementation enables a single PC to render multiple high-resolution video streams simultaneously at frame rate to arbitrary screen locations, leaving the CPU largely free to do video decompression and other processing.",2006,0,
1239,1240,3DPPS for early detection of arcing faults,"New approach to high impedance fault detection, which allows for detecting it basing on yet some random arcing at the beginning of the fault, is presented in this paper. The proposed solution was developed within novel protection methodology - 3D power protection scheme (3DPPS). The identification of the fault is based on monitoring of symmetry deviations of three phase voltage or current signals. Fundamental signal components carry the biggest amount of information on the actual state of the protected system and are processed in order to extract out the necessary information proving an occurrence of a high impedance fault that must be cleared for safety purposes.",2010,0,
1240,1241,An analysis of fault effects and propagations in ZPU: The world's smallest 32 bit CPU,"This paper presents an analysis of the effects and propagations of transient faults by simulation-based fault injection into the ZPU processor. This analysis is done by injecting 5800 transient faults into the main components of ZPU processor that is described in VHDL language. The sensitivity level of various points of ZPU processor such as PC, SP, IR, Controller, and ALU against fault manifestation is considered and evaluated. The behavior of ZPU processor against injected faults is reported. Besides, it is shown that about 50.25% of faults are recovered during simulation time; 46.47% of faults are effective and the remainders 3.28% of faults are latent. Moreover, a comparison of the behavior of ZPU processor in fault injection experiments against some common microprocessors is done. The results will be used in the future research for proposing a fault-tolerant mechanism for ZPU processor.",2010,0,
1241,1242,Design of the distributed fault recorder based on TCP/IP,"A new kind of fault recorder is presented to overcome the shortcomings of existing fault recorders in power system, such as unreasonable structure design, low communication speed, small data storage capacity, low reliability and so on. The recorder, with MC68332 as kernel, adopts high speed synchro-sampling and computer network communication techniques. The network communication based on TCP/IP between the master station and recording modules is adopted. The principles, hardware and software design are introduced in detail. A RTOS, named Vxworks and modular design are applied, which make the system functions easy to be expanded and maintained. The recorder has been put into practical operation in some substations. The operation results show that it has the advantages of easy to be extended, convenient to be installed, better anti-interference, high reliability, etc.",2008,0,
1242,1243,An enhancement of fault-tolerant routing protocol for Wireless Sensor Network,"As more and more real Wireless Sensor Network's (WSN) applications have been tested and deployed over the last decade, the research community of WSN realizes that several issues need to be revisited from practical angles, such as reliability and availability. Furthermore, fault-tolerance is one of the main issues in WSNs since it becomes critical in real deployed environments where network stability and reduced inaccessibility times are important. Basically, wireless sensor networks suffer from resource limitations, high failure rates and faults caused by the defective nature of wireless communication and the wireless sensor itself. This can lead to situations, where nodes are often interrupted during data transmission and blind spots occur in the network by isolating some of the devices. In this paper, we address the reliability issue by designing an enhanced fault-tolerant mechanism for Ad hoc On-Demand Distance Vector (AODV) routing protocol applied in WSN called the ENhanced FAult-Tolerant AODV (ENFAT-AODV) routing protocol. We apply a backup route technique by creating a backup path for every node on a main path of data transmission. When a node gets failure to deliver a data packet through the main path, it immediately utilizes its backup route to become a new main path for the next coming data packet delivery to reduce a number of data packets dropped and to maintain the continuity of data packet transmission in presence of some faults (node or link failures). Furthermore, with increased failure rate, this proposed routing protocol improves the throughput, reduces the average jitter, provides low control overhead and decreases the number of data packets dropped in the network. As a result, the reliability, availability and maintainability of the network are achieved. The simulation results show that our proposed routing protocol is better than the original AODV routing.",2010,0,
1243,1244,A Hybrid Approach to Fault Detection and Correction in SoCs,"The reliability of Systems-on-Chip (SoCs) is very important with respect to their use in different types of critical applications. Several fault tolerance techniques have been proposed to improve their fault detection and correction capabilities. These approaches can be classified in two basic categories: software-based and hardware-based techniques. In this paper, we propose a hybrid approach to provide fault detection and correction capabilities of transient faults for processor-based SoCs. This solution improves a previous one, aimed at fault detection only, and combines some modifications of the source code at high level with the introduction of an Infrastructure Intellectual Property (TIP). The main advantage of the proposed method lies in the fact that it does not require modifying the microprocessor core. Experimental results are provided to evaluate the effectiveness of the proposed method.",2007,0,
1244,1245,Overflow Detection and Correction in a Fixed-Point Multiplier,"The fixed-point binary representation, an integer format with an implied binary point, is an alternative to the IEEE floating-point binary layout. Systems that do not support the IEEE floating-point format, e.g., mobile devices, use the fixed-point format because it fits well into integer data paths whereas floating-point requires its own data path. Software developers who port to fixed-point systems often face issues when balancing range and precision. Those issues, overflow and large rounding errors, often arise from arithmetic operations, making debugging more difficult. The proposed solution limits hardware support to a set of fixed-point formats and adjusts the format of the output based on the user supplied format and overflow. The format of the result is readjusted on overflow in order to return a useful result but with the sacrifice of precision. In addition to the corrected result, the overflow flag is raised so the software and subsequent logic are aware of the readjustment in the result's format. This work has been implemented in a fixed-point multiplier because multiplication yields the largest overflow among the four basic arithmetic operations. In order to detect overflow early, the fixed-point multiplier adopts preliminary overflow detection. With the idea of taking the burden of fixed-point scaling off of the programmer, the fixed-point multiplier with overflow detection and correction provides a starting point towards mitigating fixed-point errors.",2007,0,
1245,1246,Notice of Violation of IEEE Publication Principles<BR>Using Current Signature Analysis Technology to Reliably Detect Cage Winding Defects in Squirrel-Cage Induction Motors,"Notice of Violation of IEEE Publication Principles<BR><BR>""Using Current Signature Analysis Technology to Reliably Detect Cage Winding Defects in Squirrel-Cage Induction Motors""<BR>by I.M. Culbert and W. Rhodes<BR>in the IEEE Transactions on Industry Applications, Vol. 43, No. 2, March/April 2007<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains portions of original text from the papers cited below. The original text was copied without attribution and without permission.<BR><BR>Figure 5:<BR>""Development of a Tool to Detect Faults in Induction Motors via Current Signature Analysis"",<BR>by M. Fenger, B. A. Lloyd, and W. T. Thomson<BR>in the Proceedings of the IEEE-IAS/PCA Cement Industry Conference, Dallas, TX, May 4-9, 2003, pp. 37-46.<BR><BR>Equation 3:<BR>""Case Histories of Rotor Winding fault diagnosis in induction motors"",<BR>by W. T. Thomson, and D. Rankin<BR>in the Proceedings of the International Conference on Condition Monitoring, University College of Swansea, March 31-April 3, 1987, pp. 798-819.<BR><BR>This paper will demonstrate, through industrial case histories, the application of current signature analysis (CSA) technology to reliably diagnose rotor winding problems in squirrel-cage motors. Many traditional CSA methods result in false alarms and/or misdiagnosis of healthy machines due to the presence of current components in the broken cage winding frequency domain, which are not the result of such defects. Such components can result from operating conditions, motor design, and drive components such as mechanical load fluctuations, speed-reducing gearboxes, etc. Due to theoretical advancements, it is now possible to predict many of these current components, thus making CSA testing less error prone and therefore a much more reliable technology. Reliable detection of the in- eption of broken cage winding problems, or broken rotor bars, prior to failure allows for remedial actions to be taken to avoid significant costs associated with consequential motor component damage and unplanned downtime associated with such in-service failures",2007,0,
1246,1247,Development of an ATP-based fault simulator for underground power cables,The paper reports the development of an ATP-based fault simulator for underground power cables. Emphasis is given to the design philosophy and a detailed description of the user interface which has been developed. Case studies are also included to illustrate the applications of the software,2000,0,
1247,1248,Enhance Fault Localization Using a 3D Surface Representation,"Debugging is a difficult and time-consuming task in software engineering. To locate faults in programs, a statistical fault localization technique makes use of program execution statistics and employs a suspiciousness function to assess the relation between program elements and faults. In this paper, we develop a novel localization technique by using a 3D surface to visualize previous suspiciousness functions and using fault patterns to enhance such a 3D surface. By clustering realistic faults, we determine various fault patterns and use 3D points to represent them. We employ spline method to construct a 3D surface from those 3D points and build our suspiciousness function. Empirical evaluation on a common data set, Siemens suite, shows that the result of our technique is more effective than four existing representative such techniques.",2010,0,
1248,1249,Error Sector Inventory on Optical Disc for Defect Avoidance during Recording / Playback Digest of Technical Papers,"This paper proposes a method to create on-the-fly inventory of error sectors, so that subsequent recordings / playback are free from the audio-video hiccups and retries and still compliant with DVD-Video / DVD+RW logical standard. This will in-turn aid in creating good user experience (no audio-video hiccups) and helps in using processor MIPS efficiently (no retries).",2008,0,
1249,1250,Research on Error Analysis and Calibration Method of Laser Scan Range Finder,"The technology of laser scan range finder is researched, a line laser is used to survey the object section outlines. The system is composed of line laser, CCD camera, Frame Grabber, computer and the software. The mathematical model of the system is defined and studied, the error is analysed, the calibration method is researched. the system is calibrated and tested, the result showed that the survey data moved towards with the stripe shape tallies and the system achieve the accuracy requirements.",2010,0,
1250,1251,Towards latent faults prevision in an automatic controlled plant,The paper presents an approach to develop a program for the latent fault prevision dedicated to an automatic controlled plant. The new approach is based on a soft-computing processing of a set of variables to work out a quality index for evaluating the efficiency of the plant operation. The trend of this index is the base for the prevision of latent faults. The aim of the approach is to realise an intelligent supervision program to insert in the existing automation system. A robotic soldering cell is considered.,2000,0,
1251,1252,A QoS-aware fault tolerant middleware for dependable service composition,"Based on the framework of service-oriented architecture (SOA), complex distributed systems can be dynamically and automatically composed by integrating distributed Web services provided by different organizations, making dependability of the distributed SOA systems a big challenge. In this paper, we propose a QoS-aware fault tolerant middleware to attack this critical problem. Our middleware includes a user-collaborated QoS model, various fault tolerance strategies, and a context-aware algorithm in determining optimal fault tolerance strategy for both stateless and stateful Web services. The benefits of the proposed middleware are demonstrated by experiments, and the performance of the optimal fault tolerance strategy selection algorithm is investigated extensively. As illustrated by the experimental results, fault tolerance for the distributed SOA systems can be efficient, effective and optimized by the proposed middleware.",2009,0,
1252,1253,Development of a novel humidity sensor with error-compensated measurement system,"This paper documents the creation of a complete PC-based humidity sensing system, implemented using LabVIEW from National Instruments. The humidity sensor, which has a measured sensitivity of 0.25%/RH%, is manufactured by thin film technology from a novel combination of SiO/In<sub>2</sub>O<sub>3</sub>. Both the humidity sensor and a standard temperature sensor are interfaced to a PC using a front-end signal conditioning circuit. The entire system has been analyzed mathematically and the necessary algorithms for error-compensation have been developed. The resulting measurement system is efficient, accurate and flexible.",2002,0,
1253,1254,An experiment family to investigate the defect detection effect of tool-support for requirements inspection,"The inspection of software products can help to find defects early in the development process and to gather valuable information on product quality. An inspection is rather resource intensive and involves several tedious tasks like navigating, sorting, or checking. Tool support is thus hoped to increase effectiveness and efficiency. However, little empirical work is available that directly compares paper-based (i.e., manual) and tool-based software inspections. Existing reports on tool support for inspection generally tend to focus on code inspections while little can be found on requirements or design inspection. We report on an experiment family: two experiments on paper-based inspection and a third experiment to empirically investigate the effect of tool support regarding defect detection effectiveness and inspection effort in an academic environment with 40 subjects. Main results of the experiment family are: (a) The effectiveness is similar for manual and tool-supported inspections; (b) the inspection effort and defect overlap decreased significantly with tool support, while (c) efficiency increased considerably with tool support.",2003,0,
1254,1255,An Experimental Study of Packet Loss and Forward Error Correction in Video Multicast over IEEE 802.11b Network,"Video multicast over wireless local area networks (WLANs) faces many challenges due to varying channel conditions and limited bandwidth. A promising solution to this problem is the use of packet level forward error correction (FEC) mechanisms. However, the adjustment of the FEC rate is not a trivial issue due to the dynamic wireless environment. This decision becomes more complicated if we consider the multi-rate capability of the existing wireless LAN technology that adjusts the transmission rates based on the channel conditions and the coverage range. In order to explore the above issues we conducted an experimental study of the packet loss behavior of the IEEE 802.11b protocol. In our experiments we considered different transmission rates under the broadcast mode in indoor and outdoor environments. We further explored the effectiveness of packet level FEC for video multicast over wireless networks with multi-rate capability. In order to evaluate the system quantitatively, we implemented a prototype using open source drivers and socket programming. Based on the experimental results, we provide guidelines on how to efficiently use FEC for wireless video multicast in order to improve the overall system performance. We show that the Packet Error Rate (PER) increases exponentially with distance and using a higher transmission rate together with stronger FEC is more efficient than using a lower transmission rate with weaker FEC for video multicast.",2009,0,
1255,1256,Mechanical fault detection in induction motors,"This paper presents a study to detect mechanical irregularities in low voltage random wound induction motors by means of stator current monitoring and spectrum analysis. An analysis of the MMF and permeance functions to classify frequency components of the stator field in both the stator and rotor reference frame is presented. In addition, a test rig designed to introduce different degrees of static eccentricity in the motor with new movable bearing housings is described in detail. Experimental tests prove the theoretical analysis discussed and significant results are presented.",2003,0,
1256,1257,Error Separation and Compensation of Inductosyn Angle Measuring System,"The long period error and short period error of the inductosyn have been studied. And the study result is that the long period error mainly includes the first order error and secondary error in the period of 360A and the short period error mainly includes first order, secondary, third, fifth harmonic error, and so on. A novel model of error separation and compensation is firstly presented according to the error characteristic of inductosyn. And the fourier transform are compared with least-squares in many aspects, The implement method of the error separation based on least-squares is also discussed in detail. One new measuring method is proposed, which can use less than test position to attain the long period error and the short period error. The experiments show that the error compensation method can improve the precision of the inductosyn.",2010,0,
1257,1258,"Influence of random, pile-up and scatter corrections in the quantification properties of small-animal PET scanners","The potential of PET imaging for pre-clinical studies will be fully realized only if repeatable, reliable and accurate quantitative analysis can be performed. The characteristic blurring of PET images due to positron range and non co-linearity, as well as random, pile-up and scatter contributions, that may be significant for fully 3D PET acquisitions of small animal, make it difficult their quantitative analysis. In this work specific activity versus specific counts in the image calibration curves for 3D-OSEM reconstructions from a commercially available small animal PET scanner are determined. Both linear and non-linear calibration curves are compared and the effect of corrections for random and scatter contributions are studied. To assess the improvement in the calibration procedure when scatter and random corrections are considered, actual data from a rat tumor pre- and post-cancer therapy are analyzed. The results show that correcting for random and scatter corrections can increase the sensitivity of PET images to changes in the biological response of tumors by more than 15%, compared to uncorrected reconstructions.",2007,0,
1258,1259,The discovery of the fault location in NIGS,"A new method is discovered for calculating the fault distance of the overhead line of the Neutral Indirect Grounded System (NIGS) in power distribution networks, in which the single phase to ground fault point or distance is difficult to detect, because the zero sequence current is in lower value. It is found that the information of the fault distance is kept in the zero sequence voltage vector which may be measured at the tail terminal of the questioned line by digging the data. Then an algorithm to calculate the fault location on the overhead lines is proposed by considering that the zero sequence voltage vector at the tail terminal. The value of the zero sequence voltage is determined by the fault location, and the phase angle also contains the distance traveled by the load current to the fault point. The system analysis for parameters is conducted for the NIGS by considering line is actual and by the two terminals' parameters.",2010,0,
1259,1260,"The $100,000 Keying Error","Losing $100K hurts, but other input mistakes can cost much more.",2008,0,
1260,1261,On the design of fault-tolerant logical topologies in wavelength-routed packet networks,"In this paper, we present a new methodology for the design of fault-tolerant logical topologies in wavelength-routed optical networks supporting Internet protocol (IP) datagram flows. Our design approach generalizes the ""design protection"" concepts, and relies on the dynamic capabilities of IP to reroute datagrams when faults occur, thus achieving protection and restoration, and leading to high-performance cost-effective fault-tolerant logical topologies. In this paper, for the first time we consider resilience properties during the logical topology optimization process, thus extending the optimization of the network resilience also to the space of logical topologies. Numerical results clearly show that our approach outperforms previous ones, being able to obtain very effective survivable logical topologies with limited computational complexity.",2004,0,
1261,1262,Exploiting Parametric Power Supply and/or Temperature Variations to Improve Fault Tolerance in Digital Circuits,"The implementation of complex functionality in low-power nano-CMOS technologies leads to enhance susceptibility to parametric disturbances (environmental, and operation-dependent). The purpose of this paper is to present recent improvements on a methodology to exploit power-supply voltage and temperature variations in order to produce fault-tolerant structural solutions. First, the proposed methodology is reviewed, highlighting its characteristics and limitations. The underlying principle is to introduce on-line additional tolerance, by dynamically controlling the time of the clock edge trigger driving specific memory cells. Second, it is shown that the proposed methodology is still useful in the presence of process variations. Third, discussion and preliminary results on the automatic selection (at gate level) of critical FF for which DDB insertion should take place are presented. Finally, it is shown that parametric delay tolerance insertion does not necessarily reduce delay fault detection, as multi-vdd or multi-frequency self-test can be used to recover detection capability.",2008,0,
1262,1263,Comparing Fault-based Testing Strategies of General Boolean Specifications,Testing Boolean specifications in general form (GF) by the IDNF-oriented approaches always results in superabundant cost and missing detection of some faults. This paper proposes GF-oriented approaches to improve them. The experimental results show that the GF-oriented strategies could enhance the fault detection capability and reduce the sizes of test sets.,2007,0,
1263,1264,A fast algorithm for SVPWM in three phase power factor correction application,"A novel algorithm for space vector pulse width modulation in three phase power factor correction applications is proposed. The durations for the active vectors that formed the sector containing the desired reference voltage vector are calculated directly by matrix pre-decomposing, without looking up sinusoidal and tangential tables, based on TMS320F240. Therefore running speed and control precise of the program can be improved greatly. As a result the switching frequency and power density of the rectifier is increased considerably. A new method for detecting sector is given as well. Simulated and experimental results are provided to verify the proposed algorithm in the end of the paper.",2004,0,
1264,1265,Faults Coverage Improvement Based on Fault Simulation and Partial Duplication,"A method how to improve the coverage of single faults in combinational circuits is proposed. The method is based on Concurrent Error Detection, but uses a fault simulation to find Critical points - the places, where faults are difficult to detect. The partial duplication of the design with regard to these critical points is able to increase the faults coverage with a low area overhead cost. Due to higher fault coverage we can increase the dependability parameters. The proposed modification is tested on the railway station safety devices designs implemented in the FPGA.",2010,0,
1265,1266,Using Redundant Threads for Fault Tolerance of OpenMP Programs,"As the wide application of multi-core processor architecture in the domain of high performance computing, fault tolerance for shared memory parallel programs becomes a hot spot of research. For years, checkpointing has been the dominant fault tolerance technology in this field, and recently, many research works have been engaged with it. However, to those programs which deal with large amount of data, checkpointing may induce massive I/O transfer, which will adversely affect scalability. To deal with such a problem, this paper proposes a fault tolerance approach, making use of redundancy, for shared memory parallel programs. Our scheme avoids saving and restoring computational state during the program's execution, hence does not involve I/O operations, so presents explicit advantage over checkpointing in scalability. In this paper, we introduce our approach and the related compiler tool in detail, and give the experimental evaluation result.",2010,0,
1266,1267,A mixed-integer formulation for fault detection and diagnosis: modelling and an illustrative example,"In this paper, we propose a mixed integer optimization for diagnosis of fault events which changes the structure of the system model. The proposed approach aims it selecting the right model of the systems among a bank of models when some faults occur. This approach suits to the case where both abrupt faults (intermittent or permanent such as saturation or sudden shutdown) and incipient faults (continuous) are considered. The optimization helps finding the best combination of faults that have occurred. So doing, we go further than analyzing incidence matrix of residual. Also, this approach allows to introduces fault occurrence logics such as the ones encountered when establishing fault trees.",2002,0,
1267,1268,A Systematic Approach of Improving a Fault Tolerant Fuel Cell Bus Powertrain Safety Level,"This paper focuses on the safety issue of fuel cell bus powertrain system. It demonstrates a systematic way of evaluation and refinement of the safety degree to advance the confidence of a kind of fuel cell bus powertrain system, and meanwhile a specific approach of safety level evaluation is used. Via functionally summing up the potential faults which may appear and cause economic losses or occupant injury, the deficiency of the powertrain is located. Based on the occurrence, the severity and the detecting possibility, the numerical expressions of the faults property are measured. Through combining these evaluating results, a general safety level can be achieved, which directly reflects the inappropriateness that should be adjusted or redesigned. Continuous improvement, including using the passive methods of hardware redundancy and the active methods of fault diagnosis, detection and fault tolerant control in the electric control system is carried out, leading to a much more reliable system and the updated safety level shows the enhanced confidence of the fuel cell powertrain system. In order to get a more practical control system, a hardware in the loop test bench is set up to test the performance of the controllers, the core of the powertrain system, under different working conditions, especially those when faults appear. The actual execution of the approach shows that it is an objective analysis method for the innovative powertrain system and can help to improve the system reliability of the fuel cell bus systematically.",2006,0,
1268,1269,Error Estimation for the Computation of Force Using the Virtual Work Method on Finite Element Models,"In most finite element analysis, the numerical error range of the computed force must be specified. In this paper, an error estimation method for force computation using the virtual work method is presented. The merits of the proposed method include its simplicity and its applicability in force computation when the objects being studied are touching other objects.",2009,0,
1269,1270,Analytically redundant controllers for fault tolerance: Implementation with separation of concerns,"Diversity or redundancy based software fault tolerance encompasses the development of application domain specific variants and error detection mechanisms. In this regard, this paper presents an analytical design strategy to develop the variants for a fault tolerant real-time control system. This work also presents a generalized error detection mechanism based on the stability performance of a designed controller using the Lyapunov Stability Criterion. The diverse redundant fault tolerance is implemented with an aspect oriented compiler to separate and thus reduce this additional complexity. A Mathematical Model of an Inverted Pendulum System has been used as a case study to demonstrate the proposed design framework.",2010,0,
1270,1271,Spatial error concealment based on directional decision and intra prediction,"The paper presents a novel spatial error concealment algorithm based on directional decision and intra prediction. Unlike previous approaches that simultaneously recover the pixels inside a missing macroblock (MB), we propose to recover them 44 block by 44 block. Each missing 1616 MB in an intra frame is first divided into 16 blocks, each with size 44, then recovered block by block using Intra_44 prediction. Previously-recovered blocks can be used in the recovery process afterwards. The principle advantage of this approach is the improved capability of recovering MB with edges and the lower computational complexity. The proposed algorithm has been tested on the H.264/AVC reference software JM7.2. Experimental results demonstrate the advantage of the proposed method.",2005,0,
1271,1272,Research and Assessment of the Reliability of a Fault Tolerant Model Using AADL,"In order to solve the problem of the assessment of the reliability of the fault tolerant system, the work in this paper is devoted to analyze a subsystem of ATC (air traffic control system), and use AADL (architecture analysis and design language) to build its model. After describing the various software and hardware error states and as well as error propagation from hardware to software, the work builds the AADL error model and convert it to GSPN (general stochastic Petri net). Using current Petri Net technology to assess the reliability of the fault tolerant system which is based on ATC as the background, this paper receives good result of the experiment.",2008,0,
1272,1273,Scheduling and voltage scaling for energy/reliability trade-offs in fault-tolerant time-triggered embedded systems,"In this paper we present an approach to the scheduling and voltage scaling of low-power fault-tolerant hard real-time applications mapped on distributed heterogeneous embedded systems. Processes and messages are statically scheduled, and we use process re-execution for recovering from multiple transient faults. Addressing simultaneously energy and reliability is especially challenging because lowering the voltage to reduce the energy consumption has been shown to increase the transient fault rates. In addition, time-redundancy based fault-tolerance techniques such as re-execution and dynamic voltage scaling-based low-power techniques are competing for the slack in the schedules. Our approach decides the voltage levels and start times of processes and the transmission times of messages, such that the transient faults are tolerated, the timing constraints of the application are satisfied and the energy is minimized. We present a constraint logic programming-based approach which is able to find reliable and schedulable implementations within limited energy and hardware resources.",2007,0,
1273,1274,Effective Post-BIST Fault Diagnosis for Multiple Faults,"With the increasing complexity of LSI, built-in self test (BIST) is one of the promising techniques in the production test. From our observation during the manufacturing test, multiple stuck-at faults often exist in the failed chips during the yield ramp-up. Therefore the authors propose a method for diagnosing multiple stuck-at faults based on the compressed responses from BIST. The fault diagnosis based on the compressed responses from BIST was called the post-BIST fault diagnosis (Takahashi et al., 2005, Takamatsu, 2005). The efficiency on the success ratio and the feasibility of diagnosing large circuits are discussed. From the experimental results for ISCAS and STARC03 (Sato et al., 2005) benchmark circuits, it is clear that high success ratios that are about 98% are obtained by the proposed diagnosis method. From the experimental result for the large circuits with 100K gates, the feasibility of diagnosing the large circuits within the practical CPU times can be confirmed. The feasibility of diagnosing multiple stuck-at faults on the post-BIST fault diagnosis was proven",2006,0,
1274,1275,An Approach of Fault Diagnosis for System Based on Fuzzy Fault Tree,"Due to the complicacy of the communication control system (CCS) structure and the variations in operating conditions, the occurrence of a fault inside CCS is uncertain and random. Aiming at limitations of the current fault diagnosis of CCS, the paper presents an approach to fault diagnosis of fuzzy fusion based on fuzzy fault tree. Elaborated method considers the characteristic of diagnostic object to establish fuzzy fault tree, convert the index of fault rate into fuzzy number of fault rate, perform the fuzzy analysis for the fault tree, determine the confidence interval of probability of top event, and achieve fuzzy reasoning diagnosis result. The details of fuzzy number design are described in the paper and an application example of the method is also provided. The results show that the proposed fuzzy fault tree analysis method is effective and available for fault diagnose of CCS.",2008,0,
1275,1276,Sequential Element Design With Built-In Soft Error Resilience,"This paper presents a built-in soft error resilience (BISER) technique for correcting radiation-induced soft errors in latches and flip-flops. The presented error-correcting latch and flip-flop designs are power efficient, introduce minimal speed penalty, and employ reuse of on-chip scan design-for-testability and design-for-debug resources to minimize area overheads. Circuit simulations using a sub-90-nm technology show that the presented designs achieve more than a 20-fold reduction in cell-level soft error rate (SER). Fault injection experiments conducted on a microprocessor model further demonstrate that chip-level SER improvement is tunable by selective placement of the presented error-correcting designs. When coupled with error correction code to protect in-pipeline memories, the BISER flip-flop design improves chip-level SER by 10 times over an unprotected pipeline with the flip-flops contributing an extra 7-10.5% in power. When only soft errors in flips-flops are considered, the BISER technique improves chip-level SER by 10 times with an increased power of 10.3%. The error correction mechanism is configurable (i.e., can be turned on or off) which enables the use of the presented techniques for designs that can target multiple applications with a wide range of reliability requirements",2006,0,
1276,1277,Data hiding for error concealment in H.264/AVC,"Recently, data hiding has been proposed to improve the performance of error concealment algorithms. In this paper, a new data hiding-based error concealment algorithm is proposed, that allows the increase of video quality in H.264/AVC wireless video transmission and real-time applications. Data hiding is used for carrying to the decoder the values of some inner pixels to be used to reconstruct lost macro blocks into intra frames through a bi-linear interpolation process.",2004,0,
1277,1278,Mitigating Soft Errors in System-on-Chip Design,"With the continuous downscaling of CMOS technologies, the reliability has become a major bottleneck in the evolution of the next generation scaling. Technology trends such as transistor downsizing, use of new materials and high performance computer architecture continue to increase the sensitivity of systems to soft errors. Today the technologies are moving into the period of nanotechnologies and system-on-chip (SoC) designs are widely used in most of the applications, the issues of soft errors and reliability in complex SoC designs are set to become and increasingly challenging. This paper gives a review to the soft error in SoC designs and then presents the fault tolerant solution.",2008,0,
1278,1279,Evolution and Search Based Metrics to Improve Defects Prediction,"Testing activity is the most widely adopted practice to ensure software quality. Testing effort should be focused on defect prone and critical resources i.e., on resources highly coupled with other entities of the software application.In this paper, we used search based techniques to define software metrics accounting for the role a class plays in the class diagram and for its evolution over time. We applied Chidamber and Kemerer and the newly defined metrics to Rhino, a Java ECMA script interpreter, to predict version 1.6R5 defect prone classes. Preliminary results show that the new metrics favorably compare with traditional object oriented metrics.",2009,0,
1279,1280,"Cache and memory error detection, correction, and reduction techniques for terrestrial servers and workstations","As the size of the SRAM cache and DRAM memory grows in servers and workstations, cosmic-ray errors are becoming a major concern for systems designers and end users. Several techniques exist to detect and mitigate the occurrence of cosmic-ray upset, such as error detection, error correction, cache scrubbing, and array interleaving. This paper covers the tradeoffs of these techniques in terms of area, power, and performance penalties versus increased reliability. In most system applications, a combination of several techniques is required to meet the necessary reliability and data-integrity targets.",2005,0,
1280,1281,DSP-Based Sensorless Electric Motor Fault Diagnosis Tools for Electric and Hybrid Electric Vehicle Powertrain Applications,"The integrity of electric motors in work and passenger vehicles can best be maintained by frequently monitoring its condition. In this paper, a signal processing-based motor fault diagnosis scheme is presented in detail. The practicability and reliability of the proposed algorithm are tested on rotor asymmetry detection at zero speed, i.e., at startup and idle modes in the case of a vehicle. Regular rotor asymmetry tests are done when the motor is running at a certain speed under load with stationary current signal assumption. It is quite challenging to obtain these regular test conditions for long-enough periods of time during daily vehicle operations. In addition, automobile vibrations cause nonuniform air-gap motor operation, which directly affects the inductances of electric motors and results in a noisy current spectrum. Therefore, it is challenging to apply conventional rotor fault-detection methods while examining the condition of electric motors as part of the hybrid electric vehicle (HEV) powertrain. The proposed method overcomes the aforementioned problems by simply testing the rotor asymmetry at zero speed. This test can be achieved at startup or repeated during idle modes where the speed of the vehicle is zero. The proposed method can be implemented at no cost using the readily available electric motor inverter sensors and microprocessing unit. Induction motor fault signatures are experimentally tested online by employing the drive-embedded master processor (TMS320F2812 DSP) to prove the effectiveness of the proposed method.",2009,0,
1281,1282,Inverse Fault Detection and Diagnosis Problem in Discrete Dynamic Systems,This paper investigates an inverse fault detection and diagnosis problem in discrete dynamic systems. The problem is how to adjust the system parameters according to observation value of inputs and outputs so that the system is concordant. First we formulate the problem as a least square problem with interval coefficients. Then two algorithms for this problem are presented. The first algorithm based on the expected value of observation value of inputs and outputs. We are only required to solve a classical least square problem in this algorithm and the algorithm is robust. The second algorithm by using linear programming approach can deal with large scale systems and suit for on line adjustment.,2007,0,
1282,1283,Acceleration of a model based scatter correction technique for Positron Emission Tomography using high performance computing technique,"Positron Emission Tomography (PET) is a widely used and powerful metabolic imaging technique for functional diagnosis of organs. Compton scattering is a physical effect that results in distortions in the reconstructed image. The model based, so-called, Scatter Simulation (SSS) algorithm is an appropriate solution for scatter correction. However, the SSS algorithm is extremely computation intensive. The application of the SSS algorithm in clinical environment requires the application of high performance computing (HPC) techniques. In this work we give a survey about different high performance computing techniques and introduce the selection process of optimal HPC platform for the implementation of the Single Scatter Simulation algorithm.",2010,0,
1283,1284,What Makes a Good Bug Report?,"In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.",2010,0,
1284,1285,The Measure of human error: Direct and indirect performance shaping factors,"The goal of performance shaping factors (PSFs) is to provide measures to account for human performance. PSFs fall into two categoriesdirect and indirect measures of human performance. While some PSFs such as time to complete a task are directly measurable, other PSFs, such as fitness for duty, can only be measured indirectly through other measures and PSFs, such as through fatigue measures. This paper explores the role of direct and indirect measures in human reliability analysis (HRA) and the implications that measurement theory has on analyses and applications using PSFs. The paper concludes with suggestions for maximizing the reliability and validity of PSFs.",2007,0,
1285,1286,Fault-tolerant design of the IBM pSeries 690 system using POWER4 processor technology,"The POWER4-based p690 systems offer the highest performance of the IBM eServer pSeriesTM line of computers. Within the general-purpose UNIX server market, they also offer the highest levels of concurrent error detection, fault isolation, recovery, and availability. High availability is achieved by minimizing component failure rates through improvements in the base technology, and through design techniques that permit hard- and soft-failure detection, recovery, and isolation, repair deferral, and component replacement concurrent with system operation. In this paper, we discuss the fault-tolerant design techniques that were used for array, logic, storage, and I/O subsystems for the p690. We also present the diagnostic strategy, fault-isolation, and recovery techniques. New features such as POWER4 synchronous machine-check interrupt, PCI bus error recovery, array dynamic redundancy, and minimum-element dynamic reconfiguration are described. The design process used to verify error detecti on, fault isolation, and recovery is also described.",2002,0,
1286,1287,Error Resilient Coding Based on Reversible Data Embedding Technique for H. 264/AVC Video,"In this paper, a prescription-based error concealment (PEC) method is proposed. PEC relies on pre-analyses of the concealment error image (CEI) for I-frames and the optimal error concealment scheme for P-frames at encoder side. CEI is used to enhance the image quality at decoder side after error concealment (by spatial interpolation or zero motion) of the corrupted intra-coded MBs. A set of pre-selected error concealment methods is evaluated for each corrupted inter-coded MB to determine the optimal one for the decoder. Both the CEI and the scheme indices are considered as the prescriptions for decoder and transmitted along with the video bit stream based on a reversible data embedding technique. Experiments show that the proposed method is capable of achieving PSNR improvement of up to 1.48 dB, at a considerable bit-rate, when the packet loss rate is 20%",2005,0,
1287,1288,Emulation of faults and remedial control strategies in a multiphase power converter drive used to analyse fault tolerant drive systems for Aerospace applications,"This paper describes how an experimental test rig, a multiphase power converter drive and its control have been used to emulate failures (of the converter and machine) and control strategies to study a way of achieving fault tolerant drive systems employed especially in Aerospace applications. Experimental results which validate simulation of the emulated faults are presented.",2009,0,
1288,1289,Data diverse fault tolerant architecture for component based systems,"Of late, component based software design has become a major focus in software engineering research and computing practice. These software components are used in a wide range of applications some of which may have mission critical requirements. In order to achieve required level of reliability, these component-based designs have to incorporate special measures to cope up with software faults. This paper presents a fault tolerant component based data driven architecture that is based on C2 architectural framework and implements data diverse fault tolerance strategies. The proposed design makes a trade-off between platform flexibility, reliability and efficiency at run time and exhibits its ability to tolerate faults in a cost effective manner. Application of proposed design is exhibited with a case study.",2009,0,
1289,1290,Rail defect diagnosis using wavelet packet decomposition,"One of the basic tasks in railway maintenance is inspection of the rail in order to detect defects. Rail defects have different properties and are divided into various categories with regard to the type and position of defects on the rail. This paper presents an approach for the detection of defects in rail based on wavelet transformation. Multiresolution signal decomposition based on wavelet transform or wavelet packet provides a set of decomposed signals at distinct frequency bands, which contains independent dynamic information due to the orthogonality of wavelet functions. Wavelet transform and wavelet packet in tandem with various signal processing methods, such as autoregressive spectrum, energy monitoring, fractal dimension, etc., can produce desirable results for condition monitoring and fault diagnosis. Defect detection is based on decomposition of the signal acquired by means of magnetic coil and Hall sensors from the railroad rail, and then applying wavelet coefficients to the extracted signals. Comparing these extracted coefficients provides an indication of the healthy rail from defective rail. Experimental results are presented for healthy rail and some of the more common defects. Deviation of wavelet coefficients in the healthy rail case from the case with defects shows that it is possible to classify healthy rails from defective ones.",2003,0,
1290,1291,Communication strategy and fault-tolerance abilities development in bio-inspired hardware systems with FPGA-based artificial cell network,"The paper deals through computer-aided modeling, numerical simulation and experimental research with the bio-inspired digital systems, in order to implement VLSI hardware which exhibits the abilities of living organisms, such as: evolution capabilities, self-healing and fault-tolerance. The theoretical backgrounds of the work are founded in cellular embryology's basic concepts. In the first stage of the researches a new model for an FPGA-based artificial cell is proposed and developed. Also a new communication strategy inside the cell networks is presented, in order to reproduce with high fidelity the complex phenomena and interaction rules in bio-inspired hardware systems. In the next steps the fault-tolerance and self-healing phenomena between these cells in a bi- dimensional structure is careful analyzed and simulated. The final purpose is to design a bio-inspired hardware system (embryonic machine) with programmable FPGA arrays, for study and experiment basic properties of living organisms.",2008,0,
1291,1292,Novel Convergence Model for Efficient Error Concealment using Information Hiding in Multimedia Streams,Error concealment using information hiding has been an efficient tool to combat channel impairments that degrade the transmitted data quality by introducing channel errors/packet losses. The proposed model takes a stream of multimedia content and the binarised stream is subjected to bit level enhanced mapping procedure (PRASAN - Enhanced NFD approach) accompanied with a set of convergence models that ensure a high degree of convergence for a given error norm. The mapping is performed between the current frames with respect to the previous frame in case of video data. This approach often referred to as the correlation generation is followed by convergence mathematical function generation. This function is derived based on trying out the various convergence methodologies in a weighted round robin environment and choosing the best matching function by computing the mean square error. This error is termed map-fault and is kept a minimum. The test data taken are subjected to noisy channel environments and the power signal to noise ratios obtained experimentally support firmly the advantage of the proposed methodology in comparison to existing approaches,2007,0,
1292,1293,A Pervasive Temporal Error Concealment Algorithm for H.264/AVC Decoder,"In real-time video transmission over error-prone network, the packet loss cannot be avoided which causes the video quality reduction in the destination. In this paper, we propose a pervasive temporal error concealment (PTEC) algorithm for H.264/AVC Inter frame decoding to eliminate the error effect for Human Visual System (HVS). To increase the error concealment (EC) accuracy, the 4x4 block size is used as the basic motion vector (MV) recovery unit, and the MV of the lost macroblock (MB) is recovered by employing the MV information of the neighboring intact MBs. The simulation results show that the proposed algorithm can achieve better EC performance compared with the existing TEC methods. Because of its simple composition, it is pervasive to be used in the real-time multimedia communication systems with the video coding standard H.264/AVC.",2010,0,
1293,1294,Bug-Inducing Language Constructs,"Reducing bugs in software is a key issue in software development. Many techniques and tools have been developed to automatically identify bugs. These techniques vary in their complexity, accuracy and cost. In this paper we empirically investigate the language constructs which frequently contribute to bugs. Revision histories of eight open source projects developed in multiple languages are processed to extract bug-inducing language constructs. Twenty six different language constructs and syntax elements are identified. We find that most frequent bug-inducing language constructs are function calls, assignments, conditions, pointers, use of NULL, variable declaration, function declaration and return statement. These language constructs account for more than 70 percent of bug-inducing hunks. Different projects are statistically correlated in terms of frequencies of bug-inducing language constructs. Developers within a project and between different projects also have similar frequencies of bug-inducing language constructs. Quality assurance developers can focus code reviews on these frequent bug-inducing language constructs before committing changes.",2009,0,
1294,1295,Transient stability prediction algorithm based on post-fault recovery voltage measurements,This paper presents a novel technique for predicting transient stability status of a power system following a large disturbance. The prediction is based on the synchronously measured samples of the fundamental frequency voltage magnitudes at each generation station. The voltage samples taken immediately after clearing the faults are input to a support vector machine classifier to identify the transient stability condition. The classifier is trained using examples of the post-fault recovery voltage measurements (inputs) and the corresponding stability status (output) determined using a power angle-based stability index. Studies with the New England 39-bus system indicate that the proposed algorithm can correctly recognize when the power system is approaching to the transient instability.,2009,0,
1295,1296,Fast vignetting correction and color matching for panoramic image stitching,"When images are stitched together to form a panorama there is often color mismatch between the source images due to vignetting and differences in exposure and white balance between images. In this paper a low complexity method is proposed to correct vignetting and differences in color between images, producing panoramas that look consistent across all source images. Unlike most previous methods which require complex non-linear optimization to solve for correction parameters, our method requires only linear regressions with a low number of parameters, resulting in a fast, computationally efficient method. Experimental results show the proposed method effectively removes vignetting effects and produces images that are highly visually consistent in color and brightness.",2009,0,
1296,1297,A generic method for fault injection in circuits,"Microcircuits dedicated to security in smartcards are targeted by more and more sophisticated attacks like fault attacks that combine physical disturbance and cryptanalysis. The use of simulation for circuit validation considering these attacks is limited by the time needed to compute the result of the chosen fault injections. Usually, this choice is made by the user according to his knowledge of the circuit functionality. The aim of this paper is to propose a generic and semi-automatic method to reduce the number of fault injections using types of data stored in registers (latch by latch)",2006,0,
1297,1298,Scheduling for energy efficiency and fault tolerance in hard real-time systems,"This paper studies the dilemma between fault tolerance and energy efficiency in frame-based real-time systems. Given a set of K tasks to be executed on a system that supports L voltage levels, the proposed heuristic-based scheduling technique minimizes the energy consumption of tasks execution when faults are absent, and preserves feasibility under the worst case of fault occurrences. The proposed technique first finds out the optimal solution in a comparable system that supports continuous voltage scaling, then converts the solution to the original system. The runtime complexity is only (LK<sup>2</sup>). Experimental results show that the proposed approach produces near-optimal results in polynomial time.",2010,0,
1298,1299,Principles of multi-level reflection for fault tolerant architectures,"We present the principles of multi-level reflection as an enabling technology for the design and implementation of adaptive fault tolerant systems. By exhibiting the structural and behavioral aspects of a software component, the reflection paradigm enables the design and implementation of appropriate non-functional mechanisms at a meta-level. The separation of concerns provided by reflective architectures makes reflection a perfect match for fault tolerance mechanisms. However, in order to provide the necessary and sufficient information for error detection and recovery, reflection must be applied to all system layers in an orthogonal manner. This is the main motivation behind the notion of multi-level reflection that is introduced. We describe the basic concepts of this new architectural paradigm, and illustrate them with concrete examples. We also discuss some practical work that has recently been carried out to start implementing the proposed framework.",2002,0,
1299,1300,Fault Behavior in Fire Control System Based on Extendable Petri Net,"Use the Petri net theory and studied out a synthetical expression method of static state and trends behavior in fire control system fault diagnose. The method may be convenient and succinct for fault phenomenon of multilayer rank order and many route. Amount of work is simplified in the fault diagnose system. The fault information characteristic, fault information molde and propagation behavior fashion are expounded. The algorithm for system fault information characteristic is put. The Petri net molde of fault diagnose administrative levels and mixed fault is set up. The fault information propagation route is gained in system analysis. The trends transition of fault behavior is analyzed, to find the solution of system the smallest cut set and path.",2010,0,
1300,1301,The impact of technology scaling on soft error rate performance and limits to the efficacy of error correction,The soft error rate (SER) of advanced CMOS devices is higher than all other reliability mechanisms combined. Memories can be protected with error correction circuitry but SER in logic may limit future product reliability. Memory and logic scaling trends are discussed along with a method for determining logic SER.,2002,0,
1301,1302,Using regression trees to classify fault-prone software modules,"Software faults are defects in software modules that might cause failures. Software developers tend to focus on faults, because they are closely related to the amount of rework necessary to prevent future operational software failures. The goal of this paper is to predict which modules are fault-prone and to do it early enough in the life cycle to be useful to developers. A regression tree is an algorithm represented by an abstract tree, where the response variable is a real quantity. Software modules are classified as fault-prone or not, by comparing the predicted value to a threshold. A classification rule is proposed that allows one to choose a preferred balance between the two types of misclassification rates. A case study of a very large telecommunications systems considered software modules to be fault-prone, if any faults were discovered by customers. Our research shows that classifying fault-prone modules with regression trees and the using the classification rule in this paper, resulted in predictions with satisfactory accuracy and robustness.",2002,0,
1302,1303,An RT-level fault model with high gate level correlation,"With the advent of new RT-level design and test flows, new tools are needed to migrate at the RT-level the activities of fault simulation testability analysis, and test pattern generation. This paper focuses on fault simulation at the RT-level, and aims at exploiting the capabilities of VHDL simulators to compute faulty responses. The simulator was implemented as a phototypical tool, and experimental results show that simulation of a faulty circuit is no more costly than simulation of the original circuit. The reliability of the fault coverage figures computed at the RT-level is increased thanks to an analysis of inherent VHDL redundancies, and by foreseeing classical synthesis optimizations. A set of rules is used to compute a fault list that exhibits good correlation with stuck-at faults",2000,0,
1303,1304,Spatial Optical Distortion Correction in an FPGA,"Due to the complexities of the image processing algorithms, correcting spatial distortion of optical images quickly and efficiently is a major challenge. This paper describes an efficient pipelined parallel architecture for optical distortion correction in imaging systems using a low cost FPGA device. The proposed architecture produces a fast, almost realtime solution for the correction of image distortion implemented using VHDL HDL with a single Xilinx FPGA XCS31000-4 device. The experimental results show that the barrel and pincushion distortion can be corrected with a very low residual error. The system architecture can be applied to other imaging processing algorithms in optical systems",2006,0,
1304,1305,A new algorithm for atmospheric correction of the multiangular and hyperspectral data acquired during the DAISEX campaign,"The main scientific objective of DAISEX (Digital Airborne Spectrometer Experiment) was to demonstrate the retrieval of geo/biophysical variables from imaging spectrometer data. Target variables included surface temperature, Leaf Area Index (LAI), canopy biomass, leaf water content, canopy height, canopy structure and soil properties. The imaging spectrometers used for DAISEX were the DAIS-7915, HyMap and POLDER. The campaign took place during the summers of 1998, 1999 and 2000 in Barrax (Spain) and Colmar (France). A new algorithm is under development for the atmospheric correction of the hyperspectral and multiangular data acquired during this campaign. This algorithm is intended to improve the current atmospheric correction by taking into account the coupling between atmosphere and surface (including a non-Lambertian treatment of the latter). Moreover, the hyperspectral data allow to characterise in detail the absorption and the multiangular characteristics of the data allow to describe accurately the aerosol scattering. The method consists in identifying some pixels on an image with an a priori information about its BRDF and assume that the atmosphere is the same over the whole image. Applying a radiative transfer code we can reproduce the reflectance measured by the sensor by modifying the parameters describing the surface and the aerosols through an iterative process. Once the atmosphere is known the system atmosphere-surface is uncoupled and the reflectance for the whole image can be obtained.",2003,0,
1305,1306,Design by extrapolation: an evaluation of fault-tolerant avionics,"Over the past 30 years, safety-critical avionics systems such as fly-by-wire (FBW) flight controls, full-authority digital engine controls, and other systems have been introduced on many commercial and military airplanes and spacecraft. Early FBW systems, such as on the F-16 and Airbus A320, were considered revolutionary and were introduced with extreme caution. These early systems and their successors all make use of redundant and fault-tolerant avionics to provide the required dependability and safety, but have used significantly different architectures. This paper examines the different levels of criticality and fault tolerance required by different types of avionics systems, establishes architectural categories of fault-tolerant architectures, and identifies the discriminating features of the different approaches. Examples of discriminators include the level of redundancy, methods of engaging backup systems, protection from software errors, and the use of dissimilar hardware and software. The strengths and weaknesses of the different approaches are identified. The paper concludes with some speculation on trends for future systems based on this evaluation of previous systems",2001,0,
1306,1307,Do stack traces help developers fix bugs?,"A widely shared belief in the software engineering community is that stack traces are much sought after by developers to support them in debugging. But limited empirical evidence is available to confirm the value of stack traces to developers. In this paper, we seek to provide such evidence by conducting an empirical study on the usage of stack traces by developers from the ECLIPSE project. Our results provide strong evidence to this effect and also throws light on some of the patterns in bug fixing using stack traces. We expect the findings of our study to further emphasize the importance of adding stack traces to bug reports and that in the future, software vendors will provide more support in their products to help general users make such information available when filing bug reports.",2010,0,
1307,1308,"Using composition to design secure, fault-tolerant systems","Complex systems must be analyzed in smaller pieces. Analysis must support both bottom-up (composition) and top-down (refinement) development, and it must support the consideration of several critical properties, e.g., functional correctness, fault tolerance and security, as appropriate. We describe a mathematical framework for performing composition and refinement analysis and discuss some lessons learned from its application. The framework is written and verified in PVS",2000,0,
1308,1309,Emission-based scatter correction in SPECT imaging,"Scatter correction in single photon emission computed tomography (SPECT) has been focused on either using multiple-window acquisition technique or the scatter modeling technique in iterative image re-construction. We propose a technique that uses only the emission data for scatter correction in SPECT. We assume that the scatter data can be approximated by convolving the primary data with a scatter kernel followed by the normalization using the scatter-to-primary ratio (SPR). Since the emission data is the superposition of the primary data and the scatter data, the convolution normalization process approximately results in the sum of the scatter data and a convolved version of scatter data with the kernel. By applying a proper scaling factor, we can make the estimation approximately equal to or less than the scatter data anywhere in the projection domain. Phantom and patient cardiac SPECT studies show that using the proposed emission-based scatter estimation can effectively reduce the scatter-introduced background in the reconstructed images. And additionally, the computational time for scatter correction is negligible as compared to no scatter correction in iterative image reconstruction.",2010,0,
1309,1310,Bio - Inspired & Traditional Approaches to Obtain Fault Tolerance,"Applying some observable phenomena from cells, focused on their organization, function, control and healing mechanisms, a simple fault tolerant implementation can be obtained. Traditionally, fault tolerance has been added explicitly to a system by including redundant hardware and/or software, which takes over when an error has been detected. These concepts and ideas have been applied before with the triple modular redundancy. Our approach is to design systems where redundancy was incorporated implicitly into the hardware and to mix bio-inspired and traditional approaches to deal with fault tolerance. These ideas are shown using a discrete cosine transform (application) as organ, its MAC (function) interconnected as cell and parity redundancy checker (error detector) as immune system to obtain a fault tolerance design",2006,0,
1310,1311,Comparison of worst case errors in linear and neural network approximation,Sets of multivariable functions are described for which worst case errors in linear approximation are larger than those in approximation by neural networks. A theoretical framework for such a description is developed in the context of nonlinear approximation by fixed versus variable basis functions. Comparisons of approximation rates are formulated in terms of certain norms tailored to sets of basis functions. The results are applied to perceptron networks,2002,0,
1311,1312,Application of multi-agent in control and fault diagnosis systems,"Multi-agent system with distributed structure is an important research field in intelligent control and fault diagnosis system. Based on the research of cooperation and coordination function of multi-agent, a systematic structure which integrates control, diagnosis and monitoring is established and those models, cooperation strategy, reasoning machine are also designed. This new decentralization system has been successfully used in actual product line, and provides a new way for industrial control problem.",2004,0,
1312,1313,Model-based information extraction method tolerant of OCR errors for document images,"A new method for information extraction from document images is proposed in this paper as the basis for a document reader which can extract required keywords and their logical relationship from various printed documents. Such documents obtained from OCR results may have not only unknown words and compound words, but also incorrect words due to OCR errors. To cope with OCR errors, the proposed method adopts robust keyword matching which searches for a string pattern from two dimensional OCR results consisting of a set of possible character candidates. This keyword matching uses a keyword dictionary that includes incorrect words with typical OCR errors and segments of words to deal with the above difficulties. After keyword matching, a global document matching is carried out between keyword matching results in an input document and document models which consist of keyword models and their logical relationship. This global matching determines the most suitable model for the input document and solves word segmentation problems accurately even if the document has unknown words, compound words, or incorrect words. Experimental results obtained for 100 documents show that the method is robust and effective for various document structures",2001,0,
1313,1314,Notice of Retraction<BR>An intelligent method for the control of magnitude of parabolic-like transmission error of a pair of gears,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Traditionally, the control of magnitude of parabolic-like transmission error of a pair of gears requires a lot of time-consuming trial-and-error manual procedures. To advance design efficiency, this paper has proposed an intelligent method to control efficiently and accurately the magnitude of parabolic-like transmission error. The intelligent method is devised based on the development of a system of governing equations under the conditions of tooth contact and constraints for magnitude of parabolic-like transmission error. Design parameters required to be determined are transformed into the roots of the system of equations. Just applying Newton's root finding method, parameters needed to be designed are obtained automatically and efficiently. To show how to apply the proposed method, a pair of external gears composed of an involute gear and a circular-arc gear is adopted to be an example. The gear pair is verified numerically the magnitude of parabolic-like transmission error is really controlled.",2010,0,
1314,1315,Detection and Classification of Wood Defects by ANN,"X-ray as a method of measurement was adopted to detect wood defects nondestructively. Due to the intensity of x-ray that crosses the object changes, defects in wood were detected by the difference of X-ray absorption parameter, and therefore it used computer to process and analyze the image. On the basis of image processing of nondestructive testing and characteristic construction, defects mathematic model were established by using characteristic parameters. According to signal characters of nondestructive testing, artificial neural networks were set up. Meanwhile, adopt BP networks model to recognize all characteristic parameters, which reflected characters of wood defects. BP networks used coefficient matrix of each unit, including input layer, intermediate layer (concealed layer) and output layer, to get the model of input vector and finish networks recognition through the networks learning. The test results show that the method is very successful for detection and classification of wood defects",2006,0,
1315,1316,Fault isolation in discrete event systems by observational abstraction,"We propose a method for fault isolation in discrete event systems such as object oriented control systems, where the observations are the logged error messages. The method is based on automatic abstraction that preserves only the behavior relevant to fault isolation. In this way we avoid the state space explosion, and a model checker can be used to reason about the temporal properties of the system. The result is a fault isolation table that maps possible error logs to isolated faults, and fault isolation thus reduces to table lookup. The fault isolation table can also be used as an analysis tool at the design level to find both faults that cannot be isolated as well as redundant error messages.",2003,0,
1316,1317,FIONA: a fault injector for dependability evaluation of Java-based network applications,"The use of network applications for high availability systems requires the validation of its fault tolerance mechanisms to avoid unexpected behavior during execution. FIONA is a fault injection tool to experimentally validate these mechanisms of Java distributed applications. The tool uses JVMTI, a new interface for the development of debugging and monitoring tools that enables the instrumentation of Java applications. This approach provides complete transparency between the application under test and the fault injection tool, as well as portability. FIONA injects communication faults, making it possible to conduct the dependability evaluation of UDP based network protocols developed in Java.",2004,0,
1317,1318,Neural vision sensors for surface defect detection,"Vision sensors are built from a camera and intelligent hardware and/or software. Steadily decreasing microelectronic costs have spawned a large number of vision sensory applications, such as surface defect detection. A constructive method for defect detection entails a mixture of mathematical and intelligent modules. Such a heterogeneous modular system can be realized in many ways. In this paper we discuss a packet-switched implementation on a macro-enriched field-programmable gate-array.",2004,0,
1318,1319,Analysis of Error Sources Towards Improved Form Processing,"Automatic form processing is an important application of document analysis subject. Such a system requires to be trained and tested on a standard database of forms collected from real-life. However, to the best of our knowledge, the only such available databases are NIST Special Databases. These databases consist of images of synthesized form documents. On the other hand, recently we developed a form database, samples of which had been taken from the real-life. ISIFormReader, a form processing system, also developed recently, has been tested using these real-life samples. An intensive study of the processing errors showed that writers' idiosyncracies are one of the major reasons of such errors as analyzed in U. Bhattacharya, et al., (2006). In the present paper, we investigated various other sources of errors which together cause a major concern. These include sample forms which are low in contrast, noisy, smudgy, skewed, scaled disturbing its aspect ratio and so on. An analysis of errors due to similar such sources is important towards development of an improved form processing system.",2006,0,
1319,1320,A genetic algorithm for automated horizon correlation across faults in seismic images,"Finding corresponding seismic horizons which have been separated by a fault is still performed manually in geological interpretation of seismic images. The difficulties of automating this task are due to the small amount of local information typical for those images, resulting in a high degree of interpretation uncertainty. Our approach is based on a model consisting of geological and geometrical knowledge in order to support the low-level image information. Finding the geologically most probable matches of several horizons across a fault is a combinatorial optimization problem, which cannot be solved exhaustively since the number of combinations increases exponentially with the number of horizons. A genetic algorithm (GA) has been chosen as the most appropriate strategy to solve the optimization problem. Our implementation of a GA is adapted to this particular problem by introducing geological knowledge into the solution process. The results verify the suitability of the method and the appropriateness of the parameters chosen for the horizon correlation problem.",2005,0,
1320,1321,Fast and Accurate Automatic Defect CLuster Extraction for Semiconductor Wafers,"Reduction in integrated circuit (IC) half technology, which will no longer be sustainable by traditional fault isolation and failure analysis techniques. There is an urgent need for diagnostic software tools with (which manifest as clusters) observed from manufacturing defects can be traced back to a specific process, equipment or technology, a novel data mining algorithm defects from test data logs. This algorithm and provides accurate detection of 99%.",2010,0,
1321,1322,SDG-based fault diagnosis and application based on reasoning method of granular computing,"Signed directed graph (SDG) as a qualitative model is used in fault diagnosis, because it can express causal relationships among variables of large-scale complex industry systems. However, many relevant results are included in diagnostic conclusions leading to low resolution in based-SDG fault diagnosis, so to solve this problem, granule is used to formally express the elements of SDG model in this paper, after that granular base containing knowledge which reflects the causal relation of faults and symptoms is constructed. And a searching and reasoning method based on granule is used in searching of fault source, consequently fault source is obtained by searching granular base and computing the most similarity. So the resolution could be improved. A 65t/h steam boiler system is taken as an example in the paper, and its answer show the method is feasible.",2010,0,
1322,1323,Skew Detection and Correction Method of Fabric Images Based on Hough Transform,"To solve the skew situation of scanned fabric image, a method based on Hough transform for skew detection and correction in fabric images was presented. By combining the characteristics of fabric images and the weft direction information extracted by Sobel operator, this method performed hierarchical Hough transform on the weft boundary to detect the skew angle of fabric image. Finally, a rotation algorithm based on the image linear storage structure was introduced, and the skew image was corrected rapidly. The skew detect algorithm has been experimented on various skew angles of fabric image and very promising results have been achieved given more than 99% accuracy. Experimental results show that the proposed method with high adaptability is more accurate and rapidly than traditional Hough transform.",2009,0,
1323,1324,Fiber Optical Gyro Fault Diagnosis based on Wavelet Transform and Neural Network,"Fault diagnosis plays an important role in detecting the reliability of integrated navigation. This paper proposed an intelligent method which combined wavelet transform with neural network to enhance efficiency. The combined method between wavelet transform and neural network was in series. Based on Daubechies, wavelet symmetry had been constructed. Through Daubechies eight-layer wavelet decomposing, detailed information of eight layers was achieved. Then the 8-dimensional eigenvector was used to train three-layer RBF neural network as fault sample. For RBF network is good at classifying, the network can detect a fault on-line after training. At the same time, it can classify faults and alarm. Gyro signals were chosen as the simulation inputs, the results indicated the method's applicability and effectiveness.",2008,0,
1324,1325,Development of customized distribution automation system (DAS) for secure fault isolation in low voltage distribution system,"This paper presents the development of customized distribution automation system (DAS) for secure fault isolation at the low voltage (LV) down stream, 415/240 V by using the Tenaga Nasional Berhad (TNB) distribution system. It is the first DAS research work done on customer side substation for operating and controlling between the consumer side system and the substation in an automated manner. Most of the work is focused on developing very secure fault isolation whereby the fault is detected, identified, isolated and remedied in few seconds. Supervisory Control and Data Acquisition (SCADA) techniques has been utilized to build Human Machine Interface (HMI) that provides a graphical operator interface functions to monitor and control the system. Microprocessor based Remote Monitoring Devices have been used for customized software to be downloaded to the hardware. Power Line Carrier (PLC) has been used as communication media between the consumer and the substation. As result, complete DAS fault isolation system has been developed for cost reduction, maintenance time saving and less human intervention during faults.",2008,0,
1325,1326,"Faults identification, location and characterization in electrical systems using an analytical model-based approach","The start of the electrical energy market has encouraged distributors to make new investments at distribution level so as to attain higher quality levels. The service continuity is one of the aspects of greater importance in the definition of the quality of the electrical energy. For this reason, the research in the field of fault diagnostic for distribution systems is spreading ever more. This paper presents a novel methodology to identify, locate and characterize the faulty events in the electrical distribution systems. The methodology can be extended to all types of faulty events and is applicable to reconfigurable systems. After having described the guidelines of the methodology, the Authors describe the architecture of the diagnostic control system implementing the analytic procedure. Finally the results of some relevant applications are reported.",2005,0,
1326,1327,A length compensation method to eliminate the varying length defect in one dimensional fisheye views,"Graphical fisheye view is an effective technique for visualizing and navigating large information structures. However, there are still technical difficulties that hinder its broader applications. One of the prominent problems is the varying length effect seen in most fisheye views. The varying length effect refers to a phenomenon that the length (or height) of a fisheye component is not fixed, but it varies with the location of the focal point. This effect may bring some disadvantages that reduce the usability of a fisheye component. To overcome this defect, sporadical solutions have been seen for specific implementations, but a systematic method has not been seen yet. This paper proposes a length compensation method to eliminate the varying length defect for one dimensional fisheye components. The method provides solutions for handling both discrete and continuous magnifications respectively. The mathematical foundation of the method is given, and the implemented prototype proves that it is effective.",2010,0,
1327,1328,Optimizing Cauchy Reed-Solomon Codes for Fault-Tolerant Network Storage Applications,"In the past few years, all manner of storage applications, ranging from disk array systems to distributed and wide-area systems, have started to grapple with the reality of tolerating multiple simultaneous failures of storage nodes. Unlike the single failure case, which is optimally handled with RAID level-5 parity, the multiple failure case is more difficult because optimal general purpose strategies are not yet known. Erasure coding is the field of research that deals with these strategies, and this field has blossomed in recent years. Despite this research, the decades-old Reed-Solomon erasure code remains the only space-optimal (MDS) code for all but the smallest storage systems. The best performing implementations of Reed-Solomon coding employ a variant called Cauchy Reed-Solomon coding, developed in the mid 1990's. In this paper, we present an improvement to Cauchy Reed-Solomon coding that is based on optimizing the Cauchy distribution matrix. We detail an algorithm for generating good matrices and then evaluate the performance of encoding using all implementations Reed-Solomon codes, plus the best MDS codes from the literature. The improvements over the original Cauchy Reed-Solomon codes are as much as 83% in realistic scenarios, and average roughly 10% over all cases that we tested",2006,0,
1328,1329,Software Fault Protection with ARINC 653,"With flight software becoming ever more complex, assuming that it behaves perfectly is no longer realistic. At the same time Verification and Validation (V&V) is consuming up to 50% of flight software development costs. The adaptation of fault protection concepts to flight software is attractive, particularly in the context of the fault containment and health management capabilities of ARINC 653. We propose a proactive, unified, model-based approach in which the behavior of the software is monitored against a model of its expected behavior. We describe how that may be incorporated into the ARINC 653 health management architecture. We describe software capabilities that facilitate software fault protection. These capabilities include enhancements to the ARINC 653 application executive, tools for software instrumentation, and a temporal logic runtime monitoring framework for high-level specification and monitoring. We analyze the aspects of the software that should be modeled and the types of failure responses. We show how these concepts may be applied to the Mission Data System (MDS) flight software framework.",2007,0,
1329,1330,Fractal-ANN Tool for Classification of Impulse Faults in Transformers,"Transformers are impulse tested in laboratory to assess their insulation strength against atmospheric lightning strikes. Inadequate insulation may cause transformer winding to fail during such tests. Detection of such faults is an important issue for repair and maintenance of such transformers. This paper describes the application of the concept of fractal geometry to obtain the features inherent in the impulse response of transformers subjected to impulse test. Fractal features such as fractal dimension (calculated by Higuchi, Kartz, Petrosian and Box counting methods), lacunarity and entropy has been used for collection of proper features from the current waveforms. Artificial Neural Network (ANN) has been used to classify the patterns inherent in the features extracted from Fractal analysis. The complex nature of transformer winding and its impulse response gives rise to a complex non-linear pattern of fractal features. In this regard, the application of ANN for pattern classification has greatly reduced the complexity and at the same time increased the accuracy in the fault localization and identification system. The proposed tool has been tested to identify the type and location of faults by analyzing experimental impulse responses of analog and digital transformer models.",2005,0,
1330,1331,Automated Diagnosis of Product-Line Configuration Errors in Feature Models,"Feature models are widely used to model software product-line (SPL) variability. SPL variants are configured by selecting feature sets that satisfy feature model constraints. Configuration of large feature models can involve multiple stages and participants, which makes it hard to avoid conflicts and errors. New techniques are therefore needed to debug invalid configurations and derive the minimal set of changes to fix flawed configurations. This paper provides three contributions to debugging feature model configurations: (1) we present a technique for transforming a flawed feature model configuration into a constraint satisfaction problem (CSP) and show how a constraint solver can derive the minimal set of feature selection changes to fix an invalid configuration, (2) we show how this diagnosis CSP can automatically resolve conflicts between configuration participant decisions, and (3) we present experiment results that evaluate our technique. These results show that our technique scales to models with over 5,000 features, which is well beyond the size used to validate other automated techniques.",2008,0,
1331,1332,A COTS wrapping toolkit for fault tolerant applications under Windows NT,"The paper presents a software toolkit that allows one to enhance the fault tolerant characteristics of a user application running under a Windows NT platform through sets of interchangeable and customizable fault tolerant interposition agents (FTI agents). Interposition agents are non-application software programs executed in an intermediate layer between the software application and the operating system in order to wrap the application software, intercepting and possibly modifying all the communications between the application and the surrounding hardware and software environment. The process is completely transparent to both the user application and the operating system and allows the achievement of a high degree of software based reliability in a wide variety of domains",2000,0,
1332,1333,Implementation of a quantum corrections in a 3D parallel drift-diffusion simulator,"We describe an implementation of density-gradient quantum corrections in a 3D drift-diffusion (D-D) semiconductor simulator based on finite element method. Mesh efficiency of the 3D semiconductor device simulator with quantum mechanical corrections is achieved by parallelisation of the code for a memory distributed multiprocessor environment. The Poisson equation, the current continuity equation, and the density gradient equation with an appropriate finite element discretisation have to be solved iteratively. Moreover, parallel algorithms are employed to speed up the self-consistent solution. In order to test our 3D semiconductor device simulator, we have carried out a careful calibration against experimental I-V characteristics of a 67 nm Si MOSFET achieving an excellent agreement. Then we demonstrate a relative impact of quantum mechanical corrections in this device.",2007,0,
1333,1334,Error resilient H.264/AVC video over satellite for low packet loss rates,"The performance of video over satellite is simulated. The error resilience tools of intra macroblock refresh and slicing are optimized for live broadcast video over satellite. The improved performance using feedback, using a cross- layer approach, over the satellite link is also simulated. The new Inmarsat BGAN system at 256 kbit/s is used as test case. This systems operates at low loss rates guaranteeing a packet loss rate of not more than 10~3. For high-end applications as 'reporter-in-the-field' live broadcast, it is crucial to obtain high quality without increasing delay.",2007,0,
1334,1335,An effective eye gaze correction operation for video conference using antirotation formulas,"The deviated eye gaze problem in video conferencing has been known and studied for many years. This paper suggests a simple and novel approach for face reorientation in a monocular setting, which is done by performing a combined rotation and antirotation operation on the image. Our approach on face reorientation does not require 3D modeling, registration or texture mapping. It is simple, efficient and robust. To make the eye gaze correction complete, an image warping technique is used for modifying eyelids and a simple transformation is used for correcting eye glares.",2003,0,
1335,1336,Automatic Identification of Bug-Introducing Changes,"Bug-fixes are widely used for predicting bugs or finding risky parts of software. However, a bug-fix does not contain information about the change that initially introduced a bug. Such bug-introducing changes can help identify important properties of software bugs such as correlated factors or causalities. For example, they reveal which developers or what kinds of source code changes introduce more bugs. In contrast to bug-fixes that are relatively easy to obtain, the extraction of bug-introducing changes is challenging. In this paper, we present algorithms to automatically and accurately identify bug-introducing changes. We remove false positives and false negatives by using annotation graphs, by ignoring non-semantic source code changes, and outlier fixes. Additionally, we validated that the fixes we used are true fixes by a manual inspection. Altogether, our algorithms can remove about 38%~51% of false positives and 14%~15% of false negatives compared to the previous algorithm. Finally, we show applications of bug-introducing changes that demonstrate their value for research",2006,0,
1336,1337,Taming coincidental correctness: Coverage refinement with context patterns to improve fault localization,"Recent techniques for fault localization leverage code coverage to address the high cost problem of debugging. These techniques exploit the correlations between program failures and the coverage of program entities as the clue in locating faults. Experimental evidence shows that the effectiveness of these techniques can be affected adversely by coincidental correctness, which occurs when a fault is executed but no failure is detected. In this paper, we propose an approach to address this problem. We refine code coverage of test runs using control- and data-flow patterns prescribed by different fault types. We conjecture that this extra information, which we call context patterns, can strengthen the correlations between program failures and the coverage of faulty program entities, making it easier for fault localization techniques to locate the faults. To evaluate the proposed approach, we have conducted a mutation analysis on three real world programs and cross-validated the results with real faults. The experimental results consistently show that coverage refinement is effective in easing the coincidental correctness problem in fault localization techniques.",2009,0,
1337,1338,Automated fault location system for primary distribution networks,"This paper presents the development, simulation results, and field tests of an automated fault location system for primary distribution networks. This fault location system is able to identify the most probable fault locations in a fast and accurate way. It is based on measurements provided by intelligent electronic devices (IEDs) with built-in oscillography function, installed only at the substation level, and on a database that stores information about the network topology and its electrical parameters. Simulations evaluate the accuracy of the proposed system and the experimental results come from a prototype installation.",2005,0,
1338,1339,Soft error considerations for computer web servers,"Soft errors are caused by cosmic rays striking sensitive regions in electronic devices. Termed as single event upset (SEU), in the past this phenomenon mostly affected the high altitude systems or avionics. The small geometries of today's nanodevices and their use in high-density and high-complexity designs make electronic systems sensitive even to the ground-level radiation. Therefore, large computer systems like workstations or computer web servers have become major victims of single event upsets. Given that the idea of cloud computing is an unavoidable trend for the next generation internet, which might involve almost every company in the IT industry, the urgency and criticality of the reliability rise higher then ever. This paper illustrates how soft errors are a reliability concern for computer servers. The soft error reduction techniques that are significant for the IT industry are summarized and a possible soft error rate (SER) reduction method that considers the cosmic ray striking angle to redesign the circuit board layout is proposed.",2010,0,
1339,1340,An Area-Efficient Approach to Improving Register File Reliability against Transient Errors,"This paper studies approaches to exploiting the space both within or across registers efficiently for improving the register file reliability against transient errors. The idea of our approach is based on the fact that a large number of register values are narrow (i.e., less than or equal to 16 bits for a 32-bit architecture); therefore, the upper 16 bits of the registers can be used to replicate the short operands for enhancing register integrity. This paper also adapts a prior register replication approach by selectively copying register values (i.e., long operands only) to the unused physical registers for enhancing reliability without incurring significant hardware cost. Our experiments indicate that on average, 993% register reads (regardless of short or long operands) can find their replicas available, implying significant improvement of register file integrity against transient errors.",2007,0,
1340,1341,Correction of the interpolation error of quadro-phase detection in interferometry,Laser interferometers using quadro-phase detectors for the fringe counting are frequently used for ultra-precise displacement measurements. The accuracy of interferometers in nanometrology is limited mainly by the interpolation error of the detector system. The new method based on applications of curve fitting using nonlinear last square method was developed for correcting the interpolation error of quadro-phase detectors. The results from the experimental data obtained in interferometrical comparator CMI IK-1 are presented.,2000,0,
1341,1342,GRIDTS: A New Approach for Fault-Tolerant Scheduling in Grid Computing,"This paper proposes GRIDTS, a grid infrastructure in which the resources select the tasks they execute, on the contrary to traditional infrastructures where schedulers find resources for the tasks. This solution allows scheduling decisions to be made with up-to-date information about the resources, which is difficult in the traditional infrastructures. Moreover, GRIDTS provides fault-tolerant scheduling by combining a set of fault tolerance techniques to cope with crash faults in components of the system. The solution is mainly based a tuple space, which supports the scheduling and also provides support for the fault tolerance mechanisms.",2007,0,
1342,1343,Combining dynamic fault trees and event trees for probabilistic risk assessment,"As system analysis methodologies, both event tree analysis (ETA) and fault tree analysis (FTA) are used in probabilistic risk assessment (PRA), especially in identifying system interrelationships due to shared events. Although there are differences between them, ETA and FTA, are so closely linked that fault trees (FT) are often used to quantify system events that are part of event tree (ET) sequences (J.D. Andrew et al., 2000). The logical processes employed to evaluate ET sequences and quantify the consequences are the same as those used in FTA. Although much work has been done to combine FT and ET, traditional methods only concentrate on combining static fault trees (SFT) and ET. Our main concern is considering how to combine dynamic fault trees (DFT) and ET. We proposed a reasonable approach in this paper, which is illustrated through a hypothetical example. Because of the complexity of dynamic systems, including the huge size and complicated dependencies, there may exist contradictions among different dynamic subsystems. The key benefit of our approach is that we avoid the generation of such contradictions in our model. Another benefit is that efficiency may be improved through modularization.",2004,0,
1343,1344,A New Diagnostic Model for Identifying Parametric Faults,"This paper presents a new approach to failure detection and isolation (FDI) for systems modeled as an interconnection of subsystems that are each subject to parametric faults. This paper develops the concept of a diagnostic model and the concept of a fault emulator which are used to model and parameterize subsystem faults. There are two stages to the FDI scheme. In the first stage there is a requirement to identify the diagnostic model. Once identified, the diagnostic model is used in the second stage to generate a residual. Artifacts within the measured residual are then used as a basis for identifying parametric faults. The scheme is distinct from others as it does not require an online recursive least squares type identifier.",2010,0,
1344,1345,A Posteriori Error Estimation and Adaptive Mesh Refinement Controlling in Finite Element Analysis of 3-D Steady State Eddy Current Fields,Several methods of <i>a posteriori</i> error estimation and adaptive refinement controlling in finite element analysis of 3-D steady-state eddy current field are described in this paper. An improved Z-Z method and a more efficient method of CIL are presented. The numerical models of TEAM Workshop Problem 7 and 21A are used to verify the validity of the presented method.,2010,0,
1345,1346,Label-based path switching and error-free forwarding in a prototype optical burst switching node using a fast 44 optical switch and shared wavelength conversion,We demonstrate for the first time optical burst switching using fast and scalable EO switch and a shared wavelength converter for contention resolution. 3.5 m label routing of variable-length bursts and error-free operation was achieved at 10 Gbps payload.,2006,0,
1346,1347,Design of fault diagnosis system for 3D Laser Scanning Machine based on internet,"According to the weakness of 3D Laser Scanning Machine in fault diagnosis, a new design approach for fault diagnosis has been developed. The new system consists of three layers, including the local fault diagnosis, fault service centre and multi-user cooperative diagnosis. The local fault diagnosis system, which is embedded in the existing CNC system, offers on-line, off-line diagnosis, fault compensation and remote communication services. With the support of the fault diagnosis database, the local fault diagnosis system chooses two agents that supply the user with a different view on problem as its intelligent analyzer to retrieve the optimal checkpoints. In addition, it builds the local fault diagnosis engine as a web service to facilitate users to share their diagnosis resource with others. The fault service center, which is supported by the customer service database, is designed to offer advanced suggestions to customers when their local fault diagnosis system fails to remove the malfunctions independently. It mainly provides four functions including fault diagnosis BBS, web-based retrieval, customer register and remote communication. It also adopts two agents to deal with web-based retrieval. The remote communication module supported by Audio/Video guide makes remote diagnosis effective. In order to acquire all of the users diagnosis resource, the customer register module registers all users web services in the registry; therefore the fault service center could bind and execute the customers web services when they are confused by a certain difficult problem. Finally, the key techniques for the system building are introduced, and the feasibility of the system is also testified.",2006,0,
1347,1348,Correction strategy for view maintenance anomaly after schema and data updating concurrently,"During maintaining the materialized view in the data warehouse, how to efficiently handle the concurrent updates is an important and intractable problem. The paper discusses typical situations that schema changes mix with data updates concurrently. And the reasons why concurrent updates result in view maintenance anomaly are analyzed. Based on the analysis, an enhanced commit agent is designed for dealing with non-order commit problem. Thus, the consistency between data warehouse and data source is guaranteed.",2005,0,
1348,1349,Assessing Fault Sensitivity in MPI Applications,"Today, clusters built from commodity PCs dominate high-performance computing, with systems containing thousands of processors now being deployed. As node counts for multi-teraflop systems grow to thousands and with proposed petaflop system likely to contain tens of thousands of nodes, the standard assumption that system hardware and software are fully reliable becomes much less credible. Concomitantly, understanding application sensitivity to system failures is critical to establishing confidence in the outputs of large-scale applications. Using software fault injection, we simulated single bit memory errors, register file upsets and MPI message payload corruption and measured the behavioral responses for a suite of MPI applications. These experiments showed that most applications are very sensitive to even single errors. Perhaps most worrisome, the errors were often undetected, yielding erroneous output with no user indicators. Encouragingly, even minimal internal application error checking and program assertions can detect some of the faults we injected.",2004,0,
1349,1350,Geometric correction of scanned topographic maps using capable input information,"For making digital maps by using the raster-vector conversion of printed binary topographic maps, one of the problems is how to correct geometric distortion that originates in the habit of individual scanner. To use innumerable resources of printed binary topographic maps effectively, we propose an interactive interface and geometric correction algorithms, which uses peculiar coordinates information in individual map. The examples prove that the interactive interface using a cross cursor is useful and efficient for getting accurate input coordinates, and the proposed correction algorithm demonstrates high accuracy of corrected coordinates.",2003,0,
1350,1351,GIS reliability analysis based trapezoid fuzzy fault tree,"GIS reliability refers to the ability to complete the requirements under the specified conditions and time. In this paper, there are five elements including object, conditions of use, use of time, functions and capabilities, to evaluate GIS reliability proposed by the combination in a engineering GIS. The paper imported trapezoid fuzzy fault tree into GIS reliability analysis for the fist time. The paper discussed how GIS reliability analysis uses trapezoid fuzzy fault tree method, mainly researches two problems that are GIS trapezoid fuzzy fault tree establishing and analysis step of GIS trapezoid fuzzy fault tree; uses example to adopt trapezoid fuzzy fault tree method computing GIS reliability analysis; because the technique considers not only GIS random uncertainty but also GIS fuzzy uncertainty, the result is precise, scientific and reasonable; finally summarizes up and points out the problems to need solving.",2010,0,
1351,1352,Fault detection methods for frequency converters fed induction machines,"The paper focuses on experimental investigation for stator faults detection and fault detection methods of electrical drive systems using voltage source inverter (VSI) fed cage rotor induction machines (CRIM). Two experimental investigations (one stator phase unbalance and one stator phase open) have been performed to study the behaviour of the electrical machine. A description of the measurement system including acquisition and processing of the data is presented and stator current signature, current Park's vector and instantaneous power as diagnostic techniques are considered.",2007,0,
1352,1353,Fault Analysis of the Stream Cipher Snow 3G,"Snow 3G is the backup encryption algorithm used in the mobile phone UMTS technology to ensure data confidentiality. Its design - a combiner with memory - is derived from the stream cipher Snow 2.0, with improvements against algebraic cryptanalysis and distinguishing attacks. No attack is known against Snow 3G today. In this paper, a fault attack against Snow 3G is proposed. Our attack recovers the secret key with only 22 fault injections.",2009,0,
1353,1354,Geometrical approach on masked gross errors for power systems state estimation,"In this paper, a geometrical based-index, called undetectability index (UI), that quantifies the inability of the traditional normalized residue test to detect single gross errors is proposed. It is shown that the error in measurements with high UI is not reflected in their residues. This masking effect is due to the ldquoproximityrdquo of a measurement to the range of the Jacobian matrix associated with the power system measurement set. A critical measurement is the limit case of measurement with high UI, that is, it belongs to the range of the Jacobian matrix, has an infinite UI index, its error is totally masked and cannot be detected in the normalized residue test at all. The set of measurements with high UI contains the critical measurements and, in general, the leverage points, however there exist measurements with high UI that are neither critical nor leverage points and whose errors are masked by the normalized residue test. In other words, the proposed index presents a more comprehensive picture of the problem of single gross error detection in power system state estimation than critical measurements and leverage points. The index calculation is very simple and is performed using routines already available in the existing state estimation software. Two small examples are presented to show the way the index works to assess the quality of measurement sets in terms of single gross error detection. The IEEE-14 bus system is used to show the efficiency of the proposed index to identify measurements whose errors are masked by the estimation processing.",2009,0,
1354,1355,A Study on the Non-Inductive Coils for Hybrid Fault Current Limiter Using Experiment and Numerical Analysis,"A Hybrid fault current limiter (FCL) proposed by our group previously is composed of a superconducting coil, a fast switch and a bypass reactor. The superconducting coil wound with two kinds of HTS wire has zero impedance when normal current flows in the coil. However, different quench characteristics of the HTS wire generate magnetic flux in the coil when fault current flows in the coil. As a result, the fast switch was opened by the repulsive force applied to the aluminum plate above the coil. In previous studies, our group verified operating characteristics and feasibility of the fast switch. In this paper, comparison of pancake type and solenoid type non-inductive coil wound with two kinds of the HTS wire was performed by using short-circuit test and finite element method. From these results, short-circuit characteristic of a coil can be acquired and magnitude of the repulsive force and magnetic field can be analysed.",2010,0,
1355,1356,Study of the impact of hardware fault on software reliability,"As software plays increasingly important roles in modern society, reliable software becomes desirable for all stakeholders. One of the root causes of software failure is the failure of the computer hardware platform on which the software resides. Traditionally, fault injection has been utilized to study the impact of these hardware failures. One issue raised with respect to the use of fault injection is the lack of prior knowledge on the faults injected, and the fact that, as a consequence, the failures observed may not represent actual operational failures. This paper proposes a simulation-based approach to explore the distribution of hardware failures caused by three primary failure mechanisms intrinsic to semiconductor devices. A dynamic failure probability for each hardware unit is calculated. This method is applied to an example Z80 system and two software segments. The results lead to the conclusion that the hardware failure profile is location related, time dependent, and software-specific",2005,0,
1356,1357,Automated fault analysis using an intelligent monitoring system,"Distribution feeders are complex systems comprised of numerous components, which are expected to function properly for decades. Electrical, mechanical and weather-related stresses combine to degrade components. Degradation accumulates over time, gradually impairing components' ability to perform properly and ultimately leading to failures, faults and outages. Work at Texas A&M has documented electrical parametric changes that occur as apparatus degrade. Taking advantage of these changes holds promise for helping utilities improve service quality and reliability, but intelligent algorithms and systems are required to acquire, analyze and otherwise manage the significant volume of data necessary to realize such benefits.",2009,0,
1357,1358,An Effective RM-Based Scheduling Algorithm for Fault-Tolerant Real-Time Systems,"Dependability is the representative property that predominantly distinguishes a hard real-time system from other computer systems besides timeliness. Primary/alternate version technique is a cost-effective means which trades the quality of computation results for promptness to tolerate the software faults. The kernel algorithm proposed in this paper employs the off-line backwards-RM scheme to pre-allocate time intervals to the alternate version and the on-line RM scheme to dispatch the primary version. The methodology is a dual-purpose strategy, which aims to (1) tolerate potential software faults by ensuring the accomplishment of the alternate version once its primary fails to execute or re-execute and (2) achieve better quality of service by maximizing the success rate of primary version.",2009,0,
1358,1359,A fast error correction technique for matrix multiplication algorithms,"Temporal redundancy techniques will no longer be able to cope with radiation induced soft errors in technologies beyond the 45 nm node, because transients will last longer than the cycle time of circuits. The use of spatial redundancy techniques will also be precluded, due to their intrinsic high power and area overheads. The use of algorithm level techniques to detect and correct errors with low cost has been proposed in previous works, using a matrix multiplication algorithm as the case study. In this paper, a new approach to deal with this problem is proposed, in which the time required to recompute the erroneous element when an error is detected is minimized.",2009,0,
1359,1360,Fault Tolerance & Testable Software Security: A Method of Quantifiable Non-Malleability with Respect to Time,"In this paper, we demonstrate there exists practical limits to the recoverability and integrity verification (non-malleability) of software with respect to time a property to the best of our knowledge not demonstrated previously; this in turn, implies practical limits to software security using current existing processing hardware. Non-malleability applied to software implies that it should be infeasible for an attacker to modify a piece of software, thus creating a software fault. Given the recoverability limitation, we demonstrate a quantifiable definition for secure software with respect to integrity/tamper resistance.",2007,0,
1360,1361,Error analysis of free-form surfaces for manufacturing applications,"Error analysis of free-form surfaces is a requirement to assure quality and to reduce manufacturing costs and rework. This paper proposes a new approach and algorithms for the error analysis of free-form surfaces. Given the measured surface as an input the approach first uses a statistical method to determine the number of test-points with suitable sample size for shape error analysis. Then, the system applies a robust mathematic model, Implicit polynomials (IP), to construct the model of the test-points. To perform detailed comparison of the shapes, the CAD model is geometrically adjusted with the input using model-based matching algorithm developed in this paper. Once the CAD model is adjusted, it is compared with input to reveal the errors between their shapes. To accomplish this task a new shape matching algorithm is developed. Experimental results on error analysis of a variety of the machined metal skin of aircraft are reported to show the validity of the proposed methodology.",2009,0,
1361,1362,Study on errors compensation of a vehicular MEMS accelerometer,"Based on micro electronic mechanical system technology, the micro inertial sensors are introduced, and the research and application conditions of MEMS micro accelerometers are also analyzed. Taking auto navigation and testing system as an example, simple mathematical models and compensational methods are developed to correct sensor errors and the validity is evaluated by experimental verification.",2005,0,
1362,1363,Control chart of mean with low alpha error probability,"A control chart of adjustment center imbalance of process with low alpha error probability and stability to unknown distribution parameter is designed. At the heart of algorithm is hypothesis check criterion. According to results of current controlled parameter measurements at every step is made a calculation of line inclination value and is tested hypothesis of its equality to null. If we accept this hypothesis, we consider the current process to be disordered.",2008,0,
1363,1364,Low-error carry-free fixed-width multipliers and their application to DCT/IDCT,"In this paper, we propose a low-error fixed-width redundant multiplier design. The design is based on the statistical analysis of the value of the truncated partial products in binary signed-digit representation with modified Booth encoding. The overall truncation error is significantly reduced with negligible hardware overhead. Simulation on DCT/IDCT of images with 256 gray levels shows our proposed multiplication design has higher PSNR/SNR.",2004,0,
1364,1365,Fault tolerant amplifier system using evolvable hardware,"This paper proposes the use evolvable hardware (EHW) for providing fault tolerance to an amplifier system in a signal-conditioning environment. The system has to maintain a given gain despite the presence of faults, without direct human intervention. The hardware setup includes a reconfigurable system on chip device and an external computer where a genetic algorithm is running. For detecting a gain fault, we propose a software-based built-in self-test strategy that establishes the actual values of gain achievable by the system. The performance evaluation of the fault tolerance strategy proposed is made by adopting two different types of fault-models. The fault simulation results show that the technique is robust and that the genetic algorithm finds the target gain with low error.",2010,0,
1365,1366,Fault Localization Based on Multi-level Similarity of Execution Traces,"Since automated fault localization can improve the efficiency of both the testing and debugging process, it is an important technique for the development of reliable software. This paper proposes a novel fault localization approach based on multi-level similarity of execution traces, which is suitable for object-oriented software. It selects useful test cases at class level and computes code suspiciousness at block level. We develop a tool that implements the approach, and conduct empirical studies to evaluate its effectiveness. The experimental results show that our approach has the potential to be effective in localizing faults for object-oriented software.",2009,0,
1366,1367,Real-Time Fisheye Lens Distortion Correction Using Automatically Generated Streaming Accelerators,"Fisheye lenses are often used in scientific or virtual reality applications to enlarge the field of view of a conventional camera. Fisheye lens distortion correction is an image processing application which transforms the distorted fisheye images back to the natural-looking perspective space. This application is characterized by non-linear streaming memory access patterns that make main memory bandwidth a key performance limiter. We have developed a fisheye lens distortion correction system on a custom board that includes a Xilinx Virtex-4 FPGA. We express the application in a high level streaming language, and we utilize Proteus, an architectural synthesis tool, to quickly explore the design space and generate the streaming accelerator best suited for our cost and performance constraints. This paper shows that appropriate ESL tools enable rapid prototyping and design of real-life, performance critical and cost sensitive systems with complex memory access patterns and hardware-software interaction mechanisms.",2009,0,
1367,1368,Voltage Sensor Fault Detection and Reconfiguration for a Doubly Fed Induction Generator,"Fault detection and reconfiguration of the control loops of a Doubly-Fed Induction Generator are described in this paper. The stator voltage is measured as well as observed. During fault free operation, the measured signal is used for the field oriented control. In case of a voltage sensor fault, the faulty measurement is identified and the control is reconfigured using the observer output. Operation without measuring the stator voltage is possible. Laboratory measurements prove this concept.",2007,0,
1368,1369,The Probabilistic Program Dependence Graph and Its Application to Fault Diagnosis,"This paper presents an innovative model of a program's internal behavior over a set of test inputs, called the probabilistic program dependence graph (PPDG), which facilitates probabilistic analysis and reasoning about uncertain program behavior, particularly that associated with faults. The PPDG construction augments the structural dependences represented by a program dependence graph with estimates of statistical dependences between node states, which are computed from the test set. The PPDG is based on the established framework of probabilistic graphical models, which are used widely in a variety of applications. This paper presents algorithms for constructing PPDGs and applying them to fault diagnosis. The paper also presents preliminary evidence indicating that a PPDG-based fault localization technique compares favorably with existing techniques. The paper also presents evidence indicating that PPDGs can be useful for fault comprehension.",2010,0,
1369,1370,A new probing scheme for fault detection and identification,"Probing technology has been used as a fault detection and identification method in computer networks and successful applications have been reported. One of the most appealing features of probing-based schemes is that it is an active approach. A set of probes can be sent on a periodic basis. If a network failure is detected, the outcomes of these probes are further analyzed to determine the root cause of the problem. However, the availability of a large set of such probes may in fact place a huge burden on management systems in terms of extra management traffic and storage space. Hence, the need of minimizing such a probing set has become highly desirable. In this work, we propose a preplanned probe selection scheme, in which a small set of probes are chosen such that it maintains the diagnostic power of the original set. The new approach is based on the constraint satisfaction problem paradigm and its powerful search techniques are exploited. The efficiency of the new algorithm has been demonstrated by the results reported.",2009,0,
1370,1371,An empirical study of software reuse vs. defect-density and stability,"The paper describes results of an empirical study, where some hypotheses about the impact of reuse on defect-density and stability, and about the impact of component size on defects and defect-density in the context of reuse are assessed, using historical data (data mining) on defects, modification rate, and software size of a large-scale telecom system developed by Ericsson. The analysis showed that reused components have lower defect-density than non-reused ones. Reused components have more defects with highest severity than the total distribution, but less defects after delivery, which shows that that these are given higher priority to fix. There are an increasing number of defects with component size for non-reused components, but not for reused components. Reused components were less modified (more stable) than non-reused ones between successive releases, even if reused components must incorporate evolving requirements from several application products. The study furthermore revealed inconsistencies and weaknesses in the existing defect reporting system, by analyzing data that was hardly treated systematically before.",2004,0,
1371,1372,"Animation can show only the presence of errors, never their absence","A formal specification animator executes and interprets traces on a specification. Similar to software testing, animation can only show the presence of errors, never their absence. However, animation is a powerful means of finding errors, and it is important that we adequately exercise a specification when we animate it. The paper outlines a systematic approach to the animation of formal specifications. We demonstrate the method on a small example, and then discuss its application to a non-trivial, system-level specification. Our aim is to provide a method for planned, documented and maintainable animation of specifications, so that we can achieve a high level of coverage, evaluate the adequacy of the animation, and repeat the process at a later time",2001,0,
1372,1373,Non-uniformity correction and calibration of a portable infrared scene projector,"A key attribute of any tester for FLIR systems is a calibrated uniform source. A uniform source ensures that any anomalies in performance are artifacts of the FLIR being tested and not the tester. Achieving a uniform source from a resistor array based portable infrared scene projector requires implementation of nonuniformity correction algorithms instead of controlling the bonding integrity of a source to a cooler, and the coating properties of the source typical of a conventional blackbody. The necessity to perform the non-uniformity correction on the scene projector is because the source is a two-dimensional array comprised of discrete resistive emitters. Ideally, each emitter of the array would have the same resistance and thus produce the same output for a given drive current. However, there are small variations from emitter to emitter over the thousands of emitters that comprise an array. Once a uniform output is achieved then the output must be calibrated for the system to be used as test equipment. Since the radiance emitted from the monolithic array is created by flowing current through micro resistors, a radiometric approach is used to calibrate the differential output of the scene projector over its dynamic range. The focus of this paper is to describe the approach and results of implementing non-uniformity correction and calibration on a portable infrared scene projector.",2002,0,
1373,1374,Effective data sharing system for fault tolerant Structural Health Monitoring system,"Structural Health Monitoring (SHM) system is a promising technology to determine the health condition of a structure and localize its damage. SHM system is widely used, especially in gigantic structures for its strong requirements of safety. Many researches about SHM system have been conducting. However, conventional SHM system did not take account of an accidental collapse caused by earthquake. As a result, there is a possibility that sensor nodes, network links, and a center server cause failures in processing or storing of data gathered in the building. These failures may lose important data that contains useful information for post-analyzing the collapse. The data includes when, where and why the structure is damaged and collapsed. Theses information have a great deal of potential for preventing similar damage or collapse and it will make great contribution to accelerate future structural researches. To solve the problem of data losses in a damage, data sharing system for SHM system is proposed. This system shares data within sensor nodes. The proposed system achieves the following three processes; node search, backup node selection and backup data transfer process. It is important to conduct these processes under the condition of limited resources due to small and not powerful sensor nodes. Backup node selection is an important process of this system. This paper mainly focused on this selection. Round-trip time, a size of free memory space of backup nodes and value of displacement caused by vibration are used for the selection. In this paper, the selection method based on displacement was checked. An experiment was conducted by using practical sensor nodes and acceleration data. From the result of this experiment, proposed system could share data by using selection method based on displacement.",2010,0,
1374,1375,Diagnosis of induction machine rotor defects from an approach of magnetically coupled multiple circuits,"The authors develop the squirrel cage induction machine models for the diagnosis of defects from an approach of magnetically coupled multiple circuits. The generalized models are established on the base of mathematical recurrences. The calculation of machine inductances (with and without rotor defects) is carried out by the tools of software MATLAB before the beginning of simulation under software SIMULINK. The experimental tests were carried out on four induction machines of power 4 kW, especially manufactured for the needs for the diagnosis and presenting defects in the rotor. The simulated results and the experimental ones are presented to confirm the validity of proposed models",2006,0,
1375,1376,Considering fault dependency and debugging time lag in reliability growth modeling during software testing,"Since the early 1970s tremendous growth has been seen in the research of software reliability growth modeling. In general, software reliability growth models (SRGMs) are applicable to the late stages of testing in software development and they can provide useful information about how to improve the reliability of software products. For most existing SRGMs, most researchers assume that faults are immediately detected and corrected. However, in practice, this assumption may not be realistic and satisfied. In this paper we first give a review of fault detection and correction processes in SRGMs. We show how several existing SRGMs based on NHPP models can be comprehensively derived by applying the time-dependent delay function. Furthermore, we show how to incorporate both failure dependency and time-dependent delay function into software reliability growth modeling. We present stochastic reliability models for software failure phenomenon based on NHPPs. Some numerical examples based on real software failure data sets are presented. The results show that the proposed framework to incorporate both failure dependency and time-dependent delay function into software reliability modeling has a useful interpretation in testing and correcting the software.",2004,0,
1376,1377,Application for fault location in electrical power distribution systems,"Fault location has been studied deeply for transmission lines due to its importance in power systems. Nowadays the problem of fault location on distribution systems is receiving special attention mainly because of the power quality regulations. In this context, this paper presents an application software developed in Matlabtrade that automatically calculates the location of a fault in a distribution power system, starting from voltages and currents measured at the line terminal and the model of the distribution power system data. The application is based on a N-ary tree structure, which is suitable to be used in this application due to the highly branched and the non- homogeneity nature of the distribution systems, and has been developed for single-phase, two-phase, two-phase-to-ground, and three-phase faults. The implemented application is tested by using fault data in a real electrical distribution power system.",2007,0,
1377,1378,Position and speed sensorless control for PMSM drive using direct position error estimation,"A new position and speed sensorless control approach is proposed for permanent magnet synchronous motor (PMSM) drives. The controller directly computes an error for the estimated rotor position and adjusts the speed according to this error. The derivation of the position error equation and an idea for eliminating the differential terms, are presented. The proposed approach is applied to a vector controlled PMSM AC drive and phase locked loop (PLL) control is employed for speed adjustment. Several simulations are carried out. The proposed control scheme is verified by experiments using a 3.7 kW salient pole PMSM",2001,0,
1378,1379,Control of a full-converter Permanent Magnet Synchronous Wind Generator with Neutral Point Clamped converters during a network fault,"This paper analyses the behaviour of a full-converter wind generation system with a back-to-back conversion structure using Neutral Point Clamped (NPC) converters during network faults. A Permanent Magnet Synchronous Generator (PMSG) is used as the generator. The main problems of this structure during voltage sags are firstly, the accumulation of power in the DC-link due to the reduction of power delivered to grid, and secondly, the control of the neutral point voltage. Three control strategies are proposed with the purpose of optimizing operation under network fault conditions. The characteristics of those strategies with regard to DC-link voltage control, torque variations required of PMSG and neutral point voltage control are also discussed.",2010,0,
1379,1380,Fault handling in embedded industrial measurement and control systems: issues and a case study,"With increasingly complex control systems used in a variety of commercial, aerospace, and military applications, system faults may occur during system operations. These faults inevitably result in abnormal operations and production shutdown or even disasters. Therefore, improving system reliability has become a major concern in safety-critical systems. This paper primarily addresses the issues in embedded fault-tolerant control system designs and presents a case study on vibration suppression in the aerospace industry. The design issues on embedded control systems such as component failures, sampling jitters and control delay, network-induced delay and packet loss in network transmission are discussed, all of which may have damaging effects on the closed-loop system performance. A case study on vibration control for a launch vehicle payload fairing using multiple embedded PZT actuators are also presented, where an adaptive actuator failure compensation scheme is successfully implemented. Fault-tolerant control turns out to be effective in creating more robust industrial measurement and control systems.",2003,0,
1380,1381,Concave and Convex Area on Planar Curve for Shape Defect Detection and Recognition,"A shape representation based on concave and convex area along a closed curve is presented. Curvature estimation is done to the input curve and searched for its critical points. Splitting the critical points into concave and convex critical points, the concave and convex area is computed. This technique is tested on shape defect detection of starfruit and also to shape recognition. In the first case, defect is measured with concave energy and obtained a stable measure, which is proportional with the defect. In shape recognition, starfruit's stem is identified to remove it from the starfruit shape, as it will contribute to false computation in defect measurement",2006,0,
1381,1382,Effect of channel estimation error onto the BER performance of PSAM-OFDM in Rayleigh fading,"In this paper, the current analysis focuses on the influence of BER performance in Rayleigh fading propagation environments, which results from the channel estimation error of pilot symbol assisted modulation (PSAM) in orthogonal frequency division multiplexing (OFDM) systems. This paper first characterizes the distribution of the amplitude and phase estimates using PSAM, and the formula for BER as a function of channel correlation and interpolation filter in time and frequency is given. Also interchannel interference due to Doppler effects is taken into account. Theoretical and simulation results show that channel estimation error leads to a 1-dB degradation in average signal-to-noise ratio for the parameters considered.",2003,0,
1382,1383,Fault-tolerant mobile agents in distributed objects systems,"A transactional agent is a mobile agent which manipulates objects in one or more than one object server so as to satisfy some constraints. There are some types of constraints depending on applications. ACID is one of the constraints, which shows traditional atomic transactions. There are other constraints like at-least-one constraint where a transaction can commit if at least one object server is successfully manipulated. An agent leaves a surrogate agent on an object server on leaving the object server A surrogate holds objects manipulated by the agent and recreates an agent if the agent is faulty. In addition, an agent is replicated by itself. Thus, transactional agents are fault-tolerant. We discuss how transactional agents with types of commitment constraints can commit. We discuss how to implement transactional agents.",2003,0,
1383,1384,"Eliminating exception handling errors with dependability cases: a comparative, empirical study","Programs fail mainly for two reasons: logic errors in the code and exception failures. Exception failures can account for up to two-thirds of system crashes, hence, are worthy of serious attention. Traditional approaches to reducing exception failures, such as code reviews, walkthroughs, and formal testing, while very useful, are limited in their ability to address a core problem: the programmer's inadequate coverage of exceptional conditions. The problem of coverage might be rooted in cognitive factors that impede the mental generation (or recollection) of exception cases that would pertain in a particular situation, resulting in insufficient software robustness. This paper describes controlled experiments for testing the hypothesis that robustness for exception failures can be improved through the use of various coverage-enhancing techniques: N-version programming, group collaboration, and dependability cases. N-version programming and collaboration are well known. Dependability cases, derived from safety cases, comprise a new methodology based on structured taxonomies and memory aids for helping software designers think about and improve exception handling coverage. All three methods showed improvements over control conditions in increasing robustness to exception failures but dependability cases proved most efficacious in terms of balancing cost and effectiveness",2000,0,
1384,1385,Decentralized Fault Detection and Management for Wireless Sensor Networks,"Wireless Sensor Networks are increasingly being deployed in long-lived, challenging application scenarios which demand a high level of availability and reliability. To achieve these characteristics in inherently unreliable and resource constrained sensor network environments, fault tolerance is required. This paper presents a generic and efficient fault tolerance algorithm for Wireless Sensor Networks. In contrast to existing approaches, the algorithm presented in this paper is entirely decentralized and can thus be used to support fully autonomic fault tolerance in sensor network environments.",2010,0,
1385,1386,Error resilience performance evaluation of H.264 I-frame and JPWL for wireless image transmission,"The visual quality obtained in wireless transmission strongly depends on the characteristics of the wireless channel and on the error resilience of the source coding. The wireless extensions of the JPEG 2000 standard (JPWL) and H.264 are the latest international standards for still image and video compression, respectively. However, few results have been reported to compare the rate-distortion (R-D) performance of JPEG 2000 and H.264. Conversely, comparative studies of error resilience between JPWL and H.264 for wireless still image transmission have not been thoroughly investigated. In this paper, we analyse the error resilience of image coding based on JPWL and H.264 I-frame coding in Rayleigh fading channels. Comprehensive objective and perceptual results are presented in relation to the error resilience performance of these two standards under various conditions. Our simulation results reveal that H.264 is more robust to transmission errors than JPWL for wireless still image transmission.",2010,0,
1386,1387,"Automotive signal fault diagnostics - part I: signal fault analysis, signal segmentation, feature extraction and quasi-optimal feature selection","The paper describes our research in vehicle signal fault diagnosis. A modern vehicle has embedded sensors, controllers and computer modules that collect a large number of different signals. These signals, ranging from simple binary modes to extremely complex spark timing signals, interact with each other either directly or indirectly. Modern vehicle fault diagnostics very much depend upon the input from vehicle signal diagnostics. Modeling vehicle engine diagnostics as a signal fault diagnostic problem requires a good understanding of signal behaviors relating to various vehicle faults. Two important tasks in vehicle signal diagnostics are to find what signal features are related to various vehicle faults, and how can these features be effectively extracted from signals. We present our research results in signal faulty behavior analysis, automatic signal segmentation, feature extraction and selection of important features. These research results have been incorporated in a novel vehicle fault diagnostic system, which is described in another paper (see Yi Lu Murphey et al., ibid., p.1076-98).",2003,0,
1387,1388,RACE: a software-based fault tolerance scheme for systematically transforming ordinary algorithms to robust algorithms,"We propose the robust algorithm-configured emulation (RACE) scheme for efficient parallel computation and communication in the presence of faults. A wide variety of algorithms originally designed for fault-free meshes, tori, and k-ary n-cubes can be transformed to corresponding robust algorithm through RACE. In particular optimal robust algorithms can be derived for total exchange (TE) and ascend/descend operations with a factor of 1+o (1) slowdown. Also, RACE can tolerate a large number of faulty elements, without relying on hardware redundancy or any assumption about the availability of a complete subarray",2001,0,
1388,1389,Efficient diagnosis of single/double bridging faults with Delta Iddq probabilistic signatures and Viterbi algorithm,"This paper presents an efficient method to diagnose single and double bridging faults. This method is based on Delta Iddq probabilistic signatures, as well as on the Viterbi algorithm, mainly used in telecommunications systems for error correction. The proposed method is a significant improvement over an existing one, based on maximum likelihood estimation. The (adapted) Viterbi algorithm takes into account useful information not considered previously. Simulation and experimental results are presented to validate the approach",2000,0,
1389,1390,More about arc-fault circuit interrupters,"Since the arc-fault circuit interrupter (AFCI) was commercially introduced in 1998, questions have arisen about how it detects arcs, whether it detects series and parallel arcs, and what types of AFCIs are available. Types other than the original branch/feeder AFCI are emerging. This paper is intended to provide an update regarding answers to those questions, following an earlier paper that introduced the basic functioning of the AFCI.",2004,0,
1390,1391,A hierarchical fault tolerant architecture for component-based service robots,"Due to the benefits of reusability and productivity, component-based approach has become the primary technology in service robot system development. However, because component developer cannot foresee the integration and operating condition of the components, they cannot provide appropriate fault tolerance function, which is crucial for commercial success of service robots. The recently proposed robot software frames such as MSRDS (Microsoft Robotics Developer Studio), RTC (Robot Technology Component), and OPRoS (Open Platform for Robotic Services) are very limited in fault tolerance support. In this paper, we present a hierarchically-structured fault tolerant architecture for component-based robot systems. The framework integrates widely-used, representative fault tolerance measures for fault detection, isolation, and recovery. The system integrators can construct fault tolerance applications from non-fault-aware components, by declaring fault handling rules in configuration descriptors or/and adding simple helper components, considering the constraints of components and the operating environment. To demonstrate the feasibility and benefits, a fault tolerant framework engine and test robot systems are implemented for OPRoS. The experiment results with various simulated fault scenarios validate the feasibility, effectiveness and real-time performance of the proposed approach.",2010,0,
1391,1392,"Optimized reasoning-based diagnosis for non-random, board-level, production defects","The ""back-end"" costs associated with debug of functional test failures can be one of the highest cost adders in the manufacturing process. As boards become more dense and more complex, debug of functional failures will become more and more difficult. Test strategies try to detect and diagnose failures early on in the test process (component and structural tests), but inevitably some defects are not detected until functional testing is done on the board. Finding these defects usually requires an ""expert"", with engineering level skills in both hardware and software. Depending on the complexity of the product, it could take several months (even years) to develop this level of expertise. During the initial product ramp, this expertise is usually most needed and often unavailable. Debug time is usually very long and scrap rates are generally high. This paper will provide an overview of reasoning-based diagnosis techniques and how they can significantly decrease debug time, especially during new product introduction. Because these engines are ""model-based"", there is no guarantee how they will perform in real life. In almost all cases, the reasoning engine will have to be modified based on instances where the reasoning engine could not correctly identify the failing component. Making these adjustments to the reasoning is a very complex and sometimes risky endeavor. While the new model may correctly identify the previously missed failure, the reasoning may have been altered to a point where several other diagnoses have now been unknowingly compromised. This paper will propose enhancements to the reasoning engine that will allow a simpler approach to adapting to diagnostic escapes without risking compromises to the original diagnostic engine",2005,0,
1392,1393,Online assessment of fault current considering substation topologies,"Changes to the power system, especially the installation of new generation sources and new operating conditions, result in higher fault currents. Circuit breakers, which are previously designed and coordinated for the power systems before the appearances of new generation sources and new operating conditions, may have inadequate fault current interruption capability. This paper discusses those operating conditions that result in interruption capability violations of circuit breakers, based on substation topologies/circuit breaker connections. An online application of fault current assessment is proposed to identify those unsafe operating conditions limited by circuit breaker interruption capabilities. Based on the findings of the paper, this application can give system operators suggestions of remedy actions that eliminate unsafe operating conditions by changing substation topologies. The design of such an online application is described and a MATLAB version is tested on IEEE 14-bus system.",2010,0,
1393,1394,Cascaded H-bridge Multilevel Inverter Drives Operating under Faulty Condition with AI-Based Fault Diagnosis and Reconfiguration,"The ability of cascaded H-bridge multilevel inverter drives (MLID) to operate under faulty condition including AI-based fault diagnosis and reconfiguration system is proposed in this paper. Output phase voltages of a MLID can be used as valuable information to diagnose faults and their locations. It is difficult to diagnose a MLID system using a mathematical model because MLID systems consist of many switching devices and their system complexity has a nonlinear factor. Therefore, a neural network (NN) classification is applied to the fault diagnosis of a MLID system. Multilayer perceptron (MLP) networks are used to identify the type and location of occurring faults. The principal component analysis (PCA) is utilized in the feature extraction process to reduce the NN input size. A lower dimensional input space will also usually reduce the time necessary to train a NN, and the reduced noise may improve the mapping performance. The genetic algorithm (GA) is also applied to select the valuable principal components to train the NN. A reconfiguration technique is also proposed. The proposed system is validated with simulation and experimental results. The proposed fault diagnostic system requires about 6 cycles (~100 ms at 60 Hz) to clear an open circuit and about 9 cycles (~150 ms at 60 Hz) to clear a short circuit fault. The experiment and simulation results are in good agreement with each other, and the results show that the proposed system performs satisfactorily to detect the fault type, fault location, and reconfiguration.",2007,0,
1394,1395,Capacity and error probability analysis for orthogonal space-time block codes over fading channels,"The capacity and error probability of orthogonal space-time block codes (STBCs) are considered for pulse-amplitude modulation/phase shift keying/quadrature-amplitude modulation (PAM/PSK/QAM) in fading channels. The approach is based on an equivalent scalar additive white Gaussian noise channel with a channel gain proportional to the Frobenius norm of the matrix channel for the STBC. Using this effective channel, capacity and probability of error expressions are derived for PSK/PAM/QAM modulation with space-time block coding. Rayleigh-, Ricean-, and Nakagami-fading channels are considered. As an application, these results are extended to obtain the capacity and probability of error for a multiuser direct sequence code-division multiple-access system employing space-time block coding.",2005,0,
1395,1396,A high efficient boost converter with power factor correction,"Boost converter is widely used as active power factor correction (PFC) pre-regulator. Its input voltage range is universal (90-265 V), and its output voltage is regulated at about 380 V. At low line (90 V) the switch's rms current is high, so the conduction loss of power switch MOSFET is large and the efficiency of whole converter is very low. This paper proposes a new control method that the output voltage varies with the input voltage change. Under this control the MOSFET's on-time is shortened, and the switch's RMS current decreases, which reduces the conduction loss and increases the boost converter efficiency. The distribution of power loss is analyzed by computing software (mathcad 2000) and the realization of this special control method is given. A 1200 W boost power factor corrector with average current control is built up. In order to improve the diode's turn-off loss the performance of a 600 V, 12 A silicon carbide (SiC) Schottky diode is also experimentally evaluated. Measurements of overall efficiency and reverse recovery behavior are compared between SiC diode and fast recovery diode.",2004,0,
1396,1397,Modeling Alpha and Neutron Induced Soft Errors in Static Random Access Memories,Experimental thermal neutron and alpha soft error test results of a 4 Mbit SRAM fabricated on a 0.25 mum process are evaluated using Vanderbilt University's RADSAFE toolkit. The capabilities of the radiation transport code are demonstrated by accurately reproducing experimental results and predicting operational soft error rates for the memory.,2007,0,
1397,1398,Byzantine Fault-Tolerant Web Services for n-Tier and Service Oriented Architectures,"Mission-critical services must be replicated to guarantee correctness and high availability in spite of arbitrary (Byzantine) faults. Traditional Byzantine fault tolerance protocols suffer from several major limitations. Some protocols do not support interoperability between replicated services. Other protocols provide poor fault isolation between services leading to cascading failures across organizational and application boundaries. Moreover, traditional protocols are unsuitable for applications with tiered architectures, long-running threads of computation, or asynchronous interaction between services. We present Perpetual, a protocol that supports Byzantine fault-tolerant execution of replicated services while enforcing strict fault isolation. Perpetual enables interaction between replicated services that may invoke and process remote requests asynchronously in long-running threads of computation. We present a modular implementation, an Axis2 Web Services extension, and experimental results that demonstrate only a moderate overhead due to replication.",2008,0,
1398,1399,Accurate algorithm for analysis of surface errors in reflector antennas and calculation of gain loss,"The distortion of antenna reflector can degrade the antenna performances obviously with the higher work frequency band. A new algorithm used for accurately analyzing the distorted reflectors and computing the axial, normal and radial deviations of distorted surface is developed. The analysis method with simulation values only one third of analysis results of ANSYS software is verified by calculating surface errors and gain losses of a 7.3-m parabolic antenna under different working conditions and comparing the results of accurate algorithm and ANSYS software method. Its application in both large space and ground antennas will greatly improve the design efficiency, reduce cost, and also provide the detailed precise distortion information for electrical designers.",2005,0,
1399,1400,Software Faults Diagnosis in Complex OTS Based Safety Critical Systems,"This work addresses the problem of software fault diagnosis in complex safety critical software systems. The transient manifestations of software faults represent a challenging issue since they hamper a complete knowledge of the system fault model at design/development time. By taking into account existing diagnosis techniques, the paper proposes a novel diagnosis approach, which combines the detection and location processes. More specifically, detection and location modules have been designed to deal with partial knowledge about the system fault model. To this aim, they are tuned during system execution in order to improve diagnosis during system lifetime. A diagnosis engine has been realized to diagnose software faults in a real world middleware platform for safety critical applications. Preliminary experimental campaigns have been conducted to evaluate the proposed approach.",2008,0,
1400,1401,Research on Multi-function Fault Management System Model Based on SNMP,"The improving network technology and application make it a challenge to the network manager. A feasible and efficient network management strategy will become an important method to insure the network running well. Therefore, itpsilas meaningful to be familiar with the network and network management technology. In accordance with practical network environment, we design and implement fault management system FMS based on SNMP. Using Client/Server architecture, our system established a distributed management model which is compose of Console/Manager/Agent, finished network status monitoring, event processing, fault alarm and log etc.",2008,0,
1401,1402,Combinational Logic Soft Error Correction,"We present two techniques for correcting radiation-induced soft errors in combinational logic - error correction using duplication, and error correction using time-shifted outputs. Simulation results show that both techniques reduce combinational logic soft error rate by more than an order of magnitude. Soft errors affecting sequential elements (latches and flip-flops) at combinational logic outputs are automatically corrected using these techniques",2006,0,
1402,1403,A new approach to fault-tolerant wormhole routing for mesh-connected parallel computers,"A new method for fault-tolerant wormhole routing in arbitrary dimensional meshes is introduced. The method was motivated by certain routing requirements of an initial design of the Blue Gene supercomputer at IBM Research. The machine is organized as a three-dimensional mesh containing many thousands of nodes and the routing method should tolerate a few percent of the nodes being faulty. There has been much work on routing methods for meshes that route messages around faults or regions of faults. The new method is to declare certain nonfaulty nodes to be ""lambs."" A lamb is used for routing but not processing, so a lamb is neither the source nor the destination of a message. The lambs are chosen so that every ""survivor node,"" a node that is neither faulty nor a lamb, can reach every survivor node by at most two rounds of dimension-ordered (such as e-cube) routing. An algorithm for finding a set of lambs is presented. The results of simulations on 2D and 3D meshes of various sizes with various numbers of random node faults are given. For example, on a 32  32  32 3D mesh with 3 percent random faults and using at most two rounds of e-cube routing for each message, the average number of lambs is less than 68, which is less than 7 percent of the number 983 of faults and less than 0.21 percent of the number 32,768 of nodes.",2004,0,
1403,1404,Design and application of Gray Field<sup>TM</sup> technology for defect inspection systems,"There has been increased interest in optical inspection tools that utilize UV illumination. This originates from the belief that diffraction limits will render tools employing longer wavelengths blind to many defects identified as being critical. In response to concerns over the applicability of UV illumination to rapid defect detection we performed a series of experiments to explore and develop new inspection techniques to provide the capability of detecting the dimensionally challenging defects associated with advanced technology nodes while maintaining high speeds needed for chipmakers' volume production lines. Initial results indicated that by radically redesigning the collection optics to a multiple perspective configuration that compiles information from six different scattering and reflecting directions, improved sensitivity, noise rejection and wafer throughput could be realized while using laser scanning illumination in the visible region of the spectrum. Also, defects that traditionally could only be observed in bright field tools were now detectable with ease at production worthy throughputs. Results are presented that show the optical experimental design data and simulations, and are corroborated by examples of defects from the resulting production defect inspection system, Compass<sup>TM</sup>. In addition, electron micrographs of a range of detected defects are presented that show the system versatility and the exact nature of the defects, thus allowing a clear understanding of the increase in sensitivity, speed and dimensional range these tools provide over traditional instrumentation to be made",2001,0,
1404,1405,A strategy to replace the damaged element for fault-tolerant induction motor drive,"In this paper, the best moment to replace to the damaged element in a fault-tolerant induction motor drive working with a open-loop and closed-loop control is presented, a previous stage of fault-diagnostic to detect a short-circuit or open-circuit failure in the power device is considered. The technique is based on the connection of bidirectional switches to electrically isolate the damaged element by mean of fuse blown corresponding and to replace only the damaged device by another healthful one at the most suitable moment, the main issue is to diminish the tracking error of the motor current during the fault transient. Experimental and simulation results are obtained in order to validate the technique proposed.",2008,0,
1405,1406,Boron emitters: Defects at the silicon - silicon dioxide interface,"An investigation of defects caused by boron diffusion into silicon is presented, using two techniques to directly compare the defects at an undiffused and lightly boron diffused Si-SiO<inf>2</inf> interface. The first technique uses field effect passivation induced by a MOS structure; the second uses Electron Paramagnetic Resonance measurements to determine the concentration of unpassivated P<inf>b</inf> centers on <111> oriented surfaces. It is found that additional defects introduced by the boron diffusion account for a relatively small proportion of total recombination at a well passivated <100> interface, while for more at <111> interfaces, as both the defect density and recombination increase by a factor of more than 2. The effect of the addition of LPCVD nitride on top of oxide layers is also explored. We show that exposure of samples to hot phosphoric acid (used to selectively remove silicon nitride) leads to significant changes to the Si-SiO2 interface, so that this treatment cannot be considered noninvasive.",2008,0,
1406,1407,Fault-oriented software robustness assessment for multicast protocols,"This paper reports a systematic approach for detecting software defects in multicast protocol implementations. We deploy a fault-oriented methodology and an integrated test system targeting software robustness vulnerabilities. The primary method is to assess protocol implementation by non-traditional interface fault injection that simulates network attacks. The test system includes a novel packet driving engine, a PDU generator based on Strengthened BNF notation and a few auxiliary tools. We apply it to two multicast protocols, IGMP and PIM-DM, and investigate their behaviors under active functional attacks. Our study proves its effectiveness for promoting production of more reliable multicast software.",2003,0,
1407,1408,A Distributed Replication Strategy Evaluation and Selection Framework for Fault Tolerant Web Services,"Redundancy-based fault tolerance strategies are proposed for building reliable Service-Oriented Architectures/Applications (SOA), which are usually developed on the unpredictable remote Web services. This paper proposes and implements a distributed replication strategy evaluation and selection framework for fault tolerant Web services. Based on this framework, we provide a systematic comparison of various replication strategies by theoretical formula and real-world experiments. Moreover, a user participated strategy selection algorithm is designed and verified. Experiments are conducted to illustrate the advantage of this framework. In these experiments, users from six different locations all over the world perform evaluation of Web services distributed in six countries. Over 1,000,000 test cases are executed in a collaborative manner and detailed results are also provided.",2008,0,
1408,1409,Fault-tolerance by regeneration: using development to achieve robust self-healing neural networks,"Opposed to the standard paradigm of 'fault-tolerance by redundancy', ontogeny offers the possibility to engineer artificial organisms which can re-grow faulty components. Similar to what happens in nature, organisms display self-healing: a homeostatic process which allows proper operation while suffering faults. In this paper we present a system which evolves developing spiking neural networks capable of controlling simulated Khepera robots in a wall avoidance task. Development is controlled by a decentralized process executed by each cell's identical growth program. To test the system's self-healing capability, networks are (1) subjected to random faults during development and (2) mutilated during operation. Results demonstrate how development can (i) rapidly produce proper neuro-controllers and (ii) re-grow neurons to recover normal operation. These results show that development, originally proposed to increase the evolvability of large phenotypes, also allow the production of artifacts with sustained fault-tolerance. These artifacts would be especially well-suited for tasks that require long periods of operation in absence of external maintenance.",2005,0,
1409,1410,AppWatch: detecting kernel bug for protecting consumer electronics applications,"Most consumer electronics products are equipped with diverse devices since they try to provide more services following the convergence trends. Device drivers for those devices are known to cause system failures. Most previous approaches to enhance reliability have been concerned with the kernel, not with applications. In consumer electronics, however, a main application plays a core role of the product. This paper proposes a new mechanism called AppWatch to keep a consumer electronics application reliable against misbehavior of device drivers. AppWatch exploits page management mechanism of the operating system to protect the address space of the application. Since AppWatch can be implemented at a low engineering cost, it is applicable to most systems only if they have the virtual memory system. AppWatch also provides selective protection of applications so that other unprotected applications are isolated from performance loss, if any. We have tested AppWatch in a consumer electronics environment. The result shows that AppWatch effectively protects application programs at a reasonable performance overhead in most workloads, whereas data-intensive workloads show high overhead. AppWatch also protects applications with little performance interference to other unprotected applications.",2010,0,
1410,1411,Fault-Tolerant Operation of a Battery-Energy-Storage System Based on a Multilevel Cascade PWM Converter With Star Configuration,"This paper focuses on fault-tolerant control for a battery-energy-storage system based on a multilevel cascade pulsewidth-modulation (PWM) converter with star configuration. During the occurrence of a single-converter-cell or single-battery-unit fault, the fault-tolerant control enables continuous operation and maintains state-of-charge balancing of the remaining healthy battery units. This enhances both system reliability and availability. A 200-V, 10-kW, 3.6-kWh laboratory system combining a three-phase cascade PWM converter with nine nickel-metal-hydride battery units is designed, constructed, and tested to verify the validity and effectiveness of the proposed fault-tolerant control.",2010,0,
1411,1412,"Combining Duplication, Partial Reconfiguration and Software for On-line Error Diagnosis and Recovery in SRAM-Based FPGAs","SRAM-based FPGAs are susceptible to Single-Event Upsets (SEUs) in radiation-exposed environments due to their configuration memory. We propose a new scheme for the diagnosis and recovery from upsets that combines i) duplication of the core to be protected, ii) partial reconfiguration to reconfigure the faulty part only, and iii) hardcore processor(s) for deciding when and which part will be reconfigured; executing the application in software instead of hardware during fault handling; and controlling the reconfiguration. A hardcore processor has smaller cross section and it is less susceptible than reconfigurable resources. Thus it can temporarily undertake the execution during upset conditions. Real experiments demonstrate that our approach is feasible and an area reduction of more than 40% over the dominant Triple Modular Redundancy (TMR) solution can be achieved at the cost of a reduction in the processing rate of the input.",2010,0,
1412,1413,A generic real-time computer Simulation model for Superconducting fault current limiters and its application in system protection studies,"A model for the SCFCL suitable for use in real time computer simulation is presented. The model accounts for the highly nonlinear quench behavior of BSCCO and includes the thermal aspects of the transient phenomena when the SCFCL is activated. Implemented in the RTDS real-time simulation tool the model has been validated against published BSCCO characteristics. As an example for an application in protection system studies, the effect of an SCFCL on a utility type impedance relay has been investigated using a real time hardware-in-the-loop (RT-HIL) experiment. The test setup is described and initial results are presented. They illustrate the effect of how the relay misinterprets the dynamically changing SCFCL impedance as an apparently more distant fault location. It is expected that the new real-time SCFCL model will provide a valuable tool not only for further protection system studies but for a wide range of RT-HIL experiments of power systems.",2005,0,
1413,1414,New fault tolerant robotic central controller for space robot system based on ARM processor,"A new fault tolerant robotic central controller with dual processing modules is introduced. Each processing module is composed of 32 bit ARM RISC processor and other commercial-off-the-shelf (COTS) devices. As well as, a set of fault handling mechanisms is implemented in the robotic central controller, which can tolerate a single fault. The robotic central controller software based on VxWorks is organized around a set of processes that communicate between each other through a routing process. Considering the demanding of the extremely tight constraints on mass, volume, power consumption and space environmental conditions, the new fault tolerant robotic central controller has been developed. Its excellent data processing capability is enough to meet the space robot missions.",2008,0,
1414,1415,Scheduling of Fault-Tolerant Embedded Systems with Soft and Hard Timing Constraints,"In this paper we present an approach to the synthesis of fault-tolerant schedules for embedded applications with soft and hard real-time constraints. We are interested to guarantee the deadlines for the hard processes even in the case of faults, while maximizing the overall utility. We use time/utility functions to capture the utility of soft processes. Process re-execution is employed to recover from multiple faults. A single static schedule computed off-line is not fault tolerant and is pessimistic in terms of utility, while a purely online approach, which computes a new schedule every time a process fails or completes, incurs an unacceptable overhead. Thus, we use a quasi-static scheduling strategy, where a set of schedules is synthesized off-line and, at run time, the scheduler will select the right schedule based on the occurrence of faults and the actual execution times of processes. The proposed schedule synthesis heuristics have been evaluated using extensive experiments.",2008,0,
1415,1416,Software implemented fault injection for safety-critical distributed systems by means of mobile agents,"The availability of inexpensive powerful microprocessors leads to increasing deployment of those electronic devices in ever new application areas. Currently, the automotive industry considers the replacement of mechanical or hydraulic implementations of safety-critical automotive systems (e.g., braking, steering) by electronic counterparts (so-called ""by-wire systems"") for safety, comfort, and cost reasons. In order to remain operational in the presence of faults, these kinds of systems are built as fault-tolerant distributed real-time systems consisting of interconnected control units. To assure the correct operation of the fault tolerance mechanisms, software implemented fault injection provides low cost and easy to control techniques to test the system under faulty conditions. In this paper we propose a distributed software implemented fault injection framework based on the mobile agent approach. Software agents are designed to utilize the real-time system's global time and messages to trigger the fault injection experiments. We introduce a lightweight agent implementation language to model the fault injection and the concerned system resources, agent migration and logging of the fault injection experiments.",2004,0,
1416,1417,Error rates of M-ary signals with. Multichannel reception in Nakagami-m fading channels,"In this letter, we present closed. form expressions for the exact average symbol-error rate (SER) of M-ary modulations with multichannel reception over Nakagami-m fading channels. The derived expressions extend already available results for the nondiversity case, to maximal-ratio combining-(MRC) and postdetection equal-gain combining (EGC) diversity systems. The average SERs are given in terms of Lauricella's multivariate hypergeometric function F<sub>D</sub> <sup>(n) </sup>. This function exhibits a finite integral representation that can be used for fast and accurate numerical computation of the derived expressions",2006,0,
1417,1418,&Rscr;&epsi;&Lscr;: a fault tolerance linguistic structure for distributed applications,"The embedding of fault tolerance provisions into the application layer of a programming language is a non-trivial task that has not found a satisfactory solution yet. Such a solution is very important, and the lack of a simple, coherent and effective structuring technique for fault tolerance has been termed by researchers in this field as the ""software bottleneck of system development"". The aim of this paper is to report on the current status of a novel fault tolerance linguistic structure for distributed applications characterized by soft real-time requirements. A compliant prototype architecture is also described. The key aspect of this structure is that it allows one to decompose the target fault-tolerant application into three distinct components respectively responsible for: (1) the functional service, (2) the management of the fault tolerance provisions, and (3) the adaptation to the current environmental conditions. The paper also briefly mentions a few case studies and preliminary results obtained exercising the prototype",2002,0,
1418,1419,Assessing Failure of Bridge Construction Using Fuzzy Fault Tree Analysis,Estimating exact probabilities of occurrence of bridge failure for the use in the conventional fault tree analysis (FTA) is difficult when fault events are imprecise such as human error. A fuzzy FTA model employing fuzzy sets and possibility theory to tackle this problem is proposed. An example of the collapse of cantilever gantry during construction demonstrates the capability of this approach that can assist safety engineer to better evaluate bridge performance.,2007,0,
1419,1420,Image mosaic method based on the image geometric correction for traffic accident scene,Image mosaic is one of important technologies for image processing. It is normally used to make up a seamless and high resolution image. There are some algorithms that deal with the image mosaic. But the most make simply two or more images seamlessly form a large image for a holographic scenic display. The post-processing on the photo from a traffic accident scene is required to reflect and allow inspectors to accurately determine the actual distance between objects in the scene. Therefore the images taken from the traffic accident scene need to be corrected before being spliced to each other. The image correction allows the information on the scene to be correctly displayed. The splicing on the corrected images ensures a thorough view and complete information gain that covers the whole scene.,2010,0,
1420,1421,Full contrast transfer function correction in 3D cryo-EM reconstruction,"Over the past years electron cryo-microscopy (cryo-EM) has established itself as an important tool in studying the three dimensional structure of biological molecules up to the resolution of 6-9 A. However, as we pursue even higher resolution (i.e., 3-4 A), the depth-of-field problem inherent in the contrast transfer function emerges as a limiting factor. This problem has been previously addressed in the research community (Jensen, G.J., 2000; DeRosier, D.J., 2000; Zhou, Z.H. and Chiu, W., 2003; Cohen, H.A. et al., 1984). We develop a full theoretical solution to this problem. We show that the projected image from the electron microscope corresponds to neither a slice, nor an Ewald sphere, in the Fourier space, but a pair of quadratic surfaces in that space. The general solutions to this problem for both single and double defocus exposures are developed. Simulations show the correctness of the theory.",2004,0,
1421,1422,2D Photonic Defect Layers in 3D Inverted Opals on Si Platforms,"Dielectric spheres synthesised for the fabrication of self-organized photonic crystals such as opals offer large opportunities for the design of novel nanophotonic devices. In this paper, we show a hexagonal superlattice monolayer of dielectric spheres inscribed on a 3D colloidal photonic crystal by e-beam lithography. The crystal is produced by a variation of the vertical drawing deposition method assisted by an acoustic field. The structures were chosen after simulations showed that a hexagonal super-lattice monolayer in air exhibits an even photonic band gap below the light cone if the refractive index of the spheres is higher than 1.93",2006,0,
1422,1423,A new motion compensation approach for error resilient video coding,"Multihypothesis motion-compensated prediction (MHMCP) can be used as an error resilience technique for video coding. Motivated by MHMCP, we propose a new error resilience approach named alternative motion-compensated prediction (AMCP), where two-hypothesis and one-hypothesis predictions are alternatively used with some mechanism. Both theory and simulation results show that in case of one frame loss, the expected converged error using AMCP is smaller than that using two-hypothesis MCP.",2005,0,
1423,1424,Generic Fault-Tolerance Mechanisms Using the Concept of Logical Execution Time,"Model-based development has become state of the art in software engineering. Unfortunately, the used code generators often focus on the pure application functionality. Features like automatic generation of fault-tolerance mechanisms are not covered. One main reason is the inadequacy of the used models. An adequate model must have amongst others explicit execution semantics and must be suited to support replica determinism and automatic state synchronization. These requirements are fulfilled when using the concept of logical execution time, a time-triggered approach. This approach hides the implementation details like the physical execution from the user, In contrast to other time-triggered paradigms. Within this paper, we present a solution to exploit this concept to realize major fault-tolerance mechanisms in a generic way.",2007,0,
1424,1425,TPT-RAID: a High Performance Box-Fault Tolerant Storage System,"TPT-RAID is a multi-box RAID wherein each ECC group comprises at most one block from any given storage box, and can thus tolerate a box failure. It extends the idea of an out-of band SAN controller into the RAID: data is sent directly between hosts and targets and among targets, and the RAID controller supervises ECC calculation by the targets. By preventing a communication bottleneck in the controller, excellent scalability is achieved while retaining the simplicity of centralized control. TPT-RAID, whose controller can be a software module within an out-of-band SAN controller, moreover conforms to a conventional switched network architecture, whereas an in-band RAID controller would either constitute a communication bottleneck or would have to also be a full-fledged router. The design is validated in an InfiniBand-based prototype using /SCSI and /SER, and required changes to relevant protocols are introduced.",2007,0,
1425,1426,Novel algorithms for earth fault indication based on monitoring of shunt resistance of MV feeder as a part of relay protection,Novel methods for very high-resistance earth fault identification and location in isolated or high impedance earthed distribution systems have been developed. The novel indication algorithms are able to detect and locate faults up to some hundreds of kilo-ohms. These algorithms were implemented in a microprocessor-based feeder terminal. The indication methods have proven to be very appropriate for the implementation and the preliminary results from the field installation and field experiments are promising,2001,0,
1426,1427,Genetic programming approach for fault modeling of electronic hardware,"This paper presents two variants of genetic programming (GP) approaches for intelligent online performance monitoring of electronic circuits and systems. Reliability modeling of electronic circuits can be best performed by the stressor - susceptibility interaction model. A circuit or a system is deemed to be failed once the stressor has exceeded the susceptibility limits. For on-line prediction, validated stressor vectors may be obtained by direct measurements or sensors, which after preprocessing and standardization are fed into the GP models. Empirical results are compared with artificial neural networks trained using backpropagation algorithm. The performance of the proposed method is evaluated by comparing the experiment results with the actual failure model values. The developed model reveals that GP could play an important role for future fault monitoring systems.",2005,0,
1427,1428,Petri sub-nets for minpath-based fault trees,"The choice of proper forms of fault tree (FT) and success tree (ST) representations, respectively inside of Petri nets (PNs) in the field of R&M modeling is not a trivial problem, because there is the danger of stray tokens inside of the sub-PNs of those trees which would disturb the system model's proper operation in the long run. The author has been advocating the use of disjunctive normal forms (DNFs=sum-of-products forms). However, typically in the field of graph connectivity problems the initially found FTs usually result from minpaths rather than from mincuts. The Boolean FT functions are therefore initially conjunctive normal forms (CNFs=product-of-sums forms). As the main result of this paper it is shown that for such FTs, sub-PNs can be designed systematically, even though they are not quite as simple as sub-PNs for FTs of DNFs. The main point is to allow for extra FT input places, and to gather all the tokens corresponding to the single variables of the diverse sums once the repairs of the corresponding components are finished. This way no stray tokens remain inside of the FT's sub-PN. As a consequence of the duality between FTs and STs, and since both trees are usually inserted in the overall system PN model, it suffices to find a DNF or a CNF of either tree's Boolean function. A CNF or a DNF of the other tree is then readily found via Shonnon's inversion theorem, i.e., it needs no complex Boolean algebra manipulations. The general results are formulated as PN design rules",2001,0,
1428,1429,A Mobile Agent-Based Architecture for Fault Tolerance in Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) are prone to failures as they are usually deployed in remote and unattended environments. To mitigate the effect of these failures, fault tolerance becomes imperative. Nonetheless, it remains to be a second tier activityit should not undermine the execution of the mission oriented tasks of WSNs through overly taxing their resources. We define architecture for fault tolerance in WSNs that is based on a federation of mobile agents that is used both for diagnostic intelligence and repair regimen, focusing on being lightweight in energy, communication and resources. Mobile agents are classified here as local, metropolitan, and global, providing fault tolerance at node, network and functional levels. Interactions between mobile agents are inspired by honey bee dance language that builds on semantics of errors classification and their demographic distribution. Our quantitative modeling substantiates that the proposed fault tolerance framework mandates minimalist communication through contextualized bee-inspired interactions, achieving adaptive sensitivity, and hysteresis-based stability",2010,0,
1429,1430,Fault-tolerance and noise modelling in nanoscale circuit design,"Fault-tolerance in integrated circuit design has become an alarming issue for circuit designers and semiconductor industries wishing to downscale transistor dimensions to their utmost. The motivation to conduct research on fault-tolerant design is backed by the observation that the noise which was ineffective in the large-dimension circuits is expected to cause a significant downgraded performance in low-scaled transistor operation of future CMOS technology models. This paper is destined to give an overview of all the major fault-tolerance techniques and noise models proposed so far. Summing and analysing all this work, we have divided the literature into three categories and discussed their applicability in terms of proposing circuit design modifications, finding output error probability or methods proposed to achieve highly accurate simulation results.",2010,0,
1430,1431,A low complexity and efficient slice grouping method for H.264/AVC in error prone environments,"In this paper, a new method is proposed for Macroblock (MB) importance classification of inter frames. Instead of selecting the most important MBs, the least important MBs are decided first. It makes use of the properties of skip mode in the H.264/AVC standard as the first step. Because the number of MBs chosen as skip mode in a frame varies, further classification is usually required. Four other different features therefore are considered to determine the Important Factor of the remaining MBs. It has been proved that the proposed method can provide good objective and subjective video quality performance, whilst also being simple and fast.",2009,0,
1431,1432,A fast method for fault section identification in series compensated transmission lines,"In this paper, a novel and fast method for fault section identification in compensated series transmission lines based on the high frequency traveling wave has been proposed. The method uses the relation of magnitude and polarity between wavefronts of high frequency travelling waves induced by fault. For accurately and fast extracting polarity and magnitude of travelling wave, wavelet transform and modulus maxima are used. Validation of this method is carried out by PSCAD/EMTP and MATLAB simulations for typical 400 kV power system faults. Simulation results reveal high performance of the method.",2010,0,
1432,1433,Operating characteristics of the Permanent-Magnet-Biased Saturation Based Fault Current Limiter,"This paper describes a topological configuration of a fault current limiter consisting of a permanent and saturable core. The operating characteristics of this permanent-magnet-biased saturation based fault current limiter (PMFCL) are simulated in details by means of finite element method (FEM) with ANSOFT. Firstly, the relationship between the total harmonic distortion of transient limiting current and its peak value is analyzed. Secondly, the paper presents the flux density variation with different source voltages as well as magnetic flux density distribution under different operation conditions. Finally, the fault current limiting characteristics influenced by the source voltages and the number of turns of the coil are investigated. The research results present analytical basis and calculation reference for the parameter optimization of this type of PMFCL.",2008,0,
1433,1434,Embedded-software-based approach to testing crosstalk-induced faults at on-chip buses,"Crosstalk effects on long interconnects are becoming significant for high-speed circuits. This paper addresses the problem of testing crosstalk-induced faults at on-chip buses in system-on-a-chip (SOC) designs. We propose a method to self-test on-chip buses at-speed, by executing an automatically synthesized program using on-chip processor cores. The test program, executed at system operational speed, can activate and capture the worst-case crosstalk effects on buses and achieve a complete coverage of crosstalk-induced logical and delay faults. This paper discusses the method and the framework for synthesizing such a test program. Based on the bus protocol, the instruction set architecture of an on-chip processor core, and the system specification, the method generates deterministic tests in the form of instruction sequences. The synthesized test program is highly modularized and compact. The experimental results show that, for testing interconnects between a processor core and any other on-chip core, a 3 K-byte program is sufficient to achieve the complete coverage for crosstalk-induced logical and delay faults",2001,0,
1434,1435,Compact Wireless Devices with Defected-Ground Structures,"In this paper are presented some investigations on micro- strip defected ground structures (DGS). It is shown that the presence of a slot in the ground plane can substantially enhance the electric coupling, or the electric part of a mixed coupling between resonators and its external feedings. The proposed technique can eliminate the very narrow coupling gaps needed for a tight coupling and thus can relax the fab rication tolerances.",2006,0,
1435,1436,Graph fitting test method for the interpolation error of moire fringe,"In order to realize the fast test for the interpolation error, the graph fitting test method for the interpolation error of Moire fringe is put forward in this paper. Firstly, the triangular wave Moire fringe photoelectric signal of the encoder whose phase difference is 90deg are sampled to get the Lissajous graph of the two signals. Secondly, the single wave represented by the founded subsection function is used to fit the practical Moire fringe Lissajous graph. Then, the fitting result is tested to verify whether it satisfies the accuracy requirement. Lastly, the founded subsection function instead of the practical wave function is used to calculate the interpolation error. Using the graph fitting method to sample the Moire fringe single of 15-bits photoelectric encoder to get the interpolation error curve, the tested maximum interpolation error is 70rdquo and the minimum error is - 69rdquo. Comparing with the interpolation error which is received from traditional test method, the change trend of the interpolation error curve is similar, and peak-peak value is almost equality. The results of experiment indicate that: the equipment is convenient and the examination method is efficient and feasible. The measure speed is fast and the manifestation result is intuitionistic. The system can be used in the working field. The method can avoid the speed influence and realize the dynamic interpolation error measure, which is significant for the research of encoder's dynamic accuracy characteristics.",2009,0,
1436,1437,Fault tolerance in systems design in VLSI using data compression under constraints of failure probabilities,"The design of space-efficient support hardware for built-in self-testing (BIST) is of critical importance in the design and manufacture of VLSI circuits. This paper reports new space compression techniques which facilitate designing such circuits using compact test sets, with the primary objective of minimizing the storage requirements for the circuit under test (CUT) while maintaining the fault coverage information. The compaction techniques utilize the concepts of Hamming distance, sequence weights, and derived sequences in conjunction with the probabilities of error occurrence in the selection of specific gates for merger of a pair of output bit streams from the CUT. The outputs of the space compactor may eventually be fed into a time compactor (viz. syndrome counter) to derive the CUT signatures. The proposed techniques guarantee simple design with a very high fault coverage for single stuck-line faults, with low CPU simulation time, and acceptable area overhead. Design algorithms are proposed in the paper, and the simplicity and ease of their implementations are demonstrated with numerous examples. Specifically, extensive simulation runs on ISCAS 85 combinational benchmark circuits with FSIM, ATALANTA, and COMPACTEST programs confirm the usefulness of the suggested approaches",2001,0,
1437,1438,Fault Current Limiter Based on Resonant Circuit Controlled by Power Semiconductor Devices,This work presents a resonant fault current limiter (FCL) controlled by power semiconductor devices. Initially the operation of two ideal resonant circuit topologies as fault current limiter are discussed. The analysis of these circuits is used to derive an alternative topology to the fault current limiter based on the connection of a series and a parallel resonant circuit. Digital models are implemented in the SimPowerSystem/Matlab simulation package to investigate the performance of the proposed FCL to protect transmission and distribution electric networks against short circuit currents. Transfer functions of the linear limiter models are used to identify the effect of each element of the FCL over its stability and its transient response. The developed analysis will be used to derive modifications in the FCL topology in such a way to improve their dynamic response.,2007,0,
1438,1439,How Much Fault Protection is Enough - A Deep Impact Perspective,"For the deep impact project, a myriad of fault protection (FP) monitors, symptoms, alarms and responses is engineered into the spacecraft FP software, common and yet custom to the flyby and impactor mother-daughter spacecraft. Device faults and functional faults are monitored, which are mapped 1-to-n into FP symptoms, per instance of the fault. Symptoms are then mapped n-to-1 to FP alarms, further down mapped n-to-1 to FP responses. Though the final statistics of 49 monitors, 921 symptoms, 667 alarms, and 39 responses appear to be staggering, it remains debatable whether the amount of on-board autonomous fault protection is sufficient and friendly to operate",2005,0,
1439,1440,Fault-tolerant Ethernet middleware for IP-based process control networks,"We present an efficient middleware-based fault-tolerant Ethernet (FTE) developed for process control networks. Our approach is unique and practical in the sense that it requires no change to commercial off-the-shelf hardware (switch, hub, Ethernet physical link, and network interface card) and software (commercial Ethernet NIC card driver and standard protocol such as TCP/IP) yet it is transparent to IP-based applications. The FTE performs failure detection and recovery for handling multiple points of network faults and supports communications with non-FTE-capable devices. Our experimentation shows that FTE performs efficiently, achieving less than 1-ms end-to-end swap time and less than 2-sec failover time, regardless of the concurrent application and system loads. In this paper, we describe the FTE architecture, the challenging technical issues addressed, our performance evaluation results, and the lessons learned in design and development of such an open-network-based fault-tolerant network",2000,0,
1440,1441,A method for inductor core loss estimation in power factor correction applications,"Conventional core loss estimation methods exhibit limitations in dealing with important aspects of switching power converter applications such as different duty cycles, discontinuous-conduction-mode, variable switching frequency, or variable duty cycle operation. These limitations are particularly evident when trying to estimate boost inductor core loss in power factor correction circuits. This paper first presents a core loss estimation method that addresses these limitations and then demonstrates an effective technique to estimate core losses in power factor correction circuits. Finally, the authors show examples of how this method can be conveniently incorporated into simulation software to automate the core loss estimation process. The inductor models that are developed to facilitate this automatic core loss estimation and the approaches to implement the calculation in simulation software, especially a program called SIMPLIS, are also provided",2002,0,
1441,1442,Research and Implementation of Fault-Tolerant Computer Interlocking System,"A new signal control system for railway stations, fault-tolerant all- electronic computer interlocking control system, is proposed,in which the computer-based interlocking system layer is constituted through the implementation of electronic security unit replacing the Relay, and the all-electronic fault-tolerant controlling for whole system is fulfilled through two of three fault-tolerant computer system. Furthermore, the overall structure, function and fault-tolerant security designing of the system are discussed in detail. The system can meet the requirements of high reliability, availability and real-time controlling, it also can monitor the external equipments and their own equipments real-timely. The system has been put into operation and run stably and reliabl.",2010,0,
1442,1443,A new method for fault section estimation in distribution network,"Determination of fault section is a necessary step for locating the fault in the distribution power system. In this paper a new practical based method is presented for fault section estimation in distribution system. In the proposed method, at first different zones is defined using impedance classifier. Then, the suitable locations for installing the cutout fuses are determined using expert of designer. After that, special settings for cutout fuse links are determined in such a way that they operate coordinately. Finally, current waveforms are used to distinguish which cutout fuse operated or in which section fault occurred.",2010,0,
1443,1444,Detecting processor hardware faults by means of automatically generated virtual duplex systems,"A virtual duplex system (VDS) can be used to increase safety without the use of structural redundancy on a single machine. If a deterministic program P is calculating a given function f, then a VDS contains two variants P<sub>a</sub> and P<sub>b</sub> of P which are calculating the diverse functions f<sub>a</sub> and f<sub>b</sub> in sequence. If no error occurs in the process of designing and executing P<sub>a</sub> and P<sub>b</sub>, then f= f<sub>a</sub>=f<sub>b</sub> holds. A fault in the underlying processor hardware is likely to be detected by the deviation of the results, i.e. f<sub>a</sub>(i)=f<sub>b</sub>(i) for input i. Normally, VDSs are generated by manually applying different diversity techniques. This paper, in contrast, presents a new method and a tool for the automated generation of VDSs with a high detection probability for hardware faults. Moreover, for the first time the diversity techniques are selected by an optimization algorithm rather than chosen intuitively. The generated VDSs are investigated extensively by means of software implemented processor fault injection.",2002,0,
1444,1445,Two Types of Action Error: Electrophysiological Evidence for Separable Inhibitory and Sustained Attention Neural Mechanisms Producing Error on Go/No-go Tasks,"Disentangling the component processes that contribute to human executive control is a key challenge for cognitive neuroscience. Here, we employ event-related potentials to provide electrophysiological evidence that action errors during a go/no-go task can result either from sustained attention failures or from failures of response inhibition, and that these two processes are temporally and physiologically dissociable, although the behavioral errora nonintended responseis the same. Thirteen right-handed participants performed a version of a go/no-go task in which stimuli were presented in a fixed and predictable order, thus encouraging attentional drift, and a second version in which an identical set of stimuli was presented in a random order, thus placing greater emphasis on response inhibition. Electrocortical markers associated with goal maintenance (late positivity, alpha synchronization) distinguished correct and incorrect performance in the fixed condition, whereas errors in the random condition were linked to a diminished N2P3 inhibitory complex. In addition, the amplitude of the error-related negativity did not differ between correct and incorrect responses in the fixed condition, consistent with the view that errors in this condition do not arise from a failure to resolve response competition. Our data provide an electrophysiological dissociation of sustained attention and response inhibition.",2009,0,
1445,1446,Proactive Cellular Network Faults Prediction Through Mobile Intelligent Agent Technology,"Cellular network faults prediction models using mobile intelligent agent are presented in this paper. Cellular networks are uncertain and dynamic in their behaviours and therefore we use different artificial intelligent techniques to develop platform independent, autonomous, reasoning and robust agents that can report on any unforeseen anomaly within the cellular network service provider. The specific design and implementation is done using Java agent development framework (JADE). The partial results obtained from the experiments conducted are presented and discussed in this paper.",2007,0,
1446,1447,Automated bug tracking: the promise and the pitfalls,"Bug tacking systems give developers a unique and clear view into user's everyday product experiences. Adding some statistical analysis and software teams can efficiently improve product quality. It's hard to tell precisely how well the error reporting system working, but this seems to be a bug weapon that has landed a permanent spot in microsoft's arsenal. Automated bug tracking, combined with statistical reporting, plays a key role for developers at the Mozilla Foundations, best known for its open source Web browser and email software. The sparse, random sampling approach produces enough data for the team to do what it call ""statistical debugging""-bug detection through statistical analysis.",2004,0,
1447,1448,"e-SAFE: An Extensible, Secure and Fault Tolerant Storage System","This paper describes e-SAFE , a scalable utility-driven distributed storage system that offers very high availability at an archival scale and reduces management overhead such as periodic repairs. e-SAFE is designed to provide a storage utility for environments such as large-scale data centers in enterprise networks where the servers experience temporary unavailability (possibly high load, temporary downtimes due to repair or software/hardware upgrades). e-SAFE is based on a simple principle: efficiently sprinkle data all over a distributed storage and robustly reconstruct even when many of them are unavailable. e-SAFE also provides strong guarantee on data-integrity. The use of Fountain codes for replicating file data blocks, an efficient algorithm for fast parallel encoding and decoding over multiple file segments, a utility module for service differentiation and auto-adjustments of design parameters, and a background replication mechanism hiding the cost of replication and dissemination from the user, provide a fast, durable and autonomous storage solution.",2007,0,
1448,1449,Fault-Tolerant Behavior-Based Motion Control for Offroad Navigation,"Many tasks examined for robotic application like rescue missions or humanitarian demining require a robotic vehicle to navigate in unstructured natural terrain. This paper introduces a motion control for a four-wheeled offroad vehicle trying to tackle the problems arising. These include rough ground, steep slopes, wheel slippage, skidding and others that are difficult to grasp with a physical model and often impossible to acquire with sensory equipment. Therefore, a more reactive approach is chosen using a behavior-based architecture. This way a certain generalization in unknown environment is expected. The resulting behavior network is described and experiments performed in a simulation environment as well as in real world are presented. Additionally the performance of the utilized vehicle in case of mechanical or electronic defects is examined in simulation.",2005,0,
1449,1450,Intermittent faults and effects on reliability of integrated circuits,"A significant amount of research has been aimed at analyzing the effects of high energy particles on semiconductor devices. However, less attention has been given to the intermittent faults. Field collected data and failure analysis results presented in this paper clearly show intermittent faults are a major source of errors in modern integrated circuits. The root cause for these faults ranges from manufacturing residuals to oxide breakdown. Burstiness and high error rates are specific manifestations of the intermittent faults. They may be activated and deactivated by voltage, frequency, and operating temperature variations. The aggressive scaling of semiconductor devices and the higher circuit complexity are expected to increase the likelihood of occurrence of the intermittent faults, despite the extensive use of fault avoidance techniques. Herein we discuss the effectiveness of several fault tolerant approaches, taking into consideration the specifics of the errors generated by intermittent faults. Several solutions, previously proposed for handling particle induced soft errors, are exclusively based on software and too slow for handling large bursts of errors. As a result, hardware implemented fault tolerant techniques, such as error detecting and correcting codes, self checking, and hardware implemented instruction retry, are necessary for mitigating the impact of the intermittent faults, both in the case of microprocessors, and other complex integrated circuits.",2008,0,
1450,1451,Research on the Application of Data Mining in Software Testing and Defects Analysis,"The high dependability software is not only one of software technique development commanding points, but also is the software industry development essential foundation, this paper summarizes the data mining to face the detect of the software credibility test, the appraisal and the technical aspect newest research, elaborated the data mining technology in the software flaw test application, including flaw test in commonly used data mining method, data mining system and software testing management system. Introduced specifically in view of the software flaw's different classification based on the connection rule's software flaw parsing technique's application, proposed based on the association rule's software detect evaluation method, the purpose of which is to decrease software defects and to achieve the rapid growth of software dependability.",2009,0,
1451,1452,Studying the fault-detection effectiveness of GUI test cases for rapidly evolving software,"Software is increasingly being developed/maintained by multiple, often geographically distributed developers working concurrently. Consequently, rapid-feedback-based quality assurance mechanisms such as daily builds and smoke regression tests, which help to detect and eliminate defects early during software development and maintenance, have become important. This paper addresses a major weakness of current smoke regression testing techniques, i.e., their inability to automatically (re)test graphical user interfaces (GUIs). Several contributions are made to the area of GUI smoke testing. First, the requirements for GUI smoke testing are identified and a GUI smoke test is formally defined as a specialized sequence of events. Second, a GUI smoke regression testing process called daily automated regression tester (DART) that automates GUI smoke testing is presented. Third, the interplay between several characteristics of GUI smoke test suites including their size, fault detection ability, and test oracles is empirically studied. The results show that: 1) the entire smoke testing process is feasible in terms of execution time, storage space, and manual effort, 2) smoke tests cannot cover certain parts of the application code, 3) having comprehensive test oracles may make up for not having long smoke test cases, and 4) using certain oracles can make up for not having large smoke test suites.",2005,0,
1452,1453,Performance enhancement defect tolerance in the cell matrix architecture,"This research concentrates on the area of fault tolerant circuit implementation in a field programmable type architecture, In particular, an architecture called the Cell Matrix, presented as a fault tolerant alternative to field programmable gate arrays using their Supercell approach, is studied. Architectural constraints to implement fault tolerant circuit design in this architecture are discussed. Some modifications of its basic Structure, such as the integration of circuitry for error correction and scan path, to enhance fault tolerant circuits design are introduced and are compared to the Supercell approach.",2004,0,
1453,1454,Fault management using passive testing for mobile IPv6 networks,"In this paper, we employ the communicating finite state machine (CFSM) model for networks to investigate fault management using passive testing. First, we introduce the concept of passive testing. Then, we introduce the CFSM model, the observer model and the fault model with necessary assumptions. We introduce the fault detection algorithm using passive testing. Then, we briefly present our new passive testing approach for fault location, fault identification, and fault coverage based on the CFSM model. We illustrate the effectiveness of our new technique through simulation of a practical protocol example, a 4-node mobile IPv6 network. Finally, conclusions and potential extensions are discussed",2001,0,
1454,1455,Electrical model for program disturb faults in non-volatile memories,Non-volatile memories (NVMs) are susceptible to special type of faults known as disturb faults. A class of these disturb faults are faults induced by high electric field stress known as program disturbs. In this paper we discuss the physical nature of the defects that are responsible for these faults in flash memories. We develop an electrical fault model for defects and simulate faulty cell behavior based on physical defect location (in gate oxide). We also evaluate the impact of these defects on cell performance. The modeling technique is flexible and applicable under different disturb conditions and defect characteristics.,2003,0,
1455,1456,Reliable JPEG 2000 wireless imaging by means of error-correcting MQ coder,"A new error resilience tool is proposed for robust JPEG 2000 imaging over noisy channels. In particular, a modified encoder, based on an MQ arithmetic coder with forbidden symbol, is introduced, along with a maximum likelihood error-correcting MQ decoder. The proposed technique features error detection, error concealment and error correction capability, thus adding new useful functionalities to JPEG 2000. Experimental results show that this technique largely outperforms the standard JPEG 2000 error resilience tools for error concealment and hard/soft channel decoding.",2004,0,
1456,1457,A Resource Management System for Fault Tolerance in Grid Computing,"In grid computing, resource management and fault tolerance services are important issues. The availability of the selected resources for job execution is a primary factor that determines the computing performance. The failure occurrence of resources in the grid computing is higher than in a tradition parallel computing. Since the failure of resources affects job execution fatally, fault tolerance service is essential in computational grids. And grid services are often expected to meet some minimum levels of quality of service (QoS) for desirable operation. However Globus toolkit does not provide fault tolerance service that supports fault detection service and management service and satisfies QoS requirement. Thus this paper proposes fault tolerance service to satisfy QoS requirement in computational grids. In order to provide fault tolerance service and satisfy QoS requirements, we expand the definition of failure, such as process failure, processor failure, and network failure. And we propose resource scheduling service, fault detection service and fault management service and show implement and experiment results.",2009,0,
1457,1458,Identification of a chemical process for fault detection application,"This paper presents the application results concerning the fault detection of a dynamic process using linear system identification and model-based residual generation techniques. The first step of the considered approach consists of identifying different families of linear models for the monitored system in order to describe the dynamic behaviour of the considered process. The second step of the scheme requires the design of output estimators (e.g., dynamic observers or Kalman filters) which are used as residual generators. The proposed fault detection and system identification schemes have been tested on a chemical process in the presence of sensor, actuator, component faults and disturbance. The results and concluding remarks have been finally reported.",2004,0,
1458,1459,A fault-tolerant system for Java/CORBA objects,"Frameworks like CORBA facilitate the development of distributed applications through the use of off-the-shelf components. Though the use of distributed components allows faster building of applications, it also reduces the application availability as failure of any component can make the application unavailable. In this paper we present the design and implementation of a fault-tolerant system for CORBA objects implemented in Java. The proposed fault tolerant system employs object replication. We use a three tier architecture in which the middle tier manages replication and acts as a proxy for replicated objects. The proxy ensures consistency and transparency. In the current implementation, the proxy uses the primary-site approach to ensure strong consistency. Saving and restoring of objects' state is done transparently and it does not require object implementation to have special functions implemented for this purpose.",2008,0,
1459,1460,A Byzantine Fault Tolerant Protocol for Composite Web Services,"This paper presents a Byzantine fault tolerant protocol for composite Web Services. We extend Castro and Liskov's well-known practical Byzantine fault tolerance method for the server client model and Tadashi Araragi's method for the agent system to a method for the composite Web Services. Different from other Byzantine tolerant methods for single web service, in composite Web Services we have to create replicas on both sides, while in the server-client model of Castro and Liskov's method, replicas are created only on the server side. We present a modular implementation, and experimental results demonstrate only a moderate overhead due to replication.",2010,0,
1460,1461,Crisp--A Fault Localization Tool for Java Programs,"Crisp is an Eclipse plug-in tool for constructing intermediate versions of a Java program that is being edited. After a long editing session, a programmer will run regression tests to make sure she has not invalidated previously tested functionality. If a test fails unexpectedly, Crisp allows the programmer to select parts of the edit that affected the failing test and to add them to the original program, creating an intermediate version guaranteed to compile. Then the programmer can re-execute the test in order to locate the exact reasons for the failure by concentrating on those affecting changes that were applied. Using Crisp, a programmer can it- eratively select, apply, and undo individual (or sets of) affecting changes and, thus effectively find a small set of failure-inducing changes. Crisp is an extension to our change impact analysis tool, Chianti, [6].",2007,0,
1461,1462,Defect tolerance of solid dielectric transmission class cable,"This paper addresses the issue of determining the level of defect that is likely to cause the failure of solid dielectric transmission class cables. It also proposes methods for predicting the level of defect that is likely to cause failure and to provide a simple analytic approximation for doing so in the case of conducting spheroids aligned with the electric field. A common assumption is that conducting particles > 100 m in length are likely to cause failure of extruded dielectric transmission cable. This analysis suggests that when the effects of operation at elevated temperature are included in the analysis, this is probably an appropriate criterion with a sound technical basis. For maximum background fields in the range of 15 kV/mm, as presently seen near the conductor shield of some transmission class cables, a worst-case particle length in the range of 0.1 mm is likely to be required to cause failure for the worst-case local polymer morphology in the range of the maximum operating temperature.",2005,0,
1462,1463,Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in SRAM-Based FPGAs,An approach combining the SRAM-based field-programmable gate array static cross-section with the results of fault injection campaigns allows predicting the error rate of any implemented application. Experimental results issued from heavy ion tests are compared with predictions to validate the proposed methodology.,2010,0,
1463,1464,A hybrid scatter correction for 3D PET based on an estimation of the distribution of unscattered coincidences: implementation on the ECAT EXACT HR+,"We implemented a hybrid scatter correction method for 3D PET that combines two scatter correction methods in a complementary way. The implemented scheme uses a method based on the discrimination of the energy of events (the estimation of trues method, ETM) and an auxiliary method (the single scatter simulation method, or the convolution-subtraction method), in an attempt to increase the accuracy of the correction over a wider range of acquisitions. The ETM takes into account the scatter from outside the field-of-view (FOV), which is not estimated with the auxiliary method. On the other hand, the auxiliary method accounts for events that have scattered with small angles, which have an energy that can not be discriminated from that of unscattered events using the ETM. The ETM uses the data acquired in an upper energy window above the photopeak (550-650 keV), to obtain a noisy estimate of the unscattered events in the standard window (350-650 keV). Our implementation uses the auxiliary method to correct the residual scatter in the upper window. After appropriate scaling, the upper window data is subtracted from the total coincidences acquired in the standard window, resulting in the final scatter estimate, after smoothing. We compare the hybrid method with the corrections used by default in the 2D and 3D modes of the ECAT EXACT HR+, using phantom measurements. Generally, the contrast was better with the hybrid method, although the relative errors of quantification were similar. We conclude that hybrid techniques such as the one implemented in this work can provide an accurate, general-purpose and practical way to correct the scatter in 3D PET, taking into account the scatter from outside the FOV",2000,0,
1464,1465,On the error performance of 8-VSB TCM decoder for ATSC terrestrial broadcasting of digital television,"The error performance of various 8-VSB TCM decoders for reception of terrestrial digital television is analyzed. In previous work, 8-state TCM decoders were proposed and implemented for terrestrial broadcasting of digital television. In this paper, the performance of a 16-state TCM decoder is analyzed and simulated. It is shown that not only a 16-state TCM decoder outperforms one with 8-states, but it also has much smaller error coefficients",2000,0,
1465,1466,Fault-Tolerant Operating Strategies Applied to Three-Phase Induction-Motor Drives,"This paper presents a comparative analysis involving several fault-tolerant operating strategies, applied to three-phase induction-motor drives, that intend to compensate for inverter faults. The results presented show the advantages and the inconveniences of several fault-tolerant drive structures, under different control techniques, such as the field-oriented control and the direct torque control. Experimental results concerning the performance of the three-phase induction motor, based on the analysis of some key parameters, like induction-motor efficiency, motor power factor, and harmonic distortion of both motor line currents and phase voltages, will be presented",2006,0,
1466,1467,Error detection and concealment for video conferencing service,"Compressed video bitstream is intended for real-time transmission over communication networks. However, it is very sensitive to transmission errors. Even a single bit error can lead to disastrous video quality degradation in both time and spatial domain. This quality deterioration is exacerbated when no error control mechanism is employed to protect coded video data against the error prone environments. In this paper, we present a new error detection and concealment algorithm to reduce the effects of transmission error in the video decoder.We verified that the proposed algorithm generates good performances in PSNR and objective visual quality through the computer simulation by H.324M mobile simulation set.",2009,0,
1467,1468,A persistent diagnostic technique for unstable defects,"We present a persistent diagnostic technique for unstable defects, such as open defects or delay defects. A new ""segment model"" diagnosis for the completely open defects is discussed. Here, we not only focus on the behavior of the principal offender, but also the behavior of the accomplices which cause the unstable behavior of the defect. In this paper, a technique using the layout information for an open fault diagnosis, and a testing method for the delay fault are discussed. Some experimental results of actual chips are shown.",2002,0,
1468,1469,Analytical redundancy based predictive fault tolerant control of a steer-by-wire system using nonlinear observer,"In this paper, a nonlinear observer based analytical redundancy methodology is presented for fault tolerant control of a steer by wire (SBW) system. A long-range predictor based on Diophantine identity has been utilized to improve the fault detection efficiency. The overall predictive fault tolerant control strategy was then implemented and validated on a steer by wire hardware in loop bench. The experimental results showed that the overall robustness of the SBW system was not sacrificed through the usage of analytical redundancy for sensors along with the designed FDIA algorithm. Moreover, the experimental results indicated that the faults could be detected faster using the developed analytical redundancy based algorithms for attenuating-type faults.",2010,0,
1469,1470,LIFTING: A Flexible Open-Source Fault Simulator,"This paper presents LIFTING (LIRMM fault simulator), an open-source simulator able to perform both logic and fault simulations for single/multiple stuck-at faults and single event upset (SEU) on digital circuits described in Verilog. Compared to existing tools, LIFTING provides several features for the analysis of the fault simulation results, meaningful for research purposes. Moreover, as an open-source tool, it can be customized to meet any user requirements. Experimental results show how LIFTING has been exploited on research fields. Eventually, execution time for large circuit simulations is comparable to the one of commercial tools.",2008,0,
1470,1471,Correction,"In the above title (ibid, vol. 44, issue 3, pp. 114-123, Jun 02), corrections were made to Equation 38. There were various typographical errors.",2007,0,
1471,1472,A classification approach for power distribution systems fault cause identification,"Power distribution systems play an important role in modern society. When distribution system outages occur, fast and proper restorations are crucial to improve the quality of services and customer satisfaction. Proper usages of outage root cause identification tools are often essential for effective outage restorations. This paper reports on the investigation and results of two popular classification methods: logistic regression (LR) and artificial neural network (ANN) applied on power distribution fault cause identification. LR is seldom used in power distribution fault diagnosis, while ANN has been extensively used in power system reliability researches. This paper discusses the practical application problems, including data insufficiency, imbalanced data constitution, and threshold setting that are often faced in power distribution fault cause identification problems. Two major distribution fault types, tree and animal contact, are used to illustrate the characteristics and effectiveness of the investigated techniques.",2006,0,
1472,1473,The Method of Error Controlling on the Vectorization of Dot Matrix Image,"A new vectorization method of dot matrix image based on error controlling is studied in this paper. The least-square algorithm is a main method of vectorization of dot matrix image. In the calculation of the fitting error by using the least-square algorithm, if the fitting error is in the range of threshold value, the result of vectorization will be considered to be right. So how to determine the threshold value is the key. In this paper, it has analyzed the vectorization errors of ideal bitmap by least-square algorithm and put forward the judgment formulas to evaluate recognition result. In order to complete vectorization automatically, it has presented a new method of the vectorization of dot matrix image-the least-square rolling recognition algorithm for double direction. It has solved the problem of selecting suitable dot group. The judgment formulas in this paper are scientific and accurate; the method of the vectorization is rapid and high efficient.",2008,0,
1473,1474,Sensor fault-tolerant vector control of induction motors,"The authors propose a multisensor switching strategy for fault-tolerant vector control of induction motors. The proposed strategy combines three current sensors and associated observers that estimate the rotor flux. The estimates provided by the observers are compared at each sampling time by a switching mechanism which selects the sensors-observer pair with the smallest error between the estimated flux magnitude and a desired flux reference. The estimates provided by the selected pair are used to implement a vector control law. The authors consider both field-oriented control and direct torque and flux control schemes. Pre-checkable conditions are derived that guarantee fault tolerance under an abrupt fault of a current sensor. These conditions are such that the observers that use measurements from the faulty sensor are automatically avoided by the switching mechanism, thus maintaining good performance levels even in the presence of a faulty sensor. Simulation results under realistic conditions illustrate the effectiveness of the scheme.",2010,0,
1474,1475,Error Simulation Analysis of Gripping Deviation of Plate Specimen Tensile Test Based on Finite Element Method,"The plate specimen tensile test is a common material mechanical property test in practical engineering, but the gripping deviation of plate specimen will bring tensile test result an error. In this paper, a finite element model of aluminum alloy notched specimen is built and a static finite element analysis is carried on, the influence of the plate specimen tensile test gripping position deviation rightness of experiment to the test result is studied. The result expresses that the gripping deviation of plate specimen will bring tensile test result an error bigger. It is necessary to raise the accuracy request of the gripping position of plate specimen tensile test.",2009,0,
1475,1476,On the efflciency of error concealment techniques in H.264/AVC coders,"In videoconferencing and video telephony applications operating in real time, a fluent transmission, even presenting visible errors, is often preferred over a correct but jerking transmission. This is the reason why error concealment techniques are adopted, within video codecs, to recover the transmission quality without affecting its fluency. In the framework of motion estimation based video codecs, like H.263 and H.264, error resilience facilities are made available in order to mitigate the effects of information loss during transmissions on packet networks. In this paper we focus on the adoption of error concealment techniques in H.264/AVC video coding, providing examples of both objective and subjective performance evaluation, when different algorithms are implemented at the decoder. Besides evaluating the hybrid concealment scheme already foreseen by the standard implementation, we also present a simple ""pure temporal"" replacement technique, which could be interesting for its good performance combined with a very low impact on the overall processing time",2005,0,
1476,1477,Semiparametric RMA Background-Correction for Oligonucleotide Arrays,Microarray technology has provided an opportunity to simultaneously monitor the expression levels of a large number of genes in response to intentional perturbations. A necessary step towards successful use of microarray technology is background correction which aims to remove noise. One of the most popular algorithms for background correction is the robust multichip average (RMA) procedure which relies on an unjustified parametric assumption. In this paper we first check the fitness of the RMA model using a graphical approach and then propose a new background correction method based on a semiparametric RMA model (semi-RMA). Evaluation of the proposed approach based on spike-in data and MAQC (microarray quality control project) data shows our semi-RMA model provides a better fit to microarray data than other approaches.,2007,0,
1477,1478,Architecture-Level Soft Error Analysis: Examining the Limits of Common Assumptions,"This paper concerns the validity of a widely used method for estimating the architecture-level mean time to failure (<i>MTTF</i>) due to soft errors. The method first calculates the failure rate for an architecture-level component as the product of its raw error rate and an architecture vulnerability factor (<i>AVF</i>). Next, the method calculates the system failure rate as the sum of the failure rates (<i>SOFR</i>) of all components, and the system <i>MTTF</i> as the reciprocal of this failure rate. Both steps make significant assumptions. We investigate the validity of the <i>AVF+SOFR</i> method across a large design space, using both mathematical and experimental techniques with real program traces from <i>SPEC</i> 2000 benchmarks and synthesized traces to simulate longer real-world workloads. We show that <i>AVF+SOFR</i> is valid for most of the realistic cases under current raw error rates. However, for some realistic combinations of large systems, long-running workloads with large phases, and/or large raw error rates, the <i>MTTF</i> calculated using <i>AVF+SOFR</i> shows significant-discrepancies from that using first principles. We also show that SoftArch, a previously proposed alternative method that does not make the <i>AVF+SOFR</i> assumptions, does not exhibit the above discrepancies.",2007,0,
1478,1479,Fault detection and diagnosis system for air-conditioning units using recurrent type neural network,"The air-conditioning systems of buildings have been diversified in recent years, and the complexity of the systems has increased. At the same time, stability in the system and low running cost are demanded. To solve these problems, various research projects have been done. The development of the energy load prediction systems and the fault detection and diagnosis systems have received great attention. The authors propose a real time fault diagnosis system for air conditioning units (the heating unit, the cooling unit, the air intake unit, and the air-recycling unit) using a recurrent type neural network",2000,0,
1479,1480,"REDFLAG a Run-timE, Distributed, Flexible, Lightweight, And Generic fault detection service for data-driven wireless sensor applications","Increased interest in Wireless Sensor Networks (WSNs) by scientists and engineers is forcing WSN research to focus on application requirements. Data is available as never before in many fields of study; practitioners are now burdened with the challenge of doing data-rich research rather than being data-starved. In-situ sensors can be prone to errors, links between nodes are often unreliable, and nodes may become unresponsive in harsh environments, leaving to researchers the onerous task of deciphering often anomalous data. Presented here is the REDFLAG fault detection service for WSN applications, a Run-timE, Distributed, Flexible, detector of faults, that is also Lightweight And Generic. REDFLAG addresses the two most worrisome issues in data-driven wireless sensor applications: abnormal data and missing data. REDFLAG exposes faults as they occur by using distributed algorithms in order to conserve energy. Simulation results show that REDFLAG is lightweight both in terms of footprint and required power resources while ensuring satisfactory detection and diagnosis accuracy. Because REDFLAG is unrestrictive, it is generically available to a myriad of applications and scenarios.",2009,0,
1480,1481,RFID and IPv6-enabled Ubiquitous Medication Error and Compliance Monitoring System,Became of the world's rapidly-aging population' costs for healthcare are getting higher and higher due partially to people who failed to comply with their medication regimens costing billion per year and affecting few million patients. This paper presents a combination of IPv6 network and RFID-based medication error monitoring system integrating with Wi-Fi and GSM wireless communication techniques that is able to collect user's medicine-taking records any time - anywhere as reference for the purpose of proper diagnosis and reduce the healthcare costs as a result.,2007,0,
1481,1482,A new approach for mitigating carrier phase multipath errors in multi-gnss real-time kinematic (RTK) receivers,"In this paper, we introduce a new approach for RTK positioning using triple-frequency combinations of GNSS measurements in presence of carrier phase multipath. The proposed method is based on a modification of the LAMBDA method, where the a-priori information on multipath errors is exploited as a constraint in the optimization and ambiguities search process to mitigate the effect of multipath. Triple-frequency combinations of measurements is used to formulate a new carrier phase multipath index, then incorporate it as additional constraint in the LAMBDA method cost function for multi-frequency ambiguity resolution. Simulations and real experiments shows the effectiveness of the developed scheme.",2010,0,
1482,1483,Microstrip Monopole Antenna With Enhanced Bandwidth Using Defected Ground Structure,"In this letter, a double U-shaped defected ground structure (DGS) is proposed to broaden impedance bandwidth of a microstrip-fed monopole antenna. The antenna structure consists of a simple trapezoid monopole with a DGS microstrip feedline for excitation and impedance bandwidth broadening. Measurement shows that the antenna has 10-dB return loss from 790 to 2060 MHz, yielding 112.4% impedance bandwidth improvement over that of traditional design.",2008,0,
1483,1484,Error whitening criterion for adaptive filtering: theory and algorithms,"Mean squared error (MSE) has been the dominant criterion in adaptive filter theory. A major drawback of the MSE criterion in linear filter adaptation is the parameter bias in the Wiener solution when the input data are contaminated with noise. We propose and analyze a new augmented MSE criterion called the Error Whitening Criterion (EWC). EWC is able to eliminate this bias when the noise is white. We will determine the analytical solution of the EWC, discuss some interesting properties, and develop stochastic gradient and other fast algorithms to calculate the EWC solution in an online fashion. The stochastic algorithms are locally computable and have structures and complexities similar to their MSE-based counterparts (LMS and NLMS). Convergence of the stochastic gradient algorithm is established with mild assumptions, and upper bounds on the step sizes are deduced for guaranteed convergence. We will briefly discuss an RLS-like Recursive Error Whitening (REW) algorithm and a minor components analysis (MCA) based EWC-total least squares (TLS) algorithm and further draw parallels between the REW algorithm and the Instrumental Variables (IV) method for system identification. Finally, we will demonstrate the noise-rejection capability of the EWC by comparing the performance with MSE criterion and TLS.",2005,0,
1484,1485,Multilevel full-chip gridless routing considering optical proximity correction,"To handle modern routing with nanometer effects, we need to consider designs of variable wire widths and spacings, for which gridless routers are desirable due to their great flexibility. The gridless routing is much more difficult than the grid-based one because the solution space of gridless routing is significantly larger than that of grid-based one. In this paper, we present the first multilevel, full-chip gridless detailed router. The router integrates global routing, detailed routing, and congestion estimation together at each level of the multilevel routing. It can handle non-uniform wire widths and consider routability and optical proximity correction (OPC). Experimental results show that our approach obtains significantly better routing solutions than previous works. For example, for a set of 11 commonly used benchmark circuits, our approach achieves 100% routing completion for all circuits while the famous state-of-the-art three-level routing and multilevel routing (multilevel global routing + flat detailed routing) cannot complete routing for any of the circuits. Besides, experimental results show that our multilevel gridless router can handle non-uniform wire widths efficiently and effectively (still maintain 100% routing completion for all circuits). In particular, our OPC-aware multilevel gridless router archives an average reduction of 11.3% pattern features and still maintains 100% routability for the 11 benchmark circuits.",2005,0,
1485,1486,A new adaptive hybrid neural network and fuzzy logic based fault classification approach for transmission lines protection,"In this paper, an adaptive hybrid neural networks and fuzzy logic based algorithm is proposed to classify fault types in transmission lines. The proposed method is able to identify all ten shunt faults in transmission lines with high level of robustness against variable conditions such as measured amplitudes and fault resistance. In this approach, a two-end unsynchronized measurement of the signals is used. For real-time estimation of unknown synchronization angle and three phase phasors a two-layer adaptive linear neural (ADALINE) network is used. The estimated parameters are fed to a fuzzy logic system to classify fault types. This method is feasible to be used in digital distance relays which are able to be programmed, to share and discourse data with all protective and monitoring devices. The proposed method is evaluated by a number of simulations conducted in PSCAD/EMTDC and MATLAB software.",2008,0,
1486,1487,Frame loss error concealment for spatial scalability using hallucination,"We present a new error concealment algorithm for spatially scalable video coding with frame loss in the enhancement layer, based on the technique of hallucination. For a lost enhancement layer frame, the error concealment is done as hallucinating its base layer frame, using the database trained from previously decoded frames nearby to the lost one. Simulation results show that the proposed method could out-perform the state-of-the-art error concealment algorithms of SVC significantly.",2009,0,
1487,1488,A Comparison of Mandani and Sugeno Inference Systems for a Space Fault Detection Application,"This research provides a comparison between the performances of TSK (Takagi, Sugeno, Kang)-type versus Mandani-type fuzzy inference systems. The main motivation behind this research was to assess which approach provides the best performance for a gyroscope fault-detection application, developed in 2002 for the European Space Agency (ESA) satellite ENVISAT. Due to the importance of performance in online systems we compare the application, developed with Mamdani model, with a TSK formulation using three types of tests: processing time for both systems, robustness in the presence of randomly generated noise; and sensitivity analysis of the systems' behaviors to changes in input data. The results show that the TSK model perform better in all three tests, hence we may conclude that replacing a Mamdani system with an equivalent TSK system could be a good option to improve the overall performance of a fuzzy inference system.",2006,0,
1488,1489,Magnetic Field Sensor Using a Physical Model to Pre-Calculate the Magnetic Field and to Remove Systematic Error due to Physical Parameters,"Speed and angle measurements of rotating shafts are very important in automotive applications. Typical sensing arrangements for angular measurements using the magnetic principle are analyzed in this paper. It is shown that such sensor arrangements are prone to phase errors. The phase error mainly depends on the distance between sensor element and rotating shaft. By employing finite element simulations, a variety of frequently used magnetic field sensor configurations are investigated. Measurements complement the simulations and confirm correct simulation results. Due to mounting tolerances and mechanical vibrations in automotive applications the sensor distance is varying and dynamic phase errors appear. The accuracy of measurement can only be improved if these errors get compensated. This can be done with the help of digital signal processing.",2007,0,
1489,1490,BLoG: Post-Silicon bug localization in processors using bug localization graphs,"Post-silicon bug localization - the process of identifying the location of a detected hardware bug and the cycle(s) during which the bug produces error(s) - is a major bottleneck for complex integrated circuits. Instruction Footprint Recording and Analysis (IFRA) is a promising post-silicon bug localization technique for complex processor cores. However, applying IFRA to new processor microarchitectures can be challenging due to the manual effort required to implement special microarchitecture-dependent analysis techniques for bug localization. This paper presents the Bug Localization Graph (BLoG) framework that enables application of IFRA to new processor microarchitectures with reduced manual effort. Results obtained from an industrial microarchitectural simulator modeling a state-of-the-art complex commercial microarchitecture (Intel Nehalem, the foundation for the Intel CoreTM i7 and CoreTM i5 processor families) demonstrate that BLoG-assisted IFRA enables effective and efficient post-silicon bug localization for complex processors with high bug localization accuracy at low cost.",2010,0,
1490,1491,FlowChecker: Detecting Bugs in MPI Libraries via Message Flow Checking,"Many MPI libraries have suffered from software bugs, which severely impact the productivity of a large number of users. This paper presents a new method called FlowChecker for detecting communication-related bugs inMPI libraries. The main idea is to extract program intentions of message passing (MPintentions), and to check whether theseMP-intentions are fulfilled correctly by the underlying MPI libraries, i.e., whether messages are delivered correctly from specified sources to specified destinations. If not, FlowChecker reports the bugs and provides diagnostic information. We have built a FlowChecker prototype on Linux and evaluated it with five real-world bug cases in three widely-used MPI libraries, including Open MPI, MPICH2, and MVAPICH2. Our experimental results show that FlowChecker effectively detects all five evaluated bug cases and provides useful diagnostic information. Additionally, our experiments with HPL and NPB show that FlowChecker incurs low runtime overhead (0.9-9.7% on three MPI libraries).",2010,0,
1491,1492,A H.263 compatible error resilient video coder,"We present an error resilient video coder compatible with the ITU-T H.263 standard. Resynchronization flag insertion, error detection, localization and concealment in the decoder, and dynamic programming mode selection based on error tracking are the three main adopted error-resilient strategies. An information feedback method, which utilizes the H.263 video bit stream but does not modify its syntax, is described. Simulation results for the binary symmetric channel (BSC) with random bit errors are given to show the robustness of the proposed video coder",2000,0,
1492,1493,Evaluating the Use of Requirement Error Abstraction and Classification Method for Preventing Errors during Artifact Creation: A Feasibility Study,"Defect prevention techniques can be used during the creation of software artifacts to help developers create high-quality artifacts. These artifacts should have fewer faults that must be removed during inspection and testing. The Requirement Error Taxonomy that we have developed helps focus developers' attention on common errors that can occur during requirements engineering. Our claim is that, by focusing on those errors, the developers will be less likely to commit them. This paper investigates the usefulness of the Requirement Error Taxonomy as a defect prevention technique. The goal was to determine if making requirements engineers' familiar with the Requirement Error Taxonomy would reduce the likelihood that they commit errors while developing a requirements document. We conducted an empirical study in which the participants were given the opportunity to learn how to use the Requirement Error Taxonomy by employing it during the inspection of a requirements document. Then, in teams of four, they developed their own requirements document. This requirements document was then evaluated by other students to identify any errors made. The hypothesis was that participants who find more errors during the inspection of a requirements document would make fewer errors when creating their own requirements document. The overall result supports this hypothesis.",2010,0,
1493,1494,Importance sampling for error event analysis of HMM frequency line trackers,"This paper considers the problem of designing efficient and systematic importance sampling (IS) schemes for the performance study of hidden Markov model (HMM) based trackers. Importance sampling (IS) is a powerful Monte Carlo (MC) variance reduction technique, which can require orders of magnitude fewer simulation trials than ordinary MC to obtain the same specified precision. We present an IS technique applicable to error event analysis of HMM based trackers. Specifically, we use conditional IS to extend our work in another of our paper to estimate average error event probabilities. In addition, we derive upper bounds on these error probabilities, which are then used to verify the simulations. The power and accuracy of the proposed method is illustrated by application to an HMM frequency tracker.",2002,0,
1494,1495,Towards Autonomic Fault Recovery in System-S,"System-S is a stream processing infrastructure which enables program fragments to be distributed and connected to form complex applications. There may be potentially tens of thousands of interdependent and heterogeneous program fragments running across thousands of nodes. While the scale and interconnection imply the need for automation to manage the program fragments, the need is intensified because the applications operate on live streaming data and thus need to be highly available. System-S has been designed with components that autonomically manage the program fragments, but the system components themselves are also susceptible to failures which can jeopardize the system and its applications. The work we present addresses the self healing nature of these management components in System-S. In particular, we show how one key component of System-S, the job management orchestrator, can be abruptly terminated and then recover without interrupting any of the running program fragments by reconciling with other autonomous system components. We also describe techniques that we have developed to validate that the system is able to autonomically respond to a wide variety of error conditions including the abrupt termination and recovery of key system components. Finally, we show the performance of the job management orchestrator recovery for a variety of workloads.",2007,0,
1495,1496,A novel gray clustering filtering algorithms for identifying the false alert in aircraft long-distance fault diagnosis,"The fault report is downloaded from the aircraft with ACARS for the line maintenance. This is widely attended currently. But the false alert often occurs in the fault report and drop the maintenance efficiency Aimed at the problem, the gray clustering filtering algorithms is set up based on gray cluster and filter theory .The algorithms can identify the false alert in the fault report effectively.",2007,0,
1496,1497,Research and Development of Thermal Error Compensation Embedded in CNC System,"The effective compensation upon machining error, which is given rise to by machine tool's thermal deformation, is an important fashion of improving machining efficiency in CNC system. The external thermal error compensation method is mostly adopted because of the closure in conventional CNC system. In order to solve embedded integral problem of real-time thermal error compensation function in CNC system, an concentrating approach is introduced by integral design. Of a concentrating manner, the thermal deformation error of X-axis screw is modeled and analyzed in THK6370 Horizontal processing centre. Not only is the inserted thermal error real-time compensation function realized in complete open CNC system, but also can the thermal error on-line real-time compensation be executed. Finally, the experiment validates the concentrating mode and the effective compensation of thermal deformation error can be implemented upon THK6370 horizontal machining centre X-axis screw.",2010,0,
1497,1498,The fault-tolerant technique in the Rotor Current Controller in Induction Wind Generator,"The thesis expounds the importance of the rotor current controller in induction wind generator in wind generator sets. The topic analyses the kind and the source of the faults of the rotor current controller. Also it design the method of the technology of tolerating faults in the rotor current controller. Some kinds of the technology of tolerating faults are used in hardware and software. So the system can identify the kinds of the fault and eliminate the fault, then making the system can control itself and achieve the best work state again. Using mix redundant methods in software part in order to protect the program running reliably and disposing dates correctly. The system which can be controlled remotely and showed communicates with the computer and can send the result of diagnosis to it in time.",2009,0,
1498,1499,An ACS robotic control algorithm with fault tolerant capabilities,"This paper demonstrates that an adaptive computing system (ACS) is good platform for implementing robotic control algorithms. We show that an ACS can be used to provide both good performance and high dependability. An example of an FPGA-implemented dependable control algorithm is presented. The flexibility of ACS is exploited by choosing the best precision for our application. This reduces the amount of required hardware and improves performance. Results obtained from a WILDFORCE emulation platform showed that even using 0.35 m technology, an FPGA-implemented control algorithm has comparable performance with the software-implemented control algorithm in a 0.25 m microprocessor. Different voting schemes are used in conjunction with multi-threading and combinational redundancy to add fault tolerance to the robotic controller. Error-injection experiments demonstrate that robotic control algorithms with fault tolerance techniques are orders of magnitude less vulnerable to faults compared to algorithms without any fault tolerant features",2000,0,
1499,1500,Error-free simplification of transparent Mamdani systems,"This paper shows that combinatorial complexity of fuzzy systems is at least in part caused by redundancy in these systems and presents the algorithm and its implementation for detection and removal of such redundancy for a special class of Mamdani systems. Performance of the simplification algorithm is demonstrated with uniformly impressive results on acknowledged benchmarks coming from different areas of engineering - truck backer-upper control, Mackey-Glass time series prediction and iris data classification.",2008,0,
1500,1501,Meshing Simulation and Experimental Analysis of Transmission Error for Modified Spiral Bevel Gear,"Based on the meshing theory of spiral bevel gears, the Transmission error (TE)curves of two different pairs of spiral bevel gears (one pair is modified spiral bevel gears, another one is general spiral bevel gears) are achieved. According to the characteristic of theoretical TE curve, the spiral bevel gear transmission error measurement system is designed, and the comparative experiment for the two pairs of gears were carried out on the system under different loads. The real TE of the two pair of gears has acquired. The experimental results show that modified spiral bevel gear pair is better than the general one in the dynamic behavior, which was verified the new method that modifies tooth face with modified tools is useful and effective to reduce gear vibration and noise.",2010,0,
1501,1502,Architectural-Based Validation of Fault-Tolerant Software,"Many architecture-centred approaches have been proposed for constructing dependable component-based systems. However, few of them provide an integrated solution for their development that combines fault prevention, fault removal, and fault tolerance techniques. This paper proposes a rigorous development approach based on an architectural abstraction, which combines formal methods and robustness testing. The architectural abstraction assumes a crash failure semantics, and when it is instantiated as an architectural element provides the basis for architecting fault tolerant systems. The architecture is formally specified using the B-method and CSP. Assurances that the software system is indeed dependable are obtained by combining formal specification for removing ambiguities from the architectural representation, and robustness testing for validating the source code against its software architecture. The feasibility of the proposed approach is illustrated in the context of a financial critical system.",2009,0,
1502,1503,Towards Optimal Resource Allocation in Partial-Fault Tolerant Applications,"We introduce Zen, a new resource allocation framework that assigns application components to node clusters to achieve high availability for partial-fault tolerant (PFT) applications. These applications have the characteristic that under partial failures, they can still produce useful output though the output quality may be reduced. Thus, the primary goal of resource allocation for PFT applications is to prevent, delay, or minimize the impact of failures on the application output quality. This paper is the first to approach this resource allocation problem from a theoretical perspective, and obtains a series of results regarding component assignments that provide the highest service availability under the constraints imposed by the application data flow graph and the hosting clusters. We show that (1) even simple versions of this resource allocation problem are NP-Hard, (2) a 2-approximate polynomial-time algorithm works for tree topologies, and (3) a simple greedy component placement performs well in practice for general application topologies. We implement a system prototype to study the application availability achieved by Zen compared to failure-oblivious placement, replication, and Zen+replication. Our experimental results show that three PFT applications achieve significant data output quality and availability benefits using Zen.",2008,0,
1503,1504,Motion correction of head movements in PET: realisation for routine usage,With the increase of scanner resolution head motion in PET brain studies becomes an increasingly serious limitation. Methods to correct for motion have been proposed. In this work the realisation of a motion tracking system in a PET environment and the motion correction of list mode data with the MAF method is presented. In a phantom study the method is validated and the loss in image quality is documented in a phantom with simulated movements. The relevance of motion correction for patient data above the level of system resolution is studied. In a real patient study we show the effect of motion and the applicability of the presented system.,2003,0,
1504,1505,Team-based fault content estimation in the software inspection process,"The main objective of software inspection is to detect faults within a software artifact. This helps to reduce the number of faults and to increase the quality of a software product. However, although inspections have been performed with great success, and although the quality of the product is increased, it is difficult to estimate the quality. During the inspection process, attempts with objective estimations as well as with subjective estimations have been made. These methods estimate the fault content after an inspection and give a hint of the quality of the product. This paper describes an experiment conducted throughout the inspection process, where the purpose is to compare the estimation methods at different points. The experiment evaluates team estimates from subjective and objective fault content estimation methods integrated with the software inspection process. The experiment was conducted at two different universities with 82 reviewers. The result shows that objective estimates outperform subjective when point and confidence intervals are used. This contradicts the previous studies in the area.",2004,0,
1505,1506,Jointly optimized error-feedback and realization for roundoff noise minimization in state-space digital filters,"Roundoff noise (RN) is known to exist in digital filters and systems under finite-precision operations and can become a critical factor for severe performance degradation in infinite impulse response (IIR) filters and systems. In the literature, two classes of methods are available for RN reduction or minimization-one uses state-space coordinate transformation, the other uses error feedback/feed-forward of state variables. In this paper, we propose a method for the joint optimization of error feedback/feed-forward and state-space realization. It is shown that the problem at hand can be solved in an unconstrained optimization setting. With a closed-form formula for gradient evaluation and an efficient quasi-Newton solver, the unconstrained minimization problem can be solved efficiently. With the infinite-precision solution as a reference point, we then move on to derive a semidefinite programming (SDP) relaxation method for an approximate solution of optimal error-feedback matrix with sum-of-power-of-two entries under a given state-space realization. Simulations are presented to illustrate the proposed algorithms and demonstrate the performance of optimized systems.",2005,0,
1506,1507,A highly resilient routing algorithm for fault-tolerant NoCs,"Current trends in technology scaling foreshadow worsening transistor reliability as well as greater numbers of transistors in each system. The combination of these factors will soon make long-term product reliability extremely difficult in complex modern systems such as systems on a chip (SoC) and chip multiprocessor (CMP) designs, where even a single device failure can cause fatal system errors. Resiliency to device failure will be a necessary condition at future technology nodes. In this work, we present a network-on-chip (NoC) routing algorithm to boost the robustness in interconnect networks, by reconfiguring them to avoid faulty components while maintaining connectivity and correct operation. This distributed algorithm can be implemented in hardware with less than 300 gates per network router. Experimental results over a broad range of 2D-mesh and 2D-torus networks demonstrate 99.99% reliability on average when 10% of the interconnect links have failed.",2009,0,
1507,1508,Using formal verification to eliminate software errors,"The use of software in safety critical railway applications is increasing. Techniques for developing and running such software are based on reducing the probability of an error in the software causing an unsafe system failure; e.g. that the system permits a train to proceed when it shouldn't. At the same time these techniques cause problems of the opposite nature; that the systems fail to allow trains to proceed even when it is safe to proceed. Such failures, although not directly dangerous, lead to stress and delayed traffic, which in turn can cause safety to be compromised. This paper shows how the use of formal verification can solve this problem. This technique can be used to find and eliminate all errors in the software, before the system is put into service. Formal verification is one of the corner stones of Prover iLock, an off-the-shelf commercial tool suite used for developing railway signalling applications.",2008,0,
1508,1509,Fault detection in a wastewater treatment plant,The wastewater treatment plants are very unstable and the waters to be treated are ill defined. The command of those processes needs to be done with advanced methods of control and supervision. Those methods have to take into account the bad knowledge of the processes. The most important situations that can imply problems for the plant are listed. The parameters that allow the comparison between the crisis situation and normal one are measured. The fuzzy logic method is used to distinguish the different situations in order to take a decision for the working of the plant. The method is tested in simulation and on a real plant.,2001,0,
1509,1510,Study of Superconducting Fault Current Limiters for System Integration of Wind Farms,"As electrical energy will be provided from renewable sources, the connection of a large number of wind farms to existing distribution networks may lead to the increasing fault levels beyond the capacity of existing switchgear, especially in urban areas. Fault current limiters (FCLs) are essentially expected to control the prospective short circuit currents. In this paper, investigations were carried out to assess the effectiveness of the resistive superconducting fault current limiters (SFCLs) for fault level management in wind power system. System studies confirmed that the superconducting fault current limiter (SFCL) could not only control the fault currents but also suppress the inrush currents, when wind farm has adopted in the case of the system interconnection. As a result, the highly efficient operation of the wind power system becomes more possible by introducing the superconducting fault current limiters.",2010,0,
1510,1511,An Improved Knowledge Connectivity Condition for Fault-Tolerant Consensus with Unknown Participants,"For self-organized networks that possess highly decentralized and self-organized natures, neither the identity nor the number of processes is known to all participants at the beginning of the computation because no central authority exists to initialize each participant with some context information. Hence, consensus, which is essential to solving the agreement problem, in such networks cannot be achieved in the ways for traditional fixed networks. To address this problem of Consensus with Unknown Participants (CUP), a variant of the traditional consensus problem was proposed in the literature, by relaxing the requirement for the original knowledge owned by every process about all participants in the computation. Correspondingly, the CUP problem considering process crashes was also introduced, called the Fault-Tolerant Consensus with Unknown Participants (FT-CUP) problem. In this paper, we propose a knowledge connectivity condition sufficient for solving the FT-CUP problem, which is improved from the one proposed in our previous work.",2010,0,
1511,1512,A .NET framework for an integrated fault diagnosis and failure prognosis architecture,"This paper presents a .NET framework as the integrating software platform linking all constituent modules of the fault diagnosis and failure prognosis architecture. The inherent characteristics of the .NET framework provide the proposed system with a generic architecture for fault diagnosis and failure prognosis for a variety of applications. Functioning as data processing, feature extraction, fault diagnosis and failure prognosis, the corresponding modules in the system are built as .NET components that are developed separately and independently in any of the .NET languages. With the use of Bayesian estimation theory, a generic particle-filtering-based framework is integrated in the system for fault diagnosis and failure prognosis. The system is tested in two different applications - bearing spalling fault diagnosis and failure prognosis and brushless DC motor turn-to-turn winding fault diagnosis. The results suggest that the system is capable of meeting performance requirements specified by both the developer and the user for a variety of engineering systems.",2010,0,
1512,1513,Prior Training of Data Mining System for Fault Detection,"Many approaches have been used to fault discovery in complex systems. Model based reasoning; data mining analysis; rule base methods are the few among those approaches. To be successfully applied, these approaches all have to have some knowledge about the system prior to faults detection during the system run. Fault Tree Analysis shows the possible causes of a system malfunction by enumerating the suspect components and their respective failure modes that may have induced the problem. The rule based inference build the model based on the expert knowledge. Those models and methods have one thing in common; they have presumed some prior-conditions. Complex systems often use fault trees to analyze the faults. Fault diagnosis, when an error occurs, is performed by engineers and analysts performing extensive examination of all data gathered during the mission. International space station (ISS) control center operates on the data feedback from the system and decisions are made based on threshold values by using fault trees. Since those decision-making tasks are safety critical and must be done promptly, the engineers who manually analyze the data are facing the challenge of time limit. To automate this process, this paper presents an approach that uses decision trees to discover faults from data in real-time and capture the contents of fault trees as prior knowledge and use them to set the initial state of the decision trees.",2007,0,
1513,1514,Cooperating search agents explore more than defecting search agents in the Internet information access,"In the Internet Information Access Problem, information-seeking agents (software or humans) are selfishly rational in obtaining the information sought. From a single agent's perspective, sending out as many queries as possible maximizes the chance of achieving the information sought. However, if every agent does the same, the information servers will be overloaded and most of the search agents won't be able to retrieve the information. Our previous results suggest that when behaviorally similar information-seeking agents cluster together, cooperation is promoted. In these experiments, the ranges of query (i.e., maximum logical distance from the information-seeking agents to potential information severs) is fixed for each search agent; agents only inquire the severs within the distance. The article evolves the range of the access distance. When similar agents (cooperators with cooperators and defectors with defectors), cluster together, cooperators tend to access diversified information sites while defectors tend to access only common information sites, resulting in high congestion. This phenomenon can be seen in human agents as well. When an agent sees too much competition or overuse of resource, it considers alternative choices. For example, when people see a congested highway, they tend to take other routes even if the routes may be longer. A similar phenomena is observed in our experiments. The results of the research can be used to help designing the Internet search agents that are efficient and less burdensome to information servers",2001,0,
1514,1515,Fault-tolerant strategy based on dynamic model matching for dual-redundant computer in the space robot,"Based on the requirements of space robot control system such as high reliability, low power consumption, small size and real-time, this paper presents a dual redundant fault-tolerant strategy for space robot controller. According to the peculiarity of the space robot control systems and its workflow, this strategy is based on the fault-tolerant hardware architecture and the linear running model of space robot software. The two computers communicate with each other by 485 bus and heartbeat line, and share field data through CAN bus in this architecture. The imported criterions include the heartbeat signal, the pattern matching, the dynamic synchronization data and the results returned from the other device. The fault-tolerant strategy which has redundancy criterion is formulated in this paper. This design can not only ensure the time performance of switching between two computers, but also can lower the communication data and improve the reliability and effectiveness of dual-redundant fault-tolerant system.",2008,0,
1515,1516,Similarity-Guided Streamline Placement with Error Evaluation,"Most streamline generation algorithms either provide a particular density of streamlines across the domain or explicitly detect features, such as critical points, and follow customized rules to emphasize those features. However, the former generally includes many redundant streamlines, and the latter requires Boolean decisions on which points are features (and may thus suffer from robustness problems for real-world data). We take a new approach to adaptive streamline placement for steady vector fields in 2D and 3D. We define a metric for local similarity among streamlines and use this metric to grow streamlines from a dense set of candidate seed points. The metric considers not only Euclidean distance, but also a simple statistical measure of shape and directional similarity. Without explicit feature detection, our method produces streamlines that naturally accentuate regions of geometric interest. In conjunction with this method, we also propose a quantitative error metric for evaluating a streamline representation based on how well it preserves the information from the original vector field. This error metric reconstructs a vector field from points on the streamline representation and computes a difference of the reconstruction from the original vector field.",2007,0,
1516,1517,Automatic Fault Localization for SystemC TLM Designs,"To meet today's time-to-market demands catching bugs as early as possible during the design of a system is absolutely essential. In Electronic System Level (ESL) design where SystemC has become the de-facto standard due to Transaction Level Modeling (TLM), many approaches for verification have been developed. They determine an error trace which demonstrates the difference between the required and the actual behavior of the system. However, the subsequent debugging process is very time-consuming, in particular due to TLM-related faults caused by complex process synchronization and concurrency. In this paper, we present an automatic fault localization approach for SystemC TLM designs. The approach determines components that can be changed such that the intended behavior of the design is obtained removing the contradiction given by the error trace. Techniques based on Bounded Model Checking (BMC) are used to find the components. We demonstrate the quality of our approach by experimental results.",2010,0,
1517,1518,No time for bugs,Motor-sport teams expect a fast turnaround when they tune the software that goes into the electronic controllers that manage the highly tuned engines used in their cars. Subtle changes to the way the embedded computer controls the engine can make the difference between winning and being stuck in the pits. And it means that the developers at one specialist company can have just two hours to make critical changes to their code.,2004,0,
1518,1519,Fault Classification for SRAM-Based FPGAs in the Space Environment for Fault Mitigation,"This letter proposes a classification algorithm to discriminate between recoverable and not recoverable faults occurring in static random access memory (SRAM)-based field-programmable gate arrays (FPGAs), with the final aim of devising a methodology to enable the exploitation of these devices also in space applications, typically characterized by long mission times, where permanent faults become an issue. By starting from a characterization of the radiation effects and aging mechanisms, we define a controller able to classify such faults and consequently to apply the appropriate mitigation strategy.",2010,0,
1519,1520,A Three-Phases Byzantine Fault Tolerance Mechanism for HLA-Based Simulation,"A large scale HLA-based simulation (federation) is composed of a large number of simulation components (federates), which may be developed by different participants and executed at different locations. Byzantine failures, caused by malicious attacks and software/hardware bugs, might happen to federates and propagate in the federation execution. In this paper, a three-phases (i.e., failure detection, failure location, and failure recovery) Byzantine Fault Tolerance (BFT) mechanism is proposed based on the decoupled federate architecture. By combining the replication, check pointing and message logging techniques, some redundant executions of federate replicas are avoided. The BFT mechanism is implemented using both Barrier and No-Barrier federate replication structures. Protocols are also developed to remove the epidemic effect caused by Byzantine failures. As the experiment results show, the BFT mechanism using No-Barrier replication outperforms that using Barrier replication significantly in the case that federate replicas have different runtime performance.",2010,0,
1520,1521,"Network Dependability, Fault-tolerance, Reliability, Security, Survivability: A Framework for Comparative Analysis","A number of qualitative and quantitative terms are used to describe the performance of what has come to be known as information systems, networks or infrastructures. However, some of these terms either have overlapping meanings or contain ambiguities in their definitions presenting problems to those who attempt a rigorous evaluation of the performance of such systems. The phenomenon arises because the wide range of disciplines covered by the term information technology have developed their own distinct terminologies. This paper presents a systematic approach for determining common and complementary characteristics of five widely-used concepts, dependability, fault-tolerance, reliability, security, and survivability. The approach consists of comparing definitions, attributes, and evaluation measures for each of the five concepts and developing corresponding relations. Removing redundancies and clarifying ambiguities will help the mapping of broad user-specified requirements into objective performance parameters for analyzing and designing information infrastructures",2006,0,
1521,1522,Software-implemented fault detection for high-performance space applications,"We describe and test a software approach to overcoming radiation-induced errors in spaceborne applications running on commercial off-the-shelf components. The approach uses checksum methods to validate results returned by a numerical subroutine operating subject to unpredictable errors in data. We can treat subroutines that return results satisfying a necessary condition having a linear form; the checksum tests compliance with this condition. We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent infinite-precision numerical calculations. We test both the general effectiveness of the linear fault tolerant schemes we propose, and the correct behavior of our parallel implementation of them",2000,0,
1522,1523,Probabilistic analysis of CAN with faults,"As CANs (controller area networks) are being increasingly used in safety-critical applications, there is a need for accurate predictions of failure probability. In this paper we provide a general probabilistic schedulability analysis technique which is applied specifically to CANs to determine the effect of random network faults on the response times of messages. The resultant probability distribution of response times can be used to provide probabilistic guarantees of real-time behaviour in the presence of faults. The analysis is designed to have as little pessimism as possible but never be optimistic. Through simulations, this is shown to be the case. It is easy to apply and can provide useful evidence for justification of an event-triggered bus in a critical system.",2002,0,
1523,1524,Implementation of an Overblowing Correction Controller and the proposal of a quantitative assessment of the sound's pitch for the anthropomorphic saxophonist robot WAS-2,"Since 2007, our research is related to the development of an anthropomorphic saxophonist robot, which it has been designed to imitate the saxophonist playing by mechanically reproducing the organs involved for playing a saxophone. Our research aims in understanding the motor control from an engineering point of view and enabling the communication. In a previous paper, the Waseda Saxophone Robot No. 2 (WAS-2) which is composed by 22-DOFs has been presented. Moreover, a feedback error learning with dead time compensation has been implemented to control the air pressure of the robot. However, such a controller couldn't deal with the overblowing effects (unsteady tones) that are found during a musical performance. Therefore; in this paper, the implementation of an Overblowing Correction Controller (OCC) has been proposed and implemented in order to assure the steady tone during the performance by using the pitch feedback signal to detect the overblowing condition and by defining a recovery position (off-line) to correct it. Moreover, a saxophone sound evaluation function (sustain phase) has been proposed to compare the sound produced by human players and the robot. A set of experiments were carried out to verify the improvements on the musical performance of the robot and its sound has been quantitatively compared with human saxophonists. From the experimental results, we could observe improvements on the pitch (correctness) and tone stability.",2010,0,
1524,1525,A middleware aided robust and fault tolerant dynamic reconfigurable architecture,Dynamic reconfiguration enhances embedded system with at run-time adaptive functionality and is an improvement in terms of resource utilization and system adaptability. SRAM-based FPGAs provides a dynamic reconfigurable platform with high logic density. The requirements for such an embedded high flexible system based on FPGAs are robustness and reliability to prevent operation interrupts or even system failures. The complexity of a dynamic reconfigurable system with adaptive processing module demands high effort for the user. Therefore a high level abstraction of the communication issues is required to support application development by an appropriate middleware. To achieve such a flexible embedded system we present our network-on-chip (NoC) approach system-on-chip wire (SoCWire) and outline its performance and suitability for robust dynamic reconfigurable systems. Furthermore we introduce a suitable embedded middleware concept to support the system reconfiguration and the software application development process.,2009,0,
1525,1526,An analysis of the fault correction process in a large-scale SDL production model,"Improvements in the software development process depend on our ability to collect and analyze data drawn from various phases of the development life cycle. Our design metrics research team was presented with a largescale SDL production model plus the accompanying problem reports that began in the requirements phase of development. The goal of this research was to identify and measure the occurrences of faults and the efficiency of their removal by development phase in order to target software development process improvement strategies. The number and severity of problem reports were tracked by development phase and fault class. The efficiency of the fault removal process using a variety of detection methods was measured Through our analysis of the system data, the study confirms that catching faults in the phase of origin is an important goal. The faults that migrated to future phases are on average eight times more costly to repair. The study also confirms that upstream faults are the most critical faults and more importantly it identifies detailed design as the major contributor of faults, including critical faults.",2003,0,
1526,1527,Design of turbo-coded modulation for the AWGN channel with Tikhonov phase error,"We design 1-b/symbol/Hz parallel concatenated turbo-coded modulation (PCTCM) for the additive white Gaussian noise (AWGN) channel with Tikhonov phase error. Constituent recursive convolutional codes are optimized so that the turbo codes have low error floors and low convergence thresholds. The pairwise error probability based on the maximum-likelihood decoding metric is used to select codes with low error floors. We also present a Gaussian approximation method that accurately predicts convergence thresholds for PCTCM codes on the AWGN/Tikhonov channel. Simulation results show that the selected codes perform within 0.6 dB of constellation constrained capacity, and have no detectable error floor down to bit-error rates of 10<sup>-6</sup>.",2005,0,
1527,1528,Numerical prediction of static form errors in the end milling of thin-walled workpiece,"Cutting deformation is the key factor influencing the precision and quality of the machined thin-walled workpiece, and to keep the maximum surface form errors under the permissible errors is the ultimate purpose of the form errors prediction. Cutting forces are analyzed and classified into six types according to combination of cutting depth, and cutting-force model for thin-walled workpiece machining is developed, then a finite-element model is presented to analyze the surface dimensional errors in peripheral milling of aerospace thin-walled workpieces. The efficient flexible iterative algorithm is proposed to calculate the deflections and the maximum surface form errors as contrasted with the rigid iterative algorithm used in the literatures. Meanwhile, some key techniques such as the finite-element modeling of the tool-workpiece system; the determinant algorithm to judge instantaneous immersion boundaries between a cutter element and the workpiece; iterative scheme for the calculations of tool-workpiece deflections considering the former convergence cutting position; and the method for calculating the position and magnitude of the maximum surface form errors are developed and presented in detail. The presented simulation model can control the surface errors in the permissible errors region without calculating the errors all over the workpiece, hence computing speed is greatly increased. The proposed approach is validated and proved to be efficient through comparing the obtained numerical results with the test results.",2006,0,
1528,1529,A rapid prototyping system for error-resilient multi-processor systems-on-chip,"Static and dynamic variations, which have negative impact on the reliability of microelectronic systems, increase with smaller CMOS technology. Thus, further downscaling is only profitable if the costs in terms of area, energy and delay for reliability keep within limits. Therefore, the traditional worst case design methodology will become infeasible. Future architectures have to be error resilient, i.e., the hardware architecture has to tolerate autonomously transient errors. In this paper, we present an FPGA based rapid prototyping system for multi-processor systems-on-chip composed of autonomous hardware units for error-resilient processing and interconnect. This platform allows the fast architectural exploration of various error protection techniques under different failure rates on the microarchitectural level while keeping track of the system behavior. We demonstrate its applicability on a concrete wireless communication system.",2010,0,
1529,1530,Fault tolerant insertion and verification: a case study,"The particular circuit structures that allow the building of a Fault Tolerant (FT) circuit have been extensively studied in the past, but currently there is a lack of CAD support in the design and evaluation of FT circuits. The aim of the AMATISTA European project (IST project 11762) is to develop a set of tools devoted to the design of FT digital circuits. The toolset is composed of: an automatic insertion tool and a simulation tool to validate the FT design. This paper is a case study describing how this set of FTI (Fault Tolerant Insertion) and FTV (Fault Tolerant Verification) tools have been used to increase the reliability in a typical automotive application.",2002,0,
1530,1531,A new approach for real-time multiple open-circuit fault diagnosis in voltage source inverters,"Practically all the diagnostic methods for open-circuit faults in voltage source inverters (VSI) developed during the last decades, are focused on the occurrence of single faults and do not have the capability to handle and identify multiple failures. This paper presents a new method for real-time diagnostics of multiple open-circuit faults in voltage source inverters feeding ac machines. In contrast with the majority of the methods found in the literature which are based on the motor phase currents average values, the average absolute values are used here as principal quantities in order to formulate the diagnostic variables. These prove to be more robust against the issue of false alarms, carrying also information about multiple open-circuit failures. Furthermore, by the combination of these variables with the machine phase currents average values, it is possible to obtain characteristic signatures, which allow for the detection and identification of single and multiple open-circuit faults.",2010,0,
1531,1532,Fault Detection and Recovery in a Transactional Agent Model,"Servers can be fault-tolerant through replication and checkpointing technologies in the client server model. However, application programs cannot be performed and servers might block in the two-phase commitment protocol due to the client fault. In this paper, we discuss the transactional agent model to make application programs fault-tolerant by taking advantage of mobile agent technologies where a program can move from a computer to another computer in networks. Here, an application program on a faulty computer can be performed on another operational computer by moving the program. A transactional agent moves to computers where objects are locally manipulated. Objects manipulated have to be held until a transactional agent terminates. Some sibling computers which the transactional gent has visited might be faulty before the transactional agent terminates. The transactional agent has to detect faulty sibling computers and makes a decision on whether it commits/aborts or continues the computation by skipping the faulty computers depending on the commitment condition. For example, a transactional agent has to abort in the atomic commitment if a sibling computer is faulty. A transactional agent can just drop a faulty sibling computer in the at-least-one commitment. We evaluate the transactional agent model in terms of how long it takes for the transactional agent to treat faulty sibling computers .",2007,0,
1532,1533,Analysis of Fault-Tolerant Performance of a Doubly Salient Permanent-Magnet Motor Drive Using Transient Cosimulation Method,"Doubly salient permanent-magnet (DSPM) motors offer the advantages of high power density and high efficiency. In this paper, it is examined that the DSPM motor is a new class of fault-tolerant machines, a potential candidate for many applications where reliability and power density are of importance. Fault analysis is performed in a DSPM motor drive, including internal and external faults. Due to the fact that the experimentation on a true motor drive for such a purpose is impractical because of its high cost and difficulty to make, a new cosimulation model of a DSPM motor drive is developed using coupled magnetic and electric circuit solvers. Last, to improve the performance of a DSPM motor drive with an open-circuited fault, a fault compensation strategy is proposed. Simulation and experimental results are presented, showing the effectiveness of the proposed cosimulation method and the high performance of the fault-tolerant characteristic of DSPM motor drives.",2008,0,
1533,1534,Defect-based reliability analysis for mission-critical software,"Most software reliability methods have been developed to predict the reliability of a program using only data gathered during the resting and validation of a specific program. Hence, the confidence that can be attained in the reliability estimate is limited since practical resource constraints can result in a statistically small sample set. One exception is the Orthogonal Defect Classification (ODC) method, which uses data gathered from several projects to track the reliability of a new program, Combining ODC with root-cause analysis can be useful in many applications where it is important to know the reliability of a program for a specific type of a fault. By focusing on specific classes of defects, it becomes possible to (a) construct a detailed model of the defect and (b) use data from a large number of programs. In this paper, we develop one such approach and demonstrate its application to modeling Y2K defects",2000,0,
1534,1535,Lung motion correction on respiratory gated 3-D PET/CT images,"Motion is a source of degradation in positron emission tomography (PET)/computed tomography (CT) images. As the PET images represent the sum of information over the whole respiratory cycle, attenuation correction with the help of CT images may lead to false staging or quantification of the radioactive uptake especially in the case of small tumors. We present an approach avoiding these difficulties by respiratory-gating the PET data and correcting it for motion with optical flow algorithms. The resulting dataset contains all the PET information and minimal motion and, thus, allows more accurate attenuation correction and quantification.",2006,0,
1535,1536,On The Generalization of Error-Correcting WOM Codes,"WOM (write once memory) codes are codes for efficiently storing and updating data in a memory whose state transition is irreversible. Storage media that can be classified as WOM includes flash memories, optical disks and punch cards. Error-correcting WOM codes can correct errors besides its regular data updating capability. They are increasingly important for electronic memories using MLCs (multi-level cells), where the stored data are prone to errors. In this paper, we study error-correcting WOM codes that generalize the classic models. In particular, we study codes for jointly storing and updating multiple variables - instead of one variable - in WOMs with multi-level cells. The error-correcting codes we study here are also a natural extension of the recently proposed floating codes. We analyze the performance of the generalized error- correcting WOM codes and present several bounds. The number of valid states for a code is an important measure of its complexity. We present three optimal codes for storing two binary variables in n q-ary cells, where n = 1,2,3, respectively. We prove that among all the codes with the minimum number of valid states, the three codes maximize the total number of times the variables can be updated.",2007,0,
1536,1537,Vertical Velocity Measurement - Processing of Sensor Data Using Altitude Corrections,"The system was designed for sensor measurement and data transfer. A new architecture of a multisensor system for temperature measurement using wireless communications was used in the paper. There are used sensors with digital or analog outputs. The control software of the system has been created. Different software were designed for wireless units. The integrated RF chip nRF9E5 was used as wireless units. Chip ensures wireless communication between control unit and sensors as well as wireless switch unit. The control unit controls system operation, i.e. communication, sensor data processing as well as work of actuator unit. Communication is ensured in the range of 300 m in the free space. The system was designed to operate with different type of sensors. The number of sensor can be variable. The system can used PC, PDA or mobile phone to communication with control unit.",2008,0,
1537,1538,An Integrated Framework for Checking Concurrency-Related Programming Errors,"Developing concurrent programs is intrinsically difficult. They are subject to programming errors that are not present in traditional sequential programs. Our current work is to design and implement a hybrid approach that integrates static and dynamic analyses to check concurrency-related programming errors more accurately and efficiently. The experiments show that the hybrid approach is able to detect concurrency errors in unexecuted parts of the code compared to dynamic analysis, and produce fewer false alarms compared to static analysis. Our future work includes but is not limited to optimizing performance, improving accuracy, as well as locating and confirming concurrency errors.",2009,0,
1538,1539,Robust Fault Detection in a Mixed <sub>2</sub>/<sub></sub> Setting: The Discrete - Time Case,"Robust fault detection problem for discrete-time LTI systems is considered. Allowing stochastic white noises and bounded unknown deterministic disturbances to model system uncertainties, it is shown that this problem can be cast as a mixed norm H<sub>2</sub>/H<sub>alpha</sub> residual generation problem. An example is presented to illustrate the application of the results.",2006,0,
1539,1540,Preventing human errors in power grid management systems through user-interface redesign,"Supervising an energy network is a critical and complex endeavor. Decisions are made under severe time constraints and errors can be costly, often resulting in a crippling impact on the network. Furthermore, the dispatchers who oversee these networks must navigate through multiple systems and tools to access the information they need to make good decisions quickly. Thousands of dynamic variables and hundreds of network configurations need to be considered. Energy network supervision is prone to human error because it requires high dispatcher attention and memory load. This paper shows that it is possible, without massive investments in dollars or in technology, to prevent human errors and to significantly reduce decision times by redesigning the user interfaces on the computer software used by the energy system's supervisors. It illustrates how the redesign of an energy transmission protection system user interface through a cognitive ergonomics approach, eliminates the cause of human errors induced by the existing user interface and reduces time to access information by 90 percent.",2007,0,
1540,1541,Defect tolerance in hybrid nano/CMOS architecture using tagging mechanism,In this paper we propose two efficient repair techniques for hybrid nano/CMOS architecture to provide high level of defect tolerance at a modest cost. We have applied the proposed techniques to a lookup table(LUT) based Boolean logic approach. The proposed repair techniques are efficient in utilization of spare units and viable for various Boolean logic implementations. We show that the proposed techniques are capable of handling upto 20% defect ratess in hybrid nano/CMOS architecture and upto 14% defect rates for large ISCAS'85 benchmark circuits synthesized into smaller sized LUTs.,2009,0,
1541,1542,Error concealment scheme implemented in H.264/AVC,"Video transmission over noisy channels, like wireless channels, leads to errors on video. Effect of information loss is worse in case of transmission of compressed video. With growing interest in compressed video transmission over such environments, error concealment is becoming more important. In this paper we describe error concealment scheme which uses weighted pixel averaging to obtain each pixel value of lost macroblock in intra coded pictures and also method used for error concealment in inter coded pictures. This method uses boundary matching approach. We focus on performance evaluation of the error concealment technique implemented in JM reference software whereby we used extended profile in encoder.",2008,0,
1542,1543,Design of Resource Space Model in Fault Diagnosis Knowledge of Rotating Machinery,"In this paper, we introduce the resource space model (RSM) which is a novelty semantic data model, to semantically store and manage information. Then we use the methods to classify some rotating mechanical fault diagnosis knowledge in semantic, construct RSM of part rotating mechanical fault diagnosis knowledge. In the end, we simply analyze the modelpsilas semantic characteristics in searching and management.",2008,0,
1543,1544,Rotor broken bars fault diagnosis for induction machines based on the wavelet ridge energy spectrum,"A new method for rotor broken bars fault diagnosis for induction machines based on the startup electromagnetic torque signal is presented. The fault characteristic torque frequency variation during the startup can be extracted using the wavelet ridge, which can be used to identify rotor broken bars fault. The wavelet coefficients modulus indicate the signal energy of the corresponding scale, so the wavelet coefficients modulus of the fault characteristic ridge will give us the magnitude variation law of the fault characteristic torque. According to these, the wavelet ridge energy spectrum is defined. Using the wavelet ridge energy spectrum of the fault characteristic torque as the fault severity index, the number of adjacent broken rotor bars can thus be given. Experimental results verify the feasibility of the proposed fault diagnosis method",2005,0,
1544,1545,An Immune Fault Detection System with Automatic Detector Generation by Genetic Algorithms,This work deals with fault detection of electronic analog circuits. A fault detection system for analog circuits based on cross-correlation and artificial immune systems is proposed. It is capable of detecting faulty components in analog circuits by analyzing its impulse response. The use of cross-correlation for preprocessing the impulse response drastically reduces the size of the detector used by the real-valued negative selection algorithm (RNSA). The proposed method makes use of genetic algorithms to automatically generate a small number of very efficient detectors. Results have demonstrated that the proposed system is able to detect faults in a Sallen-Key bandpass filter and in a universal filter.,2007,0,
1545,1546,Fault Injection Scheme for Embedded Systems at Machine Code Level and Verification,"In order to evaluate software from the third party whose source codes are not available, after a careful analysis of the statistic data sorted by orthogonal defect classification, and the corresponding relation between patterns of high level language programs and machine codes, we propose a fault injection scheme at machine code level suitable respectively to the IA32 ARM and MIPS architecture, which takes advantage of mutating machine code. To prove the feasibility and validity of this scheme, two sets of programs are chosen as our experimental target: Set I consists of two different versions of triangle testing algorithms, and Set II is a subset of the Mibench which is a collection of performance benchmark programs designed for embedded systems; we inject both high level faults into the source code written in C language and the corresponding machine code level faults directly into the executables, and monitor their running on Linux. The results from experiments show that at least 96% of total similarity degree is obtained. Therefore, we conclude that the effect of injecting corresponding faults on both the source code level and machine code level are mostly the same. Therefore, our scheme is rather useful in analyzing system behavior under faults.",2009,0,
1546,1547,Coordinated application of multiple description scalar quantization and error concealment for error-resilient MPEG video streaming,"Historically, multiple description coding (MDC) and postprocessing error concealment (ECN) algorithms have evolved separately. In this paper, we propose a coordinated application of multiple description scalar quantizers (MDSQ) and ECN, where the smoothness of the video signal helps to compensate for the loss of descriptions. In particular, we perform a reconstruction that is consistent with the data received at the decoder. When only a single description is available, the video is reconstructed in such a way that: 1) if we were to regenerate two descriptions (from the reconstructed video), one of them would be equivalent to the received description and 2) the reconstructed video is spatiotemporally smooth. Experimental results with several video sequences demonstrated a peak signal-to-noise ratio (PSNR) improvement of 0.9-2.8 dB for intracoded frames. The PSNR improvements for intercoded frames were negligible. However, for both cases, the visual improvements were much more striking than what the PSNR improvement suggested.",2005,0,
1547,1548,Issues insufficiently resolved in Century 20 in the fault-tolerant distributed computing field,"As the 21st Century has just opened up, it is a fitting time to reflect on the evolution of the fault-tolerant distributed computing technology that occurred in the last century. The author's view of that evolution is sketched in this paper, with emphasis on the major issues that were insufficiently resolved in the 20th Century. Such issues are naturally among what the author believes to be the prime subjects that need to be addressed in this decade by the research community. A substantial part of this paper deals with the issues that need to be resolved to advance the real-time fault-tolerant distributed computing branch into a mature practicing field",2000,0,
1548,1549,A multi-agent system-based intelligent identification system for power plant control and fault-diagnosis,"A large-scale power system is required to have a new control system to operate at a higher level of automation, flexibility, and robustness. In this paper, a multi-agent system based intelligent identification system (MAS-IIS) is presented for identification and fault-diagnosis methodologies that improve the performance of the plant in a wide-range of operation. With proposed architecture of a single agent and an organization of the multi-agent system, the MAS-IIS realizes on-line adaptive identifiers for control, and off-line identifiers for fault-diagnosis in real-time power plant operation. The proposed MAS-IIS is one of the functions in multi-agent system based intelligent control systems (MAS-ICSs) which has several functions that provide efficient ways to control locally and globally, and to accommodate and overcome the complexity of large-scale distributed systems",2006,0,
1549,1550,Seeded Fault Testing and In-situ Analysis of Critical Electronic Components in EMA Power Circuitry,"An investigation into the development of feasible detection strategies capturing and trending incipient signs of failure in electronic power and control circuitry of electromechanical actuator (EMA) systems was jointly funded and conducted by Lockheed Martin Aeronautics Company, Parker Aerospace, and Impact Technologies, LLC. The objective of this study was to experimentally evaluate feature-based and efficiency-based prognostic approaches for power drive and control electronics through application of component-level Highly Accelerated Life Testing (HALT) and circuit board-level seeded fault testing. The authors of this paper discuss collaborative work identifying system-critical components through an enhanced failure mode effect and criticality assessment (FMECA++) followed by accelerated aging of these components leading to insertion into the EMA system and analysis of test results. Component accelerated aging and EMA system testing was performed at Impact's facility with test system- specific knowledge provided by Lockheed and Parker.",2008,0,
1550,1551,An extreme value injection approach with reduced learning time to make MLNs multiple-weight-fault tolerant,"We propose an efficient method for making multilayered neural networks(MLN) fault-tolerant to all multiple weight faults in an interval by injecting intentionally two extreme values in the interval in a learning phase. The degree of fault-tolerance to a multiple weight fault is measured by the number of essential multiple links. First, we analytically discuss how to choose effectively the multiple links to be injected, and present a learning algorithm for making MLNs fault tolerant to all multiple (i.e., simultaneous) faults in the interval defined by two multi-dimensional extreme points. Then it is shown that after the learning algorithm successfully finishes, MLNs become fault tolerant to all multiple faults in the interval. The time in a weight modification cycle is almost linear for the fault multiplicity. The simulation results show that the computing time drastically reduces as the multiplicity increases.",2002,0,
1551,1552,Evaluation of Error Control Mechanisms Based on System Throughput and Video Playable Frame Rate on Wireless Channel,"Error control mechanisms are widely used in video communications over wireless channels. However for improving end-to-end video quality: they consume extra bandwidth and reduce effective system throughput. In this paper, considering the parameters of system throughput and playable frame rate as evaluating metrics, we investigate the efficiency of different error control mechanisms. We develop a throughput analytical model to present system effective throughput for different error control mechanisms under different conditions. For a given packet loss probability, both optimal retransmission times in adaptive ARQ and optimal number of redundant packets in adaptive FEC for each type of frames are derived by keeping the system throughput as a constant value. Also, end to end playable frame rates for the two schemes are computed. Then which error control scheme is the most suitable for which application condition is concluded. Finally empirical simulation experimental results with various data analysis are demonstrated.",2010,0,
1552,1553,Fusion of the MR image to SPECT with possible correction for partial volume effects,"Low spatial resolution and the related partial volume effects limit the diagnostic potential of brain single photon emission computed tomography (SPECT) imaging. As a possible remedy for this problem we propose a technique for the fusion of SPECT and MR images, which requires for a given patient the SPECT data and the T1-weighted MR image. Basically, after the reconstruction and coregistration steps, the high-frequency part of the MR, which would be unrecoverable by the set SPECT acquisition system + reconstruction algorithm, is extracted and added to the SPECT image. The tuning of the weight of the MR on the resulting fused image can be performed very quickly, any iterative reconstruction algorithm can be used and, in the case that the SPECT projections are not available, the proposed technique can also be applied directly to the SPECT image, provided that the performance of the scanner is known. The procedure has the potential of increasing the diagnostic value of a SPECT image. Even in the locations of SPECT-MR mismatch it does not significantly affect quantitation over regions of interest (ROIs) whose dimensions are decidedly larger than the SPECT resolution distance. On the other hand, appreciable corrections for partial volume effects are expected in the locations where the contrast in the structural MR matches the corresponding contrast in functional activity.",2006,0,
1553,1554,Eye gaze correction with stereovision for video-teleconferencing,"The lack of eye contact in desktop video teleconferencing substantially reduces the effectiveness of video contents. While expensive and bulky hardware is available on the market to correct eye gaze, researchers have been trying to provide a practical software-based solution to bring video-teleconferencing one step closer to the mass market. This paper presents a novel approach: based on stereo analysis combined with rich domain knowledge (a personalized face model), we synthesize, using graphics hardware, a virtual video that maintains eye contact. A 3D stereo head tracker with a personalized face model is used to compute initial correspondences across two views. More correspondences are then added through template and feature matching. Finally, all the correspondence information is fused together for view synthesis using view morphing techniques. The combined methods greatly enhance the accuracy and robustness of the synthesized views. Our current system is able to generate an eye-gaze corrected video stream at five frames per second on a commodity 1 GHz PC.",2004,0,
1554,1555,Detection and Diagnosis of Static Scan Cell Internal Defect,"In this paper, we study the impact, detection and diagnosis of the defect inside a scan cell, which is called scan cell internal defect. We first use SPICE simulation to understand how a scan cell internal defect impacts the operation of a single scan cell. To study the detectability and diagnosability of a scan cell internal defect in a production test environment, we inject scan cell internal defects into a scan-based industrial design and perform fault simulation by using production scan test patterns. Next, we evaluate how effective an existing scan chain diagnosis technique based on traditional fault models can diagnose scan cell internal defect. We finally propose a new diagnosis algorithm to improve scan cell internal defect diagnostic resolution using scan cell internal fault model. Experimental results show the effectiveness of the proposed scan cell internal fault diagnosis technique.",2008,0,
1555,1556,Reproducing non-deterministic bugs with lightweight recording in production environments,"Reproducing non-deterministic bugs is challenging. Recording program execution in production environments and reproducing bugs is an effective way to re-enable cyclic debugging. Unfortunately, most current record-replay approaches introduce large perturbations to either environments and/or execution flow, in addition to performance penalty and high storage overhead, which make them impracticable to be deployed in production environments. This paper presents Snitchaser - a fully user-space record-replay tool which can faithfully reproduce bugs by replaying system calls which are recorded with negligible perturbation and recording overhead. This is achieved by 1) a novel, lightweight system call interception mechanism without patching the binary instructions to reduce the perturbation to execution flow; 2) system call latch to save signal semantic; 3) periodic checkpointing to reduce the storage overhead. Snitchaser focuses on bugs caused by asynchronous events on heavily loaded, high throughput servers. Experimental results show that Snitchaser is capable of reproducing non-deterministic bugs efficiently at nearly no performance penalty. We also present two case studies on dealing with existing bugs in Lighttpd - a popular software used in many large scale systems.",2010,0,
1556,1557,Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows,"We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.",2010,0,
1557,1558,MATLAB Design and Research of Fault Diagnosis Based on ANN for the C3I System,Artificial neural networks (ANN) are an information-processing method of a simulation of the structure for biological neurons. C<sup>3</sup>I system as a modern combat unit can control and command the army action and can communicate to others. This paper makes a research on the approach of the artificial neural network for fault diagnosis of C<sup>3</sup>I system and constructs a fault diagnosis system of C<sup>3</sup>I system with ANN. And the system can analyze fault phenomena and detect C<sup>3</sup>I system fault. It will greatly improve the response to the C<sup>3</sup>I system fault diagnosis and maintenance efficiency.,2010,0,
1558,1559,Towards a control-theoretical approach to software fault-tolerance,"Existing schemes for software fault-tolerance are based on the ideas of redundancy and diversity. Although being experimentally tested valid, existing fault-tolerant schemes are mainly ad hoc and lack theoretically rigorous foundation. They substantially increase software complexity and incur high development costs. They also impose challenges for real-time concurrent software systems where timing requirements may be stringent and faults in concurrent processes can propagate one another. In This work we treat software fault-tolerance as a robust supervisory control (RSC) problem and propose a RSC approach to software fault-tolerance. In this approach the software component under consideration is treated as a controlled object that is modeled as a generalized Kripke structure or finite-state concurrent system, and an additional safety guarder or supervisor is synthesized and compounded to the software component to guarantee the correctness of the overall software system, which is aimed to satisfy a temporal logic (CTL*) formula, even if faults occur to the software component. The proposed RSC approach requires only a single version of software and is based on a theoretically rigorous foundation. It is essentially an approach of model construction and thus complementary to the approach of model checking. It is a contribution to the theory of supervisory control, software fault-tolerance as well as the emerging area of software cybernetics that explores the interplay between software and control.",2004,0,
1559,1560,Sensitivity analysis of modular dynamic fault trees,"Dynamic fault tree analysis, as currently supported by the Galileo software package, provides an effective means for assessing the reliability of embedded computer-based systems. Dynamic fault trees extend traditional fault trees by defining special gates to capture sequential and functional dependency characteristics. A modular approach to the solution of dynamic fault trees effectively applies Binary Decision Diagram (BOD) and Markov model solution techniques to different parts of the dynamic fault tree model. Reliability analysis of a computer-based system tells only part of the story, however. Follow-up questions such as Where are the weak links in the system?, How do the results change if my input parameters change? and What is the most cost effective way to improve reliability? require a sensitivity analysis of the reliability analysis. Sensitivity analysis (often called Importance Analysis) is not a new concept, but the calculation of sensitivity measures within the modular solution methodology for dynamic and static fault trees raises some interesting issues. In this paper we address several of these issues, and present a modular technique for evaluating sensitivity, a single traversal solution to sensitivity analysis for BOD, a simplified methodology for estimating sensitivity for Markov models, and a discussion of the use of sensitivity measures in system design. The sensitivity measures for both the Binary Decision Diagram and Markov approach presented in this paper is implemented in Galileo, a software package for reliability analysis of complex computer-based systems",2000,0,
1560,1561,The Application of Nerve Net Algorithm to Reduce Vehicle Weigh in Motion System Error,"In order to improve the Vehicle Weigh in Motion System with precision, the paper introduces Nerve Net Algorithm to error analysis. The article sets up a neural network model by determining the neural network input and output variables. Then, the function is defined by net training on MATLAB software. Finally through the experimental verification of Nerve Net Algorithm improving Weigh in Motion System accuracy is feasible.",2010,0,
1561,1562,Transient fault detection via simultaneous multithreading,"Smaller feature sizes, reduced voltage levels, higher transistor counts, and reduced noise margins make future generations of microprocessors increasingly prone to transient hardware faults. Most commercial fault-tolerant computers use fully replicated hardware components to detect microprocessor faults. The components are lockstepped (cycle-by-cycle synchronized) to ensure that, in each cycle, they perform the same operation on the same inputs, producing the same outputs in the absence of faults. Unfortunately, for a given hardware budget, full replication reduces performance by statically partitioning resources among redundant operations. We demonstrate that a Simultaneous and Redundantly Threaded (SRT) processor-derived from a a Simultaneous Multithreaded (SMT) processor-provides transient fault coverage with significantly higher performance. An SRT processor provides transient fault coverage by running identical copies for the same program simultaneously as independent threads. An SRT processor provides higher performance because it dynamically schedules its hardware resources among the redundant copies. However, dynamic scheduling makes is difficult to implement lockstepping, because corresponding instructions from redundant threads may not execute in the same cycle or in the same order. This paper makes four contributions to the design of SRT processors. First, we introduce the concept of the sphere of replication, which abstract both the physical redundancy of a lockstepped system and the logical redundancy of an SRT processor. This framework aids in identifying the scope of fault coverage and the input and output values requiring special handling. Second, we identify two viable spheres of replication in an SRT processor, and show that one of them provides fault detection while checking only committed stores and uncached loads. Third, we identify the need for consistent replication of load values, and propose and evaluate two new mechanisms for satisfying thi- - s requirement. Finally, we propose and evaluate two mechanisms-slack fetch and branch outcome queue-that enhance the performance of an SRT processor by allowing one thread to prefetch cache misses and branch results for the other thread. Our results with 11 SPEC95 benchmarks show that an SRT processor can outperform an equivalently sized, on-chip, hardware-replicated solution by 16% on average, with maximum benefit of up to 29%.",2000,0,
1562,1563,Video Error Concealment Using Spatio-Temporal Boundary Matching,"Transmission of videos in error prone environments may lead to video corruption or loss. Therefore error concealment at the decoder side has to be applied. Commonly error concealment techniques make use of the surrounding correctly received image data or motion information for concealment. In this paper, a novel spatio-temporal boundary matching algorithm (STBMA) by exploiting both spatial and temporal information to reconstruct the lost motion vectors (MV) is proposed, and also introduce a new side smoothness measurement. By using the motion vector that is found by the proposed algorithm, the lost macro block (MB) can be recovered. Compared with the well known boundary matching algorithm (BMA), the proposed algorithm is able to achieve higher PSNR as well as better visual quality.",2009,0,
1563,1564,Remote Fault Estimation and Thevenin Impedance Calculation from Relays Event Reports,"One of the typical features in modern relays is the generation of event reports during a disturbance. Event reports are records of regularly taken samples of the line currents and voltages as seen by the relay during a disturbance. This paper describes a software tool developed for the use of these event reports to perform the tasks of fault classification and estimation. The program estimates the 60 Hz values of currents and voltages by applying DFT (discrete Fourier transform) to the samples recorded by the relay every quarter of a cycle. Using these values the program performs the following calculations: 1) classification of fault, 2) estimation of the fault distance from the relay location and 3) Thevenin's impedance of the system at the fault point as well as a Thevenin's equivalent impedance of the system in front and behind the relay. The program was initially tested with data generated from ATP simulations and later with data from relays installed in an operating electrical network. The results show that the software is highly reliable, providing accurate estimation of faults and Thevenin's equivalent impedances",2006,0,
1564,1565,Novel Dual-Band Filter Incorporating Defected SIR and Microstrip SIR,"This letter presents a novel approach to design dual-band bandpass filter by using defected stepped impedance resonator (DSIR) and microstrip stepped impedance resonator (MSIR). A pair of MSIRs on the upper plane forms a cross coupled filtering passage, and a pair of DSIRs at the lower plane constructs a linear phase filtering passage. Both of them are fed by a common T-shaped microstrip feed line with source-load coupling. Then they are directly combined to construct a compact dual-band filter with two passbands centering at 2.35 GHz and 3.15 GHz, respectively. The measurement results agree well with the full-wave electromagnetic designed responses.",2008,0,
1565,1566,A Fast Analytical Approach to Multi-cycle Soft Error Rate Estimation of Sequential Circuits,"In this paper, we propose a very fast analytical approach to measure the overall circuit Soft Error Rate (SER) and to identify the most vulnerable gates and flip-flops. In the proposed approach, we first compute the error propagation probability from an error site to primary outputs as well as system bistables. Then, we perform a multi-cycle error propagation analysis in the sequential circuit. The results show that the proposed approach is four to five orders of magnitude faster than the Monte Carlo (MC) simulation-based fault injection approach with 92% accuracy. This makes the proposed approach applicable to industrial-scale circuits.",2010,0,
1566,1567,An optimal point in scheduling real-time tasks process based on fault tolerant imprecise computation model,"Fault tolerance is an important issue due to the critical nature of the supported tasks of real-time computer systems, since timing constraints must not be violated. The imprecise computation technique has been proposed as a way to handle transient overload and to enhance fault tolerant of real-time systems. This paper introduces an exact theoretical analysis for the imprecise computation model based on three principles of maximize reward-based test, minimize response-time test, and minimize errors test, then finds optimal-point in scheduling process to satisfy three scheduling conditions. Further this is also demonstrated by the simulation results.",2002,0,
1567,1568,Image quality assessment: from error visibility to structural similarity,"Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu/lcv/ssim/.",2004,0,
1568,1569,Real-Time Tasks Scheduling with Value Control to Predict Timing Faults During Overload,"Modern real-time applications are very dynamic and cannot cope with the use of worst case execution time to avoid overload situations. Therefore scheduling algorithms that are able to prevent timing faults during overload are required. In this context, the value parameter has become useful to add generality and flexibility to such systems. In this paper, we present the scheduling algorithm called DMB (dynamic misses based), which is capable of dynamically changing tasks value in order to adjust their importance according to its timing faults rate. The main goal of DMB is to allow the prediction of timing faults during overloads and thereby support a dynamic tuning of tasks fault rate. It is used to enhance the features of the previously defined TAFT (time-aware fault-tolerant) scheduler. Obtained results show that DMB in conjunction with TAFT reached the most promising results during overloads, allowing to control tasks degradation in a graceful and determined way",2007,0,
1569,1570,Error Vector Magnitude Analysis for OFDM Systems,"Error vector magnitude (EVM) is a popular figure-of-merit adopted by various communication standards for evaluating in-band distortions introduced in a communication system. In this paper, we regard EVM as a random variable and investigate its statistical distributions as the result of the following distortion mechanisms: phase noise, amplitude clipping, power amplifier nonlinearities, and gain/phase imbalances in orthogonal frequency division multiplexing (OFDM) systems. We relate key parameters characterizing the various distortion mechanisms to the statistical behavior of EVM; such statistical behavior can be used to verify compliance of the transmit signals to the requirements of the standard.",2006,0,
1570,1571,Semantic Impact and Faults in Source Code Changes: An Empirical Study,"Changes to source code have become a critical factor in fault predictions. Text or syntactic approaches have been widely used. Textual analysis focuses on changed text fragments while syntactic analysis focuses on changed syntactic entities. Although both of them have demonstrated their advantages in experimental results, they only study code fragments modified during changes. Because of semantic dependencies within programs, we believe that code fragments impacted by changes are also helpful. Given a source code change, we identify its impact by program slicing along the variable def-use chains. To evaluate the effectiveness of change impacts in fault detection and prediction, we compare impacted code with changed code according to size and fault density. Our experiment on the change history of a successful industrial project shows that: fault density in changed and impacted fragments are higher than other areas; for large changes, their impacts have higher fault density than changes themselves; interferences within change impact contribute to the high fault density in large changes. Our study suggests that, like change itself, change impact is also a high priority indicator in fault prediction, especially for changes of large scales.",2009,0,
1571,1572,Efficient multiway graph partitioning method for fault section estimation in large-scale power networks,"Fault section estimation (FSE) of large-scale power networks can be implemented effectively by the distributed artificial intelligence (AI) technique. In this paper, an efficient multiway graph partitioning method is proposed to partition the large-scale power networks into the desired number of connected subnetworks with balanced working burdens in performing FSE. The number of elements at the frontier of each subnetwork is also minimised in the method. The suggested method consists of three basic steps: forming the weighted depth-first-search tree of the studied power network; partitioning the network into connected, balanced subnetworks and minimising the number of the frontier nodes of the subnetworks through iterations so as to reduce the interaction of FSE in adjacent subnetworks. The relevant mathematical model and partitioning procedure are presented. The method has been implemented with the sparse storage technique and tested in the IEEE 14-bus, 30-bus and 118-bus systems, respectively. Computer simulation results show that the proposed multiway graph partitioning method is effective for the large-scale power system FSE using the distributed AI technique",2002,0,
1572,1573,Error tolerant DNA self-assembly by link-fracturing,"This paper proposes and evaluates link fracturing as an approach for error tolerance in self-assembly by utilizing a DNA chain as a link between two blocks of molecules. Through the use of restriction enzymes, link fracturing breaks the connecting DNA chain between two blocks if an incorrect assembly has occurred due to the erroneous growth of tiles. Two error tolerant techniques are proposed by fracturing of the DNA chain links, namely 1-link and 2-link. Using the tool Xgrow, simulations under the Kinetic Tile Assembly Model (KTAM) are performed. Results show that 2-link fracturing achieves an improvement in error rate as compared to a normal assembly; moreover this is accomplished with little overhead in assembly size and execution complexity. The 1-link method shows 100% error free growth with moderate overhead as compared to normal growth and other existing error tolerant methods.",2009,0,
1573,1574,Research on reliability modeling of complex system based on dynamic fault tree,"The traditional static fault trees with AND, OR, and Voting gates cannot capture the dynamic behavior of complex computer system failure mechanisms such as sequence dependent events, spares and dynamic redundancy management, and priorities of failure events. In this paper, dynamic fault tree modeling method is applied in complex computer system. This paper starts from dynamic fault tree analysis method and its logic gates. A complex computer system simulation example is given. We establish respectively each model, and then gain system-level dynamic fault tree and modularize this model and analyze other questions. At last, we can draw a conclusion that this model method of this paper analyzes well dynamic behaviors in computer system that combines hardware, software and human reason.",2009,0,
1574,1575,Estimating the number of faults remaining in software code documents inspected with iterative code reviews,"Code review is considered an efficient method for detecting faults in a software code document. The number of faults not detected by the review should be small. Current methods for estimating this number assume reviews with several inspectors, but there are many cases where it is practical to employ only two inspectors. Sufficiently accurate estimates may be obtained by two inspectors employing an iterative code review (ICR) process. This paper introduces a new estimator for the number of undetected faults in an ICR process, so the process may be stopped when a satisfactory result is estimated. This technique employs the Kantorowitz estimator for N-fold inspections, where the N teams are replaced by N reviews. The estimator was tested for three years in an industrial project, where it produced satisfactory results. More experiments are needed in order to fully evaluate the approach.",2005,0,
1575,1576,Automatically Finding and Patching Bad Error Handling,Bad error handling is the cause of many service outages. We address this problem by a novel approach to detect and patch bad error handling automatically. Our approach uses error injection to detect bad error handling and static analysis of binary code to determine which type of patch can be instantiated. We describe several measurements regarding the effectiveness of our approach to detect and patch bad error handling in several open source programs,2006,0,
1576,1577,Adaptive Replication Based Security Aware and Fault Tolerant Job Scheduling for Grids,"Most of the existing job scheduling algorithms for grids have ignored the security problem with a handful of exceptions. Moreover, existing algorithms using fixed- number job replications will consume excessive resources when grid security level changes dynamically. In this paper, a security aware and fault tolerant scheduling (SAFTS) algorithm based on adaptive replication is proposed which schedules the jobs by matching the user security demand and resource trust level and the number of the job replications changes adaptively with the dynamic of grid security. In experiments on RSBSME (remote sensing based soil moisture extraction) workload in a real grid environment, the average job scheduling success rate is 97%, and average grid utilization is 74%. Experiment results show that performance of SAFTS is better than non-security-aware and fixed-number job replication scheduling algorithms and SAFTS is fault-tolerant and scalable.",2007,0,
1577,1578,Fault-Tolerant Middleware for Grid Computing,"The major challenge in Grid environment is fault tolerance. Faults ranging from machine crashes, media failures, operator errors and random data corruption results in loss of data, both temporarily and permanently. The paper proposes a solution for handling faults in grid environment. Fault-Tolerance using Adaptive Replication in Grid Computing (FTARG) is an adaptive replication middleware which addresses the fault tolerance of Grid based applications by providing data replication at different sites. FTARG is an Aneka based Grid middleware especially designed for high-performance Grid based applications. FTARG enables data synchronization between multiple heterogeneous databases located in the Grid by supporting a variety of synchronization modes. Experimental analysis proves that proposed FTARG handles faults in the Grid by improving the performance of data management for large scale complex grid based applications.",2010,0,
1578,1579,Auto Regressive Model and Weighted Least Squares Based Packet Video Error Concealment,"In this paper, auto regressive (AR) model is applied to error concealment for block-based packet video encoding. Each pixel within the corrupted block is restored as the weighted summation of corresponding pixels within the previous frame in a linear regression manner. Two novel algorithms using weighted least squares method are proposed to derive the AR coefficients. First, we present a coefficient derivation algorithm under the spatial continuity constraint, in which the summation of the weighted square errors within the available neighboring blocks is minimized. The confident weight of each sample is inversely proportional to the distance between the sample and the corrupted block. Second, we provide a coefficient derivation algorithm under the temporal continuity constraint, where the summation of the weighted square errors around the target pixel within the previous frame is minimized. The confident weight of each sample is proportional to the similarity of geometric proximity as well as the intensity gray level. The regression results generated by the two algorithms are then merged to form the ultimate restorations. Various experimental results demonstrate that the proposed error concealment strategy is able to increase the peak signal-to-noise ratio (PSNR) compared to other methods.",2010,0,
1579,1580,A Digital Image Scrambling Method Based on AES and Error-Correcting Code,"A new scrambling method of true color images based on AES algorithm is proposed. It takes the structure of true color images into full consideration, and decreases the complexity. The analytical method of cryptography is first used in this paper to analyze the security of disordered images; at the same time, an error-correcting code is designed to prevent passive attacks based on bit modification. Experimental results show that the method is safe, efficient and has great ability of error correcting.",2008,0,
1580,1581,Use of fault tree analysis to improve residential gateway testing,"A residential gateway, heart of the strategy of most Telcos, is a centralized intelligent device between the operator's access network and the home's network. It terminates all external access networks and enables residential services to be delivered to the consumer. Besides a plethora of useful services, the growth in market depends upon the reputation of its resilience (availability, reliability and security). This emphasizes a near zero fault design and efficient testing should be taken care before its launch into the market. This paper deals with the analysis of failures, both from test and field data, aiming to increase the efficiency of laboratory testing. Using fault tree analysis, we study the faults that have passed through the testing phase and created failures in the customer premises. With the help of defined specifications, we have identified the zones in which testing in the laboratory needs to be improved.",2007,0,
1581,1582,Induction motor mechanical fault online diagnosis with the application of artificial neural network,"An online fault diagnostic algorithm for induction motor mechanical faults is presented based on the application of artificial neural networks. Two mechanical faults, the rotor bar breakage and air gap eccentricity, are considered. New feature coefficients obtained by wavelet packet decomposition of the stator current are used together with the slip speed as the input of a multi-layer neural network. The proposed algorithm is proved to be able to distinguish healthy and faulty conditions with high accuracy",2001,0,
1582,1583,Design and implementation of Weapons Fault Diagnosis Expert System Platform,"By analyzing the distinguishing features of the current popular weapons fault diagnosis expert system, as well as using component-based software design and complex knowledge representation methods, this paper proposes a solution of overall design and a description of user interfaces about the Weapons Fault Diagnosis Expert System Platform. It studies and discusses the complex knowledge representation about the reusable expert system. The implementation essentials in a variety of reasoning mechanisms are also discussed. It is proved that, a special weapons fault diagnosis expert system can be generated using this expert system platform and the special knowledge of the weapons fault.",2010,0,
1583,1584,Clock Domain Crossing Fault Model and Coverage Metric for Validation of SoC Design,"Multiple asynchronous clock domains have been increasingly employed in system-on-chip (SoC) designs for different I/O interfaces. Functional validation is one of the most expensive tasks in the SoC design process. Simulation on register transfer level (RTL) is still the most widely used method. It is important to quantitatively measure the validation confidence and progress for clock domain crossing (CDC) designs. In this paper, we propose an efficient method for definition of CDC coverage, which can be used in RTL simulation for a multi-clock domain SoC design. First, we develop a CDC fault model to present the actual effect of metastability. Second, we use a temporal dataflow graph (TDFG) to propagate the CDC faults to observable variables. Finally, CDC coverage is defined based on the CDC faults and their observability. Our experiments on a commercial IP demonstrate that this method is useful to find CDC errors early in the design cycles",2007,0,
1584,1585,Analysis of transient performance for DFIG wind turbines under the open switch faults,"The fast development of grid-integrated wind power introduces new requirements for the operation and control of power networks. In order to maintain the reliability of a host power grid, it is preferred that the grid-connected wind turbine should restore its normal operation with minimized power losses in events of grid fault. This paper presents the results for the transient performance of a 2MW doubly-fed induction generator (DFIG), a type of variable-speed wind turbine. The paper concentrates on transient performance of the said generator technology under open-switch grid faults. The simulation was performed using MATLAB - Simulink software. The results obtained have shown that the control schemes employed for the DFIG wind turbines played an effective role in the restoration of the normal operation for the wind turbine in response to grid faults. The results for both during and after the grid fault will be discussed in this paper.",2010,0,
1585,1586,Characterization of defects in photovoltaics using thermoreflectance and electroluminescence imaging,Thermal and electroluminescence (EL) imaging techniques are widely accepted as powerful tools for analyzing solar cells. We have identified and characterized various defects in photovoltaic devices with sub-micron spatial resolution using a novel thermoreflectance imaging technique that can simultaneously obtain thermal and EL images with a mega-pixel silicon-based CCD. Linear and non-linear shunt defects are investigated as well as electroluminescent breakdown regions at reverse biases as low as -5V. Pre-breakdown sites with electroluminescence are observed. The wavelength flexibility of thermoreflectance imaging is explored and thermal images of sub-micrometer defects are obtained through glass that would typically be opaque for infrared light. Image sequences show a 10s thermal transient response of a 15m defect in a polysilicon solar cell. Nanosecond reverse bias voltage pulses are used to detect breakdown regions in thin-film a-Si solar cells with EL.,2010,0,
1586,1587,Optimized Spacecraft Fault Protection for the WISE Mission,"The WISE project is a NASA-funded medium- class Explorer mission to map the entire sky in four infrared bands during the course of a 6-month survey. Because of the mission's limited financial resources, a traditional robustness strategy of full block-redundancy was not feasible. By leveraging aspects of the mission design that tend to reduce the risk associated with certain failures, the project has been able to adopt a robustness strategy of mitigating high-risk failures, while accepting the risk of low-impact faults or unlikely faults in heritage equipment with proven reliability. The resulting WISE flight system design is primarily single-string with some select functional- and block-redundancy and includes fault tolerance measures targeted at achieving the most cost- effective risk reduction possible for the system design. The fault protection team has been challenged with balancing the risk of faults, cost, and down-time with Ground Segment capabilities, heritage, and effectively designed fault mitigations. Faults were identified via a collection of analyses, and a criticality rating was applied to each fault to assess its impact to the mission. Taking into consideration each fault's impact and time criticality, mitigations to each possible fault were considered in areas such as on-board autonomy, the addition or use of functional and block redundancy, and ground system detection. Through this exercise, the project has realized a robust and reliable system design in line with the project's risk posture and cost constraints.",2008,0,
1587,1588,Recent Developments in Single-Phase Power Factor Correction,"The development of single-phase power factor correction (PFC) technologies was traditionally driven by the need for computers, telecommunication, lighting, and other electronic devices and systems to meet harmonic current limits defined by IEC 61000-3-2 and other regulatory standards. Recently, several new applications have emerged as additional drivers for the development of the technologies. One such application is commercial transport airplanes where single-phase PFC converters capable of meeting stringent airborne power quality requirements are required for in-flight entertainment (IFE), avionics, communication, and other single-phase loads. The proliferation of variable-speed motor drives in home appliances has also generated a new need for high-power (up to a few kilowatts), high-efficiency, and low-cost single-phase PFC converters. New PFC circuits, control methods, as well as EMI modeling and design techniques are being developed in response to these new requirements, which are reviewed in this paper. Specific subjects to be covered include airborne and home appliance applications, as well as EMI modeling and EMI filter design optimization.",2007,0,
1588,1589,Concurrent Error Detection in Digit-Serial Normal Basis Multiplication over GF(2m),"Parity prediction schemes have been widely studied in the past. Recently, it has been demonstrated that this prediction scheme can achieve fault-secureness in arithmetic circuits for stuck-at and stuck-open faults. For most cryptographic applications, encryption/decryption algorithms rely on computations in very large finite fields. The hardware implementation may require millions of logic gates and this may lead to the generation of erroneous outputs by the multiplier. In this paper, a concurrent error detection (CED) technique is used in the digit-serial basis multiplier over finite fields of characteristic two. It is shown that all types of normal basis multipliers possess the same parity prediction function.",2008,0,
1589,1590,An induction motor drive system with improved fault tolerance,"This paper investigates the utilization of a simplified topology that permits the fault-tolerant operation of a three-phase induction motor drive system. When one of the inverter legs is lost, the machine can operate with only two stator windings by connecting the machine neutral to a fourth converter leg. The structure and the operation principle of the system are presented. The machine model corresponding to the asymmetric two-windings machine is developed and a suitable controller is proposed. Experimental results are presented",2001,0,
1590,1591,Using Probabilistic Characterization to Reduce Runtime Faults in HPC Systems,"The current trend in high performance computing is to aggregate ever larger numbers of processing and interconnection elements in order to achieve desired levels of computational power, This, however, also comes with a decrease in the Mean Time To Interrupt because the elements comprising these systems are not becoming significantly more robust. There is substantial evidence that the Mean Time To Interrupt vs. number of processor elements involved is quite similar over a large number of platforms. In this paper we present a system that uses hardware level monitoring coupled with statistical analysis and modeling to select processing system elements based on where they lie in the statistical distribution of similar elements. These characterizations can be used by the scheduler/resource manager to deliver a close to optimal set of processing elements given the available pool and the reliability requirements of the application.",2008,0,
1591,1592,D-Q modeling and control of a single-phase three-level boost rectifier with power factor correction and neutral-point voltage balancing,"This paper deals with the analysis, design and operation of a control system for a single-phase three-level rectifier with a neutral-point-clamped (NPC) topology. Usually the desired operating conditions for this type of converter are: unity displacement factor, output DC voltage regulation and neutral point voltage balancing. A d-q reference frame has been used in this work to model the rectifier behaviour in order to exploit the results obtained in the field of three-phase converters. In this way, a space vector modulation PWM method has been used, with the possibility of using redundant switching states to achieve charge balancing of the capacitors. The time assignment of each redundant switching state is accomplished by utilizing a closed-loop control system. Validity of the modeling and control strategies are confirmed by the transient and steady state simulation and experimental results.",2002,0,
1592,1593,Reversible Data Hiding for Audio Based on Prediction Error Expansion,"This paper proposes a reversible data hiding method for digital audio using prediction error expansion technique. Firstly, the prediction error of the original audio is obtained by applying an integer coefficient predictor. Secondly, a location map is set up to record the expandability of all audio samples, and then it is compressed by lossless compression coding and taken as a part of secret information. Finally, the reconstructed secret information is embedded into the audio using prediction error expansion technique. After extracting the embedded information, the original audio can be perfectly restored. Experimental results show that the proposed algorithm can achieve high embedding capacity while keeping good quality of the stego-audio.",2008,0,
1593,1594,Error probability and SINR analysis of optimum combining in rician fading,"This paper considers the analysis of optimum combining systems in the presence of both co-channel interference and thermal noise. We address the cases where either the desired-user or the interferers undergo Rician fading. Exact expressions are derived for the moment generating function of the SINR which apply for arbitrary numbers of antennas and interferers. Based on these, we obtain expressions for the symbol error probability with M-PSK. For the case where the desired-user undergoes Rician fading, we also derive exact closed-form expressions for the moments of the SINR. We show that these moments are directly related to the corresponding moments of a Rayleigh system via a simple scaling parameter, which is investigated in detail. Numerical results are presented to validate the analysis and to examine the impact of Rician fading on performance.",2009,0,
1594,1595,A verification of fault tree for safety integrity level evaluation,"This study focuses on a novel approach which automatically proves the correctness and completeness of fault trees based on a formal model by model checking. This study represents that the model checking technique is useful when validating the correctness of informal safety analysis such as FTA. The benefits of this study are that it provides the probability of formally validating FTA by proving correctness and completeness of the fault trees. In addition to this benefit, it is possible that the CTL technique proves the FTA based SIL.",2009,0,
1595,1596,A Millimeter Wave Direct QPSK modulator MMIC Using PIN Technology And A Novel Approach to Self Error-Correction,"In this paper we present two topologies of QPSK (Quadrature Phase Shift Keying) Modulator for direct carrier modulation at 29.5 GHz for satellite communications using PIN technology. Previously published designs use PHEMT technology. PIN diodes allow operation at high power level (several watts at Ka band) and yield better switching performance compared to PHEMT switches. Our designs exhibit wide bandwidth (3GHz) and reasonable loss (5.5 db). In addition, we present a novel method for a self error correction QPSK modulator, which can yield high quality modulator at millimeter waves.",2002,0,
1596,1597,Satellite Ozone Retrieval Under Broken Cloud Conditions: An Error Analysis Based on Monte Carlo Simulations,"This paper investigates the influence of horizontally inhomogeneous clouds on the accuracy of total ozone column retrievals from space. The focus here is on retrievals based on backscattered ultraviolet light measurements in Huggins bands in the range of 315-340 nm. It is found that simplifying the description of cloud properties in the ozone-retrieval algorithm studied can produce errors of up to 6%, depending on the error in the assumed cloud parameters. Yet another finding is the fact that independent pixel approximation suffices for ozone-retrieval algorithms. This was found using three-dimensional Monte Carlo radiative transfer calculations in the Huggins bands",2007,0,
1597,1598,Strategy to Detect Bug in Pre-silicon Phase,"Bugs still escape to post-silicon despite huge effort has been put into validating the design in pre-silicon phase. This could cost an immediate stepping while some other bugs may have a software work around. Running more tests may still miss the bugs. Therefore it is necessary to have an effective strategy during pre-silicon phase. This paper will present a strategy to derive the test points from the validation objective, and set the domain to test based on the micro-architecture, before entering simulation environment. This strategy utilized coverage based validation (CVB), with test points and domain coded as coverage points, while the test generator directed the transactions into the domain to test. This provides a comprehensive validation coverage to the design under test.",2009,0,
1598,1599,Analysis of single-event effects in embedded processors for non-uniform fault tolerant design,"Advances in silicon technology and shrinking the feature size to nanometer scale make unreliability of nano devices the most important concern of fault-tolerant designs. Design of reliable and fault-tolerant embedded processors is mostly based on developing techniques that compensate adding hardware or software redundancy. The recently-proposed redundancy techniques are generally applied uniformly to a system and lead to inefficiencies in terms of performance, power, and area. Non-uniform redundancy requires a quantitative analysis of the system behavior encountering transient faults. In this paper, we introduce a custom fault injection framework that helps to locate the most vulnerable nodes and components of embedded processors. Our framework is based on an exhaustive transient fault injection to candidate nodes which are selected from a user-defined list. Furthermore, the list of nodes containing the microarchitectural state is also defined by user to validate execution of instructions. Based on the reported results, the most vulnerable nodes, components, and instructions are found and could be used for an effective non-uniform fault-tolerant redundancy technique.",2009,0,
1599,1600,Dual-Processor Design of Energy Efficient Fault-Tolerant System,"A popular approach to guarantee fault tolerance in safety-critical applications is to run the application on two processors. A checkpoint is inserted at the completion of the primary copy. If there is no fault, the secondary processor terminates its execution. Otherwise, should the fault occur, the second processor continues and completes the application before its deadline. In this paper, we study the energy efficiency of such dual-processor system. Specifically, we first derive an optimal static voltage scaling policy for single periodic task. We then extend it to multiple periodic tasks based on worst case execution time (WCET) analysis. Finally, we discuss how to further reduce system's energy consumption at run time by taking advantage of the actual execution time which is less than the WCET. Simulation on real-life benchmark applications shows that our technique can save up to 80% energy while still providing fault tolerance",2006,0,
1600,1601,DVHMM: variable length text recognition error model,"This paper proposes a text recognition error model called the dual variable length output hidden Markov model (DVHMM) and gives a parameter estimation algorithm based on the EM algorithm. Although existing probabilistic error models are limited to substitution (1, 1), insertion (1, 0), and deletion (0, 1) errors, the DVHMM can handle error patterns of any pair (i, j) of lengths including substitution, insertion, and deletion.",2002,0,
1601,1602,A decision tree based method for fault classification in transmission lines,"In this paper a novel and accurate method is proposed for fault classification in transmission lines. The method is based on decision tree and gets the 50 up to 950 Hz phasors of voltages and currents from one end of the line, as the inputs. The method is applied to a 400 kV transmission line, and the results showed the highest possible accuracy within less than quarter of a cycle after fault inception.",2008,0,
