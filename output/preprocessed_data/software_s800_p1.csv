,record_id,title,abstract,year,label_included,duplicate_record_id
0,1,Using Developer Information as a Factor for Fault Prediction,"We have been investigating different prediction models to identify which files of a large multi-release industrial software system are most likely to contain the largest numbers of faults in the next release. To make predictions we considered a number of different file characteristics and change information about the files, and have built fully- automatable models that do not require that the user have any statistical expertise. We now consider the effect of adding developer information as a prediction factor and assess the extent to which this affects the quality of the predictions.",2007,1,
1,2,Reliability analysis of protective relays in fault information processing system in China,"The reliability indices of protective relays are first put forward in this paper. A Markov probability model is then established to evaluate the reliability of relay protection. With the state space analytical method, all the steady state probabilities and state transition probabilities can be calculated utilizing the data stored in the fault information processing system. We can get an equation that represents the influence of routine test intervals on relay unavailability. Based on this, the optimum routine test interval for protective relays can be determined. This paper also proposes an efficient method of processing large amount of information by the fault information processing system and evaluating the reliability of protective relays with it, and the corresponding software package is also developed. The application of it to an actual power system in China proves the method to be correct and effective",2006,0,
2,3,Test effort optimization by prediction and ranking of fault-prone software modules,"Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using ID3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the NASA projects data set of PROMOSE repository.",2010,1,
3,4,A Rough Set Model for Software Defect Prediction,High assurance software requires extensive and expensive assessment. Many software organizations frequently do not allocate enough resources for software quality. We research the defect detectors focusing on the data sets of software defect prediction. A rough set model is presented to deal with the attributes of data sets of software defect prediction in this paper. Appling this model to the most famous public domain data set created by the NASA's metrics data program shows its splendid performance.,2008,1,
4,5,Using Faults-Slip-Through Metric as a Predictor of Fault-Proneness,"Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (Naive Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based techniques (genetic programming and artificial immune recognition systems) on FST data collected from two large industrial projects from the telecommunication domain. Results: Using area under the receiver operating characteristic (ROC) curve and the location of (PF, PD) pairs in the ROC space, the faults slip-through metric showed impressive results with the majority of the techniques for predicting fault-prone modules at both integration and system test levels. There were, however, no statistically significant differences between the performance of different techniques based on AUC, even though certain techniques were more consistent in the classification performance at the two test levels. Conclusions: We can conclude that the faults-slip-through metric is a potentially strong predictor of fault-proneness at integration and system test levels. The faults-slip-through measurements interact in ways that is conveniently accounted for by majority of the data mining techniques.",2010,1,
5,6,Reducing Features to Improve Bug Prediction,"Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.",2009,1,
6,7,Variance Analysis in Software Fault Prediction Models,"Software fault prediction models play an important role in software quality assurance. They identify software subsystems (modules,components, classes, or files) which are likely to contain faults. These subsystems, in turn, receive additional resources for verification and validation activities. Fault prediction models are binary classifiers typically developed using one of the supervised learning techniques from either a subset of the fault data from the current project or from a similar past project. In practice, it is critical that such models provide a reliable prediction performance on the data not used in training. Variance is an important reliability indicator of software fault prediction models. However, variance is often ignored or barely mentioned in many published studies. In this paper, through the analysis of twelve data sets from a public software engineering repository from the perspective of variance, we explore the following five questions regarding fault prediction models: (1) Do different types ofclassification performance measures exhibit different variance? (2) Does the size of the data set imply a more (or less) accurate prediction performance? (3) Does the size of training subset impact model's stability? (4) Do different classifiers consistently exhibit different performance in terms of model's variance? (5) Are there differences between variance from 1000 runs and 10 runs of 10-fold cross validation experiments? Our results indicate that variance is a very important factor in understanding fault prediction models and we recommend the best practice for reporting variance in empirical software engineering studies.",2009,1,
7,8,Evaluating Defect Prediction Models for a Large Evolving Software System,"A plethora of defect prediction models has been proposed and empirically evaluated, often using standard classification performance measures. In this paper, we explore defect prediction models for a large, multi-release software system from the telecommunications domain. A history of roughly 3 years is analyzed to extract process and static code metrics that are used to build several defect prediction models with random forests. The performance of the resulting models is comparable to previously published work. Furthermore, we develop a new evaluation measure based on the comparison to an optimal model.",2009,1,
8,9,Application of neural network for predicting software development faults using object-oriented design metrics,"In this paper, we present the application of neural network for predicting software development faults including object-oriented faults. Object-oriented metrics can be used in quality estimation. In practice, quality estimation means either estimating reliability or maintainability. In the context of object-oriented metrics work, reliability is typically measured as the number of defects. Object-oriented design metrics are used as the independent variables and the number of faults is used as dependent variable in our study. Software metrics used include those concerning inheritance measures, complexity measures, coupling measures and object memory allocation measures. We also test the goodness of fit of neural network model by comparing the prediction result for software faults with multiple regression model. Our study is conducted on three industrial real-time systems that contain a number of natural faults that has been reported for three years (Mei-Huei Tang et al., 1999).",2002,1,
9,10,Using the Conceptual Cohesion of Classes for Fault Prediction in Object-Oriented Systems,"High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.",2008,1,
10,11,Software Fault Prediction using Language Processing,"Accurate prediction of faulty modules reduces the cost of software development and evolution. Two case studies with a language-processing based fault prediction measure are presented. The measure, refereed to as a QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgements of software quality. The two case studies consider the measure's application to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and QALP score. Results, while complex, show that little correlation exists in the first case study, while statistically significant correlations exists in the second. In this second study the QALP score is helpful in predicting faults in modules (files) with its usefulness growing as module size increases.",2007,1,
11,12,Mutual Fault-tolerant and Standby SCADA System Based on MAS for Multi-area Centralized Control Centers,The general policies to construct the mutual fault- tolerant and standby SCADA system based on multi-agent technology for multi- area centralized control centers were presented in the paper in order to raise the safety and operational reliability of the power grid without additional equipment investment. The economic efficiency and feasibility of the system construction based on the policies are analyzed. The architecture of MAS and the function design of the Agents are introduced in detail and the specific implementation scheme and the corresponding key technologies are elucidated. The data and application fault-tolerance of SCADA system is realized to guarantee the reliability and continuity of the power grid operation.,2006,0,
12,13,Fault Management Driven Design with Safety and Security Requirements,"This paper exemplifies principles of embedded system design that props safety and security using operational errors management in frame of a dedicated Computer-Based System architecture. After reviewing basic principles of Cyber-Physical Systems as a novel slant (or marker?) to modeling and design in this domain, attention is focused on a real-world solution of a safety and security critical embedded system application offering genuine demonstration of that approach. The contribution stresses those features that distinguish the real project from a demonstration case study.",2010,0,
13,14,Reduction of faults in software testing by fault domination,"Although mutation testing is one of the practical ways of enhancing test effectiveness in software testing, it could be sometimes infeasible in practical work for a large scale software so that the mutation testing becomes time-consuming and even in prohibited time. Therefore, the number of faults assumed to exist in the software under test should be reduced so as to be able to confine the time complexity of test within a reasonable period of time. This paper utilizes the concept of fault dominance and equivalence, which has long been employed in hardware testing, for revealing a novel way of reducing the number of faults assumed to hide in software systems. Once the number of faults assumed in software is decreased sharply, the effectiveness of mutation testing will be greatly enhanced and become a feasible way of software testing. Examples and experimental results are presented to illustrate the effectiveness and the helpfulness of the technology proposed in the paper.",2007,0,
14,15,Electrical Test Structures for the Characterisation of Optical Proximity Correction,"Simple electrical test structures have been designed that will allow the characterisation of corner serif forms of optical proximity correction. The structures measure the resistance of a short length of conducting track with a right angled corner. Varying amounts of OPC can be applied to the outer and inner corners of the feature and the effect on the resistance of the track measured. These structures have been simulated and the results are presented in this paper. In addition a preliminary test mask has been fabricated which has test structures suitable for on-mask electrical measurement. Measurement results from these structures are also presented. Furthermore structures have been characterised using an optical microscope, a dedicated optical mask metrology system, an AFM scanner and finally a FIB system. In the future the test mask will be used to print the structures using a step and scan lithography tool so that they can be measured on-wafer. Correlation of the mask and wafer results will provide a great deal of information about the effects of OPC at the CAD level and the impact on the final printed features.",2007,0,
15,16,A real-time fault diagnosis system for UPS based on FFT frequency analysis,"UPS provides emergency power when utility power is not available, so the reliability of UPS is more important than inverter drive systems. In this paper, a fault diagnosis system for UPS is proposed using FFT frequency analysis of output current of inverter side of UPS under linear and nonlinear load conditions. Software PLL for precise synchronization of one period sampling and double buffer memory for real time processing are proposed. Experimental results show the increase of even harmonics including dc offset in case of fault conditions such as increase of resistance and delay or misfiring of IGBT turn-on, and prove the possibility of UPS fault diagnosis system if the criteria for fault decision are well defined.",2010,0,
16,17,5B: emerging technologies - reliable and fault-tolerant wireless sensor networks,"Wireless sensor networks create invisible interconnections with the physical world for the measurement, monitoring, and management of data from multiple sensors and probes with little constraint on location. These networks provide distributed processing, data storage, wireless communication, and dedicated application software with high reliability, inherent redundancy, failure-tolerant security and easily encrypted privacy. They have enormous potential to transform our society and are subjects of intense current research and application development. Three enabling hardware technologies which constitute a network node are microprocessors, MEMS sensors, and low-power radios. Sensor networks represent the paradigm shift in computing where they anticipate our needs and sometimes act on our behalf. The objective of this presentation is to discuss the reliable and fault-tolerant wireless sensor networks, focusing on environmental, behavioral, and biomedical areas. Special focus will be on wearable monitors and body wireless sensor network. An example of physiological monitoring by body area network will be discussed.",2005,0,
17,18,Application method of wavelets in the fault diagnosis of motion system,"In motion system, many types of faults are related with the abnormity of torque signal. A method was presented which was based on the wavelets function. The compactly supported orthonormal wavelets were introduced. Under it, the fault could be detected, and the type of fault could be diagnosed as well. For using convenience, the flow chart of using this method was also offered. On X-Y motion control system, collision experiments were implemented for test the given method. Using the sampled torque signals, the practicability was verified from the clear diagnosis of different types of collisions.",2008,0,
18,19,Fault detection in Flexible Assembly Systems using Petri net,"A significant part of the activities in a manufacturing system involve assembly tasks. Nowadays, these tasks are object of automation due to the market increasing demand for quality, productivity and variety of the products. Consequently, the automation of assembly systems should consider flexibility to face product diversification, functionalities, delivery times, and volumes involved. However, these systems are vulnerable to faults due to the characteristic of their mechanism and the complex interaction among their control devices. In this context, the present work is focused on the modeling design of flexible assembly systems control, including the occurrence of faults. The proposed method structures a sequence of steps for the models construction of assembly processes and their fault detection, based on the theory of discrete events systems and Petri net. This work use in special, production flow schema/mark flow graph (PFS/MFG) technique to describe and model the flexible assembly systems control through a rational and systematic procedure, as well as, the processes data record based on quantitative techniques for fault detection. This approach is applied to a flexible assembly systems installed and in operation to compare the effectiveness of the developed procedure.",2008,0,
19,20,An Evaluation of Similarity Coefficients for Software Fault Localization,"Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important technique for the development of dependable software. In this paper we study different similarity coefficients that are applied in the context of a program spectral approach to software fault localization (single programming mistakes). The coefficients studied are taken from the systems diagnosis/automated debugging tools Pinpoint, Tarantula, and AMPLE, and from the molecular biology domain (the Ochiai coefficient). We evaluate these coefficients on the Siemens Suite of benchmark faults, and assess their effectiveness in terms of the position of the actual fault in the probability ranking of fault candidates produced by the diagnosis technique. Our experiments indicate that the Ochiai coefficient consistently outperforms the coefficients currently used by the tools mentioned. In terms of the amount of code that needs to be inspected, this coefficient improves 5% on average over the next best technique, and up to 30% in specific cases",2006,0,
20,21,Fault-Tolerance in Universal Middleware Bridge,Universal middleware bridge (UMB) provides seamless interoperation among heterogeneous home network middleware. There have been high demands for the UMB components (UMB core and adaptors) to have fault- tolerance capabilities. This paper presents a TMO structuring approach together with new implementation techniques for the fault-tolerant TMO-replica structuring scheme called PSTR. PSTR implementations of UMB components provide fault tolerance capabilities essential in realizing high reliability for the UMB facility.,2008,0,
21,22,Performance evaluation of a fault-tolerant irregular network,"In an attempt to improve the fault-tolerance of the Omega network, this paper examines the performance of the proposed Theta network (THN), and compares it with other networks having similar characteristics. The irregular nature of the network has the inherent advantage of improving the latency of the network. Analytical results exhibit the favorable performance of THN at low cost, making the reliability degrade gracefully with time, while maintaining full-access capability over a reasonably long time. We study methods for routing requests in the presence and absence of faulty components in THN, where 50% of the requests pass at the minimum path length of 2.",2002,0,
22,23,The effect of registration error on tracking distant augmented objects,"We conducted a user study of the effect of registration error on performance of tracking distant objects in augmented reality. Categorizing error by types that are often used as specifications, we hoped to derive some insight into the ability of users to tolerate noise, latency, and orientation error. We used measurements from actual systems to derive the parameter settings. We expected all three errors to influence userspsila ability to perform the task correctly and the precision with which they performed the task. We found that high latency had a negative impact on both performance and response time. While noise consistently interacted with the other variables, and orientation error increased user error, the differences between ldquohighrdquo and ldquolowrdquo amounts were smaller than we expected. Results of userspsila subjective rankings of these three categories of error were surprisingly mixed. Users believed noise was the most detrimental, though statistical analysis of performance refuted this belief. We interpret the results and draw insights for system design.",2008,0,
23,24,Improving Bug Assignment with Bug Tossing Graphs and Bug Similarities,"In open-source software development the bug report is usually assigned to a developer for bug fixing. A large number of bug reports are tossed (reassigned) to other developers, for example because the bugs have been assigned by mistake. The tossing events increase bug-fix time. In order to quickly identify the fixer to bug reports we present an approach based on the bug tossing history and textual similarities between bug reports. This proposed approach is evaluated on Eclipse and Mozilla. The results show that our approach can significantly improve the efficiency of bug assignment: the bug resolver is often identified with fewer tossing events.",2010,0,
24,25,Doppler estimation and correction for shallow underwater acoustic communications,"Reliable mobile underwater acoustic communication systems must compensate for strong, time-varying Doppler effects. Many Doppler correction techniques rely on a single bulk correction to compensate first-order effects. In many cases, residual higher-order effects must be tracked and corrected using other methods. The contributions of this paper are evaluations of (1) signal-to-noise ratio (SNR) performance from three Doppler estimation and correction methods and (2) communication performance of Doppler correction with static vs. adaptive equalizers. The evaluations use our publicly available shallow water experimental dataset, which consists of 360 packet transmission samples (each 0.5s long) from a five-channel receiver array.",2010,0,
25,26,Forward error protection for robust video streaming based on distributed video coding principles,"This paper proposes an error resilient coding scheme that employs distributed video coding tools. A bitstream, produced by any standard motion-compensated predictive codec (MPEG-x, H.26x), is sent over an error-prone channel. In addition, a Wyner-Ziv encoded auxiliary bitstream is sent as redundant information to serve as a forward error correction code. At the decoder side, error concealed reconstructed frames are used as side information by the Wyner-Ziv decoder, and the corrected frame is used as a reference by future frames, thus reducing drift. We explicitly target the problem of rate allocation at the encoder side, by estimating the channel induced distortion in the transform domain. Experimental results conducted over a simulated error-prone channel reveal that the proposed scheme has comparable or better performance than a scheme where forward error correction codes are used. Moreover the proposed solution shows good performance when compared to a scheme that uses the intra-macroblock refresh procedure.",2008,0,
26,27,Analysis on Interruption and Plane Layout of Shear Wall for Frame-Shear Wall Structure with Top Fault Shear Wall,"According to the force-deformation characteristics of frame-shear wall structure, the calculation and analysis model of the ""Style Box"" type layout of frame-shear wall structure is advanced with the basic and simple arrangement of frame-shear wall structure. The different plane and vertical layout with interrupting shear walls of the frame-shear wall structure with top fault shear walls is discussed primarily by using the method of plane layout and vertical interruptable position being considered at the same time. Based on the main parameters of the frame-shear wall structure with top fault shear walls in different ways, it puts forward the importance of the shear wall location of the plane layout to the overall performance of the frame-shear wall structure with top fault shear walls except the lateral stiffness of the shear wall corresponding to the shear wall interrupted ratio.",2009,0,
27,28,Behavioral modular description of fault tolerant distributed systems with AADL Behavioral Annex,"AADL is an architecture description language intended for model-based engineering of high-integrity distributed systems. The AADL Behavior Annex (AADL-BA) is an extension allowing the refinement of behavioral aspects described through an AADL architectural description. When implementing Distributed Real-time Embedded system (DRE), fault tolerance concerns are integrated by applying replication patterns. We considered a simplified design of the primary backup replication pattern as a running example to analyze the modeling capabilities of AADL and its annex. Our contribution lies in the identification of the drawbacks and benefits of this modeling language for accurate description of the synchronization mechanisms integrated in this example.",2010,0,
28,29,Reducing human error in simulation in General Motors,"We focus on the steps taken to minimize human error in simulation modeling in General Motors. While errors are costly and undesirable in any field, they are especially harmful in simulation which has been struggling to gain acceptance in the business world for a long time. The solution discussed can be summarized as ""enter the data once and use the best tool for the job"".",2003,0,
29,30,Analysis of pressure and Blanchard altitude errors computed using atmospheric data obtained from an F-18 aircraft flight,"Pressure altitude is commonly utilized as an altitude reference for an inertial navigation system (INS) to damp the error growth in the inherently unstable vertical channel. A precise altitude reference for use in the INS vertical channel can be obtained using the Blanchard algorithm, which computes altitude from atmospheric pressure, temperature, aircraft ground velocity, and wind velocity data. This paper computes both the pressure and Blanchard altitudes for an entire test flight of an F-18 aircraft from the atmospheric data measured during the flight. The flight repeats 4 cycles of a climb, level-off, dive, level-off trajectory. The altitude computed from GPS during flight is considered to be the truth altitude. The errors in the pressure and Blanchard altitudes are computed and compared. In addition both altitude errors are analyzed in order to determine the scale factor, bias offset, and time delay utilizing the least square error fit method. The Blanchard altitude is a much more precise altitude reference than pressure altitude during actual flight of an F-18 aircraft.",2002,0,
30,31,Residual error models for the SOLT and SOLR VNA calibration algorithms,"Uncertainty calculation of vector network analyzers (VNAs) using the SOLT or SOLR calibration algorithms is often performed using residual directivity, match and tracking. In the literature the uncertainty equations are often stated without a derivation from a proper model equation. In this paper we derive the model equations for both the SOLT and SOLR calibration, the two cases do not result in the same model equation. The results are also compared to the commonly used expressions for uncertainty in the EA guidelines for VNA evaluation. For one-port measurements our results confirm the expressions in the EA guide but for two-ports there are significant differences. The symbolically derived model equations are verified using numerical simulations.",2007,0,
31,32,A method for dead reckoning parameter correction in pedestrian navigation system,"This paper presents a method for correcting dead reckoning parameters, which are heading and step size, for a pedestrian navigation system. In this method, the compass bias error and the step size error can be estimated during the period that the Global Positioning System (GPS) signal is available. The errors are used for correcting those parameters to improve the accuracy of position determination using only the dead reckoning system when the GPS signal is not available. The results show that the parameters can be estimated with reasonable accuracy. Moreover, the method also helps to increase the positioning accuracy when the GPS signal is available.",2003,0,
32,33,Single-stage power factor correction converter with parallel power processing for wide line and load changes,"A new single-phase single-stage power factor correction converter with a simple auxiliary circuit is proposed. Using parallel power processing, this converter can be operated in wide line and load changes while limiting the link voltage below 400 V. Experimental results show that the measured power factor and efficiency are about 0.98 and 81%, respectively, at rated condition and the auxiliary circuit to reduce the link voltage is effective",2002,0,
33,34,Using variable-length error-correcting codes in MPEG-4 video,Reversible variable length (RVL) codes are used in MPEG-4 video coding to improve its error resilience. Algorithms used to design variable-length error-correcting (VLEC) codes are modified so as to construct efficient RVL codes with a smaller average length than those found in the literature. It is also shown that RVL codes are a special (weak) class of VLEC codes. Consequently more powerful VLEC codes can be used in the MPEG-4 codec and it is shown that performance gains of up to 20 dB in peak signal to noise ratio (PSNR) can be obtained using a soft-decision sequential decoder with relatively simple VLEC codes. This increase in performance is obtained at the expense of an order of magnitude increase in decoding complexity,2005,0,
34,35,Restoration of Directional Overcurrent Relay Coordination in Distributed Generation Systems Utilizing Fault Current Limiter,"A new approach is proposed to solve the directional overcurrent relay coordination problem, which arises from installing distributed generation (DG) in looped power delivery systems (PDS). This approach involves the implementation of a fault current limiter (FCL) to locally limit the DG fault current, and thus restore the original relay coordination. The proposed restoration approach is carried out without altering the original relay settings or disconnecting DGs from PDSs during fault. Therefore, it is applicable to both the current practice of disconnecting DGs from PDSs, and the emergent trend of keeping DGs in PDSs during fault. The process of selecting FCL impedance type (inductive or resistive) and its minimum value is illustrated. Three scenarios are discussed: no DG, the implementation of DG with FCL and without FCL. Various simulations are carried out for both single- and multi-DG existence, and different DG and fault locations. The obtained results are reported and discussed.",2008,0,
35,36,The Design Of Embedded Bus monitoring And Fault Diagnosis System Based On Protocol SAE J1939,"Embedded bus monitoring and fault diagnosis system, which was based on protocol SAE J1939 was designed in this paper. And this system took the 32-bit embedded one as a hardware platform, customized a WinCE6.0 operation system and used EVC as the tool to design the embedded application. The functions of CAN communication, protocol defamations etc were realized. Good human-computer interaction is developed and the system has already been applied on the bus.",2010,0,
36,37,A Fault Analysis and Classifier Framework for Reliability-Aware SRAM-Based FPGA Systems,"This paper presents a new framework for the analysis of SRAM-based FPGA systems with respect to their dependability properties against single, multiple and cumulative upsets errors. The aim is to offer an environment for performing fault classification and error propagation analyses for designed featuring fault detection or tolerance techniques against soft errors, where the focus is not only the overall achieved fault coverage, but an understanding of the fault/error relation inside the internal elements of the system. We propose a fault analyzer/classifier laying on top of a classical fault injection engine, used to monitor the evolution of the system after a fault as occurred, with respect to the applied reliability-oriented design technique. The paper introduces the framework and reports some experimental results of its application to a case study, to highlight the benefits of the proposed solution.",2009,0,
37,38,Stress wave analysis of turbine engine faults,"Stress Wave Analysis (SWAN) provides real-time measurement of friction and mechanical shock in operating machinery. This high frequency acoustic sensing technology filters out background levels of vibration and audible noise, and provides a graphic representation of machine health. By measuring shock and friction events, the SWAN technique is able to detect wear and damage at the earliest stages and is able to track the progression of a defect throughout the failure process. This is possible because as the damage progresses, the energy content of friction and shock events increases. This `stress wave energy' is then measured and tracked against normal machine operating conditions. This paper describes testing that was conducted on several types of aircraft and industrial gas turbine engines to demonstrate SWAN's ability to accurately detect a broad range of discrepant conditions and characterize the severity of damage",2000,0,
38,39,High-level vulnerability over space and time to insidious soft errors,"The integrity of computational results is being increasingly threatened by soft errors, especially for computations that are large-scale or performed under harsh conditions. Existing methods for soft error estimation do not clearly characterize the vulnerability associated with a particular result. 1) We propose a metric which captures the intrinsic vulnerability over space and time (VST) to soft errors that corrupt computational results. The method of VST estimation bridges the gap between the inherently low-level faults and the high-level computational failures that they eventually cause. 2) We define a model of an insidious soft error and try to clear up confusion around the concept of silent data corruption. 3) We present experimental results from three vulnerability studies involving floating-point addition, CORDIC, and FFT computations. The results show that traditional vulnerability metrics can be confounded by seemingly reliable but inefficient implementations which actually incur high vulnerability per computation. The VST method characterizes vulnerability accurately, provides a figure-of-merit for comparing alternative implementations of an algorithm, and in some cases uncovers pronounced and unexpected fluctuations in vulnerability.",2008,0,
39,40,Image Defect Recognition Based on Rough Set,"This paper applies rough set theory to recognition system for image defect, and designs a decision algorithm on rough set suitable for image defect recognition. Firstly, the image is made regionalization and sequential discrete set is proposed, the continuous attributes of image is discretized. Then the decision table model on discrete condition attributes and decision attributes is constructed. Further the condition attributes significance function and reduction algorithm is given. A novel approach for decision rule analysis and rough set recognition is proposed. Finally, this paper takes the example for fabric defect recognition to validate these algorithms. The result shows the rough set algorithm is effective for image defect recognition with less calculation and fast speed.",2009,0,
40,41,Using design patterns and constraints to automate the detection and correction of inter-class design defects,"Developing code free of defects is a major concern for the object oriented software community. The authors classify design defects as those within classes (intra-class), those among classes (inter-classes), and those of semantic nature (behavioral). Then, we introduce guidelines to automate the detection and correction of inter-class design defects. We assume that design patterns embody good architectural solutions and that a group of entities with organization similar, but not equal, to a design pattern represents an inter-class design defect. Thus, the transformation of such a group of entities, such that its organization complies exactly with a design pattern, corresponds to the correction of an inter-class design defect. We use a meta-model to describe design patterns and we exploit the descriptions to infer sets of detection and transformation rules. A constraint solver with explanations uses the descriptions and rules to recognize groups of entities with organizations similar to the described design patterns. A transformation engine modifies the source code to comply with the recognized distorted design patterns. We apply these guidelines on the Composite pattern using PTIDEJ, our prototype tool that integrates the complete guidelines",2001,0,
41,42,Error localization for robust video transmission,"The convergence of Internet, multimedia and mobile applications has led to an increased demand for efficient and reliable video data transmission over heterogeneous networks. Due to their coding efficiency, variable-length codes (VLC) are usually employed in the entropy coding stage of video compression standards. However, error propagation is a major problem associated with VLC. We propose the use of a class of self-synchronizing VLC (SSVLC) to achieve the dual goal of optimal coding efficiency and optimal error localization. Performance evaluation has confirmed that the use of SSVLC provides better performance than standard VLC techniques.",2002,0,
42,43,On the error-control coding techniques used in GSM/EDGE radio access networks,In this paper the error-control coding techniques used in GSM/EDGE Radio Access Network (GERAN) are considered. Application of coding schemes is restricted by the corresponding traffic channels (TCH). Knowing the general scheme is necessary for modeling the work of the complete error-control system. This knowledge can be useful for the implementation of educational software used for the investigation of the properties of different codecs and their characteristics in radio channels using various radio channel models.,2004,0,
43,44,A Multi-Agent Fault Detection System for Wind Turbine Defect Recognition and Diagnosis,This paper describes the use of a combination of anomaly detection and data-trending techniques encapsulated in a multi-agent framework for the development of a fault detection system for wind turbines. Its purpose is to provide early error or degradation detection and diagnosis for the internal mechanical components of the turbine with the aim of minimising overall maintenance costs for wind farm owners. The software is to be distributed and run partly on an embedded microprocessor mounted physically on the turbine and on a PC offsite. The software will corroborate events detected from the data sources on both platforms and provide information regarding incipient faults to the user through a convenient and easy to use interface.,2007,0,
44,45,Motion correction of PET images using realignment for intraframe movement,"A method is presented for the motion correction of PET images using realignment for intraframe movement. A newly introduced aspect of the method is that it corrects not only interframe but also intraframe movement, using currently used PET images (which are not corrected for intraframe movement) and motion tracking data. Although our method requires motion tracking data, it does not require very short time image acquisition nor list mode data acquisition. So, it is applicable to currently used PET images. In our method the following hypothesis is assumed. That is if no movement happens, there exists a linear model such that counts of each voxel per unit time follows the model. Model parameters may depend on the voxels. In a simple example, our method successfully corrected motion artifact and estimates the parameters accurately. It may give a simple and practical solution to the motion correction problems.",2003,0,
45,46,Study on Data Mining for Grounding Fault Line Selection in 6kV Ineffectively Grounded System of Coal Mine,"A great amount of fault wave has been recorded by the devices for detecting phase-to-ground faults in ineffectively grounded systems. However, a better method hasn't found for effectively taking advantage of these data to improve the result of fault line selection. Data mining techniques can be used for fault line selection in ineffectively grounded system to gain knowledge from the existing data and to improve the technique of fault line selection. This paper briefly describes the principles, methods and implementation of data mining techniques, classifies the fault samples of ineffectively grounded systems by using clustering analysis method, employs different fault line selection methods according to the types of faults, and consequently provides a set of criteria for modeling of typical ineffectively grounded systems and verifying the validity of real-time fault line selections. The validity of the methods has been convinced by the calculation using the data obtained from the real performance of a substation in coal mine. It has been shown to be promising to employ the data mining techniques in ineffectively grounded systems fault detection. This paper provides very good methods for resolving the difficulties with onsite tests, enhancing the techniques of fault line selection and establishing the fault detection management systems.",2010,0,
46,47,Fault-Tolerant Coverage Planning in Wireless Networks,"Typically wireless networks coverage is planned with static redundancy to compensate temporal variations in the environment. As a result, the service still is delivered but the network coverage could have entered a critical state, meaning that further changes in the environment may lead to service failure. Service failures have to be explicitly notified by the applications. Therefore, in this paper we propose a methodology for fault-tolerant coverage planning. The idea is detecting the critical state and removing it by on-line system reconfiguration, and restoration of the original static redundancy. Even in case of a failure the system automatically generates a new configuration to restore the service, leading to shorter repair times. We describe how this approach can be applied to wireless mesh networks, often used in industrial applications like manufacturing, automation and logistics. The evaluation results show that the underlying model used for error detection and system recovery is accurate enough to correctly identify the system state.",2008,0,
47,48,Detection of high impedance fault in distribution feeder using wavelet transform and artificial neural networks,"This work presents a novel analysis method that can simulate the potential effect of high impedance fault (HIF). The proposed method offers a new scheme for protecting the overhead distribution feeder. The wavelet transform (WT) method was successfully applied in many fields. The characteristics of scaling and translation of WT can be used to identify stable and transient signals. Discrete wavelet transforms (DWT) are initially used to extract distinctive features of the voltage and current signals, and are transformed into a series of detailed and approximated wavelet components. The coefficients of variation of the wavelet components are then calculated. This information is introduced into the training artificial neural networks (ANN) to determine an HIF from the operations of the switches. The simulated results clearly reveal that the proposed method can accurately identify the HIF in the distribution feeder.",2004,0,
48,49,A hierarchical framework for fault propagation analysis in complex systems,"In complex systems, there are few critical failure modes. Prognostic models are focused at predicting the evolution of those critical faults, assuming that other subsystems in the same system are performing according to their design specifications. In practice, however, all the subsystems are undergoing deterioration that might accelerate the time evolution of the critical fault mode. This paper aims at analyzing this aspect, i.e. interaction between different fault modes in various subsystems, of the failure prognostic problem. The application domain focuses on an aero propulsion system of the turbofan type. Creep in the high-pressure turbine blade is one of the most critical failure modes of aircraft engines. The effects of health deterioration of low-pressure compressor and high-pressure compressor on creep damage of high-pressure turbine blades are investigated and modeled.",2009,0,
49,50,Data Mining Using Rough Sets and Orthogonal Signal Correction-Orthogonal Partial Least Squares Analysis,"The paper put forward Data mining using rough sets and orthogonal signal correction-orthogonal partial least squares analysis (RS-OSC-OPLS/O2PLS). first, dimensionality reduction and de-noising with rough sets and orthogonal signal correction;second, Data mining using orthogonal partial least squares analysis. The method was proved to be feasible and effective after tested with 13 kinds of nationalities crowds data.",2010,0,
50,51,Estimation of Systematic Errors of MODIS Thermal Infrared Bands,"This letter reports a statistical method to estimate detector-dependent systematic error in Moderate Resolution Imaging Spectroradiometer (MODIS) thermal infrared (TIR) Bands 20-25 and 27-36. There exist scan-to-scan overlapped pixels in MODIS data. By analyzing a sufficiently large amount of those most overlapped pixels, the systematic error of each detector in the TIR bands can be estimated. The results show that the Aqua MODIS data are generally better than the Terra MODIS data in 160 MODIS TIR detectors. There are no detector-dependent systematic errors in Bands 31 and 32 for both Terra and Aqua MODIS data. The maximum detector errors are 3.00 K in Band 21 of Terra and -8.15 K in that of Aqua for brightness temperatures of more than 250 K",2006,0,
51,52,Comparing Web Services Performance and Recovery in the Presence of Faults,"Web-services are supported by a complex software infrastructure that must ensure high performance and availability to the client applications. Web services industry holds a well established platform for performance benchmarking (e.g., TPC-App and SPEC jAppServer2004 benchmarks). In addition, several studies have been published recently by main vendors focusing web services performance. However, as peak performance evaluation has been the main focus, the characterization of the impact of faults in such systems has been largely disregarded. This paper proposes an approach for the evaluation and comparison of performance and recovery time in web services infrastructures. This approach is based on fault injection and is illustrated through a concrete example of benchmarking three alternative software solutions for web services deployment.",2007,0,
52,53,Evidence-Based Analysis and Inferring Preconditions for Bug Detection,"An important part of software maintenance is fixing software errors and bugs. Static analysis based tools can tremendously help and ease software maintenance. In order to gain user acceptance, a static analysis tool for detecting bugs has to minimize the incidence of false alarms. A common cause of false alarms is the uncertainty over which inputs into a program are considered legal. In this paper we introduce evidence-based analysis to address this problem. Evidence-based analysis allows one to infer legal preconditions over inputs, without having users to explicitly specify those preconditions. We have found that the approach drastically improves the usability of such static analysis tools. In this paper we report our experience with the analysis in an industrial deployment.",2007,0,
53,54,"PSoC design in GM(1,1) error analysis and its application in temperature prediction","In the study of prediction filed, no matter what methods we used, the main purpose is to minimize the prediction error; however, the goals cannot be fulfilled completely. Even we choose GM(1,1) model, which in the newest soft computing method, we also need to minimize the prediction error. Hence, in this paper, we focus on the influence parameter alpha in GM(1,1) model in the first, then, analyze the characteristics of alpha step by step, and use numerical method to find the prediction error corresponding with alpha value. Second, after the mathematics model is presented, we use PSoC to design a GM(1,1) error analysis model, which based on the characteristic of GM(1,1) model. Also an example, which is temperature prediction case is given to assist us to implement our approach in the final section.",2008,0,
54,55,Calculation of transverse voltages of communication lines induced by the fault current of power system,"A double-line model is presented to calculate the transverse voltage of communication lines induced by the fault current of power lines. By dividing the communication line into several fictitious segments, a chain composed by the coupling P1-type circuit with distributed source is formed. The enhanced node voltage analysis (ENVA) is also developed in order to evaluate such a kind of model. The ENVA cuts the number of nodes down greatly because of treating the active and coupling impedance branches as a whole. In addition, the transverse voltages in time domain can be obtained easily from those calculated in frequency domain by means of fast Fourier transform. The numerical examples prove the validity and efficiency of the method by comparison with analytical results. The model is of significance to the design and the rights-of-way selection of power lines and communication lines.",2002,0,
55,56,General review of fault diagnostic in wind turbines,"Global wind electricity-generating capacity increased by 28.7 percent in 2008 to 120,798 Gigawatts. This represents a twelve-fold increase from a decade ago, when world wind-generating capacity stood at less than 5 GW [1]. With wind becoming a key part of the electrical mix in Denmark (20% with 3.1 GW), Spain (8% with 10 GW), and Germany (6% with 18.4 GW), wind turbine reliability is having a bigger effect on overall electrical grid system performance and reliability [1]. This shows the impact of faults and downtime on the reliability of wind turbine especially for offshore wind farms which although are some of the most environmentally friendly and efficient methods to generate electricity in the world. However, the maintenance costs are high because of their remote location. This can amount to as much as 25 to 30% of the total energy production [2]. The aim of this paper is to present an overview of fault detection in wind turbines, study and analyze the faults and their root-causes. The paper also explores different techniques used in early fault detection to form base information for future work to build a general fault diagnostic scheme for wind turbines.",2010,0,
56,57,Reconfigurable context-free grammar based data processing hardware with error recovery,"This paper presents an architecture for context-free grammar (CFG) based data processing hardware for re-configurable devices. Our system leverages on CFGs to tokenize and parse data streams into a sequence of words with corresponding semantics. Such a tokenizing and parsing engine is sufficient for processing grammatically correct input data. However, most pattern recognition applications must consider data sets that do not always conform to the predefined grammar. Therefore, we augment our system to detect and recover from grammatical errors while extracting useful information. Unlike the table look up method used in traditional CFG parsers, we map the structure of the grammar rules directly onto the field programmable gate array (FPGA). Since every part of the grammar is mapped onto independent logic, the resulting design is an efficient parallel data processing engine. To evaluate our design, we implement several XML parsers in an FPGA. Our XML parsers are able to process the full content of the packets up to 3.59 Gbps on Xilinx Virtex 4 devices",2006,0,
57,58,Autonomous Fault Recovery Technology for Achieving Fault-Tolerance in Video on Demand System,"With the advances of compression technology, storage devices and networks, video on demand (VoD) service is becoming popular. The system needs to provide continuous service and heterogeneous service levels for users. However, these requirements cannot be satisfied in conventional VoD system which is constructed on redundant content servers and centralized management. In this paper, autonomous VoD system is proposed to meet the requirements. The system is constructed on faded information field architecture. Under the proposed architecture, autonomous fault detection and fault recovery technologies are proposed to achieve fault-tolerance for continuous service. The effectiveness of the proposed technologies are proved through simulation. The results show that an average of 30% improvement in recovery time and users' video service can be recovered without stopping compared with conventional VoD system",2006,0,
58,59,Event-based fault detection of manufacturing cell: Data inconsistencies between academic assumptions and industry practice,"Some problems with event-based faults in manufacturing systems cannot be handled by existing fault detection solutions, including finding faults in event-based data for systems for which limited information is known. A new fault detection solution that finds faults in event-based data using model generation is presented here. This solution assumes that some information is known about the system from its design information and data structure. An example application of this solution is presented for a Ford machining cell that has been experiencing a gantry waiting problem. In the course of this example application, five inconsistencies were found between relatively common academic assumptions made by this fault detection solution (as well as others) and the actual cell's set-up and data. These inconsistencies and possible means of addressing them are discussed. Some of these means to resolve the inconsistencies have been implemented, and preliminary results in generating models using the fault detection solution are presented.",2010,0,
59,60,Error monitoring for optical metropolitan network services,"Service providers rely on performance monitoring capabilities not only to ensure integrity of their network but also to support service-level agreements with their customers. The depth of monitoring is directly tied to the technology and protocol used in the transport layer of the network. Next-generation services based on enterprise-centric, non-SONET/SDH protocols, such as Gigabit Ethernet and Fibre Channel, as well as managed protocol-independent wavelength transport, have created a number of challenges for service providers because of the differences in how error monitoring is performed. In this article we describe and compare protocol-dependent and protocol-independent error monitoring techniques that apply to these service offerings",2002,0,
60,61,A method of inverter circuit fault diagnosis based on BP neural network and D-S evidence theory,"With the study and analysis on intelligent fault diagnosis for inverting circuit, an improved diagnosis method combined BP neuron network and D-S evidence theory was proposed. Each measuring point was extracted by BP neural network to obtain the local diagnosis, which is adopted to design the belief function of D-S evidence theory. Multiple monitoring points' information is fused to receive the comprehensive global diagnosis result. The experimental results show that this method has the better feasibility and effectiveness on fault diagnosis in inverter's key components-inverting circuit.",2010,0,
61,62,On fault diagnosis tree and its control flow,"The coarse-grained organization of the existing fault diagnosis scheme can not realize the automatic and intelligent diagnosis. This paper provides a tree structure of fault diagnosis scheme named T04FDS, which integrates the fault classification relations and nesting relation of part and whole. By building the state transform system to represent the diagnosis process, and describing the control flow with operations of stack. Furthermore the thesis presents the physical realization of T04FDS fault diagnosis, and finally proves the feasibility of this method by giving an example of fault diagnosis scheme for computer wireless network card.",2009,0,
62,63,Master-Slave TMR Inspired Technique for Fault Tolerance of SRAM-Based FPGA,"In order to increase reliability and availability of Static-RAM based field programmable gate arrays (SRAM-based FPGAs), several methods of tolerating defects and permanent faults have been developed and applied. These methods are not well adapted for handling high fault rates for SRAM based FPGAs. In this paper, both single and double faults affecting configurable logic blocks (CLBs) are addressed. We have developed a new fault-tolerance technique that capitalizes on the partial reconfiguration capabilities of SRAM-based FPGA. The proposed fault-tolerance method is based on triple modular redundancy (TMR) combined with master-slave technique, and exploiting partial reconfiguration to tolerate permanent faults. Simulation results on reliability improvement corroborate the efficiency of the proposed method and prove that it compares favorably to previous methods.",2010,0,
63,64,Robust Speech Recognition Using a Cepstral Minimum-Mean-Square-Error-Motivated Noise Suppressor,"We present an efficient and effective nonlinear feature-domain noise suppression algorithm, motivated by the minimum-mean-square-error (MMSE) optimization criterion, for noise-robust speech recognition. Distinguishing from the log-MMSE spectral amplitude noise suppressor proposed by Ephraim and Malah (E&M), our new algorithm is aimed to minimize the error expressed explicitly for the Mel-frequency cepstra instead of discrete Fourier transform (DFT) spectra, and it operates on the Mel-frequency filter bank's output. As a consequence, the statistics used to estimate the suppression factor become vastly different from those used in the E&M log-MMSE suppressor. Our algorithm is significantly more efficient than the E&M's log-MMSE suppressor since the number of the channels in the Mel-frequency filter bank is much smaller (23 in our case) than the number of bins (256) in DFT. We have conducted extensive speech recognition experiments on the standard Aurora-3 task. The experimental results demonstrate a reduction of the recognition word error rate by 48% over the standard ICSLP02 baseline, 26% over the cepstral mean normalization baseline, and 13% over the popular E&M's log-MMSE noise suppressor. The experiments also show that our new algorithm performs slightly better than the ETSI advanced front end (AFE) on the well-matched and mid-mismatched settings, and has 8% and 10% fewer errors than our earlier SPLICE (stereo-based piecewise linear compensation for environments) system on these settings, respectively.",2008,0,
64,65,On-board fault-tolerant SAR processor for spaceborne imaging radar systems,"A real-time high-performance and fault-tolerant FPGA-based hardware architecture for the processing of synthetic aperture radar (SAR) images has been developed for advanced spaceborne radar imaging systems. In this paper, we present the integrated design approach, from top-level algorithm specifications, system architectures, design methodology, functional verification, performance validation, down to hardware design and implementation.",2005,0,
65,66,Fault-Tolerant BPEL Workflow Execution via Cloud-Aware Recovery Policies,"BPEL is the de facto standard for business process modeling in today's enterprises and is a promising candidate for the integration of business and scientific applications that run in Grid or Cloud environments. In these distributed infrastructures, the occurrence of faults is quite likely. Without sophisticated fault handling, workflows are frequently abandoned due to software or hardware failures, leading to a waste of CPU hours. The fault handling mechanisms provided by BPEL are well suited for handling faults of the business logic, but infrastructure-induced errors should be handled automatically to avoid over-complication of workflow design and keep concerns separated. This paper identifies classes of faults that can be resolved automatically by the infrastructure, and provides a policy-based approach to configure this automatic behavior without the need for adding explicit fault handling mechanisms to the BPEL process. The proposed approach provides automatic redundancy of services using a Cloud infrastructure to allow substitution of defective services. An implementation based on the ActiveBPEL engine and Amazon's Elastic Compute Cloud is presented.",2009,0,
66,67,Neural fault isolator for Wireless Sensor Networks,"Wireless sensor networks are emerging as an innovative technology that can help to improve business processes. In such environments malfunctions and break-down states must be efficiently diagnosed to reduce to a minimum the economic losses. In this paper we present a fault isolation approach based on neural networks, which utilizes only a minimum set of information such as the sensor value, node ID and timestamp as inputs. We believe that this information set could be provided by any WSN regardless of its specific implementation. This abstraction makes the fault isolator generically applicable in enterprise business systems. The neural fault isolator was evaluated in a trial with 36 nodes and has proved to be highly efficient in the isolation of failed components.",2008,0,
67,68,Test Compaction for Transition Faults under Transparent-Scan,"Transparent-scan was proposed as an approach to test generation and test compaction for scan circuits. Its effectiveness was demonstrated earlier in reducing the test application time for stuck-at faults. We show that similar advantages exist when considering transition faults. We first show that a test sequence under the transparent-scan approach can imitate the application of broadside tests for transition faults. Test compaction can proceed similar to stuck-at faults by omitting test vectors from the test sequence. A new approach for enhancing test compaction is also described, whereby additional broadside tests are embedded in the transparent-scan sequence without increasing its length or reducing its fault coverage",2006,0,
68,69,Constrained free form deformation based algorithm for geometric distortion correction of echo planar diffusion tensor images,"In order to differentiate between normal and abnormal variations in brain diffusion tensor images, it is necessary to develop medical atlases. Atlas creation requires removal of spatial distortions in individual subject diffusion weighted images. In this paper we suggest a new approach using non-linear warping based on optic flow to map both baseline and diffusion weighted echo planar images to the anatomically correct T2 weighted spin echo image. The method is readily implemented and does not require a pre-processing step of rigid alignment. A global histogram matching precedes the base line EP image correction. A Markov random field based classification algorithm was implemented to cluster T2 weighted images into four different tissue type classes. This information was then used to synthesize diffusion based image models used in the warping algorithm to correct the geometric distortions in the diffusion weighted EP images.",2004,0,
69,70,Detecting Single and Multiple Faults Using Intelligent DSP and Agents,"In this paper intelligent agents and DSP techniques are integrated to detect single and multiple faults in electrical circuits. Agents are used to model the AC electrical circuit. A DSP engine is embedded into the agents to analyse the signals, i.e. the energy transfer between the physical components. An AC to DC rectifier circuit is chosen as test-bed for the proposed solutions",2006,0,
70,71,Real-time model based sensor fault tolerant control system on a chip,"In this paper, we proposed a model based sensor fault tolerant control system embedded in a generic PIC microcontroller for use in a temperature control system. The model based fault tolerant control algorithm is embedded in the microcontroller for stand-alone real-time implementation. The algorithm consists of a PID controller element (for nominal control) and a fault compensating element. Results from simulations and real time implementation are shown to demonstrate the ease of real time implementation.",2009,0,
71,72,Efficiency enhancement of microstrip patch antenna with defected ground structure,"Defected ground structures (DGS) have been developed to improve characteristics of many microwave devices. Although the DGS has advantages in the area of the microwave filter design, microstrip antenna design for different applications such as cross polarization reduction and mutual coupling reduction etc., it can also be used for the antenna size reduction. The etching of a defect in the ground plane is a unique technique for the antenna size reduction. The DGS is easy to be an equivalent LC resonator circuit. The value of the inductance and capacitance depends on the area and size of the defect. By varying the various dimensions of the defect, the desired resonance frequency can be achieved. In this paper the effect of dumbbell shaped DGS, to the size reduction of a microstrip patch antenna is investigated. Then a cavity backed structure is used to increase the efficiency of the microstrip patch antenna, in which the electric walls are placed surrounding the patch. The simulation is carried out with IE3D full wave EM simulator.",2008,0,
72,73,Effects of Defects on the In-plane Dynamic Energy Absorption of Metal Honeycombs,"The in-plane dynamic energy absorption of metal honeycombs with defects consisting of missing cells are analyzed using explicit dynamic finite element method. Two types of structural defects (a single defect located in the center of the model and a double defect) are firstly introduced. Then the influence of the defects and the impact velocities on the energy absorption abilities of metal honeycombs is investigated. Researches show that single and isolated defects reduce the absorbed energy of cellular materials. The separation distance between two defects has little effect on the dynamic energy absorption, while the size of the single defect has great influence on it. These results will provide some useful guides for the safety evaluation and the dynamic energy absorption design of metal honeycombs.",2010,0,
73,74,Fault Diagnosis on Board for Analog to Digital Converters,"This paper describes a general purpose high reliable data acquisition system which allows A/D converter testing by histogram and two tone tests for the fault diagnosis on the same board. A reliability analysis has been carried out in order to optimize the project, the components choice and redundancy configuration. The software has been written in Matlab and LabVIEW, with an easy graphical user interface.",2007,0,
74,75,Safing and fault protection for the MESSENGER mission to Mercury,"The MErcury Surface, Space ENvironment, GEochemistry, and Ranging (MESSENGER) mission is a NASA Discovery-class, deep-space mission to orbit the planet Mercury. Its purpose is to map the planet surface using various scientific instruments and explore the interior of the planet using measurements from instruments such as a magnetometer and observation of planetary libration. This paper discusses the architecture and implementation of the methods by which faults in the MESSENGER spacecraft are detected and the effects of those faults mitigated. The responsibility of the redundant Fault Protection Processors (FPPs) is to detect faults and take autonomous corrective actions that will keep the spacecraft healthy and safe.",2002,0,
75,76,A new error concealment algorithm for H.264 video transmission,"In this paper, a new error concealment algorithm for the new coding standard H.264 is presented. The algorithm consists of a block size determination step to determine the size type of the lost block and a motion vector recovery step to find the lost motion vector from multiple reference frames. The main feature of this algorithm are as follows. In the block size determination step, we propose a criterion to determine the size type of the lost block from the current frame. In the motion vector recovery step, the optimal motion vector for the lost block chosen from multiple previous reference frames with the minimum value of the side match distortion. The proposed algorithm not only can determine the most correct mode for the lost block, but also can save much more computation time for motion vector recovery. Experimental results show that the proposed algorithm achieves 0.47 dB improvement over the conventional VM method.",2004,0,
76,77,An EMF activity tree based BPEL defect pattern testing method,"For testing BPEL defects efficiently, a novel BPEL defect pattern testing architecture based on the EMF activity tree technology is proposed. The EMF activity tree that is similar to abstract syntax tree is used to describe the BPEL service process structure. The mapping method from the DOM object tree of a BPEL file to the EMF activity tree and the recursive algorithm to generate an EMF activity tree are represented in detail. A typical EMF activity tree is shown and the visitor design pattern based traversal method is stated. The directions to enhance this technology are illustrated finally.",2010,0,
77,78,Fault tolerant generator systems for wind turbines,"The objective of this paper is to review the possibilities of applying fault tolerance in generator systems for wind turbines based on what has been presented in the literature. In order to make generator systems fault tolerant in a suitable way, it is necessary to gain insight into the probability of different failures, so that suitable measures can be taken. Therefore, a literature survey of reliability of wind turbines, electrical machines and power electronic converters is given. Five different ways of achieving fault tolerance identified in the literature are discussed together with their applicability for wind turbines: (1) converters with redundant semiconductors, (2) fault tolerant converter topologies, (3) fault tolerance by increasing the number of phases, (4) fault tolerance of switched reluctance machines, and (5) design for fault tolerance of PM machines and converters. Because converters fail more often than machines, it makes sense to use of fault tolerant converter topologies. Increasing the number of phases is a useful form of fault tolerance because it can be achieved without increasing the cost significantly.",2009,0,
78,79,Fault recovery in linear systems via intrinsic evolution,"We investigate fault recovery using reconfiguration for analog linear feedback control systems. We assume any faults occur only within the linear system and accessibility to its internal circuitry is impossible. Consequently, the only way to restore service - even degraded service - is by inserting a compensation network into the control loop. System failures are manifested by a change in the original bandwidth. The compensators are evolved intrinsically.",2004,0,
79,80,Distance estimation technique for single line-to-ground faults in a radial distribution system,A simple yet powerful algorithm to estimate the distance to a single line-to-ground fault on a distribution feeder is proposed. The algorithm is implemented in a power monitor instrument and the estimation of distance is made within the instrument itself. The algorithm is designed to work where the only available data to the instrument are a single point measurement taken at the substation and the positive and zero-sequence impedance of the primary feeder. The single point measurement consists of three-phase voltage and current waveforms. Network topology data are not available to the algorithm. The new technique accommodates computational power and data constraints while maintaining adequate accuracy of the measurements,2000,0,
80,81,Optimal Wavelet Design for Multicarrier Modulation with Time Synchronization Error,"Wavelet packet based multi-carrier modulation (WPMCM) is an efficient transmission technique which has the advantage of being a generic scheme whose characteristics can be customized to fulfill a design specification. However, WPMCM is sensitive and vulnerable to time synchronization errors because its symbols overlap. In this paper, we design new wavelets to alleviate WPMCM's vulnerability to timing errors. First, a filter design framework that facilitates the development of new wavelet bases is built. Then the expressions for errors due to time offset in WPMCM transmission are derived and stated as a convex optimization problem. Finally, an optimal filter that best handles these deleterious effects is designed by means of semi definite programming (SDP). Through computer simulations the performance advantages of the newly designed filter over standard wavelet filters are proven.",2009,0,
81,82,Optimum design of a class of fault tolerant isotropic Gough-Stewart platforms,"Optimal geometric design is of key importance to the performance of a manipulator. First, this paper extends the work in Y. Yi, et al., (2004) to generate a class of isotropic Gough-Stewart platforms (GSPs) with an odd number of struts. Then, it develops methods for finding a highly fault tolerant GSP from that class. Two optimization criteria are considered, isotropy and fault tolerance. To meet the mission critical needs imposed by laser weapons applications, nine-strut isotropic GSPs that retain kinematic stability despite the loss of any three struts are found. First, we develop methods for generating a five parameter class of isotropic nine-strut GSPs. Next, new measures of fault tolerance are introduced and used to optimize the free parameter space. The optimized design is much more fault tolerant than the GSP currently baselined for the airborne laser.",2004,0,
82,83,Designing Fault Tolerant Web Services Using BPEL,"The Web services technology provides an approach for developing distributed applications by using simple and well defined interfaces. Due to the flexibility of this architecture, it is possible to compose business processes integrating services from different domains. This paper presents an approach, which uses the specification of services orchestration, in order to create a fault tolerant model combining active and passive replication technique. This model support fault of crash. The characteristics and the results obtained by implementing this model are described along this paper.",2008,0,
83,84,Sub-cycle detection of incipient cable splice faults to prevent cable damage,"This paper presents an innovative method for subcycle detection of incipient cable failures caused by self-clearing faults occurring in cable splices due to insulation breakdown. Because of their short duration, conventional overcurrent protection will not detect these types of faults. The protection scheme described in this paper has been integrated into a universal relay platform. It is fast enough to operate for sub-cycle faults and has the logic to differentiate them from other types of faults. Imminent cable failure can be detected",2000,0,
84,85,"Software-Based Online Detection of Hardware Defects Mechanisms, Architectural Support, and Evaluation","As silicon process technology scales deeper into the nanometer regime, hardware defects are becoming more common. Such defects are bound to hinder the correct operation of future processor systems, unless new online techniques become available to detect and to tolerate them while preserving the integrity of software applications running on the system. This paper proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extension (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible: testing techniques can be modified/upgraded in the field to trade off performance with reliability without requiring any change to the hardware. We evaluated our technique on a commercial chip-multiprocessor based on Sun's Niagara and found that it can provide very high coverage, with 99.22% of all silicon defects detected. Moreover, our results show that the average performance overhead of software-based testing is only 5.5%. Based on a detailed RTL-level implementation of our technique, we find its area overhead to be quite modest, with only a 5.8% increase in total chip area.",2007,0,
85,86,Optimizing Joint Erasure- and Error-Correction Coding for Wireless Packet Transmissions,"To achieve reliable packet transmission over a wireless link without feedback, we propose a layered coding approach that uses error-correction coding within each packet and erasure-correction coding across the packets. This layered approach is also applicable to an end-to-end data transport over a network where a wireless link is the performance bottleneck. We investigate how to optimally combine the strengths of error- and erasure-correction coding to optimize the system performance with a given resource constraint, or to maximize the resource utilization efficiency subject to a prescribed performance. Our results determine the optimum tradeoff in splitting redundancy between error-correction coding and erasure-correction codes, which depends on the fading statistics and the average signal to noise ratio (SNR) of the wireless channel. For severe fading channels, such as Rayleigh fading channels, the tradeoff leans towards more redundancy on erasure-correction coding across packets, and less so on error-correction coding within each packet. For channels with better fading conditions, more redundancy can be spent on error-correction coding. The analysis has been extended to a limiting case with a large number of packets, and a scenario where only discrete rates are available via a finite number of transmission modes.",2008,0,
86,87,Modeling transformers with internal incipient faults,"Incipient fault detection in transformers can provide early warning of electrical failure and could prevent catastrophic losses. To develop transformer incipient fault detection technique, a transformer model to simulate internal incipient faults is required. This paper presents a methodology to model internal incipient winding faults in distribution transformers. These models were implemented by combining deteriorating insulation models with an internal short circuit fault model. The internal short circuit fault model was developed using finite element analysis. The deteriorating insulation model, including an aging model and an arcing model connected in parallel, was developed based on the physical behavior of aging insulation and the arcing phenomena occurring when the insulation was severely damaged. The characteristic of the incipient faults from the simulation were compared with those from some potential experimental incipient fault cases. The comparison showed the experimentally obtained characteristic's of terminal behavior of the faulted transformer were similar to the simulation results from the incipient fault models",2002,0,
87,88,An Efficient Technique for Error-Free Implementation of H.264 Using Algebraic Integer Encoding,"Video coding technology plays a key role in various multimedia applications. H.264 is the newest video coding standard and has achieved a significant improvement in coding efficiency. The 4*4 integer transform, as one of the key techniques in H.264 video compression standard, is very important for the whole performance of H.264 codec. In this paper we propose a novel algorithm for fast and error-free (infinite-precision) implementation of H.264 based on algebraic integer-encoding scheme. The proposed algorithm has regular structure. Simulation results show that this algorithm will result in reduction of computation complexity while enhancing the quality of obtained image simultaneously. Determining the quality of an image is an open problem that is highly dependent on the specific application that this image will be used for. We propose new measuring quantities for image quality.",2010,0,
88,89,Detection of Rotor Faults in Squirrel-Cage Induction Motors using Adjustable Speed Drives,"The need for detection of rotor faults at an earlier stage, so that maintenance can be planned ahead, has pushed the development of monitoring methods with increasing sensitivity and noise immunity. Addressing diagnostic techniques based on current signatures analysis (MCSA), the characteristic components introduced by specific faults in the current spectrum are investigated and a diagnosis procedure correlate the amplitudes of such components to the fault extent. In this paper, the impact of feedback control on asymmetric rotor cage induction machine behavior is analyzed. It is shown that the variables usually employed in diagnosis procedures assuming open-loop operation are no longer effective under closed-loop operation. Simulation results show that signals already present at the drive are suitable to effective diagnostic procedure. The utilization of the current regulator error signals and the influence of the regulators gains on their utilization in rotor failure detection are the aim of the present work. The use of a band-pass filter bank to detect the presence of sidebands is also proposed in the paper",2006,0,
89,90,FPGA Implementation of Wideband IQ Imbalance Correction in OFDM Receivers,"This paper describes the implementation of a digital compensation scheme, called CSAD, for correcting the effects of wideband gain and phase imbalances in dual-branch OFDM receivers. The proposed scheme is implemented on a Xilinx Virtex-4 field programmable gate array (FPGA). The flexible architecture of the implementation makes it readily adaptable for different broadband applications, such as DVB-T/H, WLAN, and WiMAX. The proposed correction scheme is resilient against multipath fading and frequency offset. When applied to DVB-T, it is shown that an 11-bit arithmetic precision is sufficient to achieve the required BER of 2x10<sup>-4</sup> at an SNR of 16.5 dB. Using this bit-precision, the implementation consumes 1686 Virtex-4 slices equivalent to about 42600 gates.",2008,0,
90,91,Error prediction for multi-classification,"This paper describes an error prediction mechanism for multiclassification systems. First, a multiclassification system is constructed by combining a suite of two-class classifiers. While training, each sub-classifier does not utilize all the training data and the remaining data are used for testing purpose. Thus, the classification system can predict its own performance after training. We have tested this mechanism on several well-known benchmark datasets. Experimental results are demonstrated for its effectiveness.",2005,0,
91,92,Automatic detection and correction of purple fringing using the gradient information and desaturation,"This paper proposes a method to automatically detect and correct purple fringing that is one of the color artifacts due to characteristics of charge coupled device sensors in a digital camera. The proposed method consists of two steps. In the first step, we detect purple fringed regions that satisfy specific properties: hue characteristics around highlight regions with large gradient magnitudes. In the second step, color correction of the purple fringed regions is made by desaturating the pixels in the detected regions. The proposed method is able to detect purple fringe artifacts more precisely than Kang's method. It can be used as a post processing in a digital camera.",2008,0,
92,93,SOA-Based Alarm Integration for Multi-Technology Network Fault Management,"In order for the service provider to offer better quality of telecom services to customers, one of the possible ways is to monitor and control all kinds of deployed network resources, which are used to support the operation of these services, and to proactively analyze and recover any trouble reported from the network resources. In this study, two system integration scenarios along with the associated interface specifications for multi-technology resource alarm notification and retrieval services have been described. An NGOSS-based development methodology was followed, and a number of useful commercials tools were introduced to facilitate the evolution and transformation of legacy BSS/OSS, so that these BSS/OSS are able to support loosely-coupled interoperability by using standards-based interfaces based on the service-oriented architecture. The functionalities of multi-technology network fault management were realized by means of JMS and Web Service techniques. The implementation described in this paper shows the feasibility of the proposed development methodology.",2008,0,
93,94,Fault Diagnosis of Bearings in Rotating Machinery Based on Vibration Power Signal Autocorrelation,"Since fault in a great number of bearings commences from a single point defect, research on this category of faults has shared a great deal in predictive diagnosis literature. Single point defects will cause certain characteristic fault frequencies to appear in machine vibration spectrum. In traditional methods, data extracted from frequency spectrum has been used to identify damaged bearing part. Because of impulsive nature of fault strikes, and complex modulations present in vibration signal, a simple spectrum analysis may result in erroneous conclusions. When a shaft rotates at constant speed, strikes due to a single point defect repeat at constant intervals. Each strike shows a high energy distribution around it. This paper considers the time intervals between successive impulses in auto-correlated vibration power signals. The most frequent interval between successive impulses determines the period of defective part. This period is related to fault frequency and therefore shows the defective part. A comparison of results extracted from the traditional and the proposed methods shows the efficiency improvement of the second method in respect of the first one",2006,0,
94,95,A Distributed Fault-Tolerant Algorithm for Event Detection Using Heterogeneous Wireless Sensor Networks,"Distributed event detection using wireless sensor networks has received growing interest in recent years. In such applications, a large number of inexpensive and unreliable sensor nodes are distributed in a geographical region to make firm and accurate local decisions about the presence or absence of specific events based on their sensor readings. However, sensor readings can be unreliable, due to either noise in the sensor readings or hardware failures in the devices, and may cause nodes to make erroneous local decisions. We present a general fault-tolerant event detection scheme that allows nodes to detect erroneous local decisions based on the local decisions reported by their neighbors. This detection scheme does not assume homogeneity of sensor nodes and can handle cases where nodes have different accuracy levels. We prove analytically that the derived fault-tolerant estimator is optimal under the maximum a posteriori (MAP) criterion. An equivalent weighted voting scheme is also derived. Further, we describe two new error models that take into account the neighbor distance and the geographical distributions of the two decision quorums. These models are particularly suitable for detection applications where the event under consideration is highly localized. Our fault-tolerant estimator is simulated using a network of 1024 nodes deployed randomly in a square region and assigned random probability of failures",2006,0,
95,96,Diagnosis of rotor faults in brushless DC (BLDC) motors operating under non-stationary conditions using windowed Fourier ridges,"There are several applications where the motor is operating in continuous non-stationary operating conditions. Actuators in the aerospace and transportation industries are examples of this kind of operation. Diagnostics of faults in such applications is, however, challenging. A novel method using windowed Fourier ridges is proposed in this paper for the detection of rotor faults in BLDC motors operating under continuous non-stationarity. Experimental results are presented to validate the concept and depict the ability of the proposed algorithm to track and identify rotor faults. The proposed algorithm is simple and can be implemented in real-time without much computational burden.",2005,0,
96,97,Supervision and fault management of process-tasks and terminology,"The supervision of technical processes is aimed at showing the present state, indicating undesired or unpermitted states, and taking appropriate actions to avoid damage or accidents. The deviations from normal process behavior result from faults and errors, which can be attributed to many causes. They may result in some shorter or longer time periods with malfunctions or failures if no counteractions are taken. One reason for supervision is to avoid these malfunctions or failures. In the following article the basic tasks of supervision are shortly described.",2007,0,
97,98,An industrial case study of implementing and validating defect classification for process improvement and quality management,"Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements",2005,0,
98,99,Evaluation of soft-bit error sequence generators at the output of the decoding process,"Soft decision decoding algorithms are widely used in modern wireless systems; convolutional and turbo codes are usually adopted as the inner scheme thanks to their capability to correct symbol errors at low SNR values. Such algorithms can achieve high coding gains using soft decoding, and modern digital hardware technology enables efficient and low cost practical implementations. We apply the experience gained in previous work, concerning the simulation of bit error processes (Costamagna, E. et al., Proc. IEEE, vol.90 p.842-59, 2002), to implement soft-bit generative models based on hidden Markov chains and chaotic attractors. Both the input and the output of the demodulation process of a GSM-GPRS and a 3GPP UMTS transceiver are observed, developing our earlier analysis (Costamagna et al., IEEE 58th VTC, 2003), and the quality of the soft-bit sequences generated for the input is evaluated comparing the sequences obtained at the output of the demodulator when simulated or target sequences are supplied at the input. Moreover, the deep significance of some statistical features exhibited by the sequences in order to describe their error burst behavior is briefly discussed.",2004,0,
99,100,Procedure call duplication: minimization of energy consumption with constrained error detection latency,"This paper presents a new software technique for detecting transient hardware errors. The objective is to guarantee data integrity in the presence of transient errors and to minimize energy consumption at the same time. Basically, we duplicate computations and compare their results to detect errors. There are three choices for duplicate computations: (1) duplicating every statement in the program and comparing their results, (2) re-executing procedures with duplicated procedure calls and comparing the results, (3) re-executing the whole program and comparing the final results. Our technique is the combination of (1) and (2): Given a program, our technique analyzes procedure call behavior of the program and determines which procedures should have duplicated statements (choice (1)) and which procedure calls should be duplicated (choice (2)) to minimize energy consumption while controlling error detection latency constraints. Then, our technique transforms the original program into the program that is able to detect errors with reduced energy consumption by re-executing the statements or procedures. In benchmark program simulation, we found that our technique saves over 25% of the required energy on average compared to previous techniques that do not take energy consumption into consideration",2001,0,
100,101,The complexity of adding failsafe fault-tolerance,"In this paper, we focus our attention on the problem of automating the addition of failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. And, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of adding failsafe fault-tolerance in distributed programs is NP-hard. Towards this end, we reduce the 3-SAT problem to the problem of adding failsafe fault-tolerance. We also identify a class of specifications, monotonic specifications and a class of programs, monotonic programs. Given a (positive) monotonic specification and a (negative) monotonic program, we show that failsafe fault-tolerance can be added in polynomial time. We note that the monotonicity restrictions are met for commonly encountered problems such as Byzantine agreement, distributed consensus, and atomic commitment. Finally, we argue that the restrictions on the specifications and programs are necessary to add failsafe fault-tolerance in polynomial time; we prove that if only one of these conditions is satisfied, the addition of failsafe fault-tolerance is still NP-hard.",2002,0,
101,102,Comparisons of error control techniques for wireless video multicasting,"This paper explores three different methods, employed separately and in combination, to improve the quality of video delivery on wireless local area networks. The approaches are: leader-driven multicast (LDM), monitoring MAC layer unicast (re)transmissions by other receivers; application-level forward error correction (FEC) using block erasure codes; negative feedback from selected receivers in the form of extra parity requests (EPR). The performance of these three methods is evaluated using both experiments on a mobile computing testbed and simulation. The results indicate that, while LDM is helpful in improving the raw packet reception rate, the combination of FEC and EPR is most effective in improving the frame delivery rate",2002,0,
102,103,Research of remote fault diagnostic system based on Grid,"So far, methods of fault diagnosis have been numerous. But as the complexity of modern equipment, the variability of fault, as well as trends in global manufacturing, it is difficult for any separate organization of independent to complete all the work of fault diagnosis. This article reviewed the equipment fault diagnostic technology in the course of development, in particular, remote fault diagnostic system based on Internet technology, and then on this basis, this paper proposed a new fault diagnostic method, that is grid-based remote fault diagnosis, which integrates the resources of related organization to work together. The architecture of this diagnostic system is proposed, and also the work flow of this system is described.",2009,0,
103,104,Finding Faults: Manual Testing vs. Random+ Testing vs. User Reports,"The usual way to compare testing strategies, whether theoretically or empirically, is to compare the number of faults they detect. To ascertain definitely that a testing strategy is better than another, this is a rather coarse criterion: shouldn't the nature of faults matter as well as their number? The empirical study reported here confirms this conjecture. An analysis of faults detected in Eiffel libraries through three different techniques-random tests, manual tests, and user incident reports-shows that each is good at uncovering significantly different kinds of faults. None of the techniques subsumes any of the others, but each brings distinct contributions.",2008,0,
104,105,Adaptive Fault Management of Parallel Applications for High-Performance Computing,"As the scale of high-performance computing (HPC) continues to grow, failure resilience of parallel applications becomes crucial. In this paper, we present FT-Pro, an adaptive fault management approach that combines proactive migration with reactive checkpointing. It aims to enable parallel applications to avoid anticipated failures via preventive migration and, in the case of unforeseeable failures, to minimize their impact through selective checkpointing. An adaptation manager is designed to make runtime decisions in response to failure prediction. Extensive experiments, by means of stochastic modeling and case studies with real applications, indicate that FT-Pro outperforms periodic checkpointing, in terms of reducing application completion times and improving resource utilization, by up to 43 percent.",2008,0,
105,106,Fault-accommodating thruster force allocation of an AUV considering thruster redundancy and saturation,"A new approach to the fault-accommodating allocation of thruster forces of an autonomous underwater vehicle (AUV) is investigated in this paper. This paper presents a framework that exploits the excess number of thrusters to accommodate thruster faults during operation. First, a redundancy resolution scheme is presented that considers the presence of an excess number of thrusters along with any thruster faults and determines the reference thruster forces to produce the desired motion. This framework is then extended to incorporate a dynamic state feedback technique to generate reference thruster forces that are within the saturation limit of each thruster. Results from both computer simulations and experiments are provided to demonstrate the viability of the proposed scheme",2002,0,
106,107,Designing Real-Time and Fault-Tolerant Middleware for Automotive Software,"Automotive software development poses a great deal of challenges to automotive manufacturers since an automobile is inherently distributed and subject to fault-tolerance and real-time requirements. Middleware is a software layer that can handle the intrinsic complexities of distributed systems and arises as an indispensable run-time platform for automotive systems. This paper explains the concept of middleware by enumerating its functions and categorizes middleware according to adopted communication models. It also extracts five essential requirements of automotive middleware and proposes a middleware design for automotive systems based on the message-oriented middleware (MOM) structure. The proposed middleware effectively addresses the derived requirements and includes many essential features such as real-time guarantee, fault-tolerance, and a global time base",2006,0,
107,108,Design of Energy-Efficient High-Speed Links via Forward Error Correction,"In this brief, we show that forward error correction (FEC) can reduce power in high-speed serial links. This is achieved by trading off the FEC coding gain with specifications on transmit swing, analog-to-digital converter (ADC) precision, jitter tolerance, receive amplification, and by enabling higher signal constellations. For a 20-in FR4 link carrying 10-Gb/s data, we demonstrate: 1) an 18-mW/Gb/s savings in the ADC; 2) a 1-mW/Gb/s reduction in transmit driver power; 3) up to 6?? improvement in transmit jitter tolerance; and 4) a 25- to 40-mV improvement in comparator offset tolerance with 3?? smaller swing.",2010,0,
108,109,Fault classification for distance protection,This paper presents an overview of power system fault classification methods and challenges. It also contains some ideas about structured testing.,2002,0,
109,110,On-line Fault Diagnosis Model of the Hydropower Units Based on MAS,"The paper introduced a novel on-line fault diagnosis system model of the hydropower units based on multi-agent system. In allusion to the classical MAS-based fault diagnosis model, it proposes a new function of information interactive between the mission-controlled subsystem and the task decomposition subsystem to increase the transmission rate of control signals and designs the status-monitoring subsystem to detect the abnormal signals directly from local to increase the fault diagnostic sensitivity. In the fault-diagnosis subsystem, a multi-agent interactive parallel structure is designed to meet the requirements of the high reliability and good real-time. A Java-based language so called as JAFMAS is used to build a multi-agent cooperation platform. Experimental results show the effectiveness and feasibility of the proposed method.",2009,0,
110,111,Non-differential protection of a generator's stator utilizing fault transients,"This paper presents a novel protection scheme for detecting faults on the stator of a generator unit which is directly connected to a distribution system. In the scheme, a multi-channel fault transient detection unit, using the outputs of the current transformers (CTs) at the output of the generator terminal, is employed to extract the fault-generated transient current signals. The detector unit is tuned to extract two bands of fault generated transient signals with different center frequencies. A spectral comparison technique is applied to firstly compute the spectral energies of the two band signals, and then the fault diagnosis determines whether it is an internal and external fault by comparing the ratio of the two signals with a predefined threshold. The scheme offers advantages of immunity to CT saturation, and is capable of detecting both low level and interturn faults. In addition, the protection scheme is also simple in application, and is cost-effective in that it only requires one set of CTs. Simulation studies show that the proposed technique can give correct responses for various fault conditions",2001,0,
111,112,Towards fault tolerance pervasive computing,"Pervasive computing exists in the user's environment, the technology is sustainable if it is invisible to the user and does not intrude the user's consciousness. This requires that functioning of the multitude of devices in the environment be oblivious to the user. Therefore, the system has to be resilient to various kinds of faults and should be able to function despite faults. In addition, pervasive computing provides a platform for context-aware computing that enables automatic configuration of a pervasive system based on the environment context. The aim of this article is to highlight the various challenges and issues that confront fault tolerance pervasive computing, discuss their implications, prevent some solutions to these problems, and describe how some of these solutions are implemented in our system.",2005,0,
112,113,Globally optimal uneven error-protected packetization of scalable code streams,"In this paper, we present a family of new algorithms for rate-fidelity optimal packetization of scalable source bit streams with uneven error protection. In the most general setting where no assumption is made on the probability function of packet loss or on the rate-fidelity function of the scalable code stream, one of our algorithms can find the globally optimal solution to the problem in O(N<sup>2</sup>L<sup>2</sup>) time, compared to a previously obtained O(N<sup>3</sup>L<sup>2</sup>) complexity, where N is the number of packets and L is the packet payload size. If the rate-fidelity function of the input is convex, the time complexity can be reduced to O(NL<sup>2</sup>) for a class of erasure channels, including channels for which the probability function of losing n packets is monotonically decreasing in n and independent erasure channels with packet erasure rate no larger than N/2(N + 1). Furthermore, our O(NL<sup>2</sup>) algorithm for the convex case can be modified to rind an approximation solution for the general case. All of our algorithms do away with the expediency of fractional bit allocation, a limitation of some existing algorithms.",2004,0,
113,114,A Fault Recovery Approach in Fault-Tolerant Processor,"A fault recovery scheme of a fault-tolerant processor for embedded systems is introduced in this paper. The microarchitecture of the fault-tolerant processor called RSED is modified from superscalar processor architecture. The fault-tolerant mechanism of RSED is implemented mainly using temporal redundancy technique. Fault recovery scheme is an important part of the fault-tolerant mechanism. In order to resolve the problem of possible single point of failures, a novel TMR approach is adopted to generate re-execution instruction address. Compared with similar works, the fault recovery scheme proposed can recover processor execution more reliably.",2009,0,
114,115,Reaching efficient fault-tolerance for cooperative applications,"Cooperative applications are widely used, e.g. as parallel calculations or distributed information processing systems. Whereby such applications meet the users demand and offer a performance improvement, the susceptibility to faults of any used computer node is raised. Often a single fault may cause a complete application failure. On the other hand, the redundancy in distributed systems can be utilized for fast fault detection and recovery. So, we followed an approach that is based an duplication of each application process to detect crashes and faulty functions of single computer nodes. We concentrate on two aspects of efficient fault-tolerance-fast fault detection and recovery without delaying the application progress significantly. The contribution of this work is first a new fault detecting protocol for duplicated processes. Secondly, we enhance a roll forward recovery scheme so that it is applicable to a set of cooperative processes in conformity to the protocol",2000,0,
115,116,Straight-Edge Extraction in Distorted Images Using Gradient Correction,"Many camera lenses, particularly low-cost or wide-angle lenses, can cause significant image distortion. This means that features extracted naively from such images will be incorrect. A traditional approach to dealing with this problem is to digitally rectify the image to correct the distortion, and then to apply computer vision processing to the corrected image. However, this is relatively expensive computationally, and can introduce additional interpolation errors. We propose instead to apply processing directly to the distorted image from the camera, modifying whatever algorithm is used to correct for the distortion during processing, without a separate rectification pass. In this paper we demonstrate the effectiveness of this approach using the particular classic problem of gradient-based extraction of straight edges. We propose a modification of the Burns line extractor that works on a distorted image by correcting the gradients on the fly using the chain rule, and correcting the pixel positions during the line-fitting stage. Experimental results on both real and synthetic images under varying distortion and noise show that our gradient-correction technique can obtain approximately a 50% reduction in computation time for straight-edge extraction, with a modest improvement in accuracy under most conditions.",2009,0,
116,117,Automatic Spelling Correction Rule Extraction and Application for Spoken-Style Korean Text,"Nowadays, spoken-style text is prevailing because lots of information are being written in spoken-style such as Short-Message-Service(SMS) messages. However, the spokenstyle text contains more spelling errors than the traditional written-style text. In this paper, we propose a rule-based spelling correction system which can automatically extract spelling correction rules from the correction corpus and apply extracted rules to spelling errors of input sentences. In order to preserve both high precision and high recall, we devise a candidate-elimination algorithm which determines appropriate context size of spelling correction rules based on rule accuracy. Experimental results showed that the proposed system can extract 42,537 spelling correction rules and apply the rules to correct spelling errors on the test corpus and thus, the rate of precision is increased from 31.08% to 79.04% on the basis of message unit.",2007,0,
117,118,Value-based scheduling of distributed fault-tolerant real-time systems with soft and hard timing constraints,We present an approach for scheduling of fault-tolerant embedded applications composed of soft and hard real-time processes running on distributed embedded systems. The hard processes are critical and must always complete on time. A soft process can complete after its deadline and its completion time is associated with a value function that characterizes its contribution to the quality-of-service of the application. We propose a quasi-static scheduling algorithm to generate a tree of fault-tolerant distributed schedules that maximize the application's quality value and guarantee hard deadlines.,2010,0,
118,119,Trend analysis techniques for incipient fault prediction,"This paper extends the application of the Laplace Test Statistic for trend analysis and prediction of incipient faults for power systems. The extensions proposed in this paper consider the situation where two parameters believed to contribute explicitly to the eventual failure are monitored. The developed extensions applied to actual incipient failure events provide promising results for prediction of the impending failure. It is demonstrated that by incorporating two parameters in the trend analysis, the robustness to outliers is increased and the flexibility is augmented by increasing the degrees of freedom in the generation of the alarm signal.",2009,0,
119,120,Notice of Retraction<BR>Comprehensive Evaluation of Certain Power Vehicle Fault Based on Rough Sets,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>With the advent of intelligence, people put forward higher request to the fault diagnosis of weapon. As the base of certain weapon system, the power vehicle can work well or not is directly relate to the tasks of the battle and training can complete smoothly or not. Firstly, the paper introduces the fundamental conception of Rough Sets and gives the method of ascertaining the evaluation value and the weights. Secondly, based on the operation characteristics of certain power vehicle, the paper analyses and creates an index system of the power vehicle, then RS is used to impersonally distribute weights. Finally, the comprehensive evaluation model based RS is given out, and the fault sequence and evaluating grade are educed. The computing results show this practical model is very significant to the weapon maintenance and offers a reference for the evaluation of civil equipment fault.",2009,0,
120,121,Fisher Discriminance of Fault Predict for Decision-Making Systems,"A new technology of fault prediction was presented based on the neural network and Fisher discriminance in statistics. First, many enough character of running situation of decision-making were extracted from the real-time observation data. Secondly, the FP software systems were designed and the algorithm of FP of decision-making systems was presented. Finally, a simply example indicated that the algorithm is effectively.",2009,0,
121,122,FAIL-MPI: How Fault-Tolerant Is Fault-Tolerant MPI?,"One of the topics of paramount importance in the development of cluster and grid middleware is the impact of faults since their occurrence in grid infrastructures and in large-scale distributed systems is common. MPI (message passing interface) is a popular abstraction for programming distributed and parallel applications. FAIL (FAult Injection Language) is an abstract language for fault occurrence description capable of expressing complex and realistic fault scenarios. In this paper, we investigate the possibility of using FAIL to inject faults in a fault-tolerant MPI implementation. Our middleware, FAIL-MPI, is used to carry quantitative and qualitative faults and stress testing",2006,0,
122,123,An Integrated Silicon Carbide (SiC) Based Single Phase Rectifier with Power Factor Correction,"Silicon carbide (SiC) based power devices exhibit superior properties such as very low switching losses, fast switching behavior, improved reliability and high temperature operation capabilities. These properties contribute toward the ability to increase switching frequency, decrease the size of passive components and switches, and reduce the need for cooling, thus making the devices an excellent candidate for AC/DC power supplies. In this paper a SiC based integrated single phase rectifier with power factor correction (PFC) is presented. The proposed topology has many advantages including fewer semiconductor components; the presence of AC side inductor resulting in reduced EMI interference, and higher performance. This approach takes advantage of the superior properties of SiC devices and the reduced number of devices in the proposed converter to achieve higher efficiency, smaller size and better performance at high temperature. A performance and efficiency evaluation of the rectifier is presented and the results are compared with benchmark Si solutions",2005,0,
123,124,Neutron Soft Errors in Xilinx FPGAs at Lawrence Berkeley National Laboratory,The Lawrence Berkeley National Laboratory cyclotron offers broad-spectrum neutrons for single event effects testing. We discuss results from this beamline for neutron soft upsets in Xilinx Virtex-4 and -5 field-programmable-gate-array (FPGA) devices.,2008,0,
124,125,TIP-OPC: a new topological invariant paradigm for pixel based optical proximity correction,"As the 193 nm lithography is likely to be used for 45 nm and even 32 nm processes, much more stringent requirement will be posed on optical proximity correction (OPC) technologies. Currently, there are two OPC approaches - the model-based OPC (MB-OPC) and the inverse lithography technology (ILT). MB-OPC generates masks which is less complex compared with ILT. But ILT produces much better results than MB-OPC in terms of contour fidelity because ILT is a pixel based method. Observing that MB-OPC preserves the mask shape topologies which leads to a lower mask complexity, we combine the strengths of both methods - the topology invariant property and the pixel based mask representation. To the best of our knowledge, it is the first time that this topological invariant pixel based OPC (TIP-OPC) paradigm is proposed, which fills the critical hole of the OPC landscape and potentially has many new applications. Our technical novelty includes the lithography friendly mask topological invariant operations, the efficient fast Fourier transform based cost function sensitivity computation and the TIP-OPC algorithm. The experimental results show that TIP-OPC can achieve much better post OPC contours compared with MB-OPC while maintaining the mask shape topologies.",2007,0,
125,126,A New Iterative Approach to the Corrective Security-Constrained Optimal Power Flow Problem,"This paper deals with techniques to solve the corrective security-constrained optimal power flow (CSCOPF) problem. To this end, we propose a new iterative approach that comprises four modules: a CSCOPF which considers only a subset of potentially binding contingencies among the postulated contingencies, a (steady-state) security analysis (SSSA), a contingency filtering (CF) technique, and an OPF variant to check post-contingency state feasibility when taking into account post-contingency corrective actions. We compare performances of our approach and its possible variants with classical CSCOPF approaches such as the direct approach and Benders decomposition (BD), on three systems of 60, 118, and 1203 buses.",2008,0,
126,127,Systems Reliability Analysis and Fault Diagnosis Based on Bayesian Networks,"This paper presents the application of Bayesian networks(BN) to the reliability analysis and fault diagnosis of systems. For systems, it is essential to do reliability analysis. Also it is necessary to do fault diagnosis when a system failed, but the better way is to do fault diagnosis before the system has failed. Bayesian networks have many special characteristics, one of them is that they are parallel structures, so the time of solving is much shorter than that of many common methods. Bayesian networks permit not only computing the reliability indices of a system but also presenting the effect of each component or some components on the system reliability to distinguish the unsubstantial part of the system, so we can know which part is the weakest one of the system. Two examples proved the validity and superiority of the method in the application of the reliability analysis and fault diagnosis of system.",2009,0,
127,128,Prediction Error Prioritizing Strategy for Fast Normalized Partial Distortion Motion Estimation Algorithm,"A prediction error prioritizing-based normalized partial distortion search algorithm for fast motion estimation is proposed in this letter. The distortion behavior of each pixel in a macroblock is first analyzed to point out the priority/order of sum of absolute difference calculation. Afterward, the normalized partial distortion search algorithm is applied for half-stop of the distortion calculation. In addition, a dynamic search range decision algorithm is adopted for automatically changing the size of the search range to further increase the motion estimation speed. The computational complexity can be reduced significantly through the proposed algorithm, though leaving a PSNR degradation that could be dismissed.",2010,0,
128,129,Fault-Tolerant Policy for Optical Network Based Distributed Computing System,"The optical network based distributed computing system has been thought as a promising technology to support large-scale data-intensive distributed applications. For such a system with so many heterogeneous resources and middlewares involved, faults seem to be inevitable. However, for those applications that need to be finished before the given deadline, a fault in the system will lead to the failure of the application. Therefore, fault-tolerant policy is necessary to improve the performance of the system when faults could happen. In this paper, we address to the fault-tolerant problem for the optical network based distributed computing system. We first propose an overlay approach which applies the existing fault-tolerant policies for distributed computing and optical network. Then we present a joint fault-tolerant policy which takes into account the fault tolerance for computing resource and network resource in the same time. We compare the performances of different polices by simulation. The simulation results show that the joint fault-tolerant policy achieves much better performances compared to overlay approaches.",2008,0,
129,130,Silicon Wafer Defect Extraction Based on Morphological Filter and Watershed Algorithm,"Defect extraction techniques are studied regarding the silicon wafer surface defect. We design a new filter based on multiple structuring elements and suggest an improved marker-based and region merging watershed. To begin with, the filter which generalized close-opening and open-closing filter based on the morphological filter with multiple structuring elements is introduced to eliminate the noise and simplify the image and morphological gradient image while preserving the details. And then in order to reduce the over-segmentation of the watershed algorithm, this paper suggests an improved marker-based and region merging method, region average gray value and edge strength criterion is used in merging operation and has a good effect on segmentation. Finally, the improved watershed algorithm is applied to the filtered gradient image to get the defect contours. The experiments show that this method can eliminate the noises and extract accurately location and closed region contours, which lays a good foundation for defect feature extraction and selection.",2008,0,
130,131,Diagnostic fault detection & intelligent reconfiguration of fuel delivery systems,"The reliable operation of an engines fuel delivery system is fundamental. A failure in the fuel system that impacts the ability to deliver fuel to the engine will have an immediate effect on system performance and safety. There are very few diagnostic systems that monitor the health of the fuel system and even fewer that can accommodate for detected faults. Current diagnostic techniques call for careful maintenance of fuel system components. These techniques tend to be backward thinking in that they are based on previous experience which is not always a good indicator for future systems. This paper describes a technique developed at the Penn State Applied Research Laboratories Condition Based Maintenance Department for fault detection and reconfiguration for fuel delivery system components. This technique has been applied to a diesel engine test rig. The test rig is fully instrumented with sensors including those for fuel pressure. Even though this technique is being applied on a diesel engine, the approach is fully compatible to any fuel delivery system",2005,0,
131,132,Evaluation of security and fault tolerance in mobile agents,"The reliable execution of a mobile agent is a very important design issue in building a mobile agent system and many fault-tolerant schemes have been proposed so far. Security is a major problem of mobile agent systems, especially when money transactions are concerned . Security for the partners involved is handled by encryption methods based on a public key authentication mechanism and by secret key encryption of the communication. In this paper, we examine qualitatively the security considerations and challenges in application development with the mobile code paradigm. We identify a simple but crucial security requirement for the general acceptance of the mobile code paradigm, and evaluate the current status of mobile code development in meeting this requirement. We find that the mobile agent approach is the most interesting and challenging branch of mobile code in the security context. Therefore, we built a simple agent- based information retrieval application, the Traveling Information Agent system, and discuss the security issues of the system in particulars.",2008,0,
132,133,"Comprehensive Analysis of Performance, Fault-Tolerance and Scalability in Grid Resource Management System","The management of the large scale heterogeneous resources is a critical issue in grid computing. The resource management system (RMS) is an essential component of grids. To ensure the QoS of the upper layer service, it raises high requirement for the performance, fault-tolerance and scalability of RMS. In this paper, we study three typical structures of RMS, including centralized, hierarchical and peer-to-peer structures, and make a comprehensive analysis of performance, fault tolerance and scalability. We put forward the performance, fault tolerance and scalability evaluation metrics of the RMS, and give the mathematical expressions and detailed calculation processes. Besides, we make further discussions on the interactions of the performance, fault-tolerance and scalability, and make a comparison of the RMSs with the three typical structures. We believe that the results of this work will help system architects make informed choices for building the RMS.",2009,0,
133,134,Recent improvements on the specification of transient-fault tolerant VHDL descriptions: a case-study for area overhead analysis,"We present a new approach to design reliable complex circuits with respect to transient faults in memory elements. These circuits are intended to be used in harmful environments like radiation. During the design flow, this methodology is also used to perform an early-estimation of the obtained reliability level. Usually, this reliability estimation step is performed in the laboratory, by means of radiation facilities (particle accelerators). By doing so, the early-estimated reliability level is used to balance the design process into a trade-off between maximum area overhead due to the insertion of redundancy and the minimum reliability required for a given application. This approach is being automated through the development of a CAD tool (FT-PRO). Finally, we present also a case-study of a simple microprocessor used to analyze the FT-PRO performance in terms of the area overhead required to implement the fault-tolerant circuit.",2000,0,
134,135,Error calculation techniques and their application to the Antenna Measurement Facility Comparison within the European Antenna Centre of Excellence,"This paper gives an overview of the ongoing activities under the Antenna Measurement activity of the Antenna Centre of Excellence (ACE) network within the EU 6th framework research program. In particular, in this work an attempt is made to establish a common uncertainty estimation criteria in spherical near field and far field antenna measurement systems. The results from this activity are important instruments to verify the measurements accuracies for antenna measurement ranges as well as to investigate and evaluate possible improvements in measurement set-ups and procedures. These results will be used in the facility comparison campaigns in order to calculate a reference pattern for each of the high accuracy reference antennas (VAST 12, SATIMO SH800 and SATIMO SH2000) measured during the last 4 years by different institutions in Europe and US.",2007,0,
135,136,GNSS pseudorange error density tracking using Dirichlet Process Mixture,"In satellite navigation system, classical localization algorithms assume that the observation noise is white-Gaussian. This assumption is not correct when the signal is reflected on the surrounding obstacles. That leads to a decrease of accuracy and of continuity of service. To enhance the localization performances, a better observation noise density can be use in an adapted filtering process. This article aims to show how the Dirich-let Process Mixture can be employed to track the observation density on-line. This sequential estimation solution is adapted when the noise is non-stationary. The approach will be tested under a simulation scenario with multiple propagation conditions. Then, this density modeling will be used in Rao-Blackwellised Particle Filter.",2010,0,
136,137,On handling dependent evidence and multiple faults in knowledge fusion for engine health management,"Diagnostic architectures that fuse outputs from multiple algorithms are described as knowledge fusion or evidence aggregation. Knowledge fusion using a statistical framework such as Dempster-Shafer (D-S) has been used in the context of engine health management. Fundamental assumptions made by this approach include the notion of independent evidence and single fault. In most real world systems, these assumptions are rarely satisfied. Relaxing the single fault assumption in D-S based knowledge fusion involves working with a hyper-power set of the frame of discernment. Computational complexity limits the practical use of such extension. In this paper, we introduce the notion of mutually exclusive diagnostic subsets. In our approach, elements of the frame of discernment are subsets of faults that cannot be mistaken for each other, rather than failure modes. These subsets are derived using a systematic analysis of connectivity and causal relationship between various components within the system. Specifically, we employ a special form of reachability analysis to derive such subsets. The theory of D-S can be extended to handle dependent evidence for simple and separable belief functions. However, in the real world the conclusions of diagnostic algorithms might not take the form of simple or separable belief functions. In this paper, we present a formal definition of algorithm dependency based on three metrics: the underlying technique an algorithm is using, the sensors it is using, and the feature of the sensor that the algorithm is using. With this formal definition, we partition evidence into highly dependent, weakly dependent and independent evidence. We present examples from a Honeywell auxiliary power unit to illustrate our modified D-S method of evidence aggregation",2006,0,
137,138,Transmission Line Fault Location Using Two-Terminal Data Without Time Synchronization,"This letter presents a new transmission line fault location method that uses current and voltage sinusoidal phasors at both ends, without necessity of data synchronization. The main difference among the classical Johns method resides in the fact that the proposed method is based on magnitude of fault point voltage and does not demand exact phase angles of the acquired signals. Simulated and real case results are presented, showing that the proposed algorithm is robust, accurate, and provides adequate performance. Practical applications confirm that the synchronization is not really necessary, making the method faster and easier to apply than classical methods in many real situations",2007,0,
138,139,Measurement-based frame error model for simulating outdoor Wi-Fi networks,"We present a measurement-based model of the frame error process on a Wi-Fi channel in rural environments. Measures are obtained in controlled conditions, and careful statistical analysis is performed on the data, providing information which the network simulation literature is lacking. Results indicate that most network simulators use a frame loss model that can miss important transmission impairments even at a short distance, particularly when considering antenna radiation pattern anisotropy and multi-rate switching.",2009,0,
139,140,A new design technique for optimum logic filter using matrix type-B error correcting coding,"A brief examination in digital communications gives that the receiver has to decide and distinguish between a number of discrete signals in background noise. For this case an optimum filter is designed and some techniques are developed as f.i. the MAP (Maximum a posteriori probability), the maximum likelihood - ML, the matched filter, the Kalman filter, etc. In this paper we introduce a new design technique, which we called the optimum logic filter (OLF), using sophisticated matrix type-B error correcting coding.",2005,0,
140,141,Incremental fault-tolerant design in an object-oriented setting,"With the increasing emphasis on dependability in complex, distributed systems, it is essential that system development can be done gradually and at different levels of detail. We propose an incremental treatment of faults as a refinement process on object-oriented system specifications. An intolerant system specification is a natural abstraction from which a fault-tolerant system can evolve. With each refinement step a fault and its treatment are introduced, so the fault-tolerance of the system increases during the design process. Different kinds of faults are identified and captured by separate refinement relations according to how the tolerant system relates to abstract properties of the intolerant one in terms of safety, and liveness. The specification language utilized is object-oriented and based upon first-order predicates on communication traces. Fault-tolerance refinement relations are formalized within this framework",2001,0,
141,142,Fault-tolerant control of PMSM drive unit,"Since the Fuel-Cell Vehicle's demonstration in the public transport, its fault diagnosis and fault tolerant control strategy become more and more important. This paper studies on the PMSM drive of FCV and presents a sensorless control algorithm in the fault mode based analytical redundancy. Simulation analysis and experiment verification are presented to compare the control algorithm using Expanded Kalman Filter (EKF) and Phase Locked Loop.",2009,0,
142,143,The Learning with Errors Problem (Invited Survey),"In this survey we describe the Learning with Errors (LWE) problem, discuss its properties, its hardness, and its cryptographic applications.",2010,0,
143,144,Group communication protocols under errors,"Group communication protocols constitute a basic building block for highly dependable distributed applications. Designing and correctly implementing a group communication system (GCS) is a difficult task. While many theoretical algorithms have been formalized and proved for correctness, only few research projects have experimentally assessed the dependability of GCS implementations under complex error scenarios. This paper describes a thorough error-injection experimental campaign conducted on Ensemble, a popular GCS. By employing synthetic benchmark applications, we stress selected components of the GCS $the group membership service, the FIFO-ordered reliable multicast - under various error models, including errors in the memory (text and heap segments) and in the network messages. The data show that about 5-6% of the failures are due to an error escaping Ensemble's error-containment mechanism and manifesting as a fail silence violation. This constitutes an impediment to achieving high dependability, the natural objective of GCSs. Our results are derived for a particular system (Ensemble), and more investigation involving other GCSs is required to generalize the conclusions. Nevertheless, through an accurate analysis of the failure causes and the error propagation patterns, this paper offers insights into the design and the implementation of robust GCSs.",2003,0,
144,145,Extended fault modeling used in the space shuttle PRA,"A probabilistic risk assessment (PRA) has been completed for the space shuttle with NASA sponsorship and involvement. This current space shuttle PRA is an advancement over past PRAs conducted for the space shuttle in the technical approaches utilized and in the direct involvement of the NASA centers and prime contractors. One of the technical advancements is the extended fault modeling techniques used. A significant portion of the data collected by NASA for the space shuttle consists of faults, which are not yet failures but have the potential of becoming failures if not corrected. This fault data consists of leaks, cracks, material anomalies, and debonding faults. Detailed, quantitative fault models were developed for the space shuttle PRA which involved assessing the severity of the fault, detection effectiveness, recurrence control effectiveness, and mission-initiation potential. Each of these attributes was transformed into a quantitative weight to provide a systematic estimate of the probability of the fault becoming a failure in a mission. Using the methodology developed, mission failure probabilities were estimated from collected fault data. The methodology is an application of counter-factual theory and defect modeling which produces consistent estimates of failure rates from fault rates. Software was developed to analyze all the relevant fault data collected for given types of faults in given systems. The software allowed the PRA to be linked to NASA's fault databases. This also allows the PRA to be updated as new fault data is collected. This fault modeling and its implementation with FRAS was an important part of the space shuttle PRA.",2004,0,
145,146,DMTracker: finding bugs in large-scale parallel programs by detecting anomaly in data movements,"While software reliability in large-scale systems becomes increasingly important, debugging in large-scale parallel systems remains a daunting task. This paper proposes an innovative technique to find hard-to-detect software bugs that can cause severe problems such as data corruptions and deadlocks in parallel programs automatically via detecting their abnormal behaviors in data movements. Based on the observation that data movements in parallel programs typically follow certain patterns, our idea is to extract data movement (DM)-based invariants at program runtime and check the violations of these invariants. These violations indicate potential bugs such as data races and memory corruption bugs that manifest themselves in data movements. We have built a tool, called DMTracker, based on the above idea: automatically extract DM-based invariants and detect the violations of them. Our experiments with two real-world bug cases in MVAPICH/MVAPICH2, a popular MPI library, have shown that DMTracker can effectively detect them and report abnormal data movements to help programmers quickly diagnose the root causes of bugs. In addition, DMTracker incurs very low runtime overhead, from 0.9% to 6.0%, in our experiments with High Performance Linpack (HPL) and NAS Parallel Benchmarks (NPB), which indicates that DMTracker can be deployed in production runs.",2007,0,
146,147,Lightweight Fault-Tolerance for Peer-to-Peer Middleware,"We address the problem of providing transparent, lightweight, fault-tolerance mechanisms for generic peer-to-peer middleware systems. The main idea is to use the peer-to-peer overlay to provide for fault-tolerance rather than support it higher up in the middleware architecture, e.g. in the form of services. To evaluate our approach we have implemented a fault-tolerant middleware prototype that uses a hierarchical peer-to-peer overlay in which the leaf peers connect to sensors that provide data streams. Clients connect to the root of the overlay and request streams that are routed upwards through intermediate peers in the overlay up to the client. We report encouraging preliminary results for latency, jitter and resource consumption for both the non-faulty and faulty cases.",2010,0,
147,148,Automated Support for Propagating Bug Fixes,"We present empirical results indicating that when programmers fix bugs, they often fail to propagate the fixes to all of the locations in a code base where they are applicable, thereby leaving instances of the bugs in the code. We propose a practical approach to help programmers to propagate many bug fixes completely. This entails first extracting a programming rule from a bug fix, in the form of a graph minor of an enhanced procedure dependence graph. Our approach assists the programmer in specifying rules by automatically matching simple rule templates; the programmer may also edit rules or compose them from scratch. A graph matching algorithm for detecting rule violations is then used to locate the places in the code base where the bug fix is applicable. Our approach does not require that rules occur repeatedly in the code base. We present empirical results indicating that the approach nevertheless exhibits good precision.",2008,0,
148,149,Bandwidth effect on distance error modeling for indoor geolocation,"In this paper we introduce a model for the distance error measured from the estimated time of arrival (TOA) of the direct path (DP) between the transmitter and the receiver in a typical multipath indoor environment. We use the results of a calibrated Ray tracing software in a sample office environment. First we divide the whole floor plan into LOS and Obstructed LOS (OLOS), and then we model the distance error in each environment considering the variation of bandwidth of the system. We show that the behavior of the distance error in LOS environment can be modeled as Gaussian, while behavior of the OLOS is a mixture of Gaussian and exponential distribution. We also related the statistics of the distributions to the bandwidth of the system.",2003,0,
149,150,Impact of solid-state fault current limiters on protection equipment in transmission and distribution systems,"Solid-state fault current limiters (SSFCLs) offer a number of benefits when incorporated within transmission and distribution systems. SSFCLs can limit the magnitude of a fault current seen by a system using different methods, such as inserting a large impedance in the current path or controlling the voltage applied to the fault. However, these two methods can introduce a few problems when SSFCLs are used in a system along with other protection equipment such as protective relays and sensors. An experiment was designed and implemented to evaluate the behavior of the protective relays in a mimic distribution system with a SSFCL. This paper introduces the details of the experiment and the result shows that the distorted current and voltage waveforms resulting from the action of the SSFCL disturb the protective equipment.",2010,0,
150,151,Considering Fault Correction Lag in Software Reliability Modeling,"The fault correction process is very important in software testing, and it has been considered into some software reliability growth models (SRGMs). In these models, the time-delay functions are often used to describe the dependency of the fault detection and correction processes. In this paper, a more direct variable ""correction lag"", which is defined as the difference between the detected and corrected fault numbers, is addressed to characterize the dependency of the two processes. We investigate the correction lag and find that it appears Bell-shaped. Therefore, we adopt the Gamma function to describe the correction lag. Based on this function, a new SRGM which includes the fault correction process is proposed. And the experimental results show that the new model gives better fit and prediction than other models.",2008,0,
151,152,Effects of Defects on the Thermal and Optical Performance of High-Brightness Light-Emitting Diodes,"Defects in terms of voids, cracks, and delaminations are often generated in light-emitting diodes (LEDs) devices and modules. During various manufacturing processes, accelerated testing, inappropriate handling, and field applications, defects are most frequently induced in the early stage of process development. One loading is due to the nonuniform loads caused by temperature, moisture, and their gradients. In this research, defects in various cases are modeled by a nonlinear finite-element method (FEM) to investigate the existence of interfaces, interfacial open and contacts in terms of thermal contact resistance, stress force nonlinearity, and optical discontinuity, in order to analyze their effects on the LED's thermal and optical performance. The simulation results show that voids and delaminations in the die attachment would enhance the thermal resistance greatly and decrease the LED's light extraction efficiency, depending on the defects' sizes and locations generated in packaging.",2009,0,
152,153,The minimum worst case error of fuzzy approximators,"The approximation capability of fuzzy systems is an important topic of research when the systems are regarded as input-output maps. By using the notion of information-based complexity (IBC), we derive the minimum worst case error of a fuzzy approximator, which is independent of the detailed construction of the fuzzy rule bases",2001,0,
153,154,Evaluation of risk in canal irrigation systems due to non-maintenance using fuzzy fault tree approach,"The safety and performance of many existing irrigation systems could be improved by doing the preventive maintenance activities. Modeling canal irrigation systems in terms of condition and performance that can be directly correlated with particular canal system maintenance activities. There are two categories of scheduled maintenance activity in irrigation maintenance systems: maintenance may be targeted towards restoring deliveries (""restorative maintenance""), or towards reducing the risk of failures (""preventative maintenance""). This paper covers the latter kind of maintenance scheduling by 'risk analysis'. The purpose of this risk analysis is to forecast the impact of preventative maintenance on deliveries from the main channel systems. After gaining the experience from the preliminary risk analysis through questionnaire based survey and failure history of past record, a 'fuzzy fault tree (FFT)' method is developed for the rapid risk assessment and it is necessitated for the irrigation system manager/engineer. 'Risk analysis' of irrigation systems from the point of view of maintenance is studied and applied to the Tirunelveli Channel Systems located in India. The effectiveness is calculated for each preventative maintenance tasks and it is ranked according to the effectiveness/cost ratio.",2003,0,
154,155,Waveform matching approach for fault diagnosis of a high-voltage transmission line employing harmony search algorithm,"An accurate and effective technology for fault diagnosis of a high-voltage transmission line plays an important role in supporting rapid system restoration. The fault diagnosis of a high-voltage transmission line involves three major tasks, namely fault-type identification, fault location and fault time estimation. The diagnosis problem is formulated as an optimisation problem in this work: the variables involved in the fault diagnosis problem, such as the fault location, and the unknown variables such as ground resistance, are taken into account as optimisation variables; the sum of the discrepancy of the approximation components of the actual and expected waveforms is taken as the optimisation objective. Then, according to the characteristics of the formulated optimisation problem, the harmony search, an effective heuristic optimisation algorithm developed in recent years, is employed to solve this problem. Test results for a sample power system have shown that the developed fault diagnosis model and method are correct and efficient.",2010,0,
155,156,Fault tolerance in autonomic computing environment,"Since the characteristic of current information systems is the dynamic change of their configurations and scales with non-stop provision of their services, the system management should inevitably rely on autonomic computing. Since fault tolerance is one of the important system management issues, it should also be incorporated in an autonomic computing environment. This paper argues what should be taken into consideration and what approach could be available to realize the fault tolerance in such environments.",2002,0,
156,157,Fault Location Using Sparse IED Recordings,"Basic goal of power system is to continuously provide electrical energy to users. Like with any other system, failures in power system can occur. In those situations it is critical that remedial actions are applied as soon as possible. To apply correct remedial actions it is very important that accurate fault condition and location are detected. In this paper, different fault location algorithms followed with description of intelligent techniques used for implementation of corresponding algorithms are presented. New approach for fault location using sparse measurements is examined. According to available data, it decides between different algorithms and selects an optimal one. New approach is developed by utilizing different data structures in order to efficiently implement algorithm decision engine, which is presented in paper.",2007,0,
157,158,A Bio-network Based Fault-Tolerant Architecture for Supervisory System,"Supervisory systems show increasing importance in many plants to ensure the secure and stable production. And the fault tolerance ability is a key problem. Concerned with the problem of low automation level and high operation cost, a layered Bio-Network is studied inspired by biology system, and Bio-Entity as its functional unit is analyzed. In the bottom layer, the communication infrastructure is built on Web Service to hide the difference among underlying heterogeneous systems. In the Bio-System layer, each Bio-Entity is delegated by an agent to gain the functions. Then, in the application layer, various services are encapsulated upon Bio-Entity. The application interface makes the integration with other systems more convenient. Based on Bio-Network, a novel framework of supervisory system for typical waterworks is proposed. The fault tolerant functions are analyzed in the context of Bio-Network. The practical application proves that Bio-Network based system configuration is improved and the cost is reduced.",2008,0,
158,159,Error Analysis of the Complex Kronecker Canonical Form,"In some interesting applications in control and system theory, i.e. in engineering, in ecology (Leslie population model), in financial/actuarial (Leontief multi input - multi output) science, linear descriptor (singular) differential/difference equations with time-invariant coefficients and (non-) consistent initial conditions have been extensively used. The solution properties of those systems are based on the Kronecker canonical form, which is an important component of the Matrix Pencil Theory. In this paper, we present some preliminary results for the error analysis of the complex Kronecker canonical form based on the Euclidean norm. Finally, under some weak assumptions an interesting new necessary condition is also derived.",2010,0,
159,160,Joint Fault-Tolerant Design of the Chinese Space Robotic Arm,"In this paper, joint reliability design for the Chinese space robotic arm has been discussed. Redundant controller unit, redundant can bus communication unit and latch-up power protection unit have been outlined. The fault tree of the joint has been built. Moreover, the new algorithm of auto-adjust thresholding value has been presented for fault detection, and the fault-tolerant strategies of joint have been proposed. Experimental results demonstrate the effectiveness of the joint fault-tolerant design",2006,0,
160,161,Fault tolerance in scalable agent support systems: integrating DARX in the AgentScape framework,"Open multi-agent systems need to cope with the characteristics of the Internet, e.g., dynamic availability of computational resources, latency, and diversity of services. Large-scale multi-agent systems employed on wide-area distributed systems are susceptible to both hardware and software failures. This paper describes AgentScape, a multi-agent system support environment, DARX, a framework for providing fault tolerance in large scale agent systems, and a design for the integration of the two.",2003,0,
161,162,An empirical validation of the relationship between the magnitude of relative error and project size,"Cost estimates are important deliverables of a software project. Consequently, a number of cost prediction models have been proposed and evaluated. The common evaluation criteria have been MMRE, MdMRE and PRED(k). MRE is the basic metric in these evaluation criteria. The implicit rationale of using a relative error measure like MRE, rather than an absolute one, is presumably to have a measure that is independent of project size. We investigate if this implicit claim holds true for several data sets: Albrecht, Kemerer, Finnish, DMR and Accenture-ERP. The results suggest that MRE is not independent of project size. Rather, MRE is larger for small projects than for large projects. A practical consequence is that a project manager predicting a small project may falsely believe in a too low MRE. Vice versa when predicting a large project. For researchers, it is important to know that MMRE is not an appropriate measure of the expected MRE of small and large projects. We recommend therefore that the data set be partitioned into two or more subsamples and that MMRE is reported per subsample. In the long term, we should consider using other evaluation criteria.",2002,0,
162,163,Exact symbol-error probability analysis for orthogonal space-time block codes: two- and higher dimensional constellations cases,"Exact expressions are obtained for the symbol-error probability of orthogonal space-time block codes at the output of the coherent maximum-likelihood decoder in the general case of arbitrary input signal constellation and code. Such expressions are derived for the cases of both deterministic (fixed) and random Rayleigh/Ricean fading channels, and both the two- and higher dimensional constellations.",2004,0,
163,164,Which concurrent error detection scheme to choose ?,"Concurrent error detection (CED) techniques (based on hardware duplication, parity codes, etc.) are widely used to enhance system dependability. All CED techniques introduce some form of redundancy. Redundant systems we subject to common-mode failures (CMFs). While most of the studies of CED techniques focus on area overhead, few analyze the CMF vulnerability of these techniques. In this paper, we present simulation results to quantitatively compare various CED schemes based on their area overhead and the protection (data integrity) they provide against multiple failures and CMFs. Our results indicate that, for the simulated combinational logic circuits, although diverse duplex systems (with two different implementations of the same logic function) sometimes have marginally higher area overhead, they provide significant protection against multiple failures and CMFs compared to other CED techniques like parity prediction",2000,0,
164,165,Optimal scheduling of imprecise computation tasks in the presence of multiple faults,"With the advance of applications such as multimedia, image/speech processing and real-time AI, real-time computing models allowing to express the timeliness versus precision trade-off are becoming increasingly popular. In the imprecise computation model, a task is divided into a mandatory part and an optional part. The mandatory part should be completed by the deadline even under worst-case scenario; however, the optional part refines the output of a mandatory part within the limits of the available computing capacity. A non-decreasing reward function is associated with the execution of each optional part. Since the mandatory parts have hard deadlines, provisions should be taken against faults which may occur during execution. An FT-Optimal framework allows the computation of a schedule that simultaneously maximizes the total reward and tolerates transient faults of mandatory parts. We extend the framework to a set of tasks with multiple deadlines, multiple recovery blocks and precedence constraints among them. To this aim, we first obtain the exact characterization of imprecise computation schedules which can tolerate up to k faults, without missing any deadlines of mandatory parts. Then, we show how to generate FT-Optimal schedules in an efficient way. Our solution works for both linear and general concave reward functions",2000,0,
165,166,Initial Evaluation of the Fracture Behavior of Piezoelectric Single Crystals Due to Artificial Surface Defects,"This study is part of a new research program to develop fundamental understanding of the fracture and fatigue behavior of piezoelectric single crystals through the combination of computational and experimental approaches. In this work we present 1) experimental results on the creation of artificial surface defects in piezoelectric single crystals using a focused ion beam (FIB) system and 2) initial observations on the crystal's fracture behavior under an electrical field. The major advantage of using a FIB is that one can control the size, shape, and orientation of artificial defects precisely, allowing realistic surface defects, e.g., half-penny-shaped, 100 mum long, <1 mum wide, and 50 mum deep. We have demonstrated that multiple artificial defects with varying inclination angles relative to the specimen's crystallographic orientation can be machined in a few hours. In this paper, we report the experimental details of the FIB milling, typical defect shape, and initial results on the effects of high electric field on the fracture behavior of single crystals.",2006,0,
166,167,A parallel and fault tolerant file system based on NFS servers,"One important piece of system software for clusters is the parallel file system. All current parallel file systems and parallel I/O libraries for clusters do not use standard servers, thus it is very difficult to use these systems in heterogeneous environments. However why use proprietary or special-purpose servers on the server end of a parallel file system when you have most of the necessary functionality in NFS servers already? This paper describes the fault tolerance implemented in Expand (Expandable Parallel File System), a parallel file system based on NFS servers. Expand allows the transparent use of multiple NFS servers as a single file system, providing a single name space. The different NFS servers are combined to create a distributed partition where files are stripped. Expand requires no changes to the NFS server and uses RPC operations to provide parallel access to the same file. Expand is also independent of the clients, because all operations are implemented using RPC and NFS protocol. Using this system, we can join heterogeneous servers (Linux, Solaris, Windows 2000, etc.) to provide a parallel and distributed partition. Fault tolerance is achieved using RAID techniques applied to parallel files. The paper describes the design of Expand and the evaluation of a prototype of Expand, using the MPI-IO interface. This evaluation has been made in Linux clusters and compares Expand with PVFS.",2003,0,
167,168,Detection and correction of limit cycle oscillations in second -order recursive digital filter,"In this paper the effects of limit cycle oscillations in recursive second order digital filter is studied and the remedies for curing the problems of limit cycle oscillations are described. Limit cycle deletion using state space representation for second order system implemented with finite word-length register, is depicted. Necessary and sufficient condition to prevent limit cycle oscillation has also been described.",2005,0,
168,169,A Flexible Macroblock Scheme for Unequal Error Protection,"This paper proposes an enhanced error protection scheme using flexible macroblock ordering in H.264/AVC. The algorithm uses a two-phase system. In the first phase, the importance of every macroblock is calculated based on its influence on the current frame and future frames. In the second phase, the macroblocks with the highest impact factor are grouped together in a separate slice group using the flexible macroblock ordering feature of H.264/AVC. By using an unequal error protection scheme, the slice group containing the most important macroblocks can be better protected than the other slice group. The proposed algorithm offers better concealment opportunities than the algorithms which are predefined for flexible macroblock ordering in H.264/AVC.",2006,0,
169,170,Automated defect to fault translation for ASIC standard cell libraries,"Popular generic fault models, which exhibit limited realism for different IC technologies, have been widely misused due to their simplicity and cost-effective implementation. This paper introduces a system for deriving accurate, technology specific fault models that are based on analog defect simulation. The technique is formally defined and a systematic approach is developed. It is supported by a new software tool that provides a push-button solution for the previously tedious task of obtaining accurate ASIC cell defect to fault mappings. Furthermore, upon completion of the cell defect analysis, the tool automatically generates VITAL compliant, defect-injectable, VHDL cell models",2001,0,
170,171,A comparison of neural networks and model-based methods applied for fault diagnosis of electro-hydraulic control systems,The paper aims to investigate two advanced methods used in fault diagnosis of electro-hydraulic (EH) control systems. The theoretical background of the neural network method and model-based approach are presented and the implementation of these methods is summarised with procedures in easy steps to follow for application. The pros and cons of these methods are also analysed based on fault detection capability. It is concluded that a combination of the neural network method and the model-based approach will be beneficial.,2002,0,
171,172,A Predictive Fault Tolerance Agent based on Ubiquitous Computing for A Home Study System,"DOORAE (Distance Object Oriented Collaboration Environment) is a framework for supporting development on multimedia collaborative environment. It provides functions well capable of developing multimedia distance education system for students as well as teachers. It includes session management, access control, concurrency control and handling late comers. There are two approaches to software architecture on which applications for multimedia distance education environment in situation-aware middleware are based. This paper proposes a new model of fault tolerance agent based on situation-aware ubiquitous computing for a multimedia home study system which is based on CARV.",2007,0,
172,173,Model of stator inter-turn short circuit fault in doubly-fed induction generators for wind turbine,"The doubly fed induction generator (DFIG) is an important component of wind turbine systems. It is necessary to identify incipient faults quickly. This paper proposes a complete simulation model of DFIG in wind turbine about inter-turn short circuit fault at stator windings, which is based on multi-circuit theory. A detail analysis about simulation results is presented, especially about short circuit current. By analysis, the apparent 150 Hz, 450 Hz and current phase angle difference are taken as fault features and the fault phase also can be detected by phase angle difference. Both simulated results and experimental results of emulated inter-turn short circuit fault by paralleling a resistance with phase A are carried out. They verify the preceding analysis results. Moreover, their coincidence certificates this model is good and simulation results of inter-turn short circuit fault are correct.",2004,0,
173,174,Self-healing strategies for component integration faults,"Software systems increasingly integrate Off-The-Shelf (OTS) components. However, due to the lack of knowledge about the reused OTS components, this integration is fragile and can cause in the field a lot of failures that result in dramatic consequences for users and service providers, e.g. loss of data, functionalities, money and reputation. As a consequence, dynamic and automatic fixing of integration problems in systems that include OTS components can be extremely beneficial to increase their reliability and mitigate these risks. In this paper, we present a technique for enhancing component-based systems with capabilities to self-heal common integration faults by using a predetermined set of healing strategies. The set of faults that can be healed has been determined from the analysis of the most frequent integration bugs experienced by users according to data in bug repositories available on Internet. An implementation based on AOP techniques shows the viability of this technique to heal faults in real case studies.",2008,0,
174,175,Recursive Evaluation of Fault Tolerance Mechanisms for SLA Management,"Service level agreements (SLAs) have been introduced into the grid in order to build a basis for its commercial uptake. The challenge for Grid providers in agreeing and operating SLA-bound jobs is to ensure their fulfillment even in the case of failures. Hence, fault-tolerance mechanisms are an essential means of the provider's SLA management. The high utilization of commercial operated clusters leads to scenarios in which typically a job migration effects other jobs scheduled. The effects result from the unavailability of enough free resources which would be needed to catch all resource outages. Consequently before initiating a migration, its effects for other jobs have to be compared and the initiation of fault- tolerance (FT-) mechanisms have to be evaluated recursively. This paper presents a measurement for the benefit of initiating a FT-mechanism, the recursive evaluation, and termination condition. Performing such an impact evaluation of an initiated chain of FT-mechanisms is often more profitable than performing a single FT-mechanism and accordingly this is important for the Grid commercialization.",2008,0,
175,176,Corrective control strategies in case of infeasible operating situations,"A new method that deals with power systems infeasible operating situations is proposed in this paper. In case these situations occur, appropriate corrective actions must be efficiently obtained and quickly implemented. In order to accomplish this, it is necessary (a) to quantify the systems unsolvability degree (UD), and (b) to determine a corrective control strategy to pull the system back into the feasible operation region. UD is determined through the smallest distance between the infeasible (unstable) operating point and the feasibility boundary in parameter (load) space. In this paper the control strategies can be obtained by two methods, namely the proportionality method (PM) and the nonlinear programming based method (NLPM). Capacitor banks, tap changing transformers and load shedding are the usual controls available. Simulations have been carried out, for small to large systems, under contingency and heavy load situations, in order to show the efficiency of the proposed method. It can be a very useful tool in operation planning studies, particularly in voltage stability analysis.",2001,0,
176,177,Fault detection using phenomenological models,"There exist many different established approaches to detect system faults. This paper discusses the various system models and the associated fault detection techniques. Specifically, phenomenological models are presented in detail. Fault detection using principal components analysis and the cluster and classify method is illustrated with real operational data from an electrically powered vehicle.",2003,0,
177,178,Using Search Methods for Selecting and Combining Software Sensors to Improve Fault Detection in Autonomic Systems,"Fault-detection approaches in autonomic systems typically rely on runtime software sensors to compute metrics for CPU utilization, memory usage, network throughput, and so on. One detection approach uses data collected by the runtime sensors to construct a convex-hull geometric object whose interior represents the normal execution of the monitored application. The approach detects faults by classifying the current application state as being either inside or outside of the convex hull. However, due to the computational complexity of creating a convex hull in multi-dimensional space, the convex-hull approach is limited to a few metrics. Therefore, not all sensors can be used to detect faults and so some must be dropped or combined with others. This paper compares the effectiveness of genetic-programming, genetic-algorithm, and random-search approaches in solving the problem of selecting sensors and combining them into metrics. These techniques are used to find 8 metrics that are derived from a set of 21 available sensors. The metrics are used to detect faults during the execution of a Java-based HTTP web server. The results of the search techniques are compared to two hand-crafted solutions specified by experts.",2010,0,
178,179,Experimental study on the impact of endoscope distortion correction on computer-assisted celiac disease diagnosis,"The impact of applying barrel distortion correction to endoscopic imagery in the context of automated celiac disease diagnosis is experimentally investigated. For a large set of feature extraction techniques, it is found that contrasting to intuition, no improvement but even significant result degradation of classification accuracy can be observed. For techniques relying on geometrical properties of the image material (shape), moderate improvements of classification accuracy can be achieved. Reasons for this somewhat unexpected results are discussed and ways how to exploit potential distortion correction benefits are sketched.",2010,0,
179,180,A comparison of phase space reconstruction and spectral coherence approaches for diagnostics of bar and end-ring connector breakage faults in polyphase induction motors using current waveforms,"Two signal (waveform) analysis approaches are investigated in this paper for motor drive fault identification-one linear and the other nonlinear. Twenty-one different motor-drive operating conditions including healthy, 1 through 10 broken bars, and 1 through 10 broken end-ring connectors are investigated. Highly accurate numerical simulations of current waveforms for the various operating conditions are generated using the time stepping coupled finite element-state space method for a 208-V, 60-Hz, 2-pole, 1.2-hp, squirrel cage 3-phase induction motor. The linear signal analysis method is based on spectral coherence, whereas the nonlinear signal analysis method is based on stochastic models of reconstructed phase spaces. Conclusions resulting from the comparisons of these two methods are drawn.",2002,0,
180,181,Ontology-based fault diagnosis for industrial control applications,"Traditional fault detection systems in industrial control applications are just able to report occurring faults. Fault diagnosis systems are more desirable for plant operators, as such systems are capable to reduce the number of occurring alarms by elimination of consecutive alarms and prioritization of critical alarms. The disadvantage of those systems is that they have to be implemented anew for every control application, as the system dependencies vary from application to application. Ontology-based fault diagnosis systems do not have this disadvantage. Only the ontology has to be created for the new system, which greatly reduces time and effort for new systems, as old ontologies can be reused for the new system.",2010,0,
181,182,Ground control for the geometric correction of PAN imagery from Indian remote sensing (IRS) satellites,"Efficacy of a hand-held global positioning system (GPS) receiver in stand-alone mode of GPS measurements was investigated by taking measurements on different dates. These measurements were compared to those observed by DGPS and total station for the validation of hand-held GPS receiver accuracy. Ground control point (GCP) coordinates were derived from 1:25,000 and 1:50,000 scale topographic maps, and by using two different types of GPS receivers- a stand-alone hand-held and dual frequency DGPS receivers. GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using affine mapping function. GCPs derived from maps yielded root mean squares (RMS) error from 15 to 35 m respectively. However, GCPs derived by DGPS or stand-alone mode hand-held GPS receiver gave RMS error in the range of 3 to 6 meter, which is very close to spatial resolution of PAN sensor imagery (5.8 m). The mean values of GCP coordinates observed with the help of hand-held GPS receiver in stand-alone mode might prove a cost effective solution for the determination of GCP coordinates in the geometric correction of current high-resolution imagery from IRS satellites.",2003,0,
182,183,Synchronization Probabilities using Conventional and MVDR Beam Forming with DOA Errors,"In this paper, code synchronization probabilities of the direct sequence spread spectrum (DS/SS) system are investigated when the receiver utilizes an adaptive antenna array. Performance studies of three beam forming algorithms in the presence of direction-of-arrival (DOA) errors are presented. The investigated algorithms are the conventional, the minimum variance distortionless response (MVDR), and the MVDR+ADL where the MVDR algorithm is enhanced against DOA error via the adaptive diagonal loading (ADL). The paper includes a large number of analytical and simulation results where the effects of DOA errors are investigated. It can be concluded that the MVDR beam former is more sensitive to DOA errors than the conventional beam former, especially, at large DOA errors and high signal-to-noise-ratio (SNR) values. However, this sensitivity can be reduced notably by using the ADL.",2007,0,
183,184,Fault Tree Reuse Across Multiple Reasoning Paradigms,"The development of a diagnostic model can be a very time-consuming and manually intensive process. One must first analyze the Test Program Set (TPS) to determine the fault tree and then integrate with that any additional knowledge that can be obtained from external data sources (such as test results, maintenance actions from the various maintenance levels, run-time failure information, etc.). The diagnostic models defined in the IEEE Std. 1232-2002 (AI-ESTATE) each define a different method that can be used for a diagnostic reasoner. It has been determined that each of these models utilize the information found in the basic TPS fault tree. As the fault tree represents hard won engineering knowledge that is expensive to reproduce, it is desirable to share the fault tree representations across multiple reasoner models. This paper will layout how each model type in the AI-ESTATE standard utilizes the fault tree to perform diagnostics and how, through the use of the XML representation of the AI-ESTATE fault tree model, that basic fault tree can be shared between reasoner models. It would also be desirable to find a way to gain that fault tree knowledge without having to manually reproduce it. As such, this paper will also describe how that information can at least be semi-automatically extracted from TPS design artifacts.",2006,0,
184,185,Position location error analysis by AOA and TDOA using a common channel model for CDMA cellular environments,"AOA and TDOA are known to be promising methods and are developed separately. Combination or cooperation of the two methods has not been possible due to lack of the applicable channel models. The COST-207 model is utilized to provide the angular information for the AOA method. The angular characteristic of the channel can be obtained from the given temporal channel model, hence fusing of the approaches is now possible. Different properties of the methods are revealed to verify the proposition in literature. As a future research area, the fusing of the two different methods for better reliability and accuracy is mentioned",2000,0,
185,186,"Mutual coupling in microstrip antenna array: evaluation, reduction, correction or compensation","Mutual coupling between the antenna elements in a microstrip antenna array is a potential source of performance degradation, particularly in a highly congested environment. The degradation includes impedance mismatching, increased side-lobe level, deviation of the radiation pattern from the desired one, and decrease of gain due to the excitation of a surface wave. To deal with these problems, the first thing is to evaluate the mutual coupling and to select the element with low mutual coupling. Then, it is still desired to reduce the mutual coupling further by taking some measures. Finally, in certain critical cases, it is necessary to involve the mutual coupling effects accurately through numerical analysis, such as ultra low side lobe arrays and adaptive ing arrays. All these issues are discussed and some numerical examples are given. Due to limited space, the paper focuses mainly on the work done in our laboratory.",2005,0,
186,187,Geometric and shading correction for images of printed materials using boundary,"A novel technique that uses boundary interpolation to correct geometric distortion and shading artifacts present in images of printed materials is presented. Unlike existing techniques, our algorithm can simultaneously correct a variety of geometric distortions, including skew, fold distortion, binder curl, and combinations of these. In addition, the same interpolation framework can be used to estimate the intrinsic illumination component of the distorted image to correct shading artifacts. We detail our algorithm for geometric and shading correction and demonstrate its usefulness on real-world and synthetic data.",2006,0,
187,188,A novel co-evolutionary approach to automatic software bug fixing,"Many tasks in software engineering are very expensive, and that has led the investigation to how to automate them. In particular, software testing can take up to half of the resources of the development of new software. Although there has been a lot of work on automating the testing phase, fixing a bug after its presence has been discovered is still a duty of the programmers. In this paper we propose an evolutionary approach to automate the task of fixing bugs. This novel evolutionary approach is based on co-evolution, in which programs and test cases co-evolve, influencing each other with the aim of fixing the bugs of the programs. This competitive co-evolution is similar to what happens in nature for predators and prey. The user needs only to provide a buggy program and a formal specification of it. No other information is required. Hence, the approach may work for any implementable software. We show some preliminary experiments in which bugs in an implementation of a sorting algorithm are automatically fixed.",2008,0,
188,189,An NN-based atmospheric correction algorithm for Landsat/TM thermal infrared data,"Land surface temperature (LST) is a key variable for studies of global or regional land surface processes, energy and water cycle, and thus, has important applications in various areas. Atmospheric correction is a major issue in LST retrieval using remote sensing data because the presence of the atmosphere always influences the radiation from the ground to the space sensor. Atmospheric correction of thermal infrared (TIR) data for land surface temperature retrieval is to estimate the three atmospheric parameters: transmittance, path radiance and the downward radiance. Typically the atmospheric parameters are obtained using atmospheric profiles combined with a radiative transfer model (RTM). But this approach is time-consuming and expensive, which is impractical for high-speed (near-realtime) operational atmospheric correction. An artificial neural network (NN) based atmospheric correction model for Landsat/TM thermal infrared data is proposed. The multi-layer feed-forward neural network (MFNN) is selected, in which the atmospheric profiles (temperature, humidity and pressure), elevation and scan angle are the input variables, and the atmospheric parameters are the output variables. The MFNN is combined with the radiative transfer simulation, using MODTRAN 4.0 and the latest global assimilated data. Finally, the transmittance and path radiance derived by the MFNN-based algorithm is compared with MODTRAN4.0 results. The RMSE for both parameters are 0.0031 and 0.035 Wm<sup>-2</sup>sr<sup>-1</sup>m<sup>-1</sup>, respectively. The results indicate that the proposed approach can be a practical method for Landsat/TM thermal data in both accuracy and efficiency.",2010,0,
189,190,Fault tree analysis of a fire hazard of a power distribution cabinet with Petri Nets,Motivation of this study is to verify system safety analysis of HAVELSAN Peace Eagle Program developed hardware items for Ground Support Systems. A preliminary hazard analysis for each of the hardware developed items are performed and safety hazard analysis models are constructed with risk assessment of hazards based on their probability of occurrences for future operational and maintenance activities. An example for this kind of analysis the system safety fault tree analysis model of a Ground Support Segment Mission Simulator subsystem Power Distribution Adapter Cabinet design with hazardous risk assessments criteria according to the military standard specifications. Same analysis approach then modeled with Petri Nets that has extensions from fault tree analysis approach and enables the modeler to represent the probability of occurrences in the system design phase. Same model can be built in the specification phase which creates the potential for early validation of the system design behavior.,2010,0,
190,191,Three-Dimensional Pareto-Optimal Design of Inductive Superconducting Fault Current Limiters,"The inductive-type superconducting fault current limiters (LSFCLs) mainly consist of a primary copper coil, a secondary complete or partial superconductor cylinder, and a closed or open magnetic iron core. Satisfactory performance of such device significantly depends on optimal selection of its employed materials and construction dimensions, as well as its electrical, thermal, and magnetic parameters. Therefore, it is very important to identify a comprehensive model describing the LSFCL behavior in a power system prior to its fabrication. When a fault occurs, the dynamic model should essentially characterize the overall phenomena to compare the simulation results by varying LSFCL parameters to maximize the merits of a fault current limiter while minimizing its drawbacks during the normal state. The principle object of this paper is to achieve a feasible and full penetrative approach in 3-D alignments, i.e., a Pareto-optimal design of LSFCLs by means of multicriteria decision-making techniques after defining the LSFCL model in a power system CAD/electromagnetic transients including dc environment.",2010,0,
191,192,Stochastic fault tree analysis with self-loop basic events,"This paper presents an analytical approach for performing fault tree analysis (FTA) with stochastic self-loop events. The proposed approach uses the flow-graph concept, and moment generating function (MGF) to develop a new stochastic FTA model for computing the probability, mean time to occurrence, and standard deviation time to occurrence of the top event. The application of the method is demonstrated by solving one example.",2005,0,
192,193,Coverage gain estimation for multi-burst forward error correction in DVB-H networks,"An approach for increasing the reception robustness of mobile broadcast streaming services has been developed for mobile broadcast systems based on time-slicing, such as DVB-H, employing multi-burst FEC at link or application layer. Multiple bursts will be encoded jointly in order to overcome burst errors caused by signal level variations. The approach shows high potential which can be characterized by a link margin gain due to reduced CNR requirements to cope with fast fading and shadowing. Nevertheless, the achieved gain depends on several system parameters (encoding period and coding rate), the physical environment (correlation of shadowing and multi-path fading) and on the mobility of the users (velocity and trajectory). The paper deals with the coverage estimation and network gain due to multi-burst FEC for vehicular users in a realistic urban scenario. Since the user behavior has to be considered the gain cannot be directly included into the link budget. Thus, a methodology has been developed in order to estimate the coverage of multi-burst FEC services based on dynamic system-level simulations. Results are shown by means of simulations in realistic scenarios and field measurements in urban environments.",2009,0,
193,194,Dynamic strength scaling for delay fault propagation in nanometer technologies,"This paper proposes an algorithm for the detection of resistive delay faults in deep submicron technology using dynamic strength scaling, which is applicable for 45 nm and below. The approach uses an advanced coding system to build logical functions that are sensitive to strength and able to detect even the slightest voltage changes in the circuit. Such changes are caused by interconnection resistive behavior and result in timing-related defects.",2009,0,
194,195,LEON3 ViP: A Virtual Platform with Fault Injection Capabilities,"In addition to functional simulation for validation of hardware/software designs, there are additional robustness requirements that need advanced simulation techniques and tools to analyze the system behavior in the presence of faults. In this paper, we present the design of a fault injection framework for LEON3, a 32bit SPARC CPU based system used by the European Space Agency, described at Transaction Level using System C. First of all an extension of a previous XML formalization of basic binary faults, like memory and CPU registers corruption, is done in order to support TLM2.0transaction's parameters corruptions. Next a novel Dynamic Binary Instrumentation (DBI) technique for C++ binaries is used to insert fault injection wrappers in SystemC transaction path. For binary faults in model components the use of TLM2.0 transport_dbg is proposed. This way each component with fault injection capabilities exposes a standard interface to allow internal component inspection and modification.",2010,0,
195,196,Research on Transformer Fault Diagnosis Expert System Based on DGA Database,"This paper analyzes and designs the transformer fault diagnosis system based on dissolved gas analysis (DGA) database in which DGA data is managed by the Oracle database. The fault diagnosis module includes the single analyzing item and the Integrated analyzing item, such as, improvement three-ratio method, grey relational entropy, fuzzy clustering, artificial neural networks, and so on. They reduce the insufficiency in diagnosis method which is used now. The system realizes each function of the modules by using the lamination method, it is able to diagnose problems existing in oil chromatogram analysis data of transformer, and the accuracy of the system is also testified by practical example.",2009,0,
196,197,Modular fault recovery in timed discrete-event systems: application to a manufacturing cell,"This paper extends the previous results of the authors on fault recovery to timed discrete-event systems (TDES), and discusses the application of the proposed methodology to a manufacturing cell. It is assumed that the plant can be modelled as a TDES, the faults are permanent, and that a diagnosis system is available that detects and isolates faults with a bounded delay (expressed in clock ticks). Thus, the combination of the plant and the diagnosis system, as the system to be controlled, has three modes: normal, transient and recovery. Initially, the plant is in the normal mode. Once a fault occurs, the system enters the transient mode. After the fault is detected and isolated by the diagnosis system, the system enters the recovery mode. This framework does not depend on the diagnosis technique used, as long as lower and upper bounds for diagnosis delay are available. A modular switching supervisory scheme is proposed to satisfy the system specifications. The design consists of a normal-transient supervisor, and multiple recovery supervisors each for recovery from a particular failure mode. The issue of the nonblocking property of the system under supervision, and also supervisor admissibility (controllability), in particular coerciveness, are studied. The proposed approach is applied to a manufacturing cell consisting of two machines and two conveyors. A modular switching supervisor is designed to ensure the specifications in the normal mode are met. In cases of failure, the supervisor sends appropriate recovery commands so that the cell can complete its production cycle",2005,0,
197,198,Managing fault-induced delayed voltage recovery in Metro Atlanta with the Barrow County SVC,"Georgia Transmission Corporation (GTC) commissioned the Barrow County Static Var Compensator (SVC) with a continuous rating of 0 to +260 Mvar in June of 2008. This paper presents the northern Metro Atlanta Georgia area transmission system, the requirements for voltage and var support, the dynamic performance study used to verify performance of the SVC, and provides an overview of the SVC design and control strategy, including the SVC's response to an actual power system disturbance. The Barrow County SVC is connected to the 230 kV bus at the Winder Primary Substation to effectively manage the exposure to fault-induced delayed voltage recovery (FIDVR), where the system voltage remains low (<80%) for several seconds following a disturbance and potentially leading to voltage collapse. The SVC configuration includes two thyristor-switched capacitors for rapid insertion of reactive power following a disturbance to decrease voltage recovery time and control the system's dynamic performance.",2009,0,
198,199,Investigation of fault tolerant of direct torque control in induction motor drive,"AC drives based on direct torque control (DTC) of induction machines are known for their high dynamic performances, obtained with very simple control schemes. So many studies have been performed with ASIC or FPGA DTC implementation. In this paper, we investigate for the tolerance of such drive to sensors defects, when the control algorithm is to be implemented in an FPGA. So, authors specially focus on the influence of the FPGA implementation design on the DTC fault tolerance. Simulations are carried out with system generator (SG) toolbox working in the MATLAB/SIMULINK environment. Results are presented and discussed to evaluate the DTC operating under considered faults.",2004,0,
199,200,Operational Fault Detection in cellular wireless base-stations,"The goal of this work is to improve availability of operational base-stations in a wireless mobile network through non-intrusive fault detection methods. Since revenue is generated only when actual customer calls are processed, we develop a scheme to minimize revenue loss by monitoring real-time mobile user call processing activity. The mobile user call load profile experienced by a base-station displays a highly non-stationary temporal behavior with time-of-day, day-of-the-week and time-of-year variations. In addition, the geographic location also impacts the traffic profile, making each base-station have its own unique traffic patterns. A hierarchical base-station fault monitoring and detection scheme has been implemented in an IS-95 CDMA Cellular network that can detect faults at - base station level, sector level, carrier level, and channel level. A statistical hypothesis test framework, based on a combination of parametric, semi-parametric and non-parametric test statistics are defined for determining faults. The fault or alarm thresholds are determined by learning expected deviations during a training phase. Additionally, fault thresholds have to adapt to spatial and temporal mobile traffic patterns that slowly changes with seasonal traffic drifts over time and increasing penetration of mobile user density. Feedback mechanisms are provided for threshold adaptation and self-management, which includes automatic recovery actions and software reconfiguration. We call this method, Operational Fault Detection (OFD). We describe the operation of a few select features from a large family of OFD features in Base Stations; summarize the algorithms, their performance and comment on future work.",2006,0,
200,201,Fault Tolerance for Manufacturing Components,"This article proposes a multiagent system for industrial production elements that transfers the concept of fault tolerance to the manufacturing levels of the organisation, acting automatically under open protocols when there is degradation or failure of any of the components, ensuring that normal operation is resumed within a delimited time. The main characteristics of this system are the drastic reduction in recovery times, the support for the significant heterogeneity existing in these scenarios and their high level of automation, while practically dispensing with the intervention of system administrators.",2006,0,
201,202,A study of student strategies for the corrective maintenance of concurrent software,"Graduates of computer science degree programs are increasingly being asked to maintain large, multi-threaded software systems; however, the maintenance of such systems is typically not well-covered by software engineering texts or curricula. We conducted a think-aloud study with 15 students in a graduate-level computer science class to discover the strategies that students apply, and to what effect, in performing corrective maintenance on concurrent software. We collected think-aloud and action protocols, and annotated the protocols for a number of behavioral attributes and maintenance strategies. We divided the protocols into groups based on the success of the participant in both diagnosing and correcting the failure. We evaluated these groups for statistically significant differences in these attributes and strategies. In this paper, we report a number of interesting observations that came from this study. All participants performed diagnostic executions of the program to aid program comprehension; however, the participants that used this as their predominant strategy for diagnosing the fault were all unsuccessful. Among the participants that successfully diagnosed the fault and displayed high confidence in their diagnosis, we found two commonalities. They all recognized that the fault involved the violation of a concurrent-programming idiom. And, they all constructed detailed behavioral models (similar to UML sequence diagrams) of execution scenarios. We present detailed analyses to explain the attributes that correlated with success or lack of success. Based on these analyses, we make recommendations for improving software engineering curriculums by better training students how to apply these strategies effectively.",2008,0,
202,203,The feasibility study on the combined equipment between micro-SMES and inductive/electronic type fault current limiter,"The concept of the combined equipment between micro-SMES and inductive/electronic type FCL is proposed in this paper. Having the multifunction for a superconducting device, the new equipment can serve as the protective component for a dual power system. The specification of a testing model was determined and the transient performance was analyzed by Matlab software. The results show that the combined equipment is realizable for a dual power system application, where it has the major function of limiting fault current (FCL function) and the minor function of maintaining power fluctuation (SMES function).",2003,0,
203,204,Effective Static Analysis to Find Concurrency Bugs in Java,"Multithreading and concurrency are core features of the Java language. However, writing a correct concurrent program is notoriously difficult and error prone. Therefore, developing effective techniques to find concurrency bugs is very important. Existing static analysis techniques for finding concurrency bugs either sacrifice precision for performance, leading to many false positives, or require sophisticated analysis that incur significant overhead. In this paper, we present a precise and efficient static concurrency bugs detector building upon the Eclipse JDT and the open source WALA toolkit (which provides advanced static analysis capabilities). Our detector uses different implementation strategies to consider different types of concurrency bugs. We either utilize JDT to syntactically examine source code, or leverage WALA to perform interprocedural data flow analysis. We describe a variety of novel heuristics and enhancements to existing analysis techniques which make our detector more practical, in terms of accuracy and performance. We also present an effective approach to create inter-procedural data flow analysis using WALA for complex analysis. Finally we justify our claims by presenting the results of applying our detector to a range of real-world applications and comparing our detector with other tools.",2010,0,
204,205,A case study of evaluation technique for soft error tolerance on SRAM-based FPGAs,"SRAM-based field programmable gate arrays (FPGAs) are vulnerable to a single event upset (SEU), which is induced by radiation effect. Therefore, the dependable design techniques become important, and the accurate dependability analysis method is required to demonstrate their robustness. Most of present analysis techniques are performed by using full reconfiguration to emulate the soft error. However, it takes long time to analyze the dependability because it requires many times of reconfiguration to complete the soft error injection. In the present paper, we construct the soft error estimation system to analyze the reliability and to reduce the estimation time. Moreover, we apply Monte Carlo simulation to our approach, and identify trade-off between accuracy of error rate and estimation time. As a result of our experimentation for 8-bit full-adder and multiplier, we can show the dependability of the implemented system. Also, the constructed system can reduce the estimation time. According to the result, when performing about 50% circuit Monte Carlo simulation, the error rate is within 20%.",2010,0,
205,206,Effects of finite weight resolution and calibration errors on the performance of adaptive array antennas,"Adaptive antennas are now used to increase the spectral efficiency in mobile telecommunication systems. A model of the received carrier-to-interference plus noise ratio (CINR) in the adaptive antenna beamformer output is derived, assuming that the weighting units are implemented in hardware, The finite resolution of weights and calibration is shown to reduce the CINR. When hardware weights are used, the phase or amplitude step size in the weights can be so large that it affects the maximum achievable CINR. It is shown how these errors makes the interfering signals leak through the beamformer and we show how the output CINR is dependent on power of the input signals. The derived model is extended to include the limited dynamic range of the receivers, by using a simulation model. The theoretical and simulated results are compared with measurements on an adaptive array antenna testbed receiver, designed for the GSM-1800 system. The theoretical model was used to find the performance limiting part in the testbed as the 1 dB resolution in the weight magnitude. Furthermore, the derived models are used in illustrative examples and can be used for system designers to balance the phase and magnitude resolution and the calibration requirements of future adaptive array antennas",2001,0,
206,207,Fault Tolerant Methods for Intermitted Failures in Virtual Large Scale Disks,"Recently, the demand of low cost large scale storages increases. We developed VLSD (Virtual Large Scale Disks) toolkit for constructing virtual disk based distributed storages, which aggregate free spaces of individual disks. However, current implementation of VLSD can mask only stop failure but cannot mask other kinds of failures such as intermitted failure. In this paper, we introduce two classes to VLSD in order to increase the intermitted fault tolerance. One is Retry Disk which retries to read/write at failures, another is VotedRAID1 which masks failures by majority voting. In this paper, we describe these classes in detail and evaluate their fault tolerance.",2010,0,
207,208,Advances in scatter correction for 3D PET/CT,"We report on several significant improvements to the implementation of image-based scatter correction for 3D PET and PET/CT. Among these advances are: a new algorithm to scale the estimated scatter sinogram to the measured data, thereby largely compensating for external scatter; the ability to handle CT image truncation during this scaling; the option to iterate the scatter calculation for improved accuracy; the use of ordered subset estimation maximization (OSEM) reconstruction for the estimated emission images from which the scatter contributions are simulated; reporting of data quality parameters such as scatter and randoms fractions, and noise equivalent count rate (NECR), for each patient bed position; and extensive quality control output. Scatter correction (2 iterations, OSEM) typically requires 15-45 sec per bed. Very good agreement between the estimated scatter and measured emission data for several typical clinical scans is reported for CPS Pico-3D and HiRez LSO PET/CT systems. The data characteristics extracted during scatter correction are useful for patient specific count rate modeling and scan optimization",2004,0,
208,209,Time and frequency domain analyses based expert system for impulse fault diagnosis in transformers,"The presence of insulation failure in the transformer winding is detected using the voltage and current oscillograms recorded during the impulse test. Fault diagnosis in transformers has several parameters such as the severity of fault, the kind of fault and the location of the fault. Detection of major faults involving a large section of the coils have never been a big issue and several visual and computational methods have already been proposed by several researchers. The present paper describes an expert system based on re-confirmative method for the diagnosis of minor insulation failures involving small number of turns in transformers during impulse tests. The proposed expert system imitates the performance of an experienced testing personnel. To identify and locate a fault, an inference engine is developed to perform deductive reasoning based on the rules in the knowledge base and different statistical techniques. The expert system includes both the time-domain and frequency-domain analyses for fault diagnosis. The basic aim of the expert system is to provide a non-expert with the necessary information and interaction in order to make fault diagnosis in a friendly windowed environment. The rules for fault diagnosis have been so designed that these are valid for the range of power transformers used in practice up to a voltage level of 33 kV. The fault diagnosis algorithm has been tested using experimental results obtained for a 3 MVA transformer and simulation results obtained for 5 and 7 MVA transformers",2002,0,
209,210,Fault Detection Using the k-Nearest Neighbor Rule for Semiconductor Manufacturing Processes,"It has been recognized that effective fault detection techniques can help semiconductor manufacturers reduce scrap, increase equipment uptime, and reduce the usage of test wafers. Traditional univariate statistical process control charts have long been used for fault detection. Recently, multivariate statistical fault detection methods such as principal component analysis (PCA)-based methods have drawn increasing interest in the semiconductor manufacturing industry. However, the unique characteristics of the semiconductor processes, such as nonlinearity in most batch processes, multimodal batch trajectories due to product mix, and process steps with variable durations, have posed some difficulties to the PCA-based methods. To explicitly account for these unique characteristics, a fault detection method using the k-nearest neighbor rule (FD-kNN) is developed in this paper. Because in fault detection faults are usually not identified and characterized beforehand, in this paper the traditional kNN algorithm is adapted such that only normal operation data is needed. Because the developed method makes use of the kNN rule, which is a nonlinear classifier, it naturally handles possible nonlinearity in the data. Also, because the FD-kNN method makes decisions based on small local neighborhoods of similar batches, it is well suited for multimodal cases. Another feature of the proposed FD-kNN method, which is essential for online fault detection, is that the data preprocessing is performed automatically without human intervention. These capabilities of the developed FD-kNN method are demonstrated by simulated illustrative examples as well as an industrial example.",2007,0,
210,211,ANN based detection of electrical faults in generator-transformer units,"In the paper a model of decision system based on ANN, will be shown. As protected object the generator-transformer unit has been taking into consideration. The range of detected faults are initially narrows to faults of electromagnetic character (three-phase, two-phase, two-phase to earth and one-phase faults) within the generator - unit transformer - high voltage transmission line configuration.",2004,0,
211,212,Research on remote intelligent fault-diagnosis of CNC lathe based on bayesian networks,"Considering the development of smart machine tools and Internet-based manufacturing and in order to manage the manufacturing process more efficiently, a unit of remote intelligent fault-diagnosis based on Bayesian Networks (BN) was designed and software based on internet was realized as well as the case study concerning CNC lathe. It is a compensation of machine tool's self-detection whose major job is to find the fault of hardware and programming. The case study proved the reliability and advantages of the intelligent model based on BN.",2010,0,
212,213,Soft error optimization of standard cell circuits based on gate sizing and multi-objective genetic algorithm,"A radiation harden technique based on gate sizing and multi-objective genetic algorithm (MOGA) is developed to optimize the soft error tolerance of standard cell circuits. Soft error rate (SER), chip area and longest path delay are selected as the optimization goals and fast fitness evaluation algorithms for the three goals are developed and embedded into the MOGA. All the three goals are optimized simultaneously by optimally sizing the gates in the circuit, which is a complex NP-Complete problem and resolved by MOGA through exploring the global design space of the circuit. Syntax analysis technique is also employed to make the proposed framework can optimize not only pure combinational logic circuit but also the combinational parts of sequential logic circuit. Optimizing experiments carried out on ISCAS'85 and ISCAS'89 standard benchmark circuits show that the proposed optimization algorithm can decrease the SER 74.25% with very limited delay overhead (0.28%). Furthermore, the algorithm can also reduce the area for most of the circuit under test by average 5.23%. The proposed technique is proved to be better than other works in delay and area overhead and suitable to direct the design of soft error tolerance integrated circuits in high reliability realms.",2009,0,
213,214,Nonlinear Systems Fault Diagnosis with Differential Elimination,"The differential elimination algorithm is used to eliminate the non-observed variables of the nonlinear systems. By incorporating the algebraic observability and diagnosability concepts and using numerical differentiation algorithms, another approach to the certain classes of nonlinear systems fault diagnosis problem is presented.",2009,0,
214,215,Fault Tolerant Actuation for Steer-by-Wire Applications,"This paper introduces a R&D project concerned with the development of a fault-tolerant actuation system for steer-by-wire applications. The essential safety and reliability requirements for automotive vehicles are assessed. General redundancy schemes and current practices are examined. The paper then focuses on the use of actuators based on permanent magnetic brushless dc motors, and analyses internal fault-tolerant potentials of the actuator technology with possible control schemes evaluated. Finally key innovations that may provide practical and affordable solutions are discussed.",2007,0,
215,216,A procedure to correct the error in the structure function based thermal measuring methods,In this paper a methodology is presented to correct the systematic error of structure function based thermal material parameter measuring methods. This error stems from the fact that it is practically impossible to avoid parallel heat-flow paths in case of forced one-dimensional heat conduction. With the presented method we show how to subtract the effect of the parallel heat-flow paths from the measured structure function. With this correction methodology the systematic error of structure function based thermal material parameter measuring methods can be practically eliminated. Application examples demonstrate the accuracy increase obtained with the use of the method.,2004,0,
216,217,Backward-compatible robust error protection of JPEG XR compressed video,"The new JPEG XR image encoding standard offers a great compression rate while maintaining a good visual quality. Nonetheless, it has low error robustness, making it unusable in case of unreliable transmission over error prone channels, e.g., wireless channels. An improvement to the standard was developed, which can correct transmission errors, both bit or packet losses, and which is fully compatible with legacy decoders. Data interleaving and channel coding can offer a good protection against transmission errors; different levels of protection can be adopted, in order to trade-off between error protection capabilities and decompressed image quality.",2010,0,
217,218,Application of fuzzy neuro for generator stator earth fault detection,"In this paper the use of a fuzzy neural net for stator earth fault detection is presented. A generator model is simulated using EMTDC software. Earth faults are simulated between 0.1% to 100% distance points from the generator neutral. The combination of both EMTDC simulation and neural network presented in this paper introduces a new, complementary method that performs better in instances where the interpretation of traditional methods is somewhat dubious.",2004,0,
218,219,Research and Realization of Digital Circuit Fault Probe Location Process,"This paper presents three core files relating to circuit fault diagnosis which is generated by LASAR (logic automated stimulus response), i.e. fault dictionary, node truth table and pin connection table, analyses the content of fault dictionary, pin connection table and node truth table, finds the necessary information for fault location, summarizes the procedure of circuit test and fault location. Finally the digital circuit diagnosis system which can locate the fault on the pin of components is designed. With the help of probe, fault location of component pins can be accurately pinpointed.",2008,0,
219,220,New results for fault detection of untimed continuous Petri nets,"In this paper we study fault diagnosis of systems modeled by untimed continuous Petri nets. In particular, we generalize our previous works in this framework where we solved this problem only for special classes of continuous Petri nets, namely state machines and backward conflict free nets. We show that the price to pay for this generalization is that only three diagnosis states can be defined, rather than four. However, this is not a significant restriction because it is in accordance with all the literature on finite state automata.",2009,0,
220,221,Application of Particle Swarm Optimization and RBF Neural Network in Fault Diagnosis of Analogue Circuits,"BP neural network has the shortcoming of over-fitting, local optimal solution, which affects the practicability of BP neural network. RBF neural network is a feedforward neural network, which has the global optimal closing ability. However, the parameters in RBF neural network need determination. Particle swarm optimization is presented to choose the parameters of RBF neural network. The particle swarm optimization-RBF neural network method has high classification performance, and is applied to fault diagnosis of analogue circuits. Finally, the result of fault diagnosis cases shows that the particle swarm optimization - RBF neural network method has higher classification than BP neural network.",2009,0,
221,222,Development of a Testbench for Validation of DMT and DT2 Fault-Tolerant Architectures on SOI PowerPC7448,The purpose of TAFT fault tolerance studies conducted at CNES is to prepare the space community for the significant evolution linked to the usage of COTS components for developing spacecraft supercomputers. CNES has patented the DMT and DT2 fault-tolerant architectures with 'light' features. The development of a DMT/DT2 testbench based on a PowerPC7448 microprocessor from e2v is presented in this paper.,2008,0,
222,223,Compact Power Divider using Defected Ground Structure for Wireless Applications,"Use of different types of defected ground structures (DCS) has been reported in this paper to design compact power dividers in microstrip medium. Unit cell's (of DGS) equivalent circuit has been used to evaluate the performance of power divider. Based on this approach, compact two-way equal power dividers have been designed in GSM (900 MHz) band. Results show a size reduction of 35% and 32% for the power dividers using T shaped DGS and split ring DGS over the conventional power divider.",2008,0,
223,224,Utilisation of motion similarity in Colour-plus-Depth 3D video for improved error resiliency,"Robust 3D stereoscopic video transmission over error-prone networks has been a challenging task. Sustainability of the perceived 3D video quality is essential in case of channel losses. Colour-plus-Depth format on the other hand, has been popular for representing the stereoscopic video, due to its flexibility, low encoding cost compared to left-right stereoscopic video and backwards compatibility. Traditionally, the similarities existing between the colour and the depth map videos are not exploited during 3D video coding. In other words, both components are encoded separately. The similarities include the similarity in motion, image gradients and segments. In this work, we propose to exploit the similarity in the motion characteristics of the colour and the depth map videos by computing only a set of motion vectors and duplicating it for the sake of error resiliency. As the previous research has shown that the stereoscopic video quality is primarily affected by the colour texture quality, especially the motion vectors are computed for the colour video component and the corresponding vectors are used to encode the depth maps. Since the colour motion vectors are protected by duplication, the results have shown that both the colour video quality and the overall stereoscopic video quality are maintained in error-prone conditions at the expense of slight loss in depth map video coding performance. Furthermore, total encoding time is reduced by not calculating the motion vectors for depth map.",2010,0,
224,225,Embryonics+immunotronics: a bio-inspired approach to fault tolerance,"Fault tolerance has always been a standard feature of electronic systems intended for long-term missions. However, the high complexity of modern systems makes the incorporation of fault tolerance a difficult task. Novel approaches to fault tolerance can be achieved by drawing inspiration from nature. Biological organisms possess characteristics such as healing and learning that can be applied to the design of fault-tolerant systems. This paper extends the work on bio-inspired fault-tolerant systems at the University of York. It is proposed that by combining embryonic arrays with an immune inspired network, it is possible to achieve systems with higher reliability",2000,0,
225,226,"Video image based attenuation correction for PETbox, a preclinical PET tomograph","PETBox is a new simplified bench top PET scanner dedicated for pre-clinical imaging of mice. It has only two facing detector heads in a static gantry. Using iterative methods, limited-angle reconstruction of 3D images is possible. The geometry of the PETBox is such that very oblique emission angles are detected traversing significant lengths of tissue, making attenuation correction necessary. To that effect, we have developed a method by which two orthogonal optical views are combined to create a 3-dimensional estimate of the subject. This estimate is used to produce attenuation correction data that significantly improve the quantitative accuracy of the reconstructed images. In this paper, we present the method and evaluate its accuracy.",2009,0,
226,227,Analysis and design of SEPIC converter in boundary conduction mode for universal-line power factor correction applications,"In this paper, a SEPIC converter operated in boundary conduction mode for power factor correction applications with arbitrary output voltage is proposed, analyzed and designed. By developing an equivalent circuit model for the coupled inductor structure, a SEPIC converter with or without coupled inductors (and ripple current steering) can be analyzed and designed in a unified framework. Power factor correction under boundary conduction operation mode can be achieved conveniently using a simple commercially available control IC. Experimental results are provided to validate the circuit design",2001,0,
227,228,Categorization of minimum error forecasting zones using a geostatistic wind model,"In this paper a geostatistic wind direction model is applied to trace a wind speed map, based on data from official measurement weather stations distributed within the region of Andalucia-Spain. Each station's performance is assessed by comparing real measurements to those resulting from the linear interpolation of the rest. Once an error is associated to the station, the error is drawn in a map, in which minimum error zones can be delimited. Frequency and wind speed in each direction are the magnitudes of interest to get a first categorization of wind resources associated to the region. The interest of the method relies in the possibility of forecasting everywhere within the region with an error inside the tolerable margins.",2009,0,
228,229,A novel feature extraction and optimisation method for neural network-based fault classification in TCSC-compensated lines,"The suitability of fault classifiers introduced hitherto to operate correctly under a real TCSC transmission system remains a challenge since the computations are determined based on a number of postulations. This paper describes an alternative approach to fault classification in TCSC tines using artificial neural networks (ANNs). Special emphasis is placed on illustrating a combined wavelet transform and selforganising map (SOM) methodology to extract, validate and optimise the key characteristics of the fault transient phenomena in a TCSC line such that the input features to the ANNs are near optimal. As a result, it is shown that the fault classification proposed provides the ability to accurately classify the fault type, obviating the need for any predefined assumptions. Extensive simulation studies have been made to verify that the proposed method is both powerful and appropriate for fault classification.",2002,0,
229,230,Thermoreflectance imaging of defects in thin-film solar cells,We have identified and characterized various defects in thin-film a-Si and CIGS solar cells with sub-micron spatial resolution using thermoreflectance imaging. A megapixel silicon-based CCD was used to obtain noncontact thermal images simultaneously with visible electroluminescence (EL) images. EL can be indicative of pre-breakdown sites due to trap assisted tunneling and stress induced leakage currents. Physical defects appear at reverse bias voltages of 8 V in a-Si samples. Linear and nonlinear shunt defects are investigated as well as electroluminescent breakdown regions at reverse biases as low as 4.5 V. Pre-breakdown sites with electroluminescence are investigated.,2010,0,
230,231,Induction Motor-Drive Systems with Fault Tolerant Inverter-Motor Capabilities,"A low-cost fault tolerant drive topology for low- speed applications such as ""self-healing/limp-home"" needs for vehicles and propulsion systems, with capabilities for mitigating transistor open-circuit switch and short-circuit switch faults is presented in this paper. The present fault tolerant topology requires only minimum hardware modifications to the conventional off-the-shelf six-switch three-phase drive, with only the addition of electronic components such as triacs/SCRs and fast-acting fuses. In addition, the present approach offers the potential of mitigating not only transistor switch faults but also drive related faults such as rectifier diode short-circuit fault or dc link capacitor fault. In this new approach, some of the drawbacks associated with the known fault mitigation techniques such as the need for accessibility to a motor neutral, overrating the motor to withstand higher fundamental rms current magnitudes above its rated rms level, the need for larger size dc link capacitors, or higher dc bus voltage, are overcome here using the present approach. Given in this paper is a complete set of simulation results that demonstrate the soundness and effectiveness of the present topology.",2007,0,
231,232,On the relationships of faults for Boolean specification based testing,"Various methods of generating test cases based on Boolean specifications have previously been proposed. These methods are fault-based in the sense that test cases are aimed at detecting particular types of faults. Empirical results suggest that these methods are good at detecting particular types of faults. However, there is no information on the ability of these test cases in detecting other types of faults. The paper summarizes the relationships of faults in a Boolean expression in the form of a hierarchy. A test case that detects the faults at the lower level of the hierarchy will always detect the faults at the upper level of the hierarchy. The hierarchy helps us to better understand the relationships of faults in a Boolean expression, and hence to select fault-detecting test cases in a more systematic and efficient manner",2001,0,
232,233,Error sources in in-plane silicon tuning-fork MEMS gyroscopes,"This paper analyzes the error sources defining tactical-grade performance in silicon, in-plane tuning-fork gyroscopes such as the Honeywell-Draper units being delivered for military applications. These analyses have not yet appeared in the literature. These units incorporate crystalline silicon anodically bonded to a glass substrate. After general descriptions of the tuning-fork gyroscope, ordering modal frequencies, fundamental dynamics, force, and fluid coupling, which dictate the need for vacuum packaging, mechanical quadrature, and electrical coupling are analyzed. Alternative strategies for handling these engineering issues are discussed by introducing the Systron Donner/BEI quartz rate sensor, a successful commercial product, and the Analog Device (ADXRS), which is designed for automotive applications.",2006,0,
233,234,Flexible Error Concealment for H.264 Based on Directional Interpolation,"The losses of packets cannot he avoided if real-time video is transported over error prone environments. To conceal missing parts of video pictures, the spatial and temporal correlation feature of natural video sequences is used. However, in some cases - for instance in case of a scene change - there is no temporal correlation available and thus spatial error concealment has to be used. This article proposes flexible spatial error concealment based on directional interpolation method that performs well also if only two neighboring boundaries are used as common for H.264 spatially predicted frames. The proposed method was implemented and tested in a H.264 codec together with other error concealment methods to evaluate their performance",2005,0,
234,235,Nonlinear observers with approximately linear error dynamics: the multivariable case,"Exact error linearization uses nonlinear input-output injection to design observers with linear error dynamics in certain coordinates. This approach can only be applied nongenerically. We propose an observer for a wider class of multivariable systems which uniformly minimizes the nonlinear part of the system that cannot be canceled by nonlinear input-output injection. Our approach is numerical, constructive, and provides locally exponentially stable error dynamics. An example compares our design with a high-gain method",2001,0,
235,236,Time-Varying Network Fault Model for the Design of Dependable Networked Embedded Systems,"Dependability is becoming a key design aspect of today networked embedded systems (NES's) due to their increasing application to safety-critical tasks. Dependability evaluation must be based on modelling and simulation of faulty application behaviors, which must be related to faulty NES behaviors under actual defects. However, NES's behave differently from traditional embedded systems when testing activities are performed on them. In particular, issues arise on the definition of correct behavior, on the best point to observe it, and on the temporal properties of the faults to be injected. The paper describes these issues, discusses some possible solutions and presents a new time-varying network-based fault model to represent failures in a more abstract and efficient way. Finally, the fault model has been used to support the design of a network-based control application where packet losses, end-to-end delay and signal distortion must be carefully controlled.",2009,0,
236,237,Pilot signal-based real-time measurement and correction of phase errors caused by microwave cable flexing in planar near-field tests,Millimeter and submillimeter wave receivers in scanning planar near-field test systems are commonly based on harmonic mixing and thus require at least one flexible microwave cable to be connected to them. The phase errors originated in these cables get multiplied and added to the phase of the final detected signal. A complete submillimeter setup with on-the-fly measurement of phase errors is presented. The novel phase error correction system is based on the use of a pilot signal to measure the phase errors caused by cable flexing. The measured phase error surface in the quiet-zone region of a 310 GHz compact antenna test range (CATR) based on a hologram is shown as an application example. The maximum measured phase error due to the cable within a 8090 cm<sup>2</sup> scan area was 38.,2003,0,
237,238,Research about Software Fault Injection Technology Based on Distributed System,"Firstly, the paper made a contrast between the current domestic and international research condition, and introduced the basic concept of fault injection and distributed system. secondly, it discussed the classification and requirements of fault injection. There are mainly three types of distributed fault, namely the memory fault, CPU fault and correspondence fault. Besides, it discussed the method of distributed software fault injection about DOCTOR and illustrated the comprehensive structure and its respective parts of DOCTOR in detail. Thirdly, it reached a conclusion about the fault model of distributed system of fault injection and its realization method.",2010,0,
238,239,Fault detection and location of open-circuited switch faults in matrix converter drive systems,"Matrix converter based electric vehicles can be effectively applied to military vehicles due to weight and volume reduction as well as high temperature operation with no dc-bus capacitors fragile in a harsh environment. For successful applications for military vehicle areas, satisfactory reliability issues have to be incorporated into the matrix converter drives. This paper proposes a fault diagnostic technique for detecting and locating open-circuited faults in switching components of matrix converter drive systems. In this paper, the fault-mode behaviors of the matrix converter are, in detail, explored under the open-circuited switch fault conditions. Based on the investigated knowledge of the converter behaviors, the proposed scheme enables the matrix converter drive to detect and exactly identify power switches in which open-circuited faults have occurred. The proposed fault diagnostic algorithm is based on monitoring nine voltage errors assigned to nine bi-directional switches of the matrix converter. The voltage error signals are constructed with simple comparison of measured input and output voltages. In case that any of bi-directional switches are associated with open-circuited switch faults, the dedicated voltage error signals rise over a certain threshold value, which can be possible to detect a fault occurrence and locate the faulty switch. Since the developed diagnostic method requires no construction of reference output voltages from the pulsewidth modulation (PWM) reference signals, it can be implemented with simple and robust features. Verification results are presented to demonstrate the feasibility of the proposed technique.",2009,0,
239,240,Provisioning fault-tolerant scheduled lightpath demands in WDM mesh networks,"In this paper, we consider the problem of routing and wavelength assignment (RWA) of fault-tolerant scheduled lightpath demands (FSLDs) in all optical wavelength division multiplexing (WDM) networks under single component failure. In scheduled traffic demands, besides the source, destination, and the number of lightpath demands between a node-pair, their set-up and tear-down times are known, in this paper, we develop integer linear programming (ILP) formulations for dedicated and shared scheduled end-to-end protection schemes under single link/node failure for scheduled traffic demand with two different objective functions: 1) minimize the total capacity required for a given traffic demand while providing 100% protection for all connections; and 2) given a certain capacity, maximize the number of demands accepted while providing 100% protection for accepted connections. The ILP solutions schedule both the primary and end-to-end protection routes and assign wavelengths for the duration of the traffic demands. As the time disjointness that could exist among fault-tolerant scheduled lightpath demands is captured in our formulations, it reduces the amount of global resources required. The numerical results obtained from CPLEX indicate that dedicated scheduled (with set-up and tear-down times) protection provides significant savings (up to 33 %) in capacity utilization over dedicated conventional (without set-up and tear-down times) end-to-end protection scheme; shared scheduled protection provides considerable savings (up to 21 %) in capacity utilization over shared conventional end-to-end protection schemes. Also the numerical results indicate that shared scheduled protection achieves the best performance followed by dedicated scheduled protection scheme, and shared conventional end-to-end protection in terms of the number of requests accepted, for a given network capacity.",2004,0,
240,241,Induced error-correcting code for 2 bit-per-cell multi-level DRAM,"Traditionally, memories employ SEC-DED (Single Error Correcting and Double Error Detecting) Error Correcting Codes (ECC). While such codes have been considered for MLDRAM (Multi-Level Dynamic Random Access Memory), their use is inefficient, due to likely double-bit errors in a single cell. For this reason we propose an induced ECC architecture that uses ECC in such a way that no common error corrupts two bits. Induced ECC allows significant increase in reliability of the MLDRAM",2001,0,
241,242,Design of Timing Error Detectors for Orthogonal Space-Time Block Codes,"We present a method for the design of low complexity timing error detectors in orthogonal space-time block coding (OSTBC) receivers. A general expression for the S-curve of timing error detectors is derived. Based on this result, we obtain sufficient conditions for a difference of threshold crossings timing estimate that is robust to channel fading. A number of timing error detectors for 3- and 4-transmit antenna codes are presented. The performance is evaluated by examining their tracking capabilities within a timing loop of an OSTBC receiver. Symbol-error-rate results are presented showing negligible loss due to timing synchronization. In addition, we study the performance as a function of the timing drift and show that the receiver is able to track up to the normalized timing drift bandwidth of 0.001",2006,0,
242,243,Multi-Agent Fault Diagnosis in Manufacturing Systems Using Soft Computing,"The expeditious and accurate diagnosis of faults in manufacturing systems is essential in order to avoid expensive downtime. Many artificial intelligence approaches to automated fault diagnosis use techniques that are too computationally complex to achieve a diagnosis in real-time or are too inflexible for dynamic systems. Other approaches use either structural or symptom-based reasoning. Functional approaches are unable to provide real-time response due to their computational complexity, whereas, symptom-based approaches are only able to handle situations specifically coded in rules. Current hybrid approaches that combine the two methods are too structured in their approach to switching between the reasoning methods and, therefore fail to provide the flexible, rapid response of humans experts. This paper presents a robust, extensible approach to fault diagnosis that allows unstructured switching between reasoning methods using multiple fuzzy intelligent agents that examine the problem domain from a variety of perspectives.",2007,0,
243,244,Spatial error concealment algorithm based on improved SUSAN operator,"In the transmission of real-time video compressed streams, error concealment method is to restore the damaged or lost data packets. This paper improves the existing spatial error concealment algorithm based on SUSAN detection operator. On the one hand, as recovering the error, the detection pixels were reduced by considering the relationship of nearby pixels. On the other hand, more associated pixels were fully considered. The experimental results show that the proposed algorithm enhances the Peak Signal to Noise Ratio in the case of reducing 4%-8% computational complexity, and is more beneficial to real-time application.",2010,0,
244,245,"Impact of Channel Errors on Decentralized Detection Performance of Wireless Sensor Networks: A Study of Binary Modulations, Rayleigh-Fading and Nonfading Channels, and Fusion-Combiners","We provide new results on the performance of wireless sensor networks in which a number of identical sensor nodes transmit their binary decisions, regarding a binary hypothesis, to a fusion center (FC) by means of a modulation scheme. Each link between a sensor and the fusion center is modeled independent and identically distibuted (i.i.d.) either as slow Rayleigh-fading or as nonfading. The FC employs a counting rule (CR) or another combining scheme to make a final decision. Main results obtained are the following: 1) in slow fading, a) the correctness of using an average bit error rate of a link, averaged with respect to the fading distribution, for assessing the performance of a CR and b) with proper choice of threshold, on/off keying (OOK), in addition to energy saving, exhibits asymptotic (large number of sensors) performance comparable to that of FSK; and 2) for a large number of sensors, a) for slow fading and a counting rule, given a minimum sensor-to-fusion link SNR, we determine a minimum sensor decision quality, in order to achieve zero asymptotic errors and b) for Rayleigh-fading and nonfading channels and PSK (FSK) modulation, using a large deviation theory, we derive asymptotic error exponents of counting rule, maximal ratio (square law), and equal gain combiners.",2008,0,
245,246,A signature-based approach for diagnosis of dynamic faults in SRAMs,"This paper focuses on diagnosis of dynamic faults in SRAMs. The current techniques for fault diagnosis are mainly based on the signature method. Here, we introduce an extension of the signature scheme by taking in account additional information related to the addressing order during March test execution. A first advantage of the proposed approach is its capability to distinguish between static and dynamic faults. Another main feature is the correct identification of the location of the failure in a given memory component: the core-cell array, write drivers, sense amplifiers, address decoders and pre- charge circuits. Moreover, since this approach does not modify the March test, there is no increase of test complexity, conversely to other existing diagnosis techniques.",2008,0,
246,247,Defect tolerance for gracefully-degradable microfluidics-based biochips,"Defect tolerance is an important design consideration for microfluidics-based biochips that are used for safety-critical applications. We propose a defect tolerance methodology based on graceful degradation and dynamic reconfiguration. We first introduce tile-based biochip architecture, which is scalable for large-scale bioassays. A clustered defect model is used to evaluate the graceful degradation method for tile-based biochips. The proposed schemes ensure that the bioassays mapped to a droplet-based microfluidic array during design can be executed on a defective biochip through operation rescheduling and/or resource rebinding. Real-life biochemical procedures, namely polymerase chain reaction (PCR) and multiplexed in-vitro diagnostics on human physiological fluids, are used to evaluate the proposed defect tolerance schemes.",2005,0,
247,248,"Decoding of the (24, 12, 8) extended golay code up to four errors","A new decoder is proposed to decode the (24, 12, 8) binary extended Golay code up to four errors. It consists of the conventional hard decoder for correcting up to three errors, the detection algorithm for four errors and the soft decoding for four errors. For a weight-4 error in a received 24-bit word, Method 1 or 2 is developed to determine all six possible error patterns. The emblematic probability value of each error pattern is then defined as the product of four individual bit-error probabilities corresponding to the locations of the four errors. The most likely one among these six error patterns is obtained by choosing the maximum of the emblematic probability values of all possible error patterns. Finally, simulation results of this decoder in additive white Gaussian noise show that at least 93% and 99% of weight-4 error patterns that occur are corrected if the two E<sub>b</sub>/N<sub>0</sub> ratios are greater than 2 and 5 dB, respectively. Consequently, the proposed method can achieve a better percentage of successful decoding for four errors at variable signal-to-noise ratios than Lu et al.'s algorithm in software. However, the speed of the method is slower than Lu et al.'s algorithm.",2009,0,
248,249,Joint Generalized Antenna Combination and Symbol Detection Based on Minimum Bit Error Rate: A Particle Swarm Optimization Approach,"In order to reduce hardware cost and achieve superior performance in multi-input multi-output (MIMO) systems, this paper proposes a novel scheme for joint antenna combination and symbol detection. More specifically, the new approach simultaneously determines the transformation weighting for antenna combination to lower the RF chains called for and to design the minimum bit error rate (MBER) detector to effectively mitigate the impairment due to interference. The joint decision statistic, however, is highly nonlinear and the particle swarm optimization (PSO) algorithm is employed to reduce the computational overhead. Conducted simulation results show that the new approach yields satisfactory performance with reduced computational overhead compared with pervious works.",2008,0,
249,250,Finite Element Analysis of Switched Reluctance Motor under Dynamic Eccentricity Fault,"This paper describes the results of a two-dimensional finite element analysis carried out on an 8/6 switched reluctance motor for studying the effects of dynamic eccentricity on the static characteristics of the motor. Flux contours, flux-linkage profiles and mutual fluxes are obtained for both healthy and faulty motor. Besides, Static torque profiles of phases are obtained for different degrees of eccentricity and it is shown that at low current; the effect of eccentricity is considerable compared to that of the rated current case. Finally, Fourier analysis of the torque profiles is performed to make their difference visible.",2006,0,
250,251,Effective congestion and error control for scalable video coding extension of the H.264/AVC,"We present an effective congestion and error control mechanism for scalable video coding (SVC) extension of the H.264/AVC video dissemination over Internet. The congestion control is used to determine the appropriate number of SVC video layers based on bandwidth inference congestion (BIC) control protocol for layered multicast scenarios and the error control is achieved by unequal forward error correction (FEC) layered protection using block erasure coding. Through the real Internet streaming experiments, we demonstrate the effectiveness of the proposed layered SVC delivery, in terms of subscription layer, average packet loss rate and PSNRs, under several layered-definition scalabilities.",2008,0,
251,252,Routability estimation of FPGA-based fault injection,"In the past years various approaches to hardware-based fault injection using FPGA-based hardware have been presented. Some approaches insert additional functions at the fault location (any location in the circuit, e.g. I/Os of components or their interconnection nets), while others utilize the reconfigurability of FPGAs. A common feature of each of these methods is the execution of hardware-based fault simulation using the stuck-at fault model at gate level. The expansion of a circuit by insertion of additional functions at the fault location constitutes an overhead of FPGA resources. An optimized mapping of the circuit into an FPGA and a routable placement in the FPGA is difficult to achieve due to the generation of additional functions at the fault locations. Therefore, an optimized assignment of the fault locations to the FPGA-resources (configurable logic blocks, look-up tables, I/O blocks, etc.) precedes and thereby guarantees the mapping and routability of very large circuits in an acceptable runtime. In this paper an approach to node assignment is introduced, which achieves a reduction in FPGA overhead as well as routability of the expanded circuit in a minimal runtime.",2003,0,
252,253,A Research on I.C. Engine Misfire Fault Diagnosis Based on Rough Sets Theory and Neural Network,"A method for diagnosis of misfire fault in internal combustion engine based on exhaust density of HC, CO2, O2 and the engines work parameters are presented in this paper. Rough sets theory is used to simplify attribute parameter reflecting exhaust emission and conditions of internal combustion engine and in which unnecessary properties are eliminated. The engines work parameters, exhaust emission with misfire fault and without fault are tested by the experimentation of CA6100 engine. A diagnosis model which describing the relationship between the misfire degree and the internal combustion engines exhaust emission and work parameters is established based on rough sets theory and RBF neural network. The model reduces the sample size, optimizes the neural network, increase the diagnosis correctness. The model is also trained by test data and MATLAB software. The model has been used to diagnosis internal combustion engine misfire fault, the result illustrates that this diagnosis model is suitable. This system can reduce input node number and overcome some shortcomings, such as neural network scale is too large and the rate of classification is slow.",2010,0,
253,254,High-Intensity Radiated Field fault-injection experiment for a fault-tolerant distributed communication system,"Safety-critical distributed flight control systems require robustness in the presence of faults. In general, these systems consist of a number of input/output (I/O) and computation nodes interacting through a fault-tolerant data communication system. The communication system transfers sensor data and control commands and can handle most faults under typical operating conditions. However, the performance of the closed-loop system can be adversely affected as a result of operating in harsh environments. In particular, High-Intensity Radiated Field (HIRF) environments have the potential to cause random fault manifestations in individual avionic components and to generate simultaneous system-wide communication faults that overwhelm existing fault management mechanisms. This paper presents the design of an experiment conducted at the NASA Langley Research Center's HIRF Laboratory to statistically characterize the faults that a HIRF environment can trigger on a single node of a distributed flight control system.",2010,0,
254,255,On-line fault diagnosis in a Petri Net framework,"The paper addresses the fault detection problem for discrete event systems modeled by Petri nets (PN). Assuming that the PN structure and initial marking are known, faults are modeled by unobservable transitions. The paper recalls a previously proposed diagnoser that works online and employs an algorithm based on the definition and solution of some integer linear programming problems to decide whether the system behavior is normal or exhibits some possible faults. To reduce the on-line computational effort, we prove some results showing that if the unobservable subnet enjoys suitable properties, the algorithm solution may be obtained with low computational complexity. We characterize the properties that the PN modeling the system fault behavior has to fulfill and suitably modify the proposed diagnoser.",2009,0,
255,256,The prediction of fault currents in a large multiwinding reactor transformer,"The fault currents occurring in power transformers are determined by the leakage reactances within the windings. Where the transformers are used in a phase-shifting mode, there is additional coupling between phases which influences the fault current. This work describes the modelling of the self and mutual inductances in a 90 MVA autotransformer with a tertiary winding, on the assumption that the airgap formed by the transformer window dictates the reluctance of the leakage flux paths. Recordings made during a short-circuit between two phases of the tertiary winding show a remarkably close comparison with the predicted waveforms.",2003,0,
256,257,How Long Will It Take to Fix This Bug?,"Predicting the time and effort for a software problem has long been a difficult task. We present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. Our technique leverages existing issue tracking systems: given a new issue report, we use the Lucene framework to search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach using effort data from the JBoss project. Given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating naive predictions by a factor of four.",2007,0,
257,258,Data reduction and clustering techniques for fault detection and diagnosis in automotives,"In this paper, we propose a data-driven method to detect anomalies in operating Parameter Identifiers (PIDs) and in the absence of any anomaly, classify faults in automotive systems by analyzing PIDs collected from the freeze frame data. We first categorize the operating parameter data using automotive domain knowledge. The dataset thus obtained is then analyzed using Principal Component Analysis (PCA) and Independent Component Analysis (ICA) for finding coherence among the PIDs. Then we use clustering algorithms based on both linear distance and information theoretic measures to assign coherent PIDs to the same class or cluster. A comparative analysis of the behavior of PIDs belonging to the same cluster can now be made for detecting anomaly in PIDs. Since a system fault is characterized by the values by of all PIDs across all the clusters, we use the joint probability distribution of the independent components of all PIDs to characterize the fault and find the divergence between the joint distributions of training and test data to classify faults. The proposed method can analyze available parameter data, categorize PIDs into informative or non-informative category, and detect fault condition from the clusters. We demonstrate the algorithm by way of an application to operating parameter data collected during faults in catalytic converters of vehicles.",2010,0,
258,259,Effect of atmospheric correction for different land use on Landsat 7 ETM+ satellite imagery,"Various changes in the atmosphere of the earth and different illuminations resulting from rough terrain change the spectral reflection values of satellite images. Studies making use of real reflection values belonging to the object will provide more accurate data. The atmospheric correction process to be applied in this study is used to prevent the negative effects resulting from atmosphere and different illuminations in order to represent the reflections from the ground on the image in the best way possible. Using atmospheric correction, differentiations in reflection values sensed by different sensors or platforms resulting from atmosphere and some technical problems will be prevented. In this study, the aim is to determine the changes in the spectral reflection values concerning land use following the atmospheric correction to be applied on Landsat image data. For this reason, atmospheric correction was applied on Landsat image data. The relations of each band with each other before and after the correction were determined. The changes between spectral reflection values of all bands before and after correction regarding three different land uses as forest, agricultural area and residential area were examined visually and statistically.",2009,0,
259,260,An Efficient Fault Tolerance Scheme for Preventing Single Event Disruptions in Reconfigurable Architectures,"Reconfigurable architectures are becoming increasingly popular with space related design engineers as they are inherently flexible to meet multiple requirements and offer significant performance and cost savings for critical applications. As the microelectronics industry has advanced, integrated circuit (IC) design and reconfigurable architectures (FPGAs, reconfigurable SoC and etc) have experienced dramatic increase in density and speed. These advancements have serious implications for the reconfigurable architectures when used in space environment where IC is subject to total ionization dose (TID) and single event effects as well. Due to transient nature of single event upsets (SEUs), these are most difficult to avoid in space-borne reconfigurable architectures. We present a unique SEU fault tolerance technique based upon double redundancy with comparison to overcome the overheads associated with the conventional schemes",2006,0,
260,261,Anshan: Wireless Sensor Networks for Equipment Fault Diagnosis in the Process Industry,"Wireless sensor networks provide an opportunity to enhance the current equipment diagnosis systems in the process industry, which have been based so far on wired networks. In this paper, we use our experience in the Anshan Iron and Steel Factory, China, as an example to present the issues from the real field of process industry, and our solutions. The challenges are three fold: First, very high reliability is required; second, energy consumption is constrained; and third, the environment is very challenging and constrained. To address these issues, it is necessary to put systematic efforts on network topology and node placement, network protocols, embedded software, and hardware. In this paper, we propose two technologies i.e. design for reliability and energy efficiency (DRE), and design for reconfiguration (DRC). Using these techniques we developed Anshan, a wireless sensor network for monitoring the temperature of rollers in a continuously annealing line and detecting equipment failures. Project Anshan includes 406 sensor nodes and has been running for four months continuously.",2008,0,
261,262,Heterogeneous Error Protection of H.264/AVC Video Using Hierarchical 16-QAM,"Heterogeneous error protection (HEP) of H.264/AVC coded video is investigated using hierarchical quadrature amplitude modulation (HQAM), which takes into consideration the non- uniformly distributed importance of intracoded frame (I-frame) and predictive coded frame (P-frame) as well as the sensitivity of the coded bitsream against transmission errors. The HQAM constellation are used to give different degrees of error protection of the most important information of the video content. The performance of the transmission system is evaluated under additive Gaussion Noise (AWGN). The simulation results indicate that the strategy produces a high quality of the reconstructed video data compared with uniform protection.",2009,0,
262,263,A design of the novel coupled line bandpass filter using defected ground structure,"In this paper, a novel coupled line bandpass filter with a DGS (Defected Ground Structure) is proposed to realize a compact size with low insertion loss characteristic. The proposed bandpass filter can provide an attenuation pole due to the resonance characteristic of the DGS. The equivalent circuit parameters for the DGS are extracted by using an EM simulation process and the circuit analysis method. The design method for the proposed 3-pole bandpass filter is derived based on coupled line filter theory and the derived equivalent circuit of the DGS. The experimental results show an excellent agreement with theoretical simulation results.",2000,0,
263,264,The use of characteristic features of wireless cellular networks for transmission of GNSS assistance and correction data,"Precise Global Navigation Satellite System (GNSS) positioning using Real Time Kinematics (RTK) correction data is currently utilized in many fields of surveying, mapping and precision agriculture. In the near future, sub decimeter precision data usage is expected to extend to autonomous vehicles navigation and public safety areas. To satisfy this increasing demand of precision positioning correction bandwidth, new techniques and protocols in assistance and correction data transmission are needed. This paper reviews one such possible technique involving sending correction dataset via public wireless cellular networks. The data will be transmitted through a hybrid system integrating correction data broadcasted in the wireless cellular network control plane with AGNSS assistance data and correction metadata in the user plane. Through this system, the bandwidth intensive, low refresh rate data of GNSS system ephemeris, reference station and satellite identification is omitted from the main data stream. Instead, a constant bit rate (CBR) stream for correction data is used and bandwidth is conserved. The results show that the proposed system can achieve scalability required for widespread usage of sub decimeter level positioning data from GNSS.",2010,0,
264,265,An on-line monitoring and multi-layer fault diagnosis system of electrical equipment based on geographic information system,"Automated mapping/facilities management/geographic information system (AM/FM/GIS), which provides a powerful way to process graphic and non-graphic information, can construct a spatial database system with topological structure and analysis function by combining diversified information in power system with geographic position-related graphic information. Based on the AM/FM/GIS and on-line monitoring system, an integrated system is put forward which can implement state monitoring, multi-layer fault diagnosis and assess the faults. By using this integrated system, latent fault and defect can be eliminated, loss due to power cut is reduced and the reliability of running power system is improved. Application indicates it is economical, pragmatic and has excellent performance.",2005,0,
265,266,Coseismic fault rupture detection and slip measurement by ASAR precise correlation using coherence maximization: application to a north-south blind fault in the vicinity of Bam (Iran),"Using the phase differences between satellite radar images recorded before and after an earthquake, interferometry allows mapping the projection along the line of sight (LOS) of the ground displacement. Acquisitions along multiple LOS theoretically allow deriving the complete deformation vector; however, due to the orbit inclination of current radar satellites, precision is poor in the north-south direction. Moreover, large deformation gradients (e.g., fault ruptures) prevent phase identification and unwrapping and cannot be measured directly by interferometry. Subpixel correlation techniques using the amplitude of the radar images allow measuring such gradients, both in slant-range and in azimuth. In this letter, we use a correlation technique based on the maximization of coherence for a radar pair in interferometric conditions, using the complex nature of the data. In the case of highly coherent areas, this technique allows estimating the relative distortion between images. Applied to ASAR images acquired before and after the December 26, 2003 Bam earthquake (Iran), we show that the near-field information retrieved by this technique is useful to constrain geophysical models. In particular, we confirm that the major gradients of ground displacement do not occur across the known fault scarp but approximately 3 km west of it, and we also estimate directly the amplitude of right lateral slip, while retrieving this value from interferometry requires passing through the use of a model for the earthquake fault and slip.",2006,0,
266,267,Reducing cost and tolerating defects in page-based intelligent memory,"Active Pages is a page-based model of intelligent memory specifically designed to support virtualized hardware resources. Previous work has shown substantial performance benefits from off loading data-intensive tasks to a memory system that implements Active Pages. With a simple VLIW processor embedded near each page on DRAM, Active Page memory systems achieve up to 1000X speedups over conventional memory systems. In this study, we examine Active Page memories that share, or multiplex, embedded VLIW processors across multiple physical Active Pages. We explore the trade-off between individual page-processor performance and page-level multiplexing. We find that hardware costs of computational logic can be reduced from 31% of DRAM chip area to 12%, through multiplexing, without significant loss in performance. Furthermore, manufacturing defects that disable up to 50% of the page processors can be tolerated through efficient resource allocation and associative multiplexing",2000,0,
267,268,A defect-to-yield correlation study for marginally printing reticle defects in the manufacture of a 16Mb flash memory device,"This paper presents a defect-to-yield correlation for marginally printing defects in a gate and a contact 4X DUV reticle by describing their respective impact on the lithography manufacturing process window of a 16Mb flash memory device. The study includes site-dependent sort yield signature analysis within the exposure field, followed by electrical bitmap and wafer strip back for the lower yielding defective sites. These defects are verified using both reticle inspection techniques and review of printed resist test wafers. Focus/exposure process windows for defect-free feature and defective feature are measured using both in-line SEM CD data and defect printability simulation software. These process window models are then compared against wafer sort yield data for correlation. A method for characterizing the lithography manufacturing process window is proposed which is robust to both marginally printing reticle defects and sources of process variability outside the lithography module",2000,0,
268,269,PEDS: A Parallel Error Detection Scheme for TCAM Devices,"Ternary content-addressable memory (TCAM) devices are increasingly used for performing high-speed packet classification. A TCAM consists of an associative memory that compares a search key in parallel against all entries. TCAMs may suffer from error events that cause ternary cells to change their value to any symbol in the ternary alphabet ""0"",""1"",""*"". Due to their parallel access feature, standard error detection schemes are not directly applicable to TCAMs; an additional difficulty is posed by the special semantic of the ""*"" symbol. This paper introduces PEDS, a novel parallel error detection scheme that locates the erroneous entries in a TCAM device. PEDS is based on applying an error-detection code to each TCAM entry, and utilizing the parallel capabilities of the TCAM, by simultaneously checking the correctness of multiple TCAM entries. A key feature of PEDS is that the number of TCAM lookup operations required to locate all errors depends on the number of symbols per entry rather than the (orders-of-magnitude larger) number of TCAM entries. For large TCAM devices, a specific instance of PEDS requires only 200 lookups for 100-symbol entries, while a naive approach may need hundreds of thousands lookups. PEDS allows flexible and dynamic selection of trade-off points between robustness, space complexity, and number of lookups.",2009,0,
269,270,Noise identification and fault diagnosis for the new products of the automobile gearbox,"A noise identification and fault diagnosis system for the new products of the automobile gearbox is introduced. The framework of the developed software is described, which includes function modules as data acquisition, feature extracting, time frequency transform, order analysis, learning and training, and so on. The prototype system has been partially put in practice in a certain automobile gear-box manufacture company.",2009,0,
270,271,Soft Defects: Challenge and Chance for Failure Analysis,"Failure analysis on advanced logic and mixed signal ICs more and more has to deal with so called 'soft defects'. In this paper, an analysis flow especially for parameter dependent scan fails is presented. For the two major localization techniques, namely soft defect localization (SDL) and internal signal measurement enhanced activation and localization procedures using test systems are proposed.",2007,0,
271,272,Layout to Logic Defect Analysis for Hierarchical Test Generation,"As shown by previous studies, shorts between the interconnect wires should be considered as the predominant cause of failures in CMOS circuits. Fault models and tools for targeting these defects, such as the bridging fault test pattern generators have been available for a long time. However, this paper proposes a new hierarchical approach based on critical area extraction for identifying the possible shorted pairs of nets on the basis of the chip layout information, combined with logic-level test pattern generation for bridging faults. Experiments on real design layouts will show that only a fraction of all the possible pairs of nets have non-zero shorting probabilities. Furthermore, it will also be proven at the logic-level that nearly all such bridging faults can be tested by a simple and robust one-pattern logic test. The methods proposed in this paper are supported by a design flow implementing existing commercial and academic CAD software.",2007,0,
272,273,A Fault Propagation Approach for Highly Distributed Service Compositions,"Today, the techniques for realizing service compositions (e.g. WS-BPEL) have become mature. Nevertheless, when it comes to execution faults within service compositions, many problems are still unsolved. Especially the propagation and global handling of errors in service compositions yet remains an open issue. In this paper, we describe some preliminary results of our ongoing work in the field of fault propagation and exception handling in service compositions. We provide some service classification criteria and show how they relate to service composition fault handling. Further, we present a fault propagation approach for service compositions.",2008,0,
273,274,Using design based binning to improve defect excursion control for 45nm production,"For advanced device (45 nm and below), we proposed a novel method to monitor systematic and random excursion. By integrating design information and defect inspection results into automated software (DBB), we can identify design/process marginality sites with defect inspection tool. In this study, we applied supervised binning function (DBC) and defect criticality index (DCI) to identify systematic and random excursion problems on 45 nm SRAM wafers. With established SPC charts, we will be able to detect future excursion problem in manufacturing line early.",2007,0,
274,275,Comparison of Outlier Detection Methods in Fault-proneness Models,"In this paper, we experimentally evaluated the effect of outlier detection methods to improve the prediction performance of fault-proneness models. Detected outliers were removed from a fit dataset before building a model. In the experiment, we compared three outlier detection methods (Mahalanobis outlier analysis (MOA), local outlier factor method (LOFM) and rule based modeling (RBM)) each applied to three well-known fault-proneness models (linear discriminant analysis (LDA), logistic regression analysis (LRA) and classification tree (CT)). As a result, MOA and RBM improved Fl-values of all models (0.04 at minimum, 0.17 at maximum and 0.10 at mean) while improvements by LOFM were relatively small (-0.01 at minimum, 0.04 at maximum and 0.01 at mean).",2007,0,
275,276,Algorithm-based fault tolerance for spaceborne computing: basis and implementations,"We describe and test the mathematical background for using checksum methods to validate results returned by a numerical subroutine operating in a fault-prone environment that causes unpredictable errors in data. We can treat subroutines whose results satisfy a necessary condition of a linear form; the checksum tests compliance with this necessary condition. These checksum schemes are called algorithm-based fault tolerance (ABFT). We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent in finite-precision numerical calculations. Two series of tests are described. The first tests the general effectiveness of the linear ABFT schemes we propose, and the second verifies the correct behavior of our parallel implementation of them. We find that under simulated fault conditions, it is possible to choose a fault detection scheme that for average case matrices can detect 99% of faults with no false alarms, and that for a worst-case matrix population can detect 80% of faults with no false alarms",2000,0,
276,277,Bug busters,"One way to deal with bugs is to avoid them entirely. The approach would be wasteful because we'd be underutilizing the many automated tools and techniques that can catch bugs for us. Most tools for eliminating bugs work by tightening the specifications of what we build. At the program code level, tighter specifications affect the operations allowed on various data types, our program's behavior, and our code's style. Furthermore, we can use many different approaches to verify that our code is on track: the programming language, its compiler, specialized tools, libraries, and embedded tests are our most obvious friends. We can delegate bug busting to code. Many libraries come with hooks or specialized builds that can catch questionable argument values, resource leaks, and wrong ordering of function calls. Bugs many be a fact of life, but they're not inevitable. We have some powerful tools to find them before they mess with our programs, and the good news is that these tools get better every year.",2006,0,
277,278,Design and validation of portable communication infrastructure for fault-tolerant cluster middleware,"We describe the communication infrastructure (CI) for our fault-tolerant cluster middleware, which is optimized for two classes of communication: for the applications and for the cluster management middleware. This CI was designed for portability and for efficient operation on top of modern user-level message passing mechanisms. We present a functional fault model for the CI and show how platform-specific faults map to this fault model. Based on this fault model, we have developed a fault injection scheme that is integrated with the CI and is thus portable across different communication technologies. We have used fault injection to validate and evaluate the implementation of the CI itself as well as the cluster management middleware in the presence of communication faults.",2002,0,
278,279,Effect of rotor position error on commutation in sensorless BLDC motor drives,"In this paper, two kinds of commutation modes of the brushless DC(BLDC) motor drives, the delaying commutation and the leading commutation, are discussed in detail. The current of the unexcited phase is calculated under an ideal operation condition, and the condition of circulating current occurring is analyzed. The result with the compensated commutation is provided. The theoretical analysis is confirmed by the experiment results.",2005,0,
279,280,A Fault-Tolerant Active Pixel Sensor to Correct In-Field Hot-Pixel Defects,"Solid-state image sensors develop in-field defects in all common environments. Experiments have demonstrated the growth of significant quantities of hot-pixel defects that degrade the dynamic range of an image sensor and potentially limit low-light imaging. Existing software- only techniques for suppressing hot-pixels are inadequate because these defective pixels saturate at relatively low illumination levels. The redundant fault-tolerant active pixel sensor design is suggested to isolate point-like hot-pixel defects. Emulated hot-pixels have been induced in hardware implementations of this pixel architecture and measurements of pixel response indicate that it generates an accurate output signal throughout the sensor's entire dynamic range, even when standard pixels would be otherwise saturated by the hot defect. A correction algorithm repairs the final image by building a simple look-up table of illumination- response of a working pixel. In emulated hot-pixels, the true illumination value can be recovered with an error of plusmn5% under typical conditions.",2007,0,
280,281,Automatic error recovery in targetless logic emulation,"Targetless logic emulation refers to a verification system in which there are no external hardware targets interfacing with the emulator. In such systems input stimuli to the DUT come either from a user provided vector file or a HDL testbench running on a software simulator and the DUT runs on hardware based logic emulator. Many users use such targetless environment for automated long running verification tests consisting of huge sets of input stimuli, consequently an automatic recovery method is of significant interest in such systems. The automatic error recovery method shall be able to complete the emulation session gracefully skipping error points and subsequently report various errors and mismatch conditions for user debug. The paper presents a novel methodology and verification infrastructure based on periodic checkpointing, which provides a robust way of error condition detection, subsequent restoration of last saved system state and resume emulation run by skipping offending operations. It does not require any special hardware extension and provides a fully customizable checkpoint frequency selection scheme. It is seen to add only a minimal overhead on overall hardware emulation speed.",2009,0,
281,282,Weather radar equation correction for frequency agile and phased array radars,"This paper presents the derivation of a correction to the Probert-Jones weather radar equation for use with advanced frequency agile, phased array radars. It is shown that two additional terms are required to account for frequency hopping and electronic beam pointing. The corrected weather radar equation provides a basis for accurate and efficient computation of a reflectivity estimate from the weather signal data samples. Lastly, an understanding of calibration requirements for these advanced weather radars is shown to follow naturally from the theoretical framework.",2007,0,
282,283,Waveform analysis of the bridge type SFCL during load changing and fault time,"DC reactor type superconducting fault current limiter (SFCL) has drawn the interest of some researchers in developing such device and more research work is being carried out in order to make it practically feasible. We have pointed out one issue that is not properly examined yet on such a device during load changing time. As we know, it is very difficult to introduce DC bias voltage to the reactor coil of the bridge type SFCL and some researchers are developing such device without using DC bias current. In such a case, the voltage drop occurs at the load terminal during the load increasing time caused by the DC reactor's inductance. By using the Electro-Magnetic Transients in DC systems which is the simulator of electric networks (EMTDC) software we carried out analysis of first few half cycles of the voltage and current waveforms after the load is increased. We also performed the same analysis for fault conditions. The peak value of the waveforms is considered in calculating the voltage drop at load terminal during the load changing time. The analysis can be used in selecting an appropriate inductance value for designing such SFCL.",2003,0,
283,284,DuoTracker: Tool Support for Software Defect Data Collection and Analysis,"In today software industry defect tracking tools either help to improve an organizationAs software development process or an individualAs software development process. No defect tracking tool currently exists that help both processes. In this paper we present DuoTracker, a tool that makes possible to track and analyze software defects for organizational and individual software process decision making. To accomplish this, DuoTracker has capabilities to classify defects in a manner that makes analysis at both organizational and individual software processes meaningful. The benefit of this approach is that software engineers are able to see how their personal software process improvement impacts their organization and vice versa. This paper shows why software engineers need to keep track of their program defects, how this is currently done, and how DuoTracker offers a new way of keeping track of software errors. Furthermore, DuoTracker is compared to other tracking tools that enable software developers to record program defects that occur during their individual software processes.",2006,0,
284,285,Investigation of the effects of transmission faults upon a renewable energy generating plant,"In recent years the number of renewable energy generators connected to Ireland's electricity grid has steadily increased. The Republic of Ireland is now expected to source 13.2% of the electricity it consumes from renewables by 2010, which represents a significant challenge to the electricity system operators and planners. This paper describes the modelling and simulation of a small hybrid wind/hydro generating plant connected to the distribution network. The effects upon the plant of transmission network faults and continuous voltage unbalance are investigated.",2005,0,
285,286,Detection of defects in wood slabs by using a microwave imaging technique,"In this paper, an experimental set up based on interrogating microwaves is used to obtain images of the cross section of dielectric cylinders. In particular, a microwave tomographic configuration is used to inspect wood slabs in order to search for defects and voids. The measured data (samples of the scattered electric field) are inverted by using an efficient reconstruction technique, which is able to handle the ill-posedness of the inverse scattering problem. The developed experimental apparatus is validated in this paper by means of several numerical simulations. Preliminary experimental results are also reported.",2007,0,
286,287,Influence of the Transmission Channel Parameters on Error Rates and Picture Quality in DVB Baseband Transmission,"The paper deals with the component analysis of DVB (digital video broadcasting) transmission model in baseband and its source, channel and link coding. The transmission channel model is based on the digital filter design and it can be designed with the variable transmission parameters (e.g. cut-off frequency) and linear distortions with additive noise and reflected signal. Results of achieved BER (bit error rate) and SER (symbol error rate) and corresponding PQE (picture quality evaluation) analysis are presented, including the evaluation of subjective picture quality influence on normalized cut-off frequency of the channel",2006,0,
287,288,Rotor position sensor fault detection Isolation and Reconfiguration of a Doubly Fed Induction Machine control,"In this paper, a Doubly Fed Induction Machine (DFIM) operating in motor mode and supplied by two Voltage Source Inverters (VSI), in stator and rotor sides, is presented. The aim is to analyze the position sensor fault effects on a Direct Torque Control (DTC) of the DFIM. This justifies the necessity of a reconfiguration control when a position sensor fault appears in order to avoid an interruption in system operations. In the other hand, this study emphasizes the close dependency between system performance and the output accuracy of the rotor position sensor. Moreover, simulation results point out the operation system deterioration in case of position sensor fault, which leads in most cases to its shut down in contrast to industrial expectations. This work presents a control reconfiguration for a DFIM speed drive when a position sensor fault occurs, in order to ensure system service continuity. For this purpose, SABER simulation results illustrate the system behavior before and after a position sensor fault. System performance preservation is carried out after control reconfiguration. The proposed solution is relevant especially due to its simplicity.",2009,0,
288,289,Computation and analysis of output error probability for C17 benchmark circuit using bayesian networks error modeling,"The reliability of digital circuits is in question since the new scaled transistor technologies continue to emerge. The major factor deteriorating the circuit performance is the random and dynamic nature of errors encountered during its operation. Output-error probability is the direct measure of circuit's reliability. Bayesian networks error modeling is the approach used to compute error probability of digital circuits. In our paper, we have used this technique to compute and analyze the output error probability of LGSynth's C17 benchmark circuit. The simulations are based on MATLAB and show important relationships among output-error probability, execution time and number of priors involved in the analysis.",2010,0,
289,290,Raising network fault management intelligence,"Most large network management centers have relatively low skilled personnel as their first level operations staff. Many organizations attempt to cope with this situation by restricting the set of problems these people have to deal with to those which are well understood and documented. Several software packages exist which can correlate and filter incoming events from the network and present a select subset to the operator. Unfortunately, programming these fault management applications requires considerable expertise and effort. Often, once the initial development is done, the implementation remains static, while the network itself is dynamic. This paper proposes a methodology for documenting known faults and responses, programming fault correlation engines, continuously examining real behavior, and feeding the result back into the programming process. This results in a continuous improvement in fault management intelligence, with corresponding improvement in network availability and thus value of the network to the organization",2000,0,
290,291,Analysis of Timing Error Aperture Jitter on the Performance of Sigma Delta ADC for Software Radio Mobile Receivers,"Jitter is the limiting effect for high speed analog-to digital converters with high resolution and wide digitization bandwidth, which are required in receivers in order to support high data rates. The rapid development of digital wireless system has led to a need of high resolution and high speed analog to digital converter. The proper selection of data converters, both analog to digital converters and digital to analog converters (DACs) is one of the most challenging steps in designing software radio. The performance of a data converter is dependent upon the accuracy and stability of the clock supplied to the circuits. When data converter employ a high sampling rate, clocking issues become magnified and significant distortion can be result. This paper describes the effect of aperture jitter on the performance of sigma delta ADC and present analytical evaluation of the performance and mean error power spectrum due to aperture jitter application has favored the use of oversampling delta sigma ADC (analog-to-digital converters) due to their better speed-accuracy tradeoff. Delta-sigma modulator is one of the key building blocks, which can be implemented using DT (discrete-time) and CT (continuous-time) techniques. Compared to their DT counterparts, CT delta-sigma modulators have recently attracted more and more attentions due to their advantages in terms of high speed, low power, low noise and intrinsic anti-aliasing capability. In this paper, we concentrate on the discrete implementation. Section 2 presents an aperture jitter effect in SDM in terms of SNR. In the last few years different authors derived formulas to quantify the SNR limiting effect of jitter in ADCs. While Walden used a worst case approach, Kobayashi presented an exact formula which allows calculating the SNR in the presence of an aperture jitter.",2009,0,
291,292,"The Impact of Tower Shadow, Yaw Error, and Wind Shears on Power Quality in a WindDiesel System","To study the impact of aerodynamic aspects of a wind turbine (WT) (i.e., tower shadow, wind shears, yaw error, and turbulence) on the power quality of a wind-diesel system, all electrical, mechanical, and aerodynamic aspects of the WT must be studied. Moreover, the contribution of the diesel generator system and its controllers should be considered. This paper describes how the aerodynamic and mechanical aspects of a WT can be simulated using TurbSim, AeroDyn, and FAST where the electrical parts of WT, diesel generator, its controllers, and electrical loads are modeled by Simulink blocks. Simulation results obtained from the model are used to observe the power and voltage variations at the WT generator terminals under different operating conditions. Furthermore, the effects of tower shadow, wind shears, yaw error, and turbulence on the power quality in a stand-alone wind-diesel system utilizing a fixed-speed WT are studied.",2009,0,
292,293,Based on Compact Type of Wavelet Neural Network Tolerance Analog Circuit Fault Diagnosis,"Based on the classical wavelet neural network, this paper put forward a sort of improved multiple-input multiple-output compact type of wavelet neural network, adopted adaptive learning rate and additional momentum BP algorithm to carry out training, studied its tolerance analog circuit fault diagnosis applications. Simulation results displayed that the compact type of wavelet neural network learning is fast, it can be effective diagnosed and located to tolerance analog circuit fault.",2009,0,
293,294,Lab VIEW based implementation of remedial action for DC arcing faults in a spacecraft,"In this paper remedial action for DC arcing faults in spacecraft has been designed and implemented using Lab VIEW. The Lab VIEW is an innovative graphical programming system designed to facilitate computer controlled data acquisition and analysis. DC arcing faults in spacecraft has been designed and implemented using the experimental data obtained at NASA Glenn research center. It is important to keep the continuity of the power supply and at the same time increase the reliability of spacecraft energy power system. In the frequency domain, fast Fourier transformation (FFT) is used for the feature extraction of the fault signal and odd harmonics frequency components of the phase currents are analyzed.",2003,0,
294,295,A Fault Tolerant Optimization Algorithm based on Evolutionary Computation,"In this paper we describe how an evolutionary algorithm is capable of running on a distributed environment with volatile resources. When executing algorithms in a desktop computing or resource harvesting context, resources can be reclaimed by their owners without warning, which may produce data loss and process to fail. The interest of the algorithm presented in the paper is that although it doesn't keep processes from failing, or data from being lost, it does improve the quality of results because of its design, not employing any special task control, checkpoint/restart or resource redundancies. By means of a series of experiments, we test the performance of the algorithm by studying the number of process failing and the quality of solutions when compared with the classic flavor of the evolutionary algorithm. The new algorithm, which shows its advantages, therefore improve dependability of distributed system",2006,0,
295,296,Efficient Error Correcting Codes for On-Chip DRAM Applications for Space Missions,"New systematic single error correcting codes-based circuits are introduced for random access memories, with ultimate minimal encoding/decoding complexity, low power and high performance. These new, codes-based circuits can be used in combinational circuits and in on-chip random access memories of reconfigurable architectures with high performance and ultimate minimum decoding/encoding complexity. Due to the overhead of parity check bits associated with the error-correcting-codes, there has always been a demand for an efficient and compact code for small memories in terms of data width. The proposed codes give improved performance even for small memories over the other codes. Area and power comparisons have been performed to benchmark the performance index of our codes. The code-centric circuits offer significant advantages over existing error correcting codes-based circuits in the literature in terms of lower size, power and cost which make them suitable for wider range of applications such as those targeting space. The paper describes the new efficient code and associated circuits for its implementation",2005,0,
296,297,Fault Diagnosis for Engine Based on EMD and Wavelet Packet BP Neural Network,"To solve the problem of fault diagnosis for engine, due to the complexity of the equipments and the particularity of the operating environments, generally speaking, there is no one-to-one correspondence between the characteristic parameters and status, so, the methods of diagnosis are very complicated. A novel fault diagnosis method based on empirical mode decomposition (EMD) and wavelet packet BP neural network is proposed in this paper. Firstly, the given signal is analyzed by wavelet packet to remove the noise; Then the de-noised data is decomposed into a number of IMFs by EMD and extract their frequency eigenvectors, then using these eigenvectors as the training samples of the BP network, training the BP network to identify the faults. Finally, the simulation experiments shows that the proposed method for fault diagnosis of engine is effective and the de-nosing process using wavelet packet transform is essential.",2009,0,
297,298,Errors estimation and minimization for the 5-axis milling machine,"This paper presents the tool path optimization algorithms to compute and estimate the non-linear inverse kinematics errors of the 5-axis milling machine. The approach is based on a global approximation of the required surface by a virtual surface constructed from the tool trajectories. Errors are computed from the difference between the required surface and the virtual surface and displayed numerically and graphically through the virtual machine simulator. The simulator is based on 3D representation and employing the inverse kinematics approach to derive the corresponding rotational and translation movement of the mechanism. The simulator makes it possible to estimate the errors of a 3D tool-path based on a prescribed set of the cutter location (CL) points as well as a set of the cutter contact (CC) points with tool inclination angle. Errors, particularly near the vicinity of the large milling errors, are minimized using a discrete algorithm based on a shortest path strategy. Furthermore, the simulator can be used to simulate the milling process, verify the final cut of the actual tool-path before testing with the real machine. Thus, it reduces the cost of iterative trial and errors.",2002,0,
298,299,Soft error assessments for servers,"In order to assess the soft error rate (SER) of a server, it is important to not only quantify the soft error contribution of the individual semiconductor components, but also to account for derating and for SER mitigation like hardening and shielding. Derating describes the fact that not every soft error has an impact. A large number of soft errors vanish based on electrical, logical or timing considerations. They have no impact. Additionally, a server can, to a large degree, be protected from the impact of soft errors by implementing error detection and correction means. In these cases the impact of the soft error is limited to the extra compute time needed for the correction. Summing up the SER contributions from transistors and circuits results in the so-called raw soft error rate, a rate which describes just the bottom layer of the system stack. Powerful protection mechanisms at higher layers can reduce that rate by several orders of magnitude. Awareness of this vertical interaction across the different layers in the system stack leads to servers optimized for robustness.",2010,0,
299,300,On the Use of Dynamic Binary Instrumentation to Perform Faults Injection in Transaction Level Models,"Transaction Level Modelling (TLM) has been widely accepted as systems modelling framework focused in system components communication. This approach allows efficient accurate estimation and rapid design space exploration. Besides of the functional simulation for validation of a hardware/software designs, there are additional reliability requirements that need advanced simulation techniques to analyze the system behaviour in the presence of faults. Several traditional VHDL fault injection mechanisms like mutants or saboteurs have been adapted to SystemC model descriptions. The main drawback of these approaches is the necessity of source code modification to carry out the fault injection campaigns. In this paper, we propose the use of Dynamic Binary Instrumentation (DBI) to perform fault injection in SystemC TLM models. DBI is a technique to intercept software routine calls allowing argument and return value corruption and data structures modification at runtime. This technique needs neither source code modifications nor recompilation of models in order to generate module mutants or in order to insert saboteurs in the signal communication path.",2009,0,
300,301,Online Fault Diagnosis of Discrete Event Systems. A Petri Net-Based Approach,"This paper is concerned with an online model-based fault diagnosis of discrete event systems. The model of the system is built using the interpreted Petri nets (IPN) formalism. The model includes the normal system states as well as all possible faulty states. Moreover, it assumes the general case when events and states are partially observed. One of the contributions of this work is a bottom-up modeling methodology. It describes the behavior of system elements using the required states variables and assigning a range to each state variable. Then, each state variable is represented by an IPN model, herein named module. Afterwards, using two composition operators over all the modules, a monolithic model for the whole system is derived. It is a very general modeling methodology that avoids tuning phases and the state combinatory found in finite state automata (FSA) approaches. Another contribution is a definition of diagnosability for IPN models built with the above methodology and a structural characterization of this property; polynomial algorithms for checking diagnosability of IPN are proposed, avoiding the reachability analysis of other approaches. The last contribution is a scheme for online diagnosis; it is based on the IPN model of the system and an efficient algorithm to detect and locate the faulty state. Note to Practitioners-The results proposed in this paper allow: 1) building discrete event system models in which faults may arise; 2) testing the diagnosability of the model; and 3) implementing an online diagnoser. The modeling methodology helps to conceive in a natural way the model from the description of the system's components leading to modules that are easily interconnected. The diagnosability test is stated as a linear programming problem which can be straightforward programmed. Finally, the algorithm for online diagnosis leads to an efficient procedure that monitors the system's outputs and handles the normal behavior model. This provides an oppo- rtune detection and location of faults occurring within the system",2007,0,
301,302,Impact of correlation errors on the optimum Kalman filter gain identification in a single sensor environment,"The impact of errors in the innovation correlation functions evaluation, related to the suboptimal filter, on the identification of the optimum steady state Kalman filter gains are investigated. This issue arises in all real time applications, where the correlations must be calculated from experimental data. An identification algorithm proposed in the literature, with formal proof of convergence, is revisited and summarized. Based on this algorithm, equations describing this impact are developed. Simulation results are presented and discussed. As contribution, experimental results of the identification algorithm, applied to estimate the states of a position servo systems, are presented.",2004,0,
302,303,A Fault Tolerant Control strategy for an unmanned aerial vehicle based on a Sequential Quadratic Programming algorithm,"In this paper a fault tolerant control strategy for the nonlinear model of an unmanned aerial vehicle (UAV) equipped with numerous redundant controls is proposed. Asymmetric actuator failures are considered and, in order to accommodate them, a sequential quadratic programming (SQP) algorithm which takes into account nonlinearities, aerodynamic and gyroscopic couplings, state and control limitations is implemented. This algorithm computes new trims such that around the new operating point, the faulty linearized model remains nearby from the fault free model. For the faulty linearized models, linear state feedback controllers based on an eigenstructure assignment method are designed to obtain soft transients during accommodation. Real time implementation of the SQP algorithm is also discussed.",2008,0,
303,304,"A Framework to Evaluate the Trade-Off among AVF, Performance and Area of Soft Error Tolerant Microprocessors","Because of the increasing susceptibility of the integrated circuits to soft errors, many techniques have been proposed in all the design levels to reduce the AVF (architecturally vulnerable factor) of microprocessors with extra performance and area overheads. These overheads have a negative impact on the reliability. Conventional reliability evaluation frameworks do not take both performance and area overheads into account. A new metric, mMWTF (modified mean work to failure), is proposed in this paper to capture the trade-off among AVF, performance and area. A quantitative approach to evaluate mMWTF is also presented, in which fault injection is used to estimate the AVF. To modify the conventional fault injection methods which inject only SEU (single event upset), a new method is proposed to injects both SEU and MBU (multi bits upset), the latter of which happens more frequently with the shrinking feature size. Because of the new metric and the new fault injection method, the framework presented in this paper is more accurate than conventional ones. As a case study, two control flow checking techniques are proposed and evaluated in this paper. The evaluation results demonstrate that the techniques with better balance among AVF, performance and area can better improve the reliability of microprocessors.",2008,0,
304,305,Falcon: fault localization in concurrent programs,"Concurrency fault are difficult to find because they usually occur under specific thread interleavings. Fault-detection tools in this area find data-access patterns among thread interleavings, but they report benign patterns as well as actual faulty patterns. Traditional fault-localization techniques have been successful in identifying faults in sequential, deterministic programs, but they cannot detect faulty data-access patterns among threads. This paper presents a new dynamic fault-localization technique that can pinpoint faulty data-access patterns in multi-threaded concurrent programs. The technique monitors memory-access sequences among threads, detects data-access patterns associated with a program's pass/fail results, and reports dataaccess patterns with suspiciousness scores. The paper also presents the description of a prototype implementation of the technique in Java, and the results of an empirical study we performed with the prototype on several Java benchmarks. The empirical study shows that the technique can effectively and efficiently localize the faults for our subjects.",2010,0,
305,306,Enhanced error concealment with mode selection,"Delay sensitive video transmission over error prone networks can suffer from packet erasures when channel conditions are not favorable. Use of error concealment (EC) at the video decoder is necessary in such cases to prevent error induced artefacts making the affected video frames visibly intolerable. This paper proposes an EC method that incorporates enhanced temporal and spatial concealment elements, the use of which is controlled by a mode selection (MS) algorithm well matched to the characteristics of the temporal concealment approach. The performance of the individual enhancements and of the MS algorithm are compared with the respective features of the method employed in the H.264 joint model (JM) decoder and with other state of the art methods. The overall performance of the proposed method is shown to offer significant gains (up to 9 dB) compared to that of the JM decoder for a wide range of natural and animation image sequences without any considerable increase in complexity",2006,0,
306,307,A Fault-Location Method for Application With Current Differential Relays of Three-Terminal Lines,This paper presents a new method for locating faults on three-terminal power lines. Estimation of a distance to fault and indication of a faulted section is performed using three-phase current from all three terminals and additionally three-phase voltage from the terminal at which a fault locator is installed. Such a set of synchronized measurements has been taken into consideration with the aim of developing a fault-location algorithm for applications with current differential relays of three-terminal lines. The delivered fault-location algorithm consists of three subroutines designated for locating faults within particular line sections and a procedure for indicating the faulted line section. Testing and evaluation of the algorithm has been performed with fault data obtained from versatile Alternate Transients Program-Electromagnetic Transients Program simulations. The sample results of the evaluation are reported and discussed.,2007,0,
307,308,Bad Words: Finding Faults in Spirit's Syslogs,"Accurate fault detection is a key element of resilient computing. Syslogs provide key information regarding faults, and are found on nearly all computing systems. Discovering new fault types requires expert human effort, however, as no previous algorithm has been shown to localize faults in time and space with an operationally acceptable false positive rate. We present experiments on three weeks of syslogs from Sandia's 512-node ""Spirit"" Linux cluster, showing one algorithm that localizes 50% of faults with 75% precision, corresponding to an excellent false positive rate of 0.05%. The salient characteristics of this algorithm are (1) calculation of nodewise information entropy, and (2) encoding of word position. The key observation is that similar computers correctly executing similar work should produce similar logs.",2008,0,
308,309,Efficient Test Pattern Compression Method Using Hard Fault Preferring,"This paper describes new compression method that is used for test pattern compaction and compression in algorithm called COMPAS, which utilizes a test data compression method based on pattern overlapping. This algorithm reorders and compresses deterministic test patterns previously generated in an ATPG by overlapping them. Independency of COMPAS on used ATPG is discussed and verified. New method improves compression ratio by preprocessing input data to determine the degree of random test resistance for each fault. This information allows the algorithm to reorder test patterns more efficiently and results to 10% compression ratio improvement in average. Compressed data sequence is well suited for decompression by the scan chains in the embedded tester cores.",2008,0,
309,310,Detection of small defects by THz-waves for non-destructive testing in dielectric layered structures,"In this study, the small defects detection in dielectric layered structures by THz waves for nondestructive testing. Finite element method were used for modelling of the structures.",2010,0,
310,311,"On line sensor fault detection, isolation and accommodation in tactical aerospace vehicle","This paper presents on line sensor fault detection, isolation (FDI) and the associated fault tolerant control (FTC) algorithm for a tactical aerospace vehicle. A study on the analytical redundancy and a sensor fault detection scheme (FDI ) into a flight control system has been performed for a tactical aerospace vehicle using the longitudinal model. There are various methods available in the academic literature to apply FDI and FTC schemes to control systems and some have already been applied to real applications. Among these, observer-based approaches have arisen as one of the most widespread. The basic ideas behind observer-based FDI schemes are the generation of residuals, and the use of an optimal threshold function to differentiate faults from disturbances. Generally, the residuals, also known as diagnostic signals, are generated from estimates of the system's measurements obtained by a Luenberger observer or a Kalman filter. The threshold function is then used to 'detect' the fault by separating the residuals from false faults and disturbances. The change in residual signal is used to detect and isolate the fault and corresponding fault tolerant control action is taken to arrest the failure of the aerospace vehicle. A closed-loop simulation with nonlinear 6-degree of freedom (6-DoF) model shows that the above FDI and FTC scheme will be able to reduce the probability of mission failure due to the fault in one of the sensors.",2004,0,
311,312,Induction machines performance evaluator 'torque speed estimation and rotor fault diagnostic',"This paper proposes a new DSP based tool for evaluating the performance of induction motors based on the data extracted from the stator current. In the proposed algorithm, a pattern recognition technique according to Bayes minimum error classifier is developed to detect incipient rotor faults such as broken rotor bars and static eccentricity in induction motors. Also, part of the algorithm is based on the acceleration method presented in the IEEE Std. 112. It helps to calculate the motor's torque using two line currents and voltages. The use of linear and quadratic time-frequency representations is investigated as a viable solution to the task at hand. Speed information is vital in this approach, so an algorithm to track the speed related saliency induced harmonics from the machine's line current spectrogram is presented. Capturing the harmonics gives the rotor speed that can also be used to extract the feature vector for diagnostic. The implementation of the algorithm on TMS320C6000 family of DSP chips is currently underway. The complete algorithm is then be used to obtain the induction motor's performance curves. This is a complete stand-alone panel mounted induction motor diagnostic tool currently being developed in their lab. This package will be used in conjunction with a drive system (inverter) for online performance monitoring and preventing unwanted shutdown of the induction motor. The difficulties encountered, including a limited dynamic range and the presence of cross terms, are addressed and the suggested solution is provided. Experimental results corroborating the proposed algorithm are presented, and a discussion of the advantages and disadvantages of such an approach is touched upon",2002,0,
312,313,Correction of MR k-space data corrupted by spike noise,"Magnetic resonance images are reconstructed from digitized raw data, which are collected in the spatial-frequency domain (also called k-space). Occasionally, single or multiple data points in the k-space data are corrupted by spike noise, causing striation artifacts in images. Thresholding methods for detecting corrupted data points can fail because of small alterations, especially for data points in the low spatial frequency area where the k-space variation is large. Restoration of corrupted data points using interpolations of neighboring pixels can give incorrect results. The authors propose a Fourier transform method for detecting and restoring corrupted data points using a window filter derived from the striation-artifact structure in an image or an intermediate domain. The method provides an analytical solution for the alteration at each corrupted data point. It can effectively restore corrupted k-space data, removing striation artifacts in images, provided that the following 3 conditions are satisfied. First, a region of known signal distribution (for example, air background) is visible in either the image or the intermediate domain so that it can be selected using a window filter. Second, multiple spikes are separated by the full-width at half-maximum of the point spread function for the window filter. Third, the magnitude of a spike is larger than the minimum detectable value determined by the window filter and the standard deviation of k-space random noise.",2000,0,
313,314,Initial Experiences with a New FPGA Based Traveling Wave Fault Recorder Installed on a MV Distribution Network,"This paper presents the initial results obtained from a newly developed FPGA based traveling wave fault recorder installed on a medium voltage (MV) distribution line. The recorder is capable of recording six input signals, simultaneously sampling at 40 mega samples per second (MSPS) and at 14 bit resolution. It uses high bandwidth 17 MHz Rogowski coils connected to the secondary of a current transformer inside the substation to acquire the high frequency traveling wave components. Initial results during the testing phase show that recorder is capable of recording high fidelity signals relating to switching events. It has also highlighted that the distribution line is subject to many other transient phenomena in addition to faults and switching events which must be taken into consideration when choosing a suitable triggering mechanism.",2008,0,
314,315,Evaluation of the Low Frame Error Rate Performance of LDPC Codes Using Importance Sampling,"We present an importance sampling method for the evaluation of the low frame error rate (FER) performance of LDPC codes under iterative decoding. It relies on a combinatorial characterization of absorbing sets, which are the dominant cause of decoder failure in the low FER region. The biased density in the importance sampling scheme is a mean-shifted version of the original Gaussian density, which is suitably centered between a codeword and a dominant absorbing set. This choice of biased density yields an unbiased estimator for the FER with a variance lower by several orders of magnitude than the standard Monte Carlo estimator. Using this importance sampling scheme in software, we obtain good agreement with the experimental results obtained from a fast hardware emulator of the decoder.",2007,0,
315,316,A New Method for Earth Fault Line Detection Based on Two-Dimensional Wavelet Transform in Distribution Automation,"A novel method based on two-dimensional wavelet transform to detect single-phase faults in distribution systems is proposed in this paper. After structuring analytic signals of zero sequence current, the two-dimensional wavelet transform is applied. Thus the analysis of combined signal of amplitude and phase is realized. Compared with the use of single amplitude or single phase, combined signal carries more details of transient signal. Theoretical analysis and MATLAB based simulation show that the presented method can exactly and effectively choose the faulty line in single phase-to-ground fault",2005,0,
316,317,An FFT-based method to evaluate and compensate gain and offset errors of interleaved ADC systems,"Interleaved analog-digital converter (ADC) systems can be used to increase the sampling rate for a given ADC implementation technique. In theory, the maximum sampling rate that can be achieved is limited only by the bandwidth and the practical limits related to the power and space of integrated circuits. In this paper, a solution to increase the sampling rate of a digitizing system based on interleaved ADCs is presented. An error analysis, which takes into consideration offset and gain errors of the different ADC channels, is performed in order to quantify the effect of such errors in the system's performance. A software method based on the fast Fourier transform is presented for offset and gain error compensation of interleaved ADC associations. Numerical simulations and experimental results are used to validate the theory and the proposed compensation algorithm.",2004,0,
317,318,Fault Diagnosis of Generator Based on D-S Evidence Theory,"It is difficult to identify the fault type with the signal gathered from the sensors. In this paper, a new fusion algorithm based on the Dempster-Shafer theory of evidence and neural networks is brought forward. This method combines the advantages of D-S evidence theory and the BP neural network. Neural networks are used to pretreated the data gathered from the embedded sensors in the monitoring system of hydropower plant. Compared with the approaches that only adopt D-S evidence theory or neural networks, the accuracy of diagnostic results is obviously improved, and the signals analysis proved this conclusion. This method has been applied in the monitoring system of JiLin FengMan hydropower plant successfully.",2008,0,
318,319,Improved error bounds for the erasure/list scheme: the binary and spherical cases,We derive improved bounds on the error and erasure rate for spherical codes and for binary linear codes under Forney's erasure/list decoding scheme and prove some related results.,2004,0,
319,320,Faulted phase selection on double circuit transmission line using wavelet transform and neural network,"Modern numerical relays often incorporate the logic for combined single and three-phase auto-reclosing scheme; single phase to earth faults initiate single-phase tripping and reclosure, and all the other faults initiate three-phase tripping and reclosure. Accurate faulted phase selection is required for such a scheme. This paper presents a novel scheme for detection and classification of faults on double circuit transmission line. The proposed approach uses combination of wavelet transform and neural network, to solve the problem. While wavelet transform is a powerful mathematical tool which can be employed as a fast and very effective means of analyzing power system transient signals, artificial neural network has a ability to classify non-linear relationship between measured signals by identifying different patterns of the associated signals. The proposed algorithm consists of time-frequency analysis of fault generated transients using wavelet transform, followed by pattern recognition using artificial neural network to identify the faulted phase. MATLAB/Simulink software is used to generate fault signals and verify the correctness of the algorithm. The adaptive discrimination scheme is tested by simulating different types of fault and varying fault resistance, fault location and fault inception time, on a given power system model. The simulation results show that the proposed phase selector scheme is able to identify faulted phase on the double circuit transmission line rapidly and correctly.",2009,0,
320,321,Algorithm-Based Fault Tolerance for Fail-Stop Failures,"Fail-stop failures in distributed environments are often tolerated by checkpointing or message logging. In this paper, we show that fail-stop process failures in ScaLAPACK matrix-matrix multiplication kennel can be tolerated without checkpointing or message logging. It has been proved in previous algorithm-based fault tolerance that, for matrix-matrix multiplication, the checksum relationship in the input checksum matrices is preserved at the end of the computation no mater which algorithm is chosen. From this checksum relationship in the final computation results, processor miscalculations can be detected, located, and corrected at the end of the computation. However, whether this checksum relationship can be maintained in the middle of the computation or not remains open. In this paper, we first demonstrate that, for many matrix matrix multiplication algorithms, the checksum relationship in the input checksum matrices is not maintained in the middle of the computation. We then prove that, however, for the outer product version algorithm, the checksum relationship in the input checksum matrices can be maintained in the middle of the computation. Based on this checksum relationship maintained in the middle of the computation, we demonstrate that fail-stop process failures (which are often tolerated by checkpointing or message logging) in ScaLAPACK matrix-matrix multiplication can be tolerated without checkpointing or message logging.",2008,0,
321,322,Maintaining Consistency between Loosely Coupled Services in the Presence of Timing Constraints and Validation Errors,"Loose coupling is often cited as a defining characteristic of service-oriented architectures. Interactions between services take place via messages in an asynchronous environment where communication and processing delays can be unpredictable; further, interacting parties are not required to be on-line at the same time. Despite loose coupling, many service interactions have timing and validation constraints. For example, business interactions that take place using RosettaNet partner interface processes (PIPs) such as request price and availability, request purchase order, notify of invoice, etc. have to meet several timing and message validation constraints. A failure to deliver a valid message within its time constraint could cause mutually conflicting views of an interaction. For example, one party can regard it as timely whilst the other party regards it as untimely, leading to application level inconsistencies. The paper describes how business interactions, such as PIPs can be wrapped by simple handshake synchronisation protocols to provide bilateral consistency, thereby simplifying the task of coordinating peer-to-peer business processes",2006,0,
322,323,Compact Microstrip Quasi-Elliptic Bandpass Filter Using Open-Loop Dumbbell Shaped Defected Ground Structure,"A novel square open-loop dumbbell-shaped defected ground structure (DGS) unit is proposed. This unit provides a quasi-elliptic bandpass characteristic and the two transmission zeros near the passband edges can be controlled by the dimensions of DGS. Two quasi-elliptic bandpass filters using one and two DGS units; centered at 1.5 GHz were designed and implemented. Both the simulation and experimental results show that the DGS filter response is in good accordance with ideal quasi-elliptic model. The prototype filter with two DGS units yields higher order quasi-elliptic filtering and reports the measured 0.72 dB as insertion loss, 34 dB as matching, 51.8 % fractional bandwidth and about 20 dB stopband attenuation up to 10 GHz",2006,0,
323,324,Analyzing the soft error resilience of linear solvers on multicore multiprocessors,"As chip transistor densities continue to increase, soft errors (bit flips) are becoming a significant concern in networked multiprocessors with multicore nodes. Large cache structures in multicore processors are especially susceptible to soft errors as they occupy a significant portion of the chip area. In this paper, we consider the impacts of soft errors in caches on the resilience and energy efficiency of sparse linear solvers. In particular, we focus on two widely used sparse iterative solvers, namely Conjugate Gradient (CG) and Generalized Minimum Residuals (GMRES). We propose two adaptive schemes, (i) a Write Eviction Hybrid ECC (WEH-ECC) scheme for the L1 cache and (ii) a Prefetcher Based Adaptive ECC (PBA-ECC) scheme for the L2 cache, and evaluate the energy and reliability trade-offs they bring in the context of GMRES and CG solvers. Our evaluations indicate that WEH-ECC reduces the CG and GMRES soft error vulnerability by a factor of 18 to 220 in L1 cache, relative to an unprotected L1 cache, and energy consumption by 16%, relative to a cache with strong protection. The PBA-ECC scheme reduces the CG and GMRES soft error vulnerability by a factor of 9 A 10<sup>3</sup> to 8.6 A 10<sup>9</sup>, relative to an unprotected L2 cache, and reduces the energy consumption by 8.5%, relative to a cache with strong ECC protection. Our energy overheads over unprotected L1 and L2 caches are 5% and 14% respectively.",2010,0,
324,325,Application-driven co-design of fault-tolerant industrial systems,"This paper presents a novel methodology for the HW/SW co-design of fault tolerant embedded systems that pursues the mitigation of radiation-induced upset events (which are a class of Single Event Effects - SEEs) on critical industrial applications. The proposal combines the flexibility and low cost of Software Implemented Hardware Fault Tolerance (SIHFT) techniques with the high reliability of selective hardware replication. The co-design flow is supported by a hardening platform that comprises an automatic software hardening environment and a hardware tool able to emulate Single Event Upsets (SEUs). As a case study, we selected a soft-micro (PicoBlaze) widely used in FPGA-based industrial systems, and a fault tolerant version of the matrix multiplication algorithm was developed. Using the proposed methodology, the design was guided by the requirements of the application, leading us to explore several trade-offs among reliability, performance and cost.",2010,0,
325,326,Defect-Tolerant Design and Optimization of a Digital Microfluidic Biochip for Protein Crystallization,"Protein crystallization is a commonly used technique for protein analysis and subsequent drug design. It predicts the 3-D arrangement of the constituent amino acids, which in turn indicates the specific biological function of a protein. Protein crystallization experiments are typically carried out in well-plates in the laboratory. As a result, these experiments are slow, expensive, and error-prone due to the need for repeated human intervention. Recently, droplet-based AdigitalA microfluidics have been used for executing protein assays on a chip. Protein samples in the form of nanoliter-volume droplets are manipulated using the principle of electrowetting-on-dielectric. We present the design of a multi-well-plate microfluidic biochip for protein crystallization; this biochip can transfer protein samples, prepare candidate solutions, and carry out crystallization automatically. To reduce the manufacturing cost of such devices, we present an efficient algorithm to generate a pin-assignment plan for the proposed design. The resulting biochip enables control of a large number of on-chip electrodes using only a small number of pins. Based on the pin-constrained chip design, we present an efficient shuttle-passenger-like droplet manipulation method and test procedure to achieve high-throughput and defect-tolerant well loading.",2010,0,
326,327,Influence of the AC system faults on HVDC system and recommendations for improvement,"The interaction between AC and DC systems in a long distance bulk power transmission system is very complicated. In this paper, taking some cases in China Southern Power Grid as examples, the main functions of the DC control system operating during the AC faults is discussed. During the AC faults, the protection and monitoring system for DC converter and the protection system for converter transformers and auxiliary transformers may work incorrectly, reasons for these cases are analyzed and recommendations are given to solve the defects. Experience from these cases will help us to improve the ability of operation and maintenance to insure the safety and provide useful references for the design of HVDC and the coordination of AC/DC system in China.",2009,0,
327,328,Dense error correction via l1-minimization,"We study the problem of recovering a non-negative sparse signal x isin Ropf<sup>n</sup> from highly corrupted linear measurements y = Ax+e isin Ropf<sup>m</sup>, where e is an unknown (and unbounded) error. Motivated by an observation from computer vision, we prove that for highly correlated dictionaries A, any non-negative, sufficiently sparse signal x can be recovered by solving an lscr<sup>1</sup>-minimization problem: min ||x||<sub>1</sub> + ||e||<sub>1</sub> subject to y = Ax + e. If the fraction rho of errors is bounded away from one and the support of x grows sublinearly in the dimension m of the observation, for large m, the above lscr<sup>1</sup>-minimization recovers all sparse signals x from almost all sign-and-support patterns of e. This suggests that accurate and efficient recovery of sparse signals is possible even with nearly 100% of the observations corrupted.",2009,0,
328,329,Efficient stimuli generators for detection of path delay faults,"This paper presents a way to construct accumulator based test vector generators intended for efficient detection of path delay faults. Experiments conducted using our path delay fault simulator, GFault, shows that our proposed generator can give as much as 30times reduction in test time for circuits in the ISCAS85 benchmark suite compared to an accumulator based pseudo random generator",2005,0,
329,330,A fault analysis and design consideration of pulsed-power supply for high-power laser,"According to the requirements of driving flashlamps, the design of a pulsed-power supply (PPS), based on capacitors as energy storage elements, is presented. Special consideration is given to some possible faults such as capacitor internal short-circuit, bus bar breakdown to ground, flashlamp sudden short or break (open circuit), and closing switch restrike in the preionization branch. These faults were analyzed in detail, and both fault current and voltage waveforms are shown through circuit simulation. Based on the analysis and computation undertaken, the pulsed-power system design and protection requirements are proposed. The preliminary experiments undertaken after circuit simulation demonstrated that the design of the PPS met the project requirements.",2003,0,
330,331,A fault tolerant control architecture for automated highway systems,A hierarchical controller for dealing with faults and adverse environmental conditions on an automated highway system is proposed. The controller extends a previous control hierarchy designed to work under normal conditions of operation. The faults are classified according to the capabilities remaining on the vehicle or roadside after the fault has occurred. Information about these capabilities is used by supervisors in each of the layers of the hierarchy to select appropriate fault handling strategies. We outline the strategies needed by the supervisors and give examples of their detailed operation,2000,0,
331,332,Analysis of interconnect crosstalk defect coverage of test sets,"This paper addresses the problem of evaluating the effectiveness of test sets to detect crosstalk defects in interconnects of deep sub-micron circuits. The fast and accurate estimation technique will enable: (a) evaluation of different existing tests, like functional, scan, logic BIST, and delay tests, for effective testing of crosstalk defects in interconnects, and (b) development of crosstalk tests if the existing tests are not sufficient, thereby minimizing the cost of interconnect testing. Based on a covering relationship we establish between transition tests in detecting crosstalk defects, we develop an abstract crosstalk fault model for circuit interconnects. Based on this fault model, and the covering relationship, we develop a fast and efficient method to estimate the fault coverage of any general test set. We also develop a simulation-based technique to calculate the probability of occurrence of the defects corresponding to each fault, which enables the fault coverage analysis technique to produce accurate estimates of the actual crosstalk defect coverage of a given test set. The crosstalk test and fault properties, as well as the accuracy of the proposed crosstalk coverage analysis techniques, have been validated through extensive simulation experiments. The experiments also demonstrate that the proposed crosstalk techniques are orders of magnitude faster than the alternative method of SPICE-level simulation. Finally, we demonstrate the practical applicability of the proposed fault coverage analysis technique by using it to evaluate the crosstalk fault coverage of logic BIST tests for the buses in a DSP core",2000,0,
332,333,Atmospheric correction of AMSR-E brightness temperatures for dry snow cover mapping,"Differences between the brightness temperatures (spectral gradient) collected by the Advanced Microwave Scanning Radiometer for EOS (AMSR-E) at 18.7 and 36.5 GHz are used to map the snow-covered area (SCA) over a region including the western U.S. The brightness temperatures are corrected to take into account for atmospheric effects by means of a simplified radiative transfer equation whose parameters are stratified using rawinsonde data collected from a few stations. The surface emissivity is estimated from the model, and the brightness temperatures at the surface are computed as the product of the surface temperature and the computed emissivity. The SCA derived from microwave data is compared with that obtained from the Moderate Resolution Imaging Spectroradiometer for both cases of corrected and noncorrected brightness temperatures. The improvement to the SCA retrievals based on the corrected brightness temperatures shows an average value around 7%",2006,0,
333,334,A low-tech solution to avoid the severe impact of transient errors on the IP interconnect,"There are many sources of failure within a system-on-chip (SoC), so it is important to look beyond the processor core at other components that affect the reliable operation of the SoC, such as the fabric included in every one that connects the IP together. We use ARM's AMBA 3 AXI bus matrix to demonstrate that the impact of errors on the IP interconnect can be severe: possibly causing deadlock or memory corruption. We consider the detection of 1-bit transient faults without changing the IP that connects to the bus matrix or the AMBA 3 standard and without adding extra latency while keeping the performance and area overhead low. We explore what can be done under these constraints and propose a combination of techniques for a low-tech solution to detect these rare events.",2009,0,
334,335,The study of analog circuit fault diagnosis method based on circuit transform function,"This is the paper use Laplace Transfer to compute the analog circuit transform function, and compute the fault transform functions with different kinds of fault to generate the fault diagnosis table. The table is used to complete fault diagnosis. At last the software Multisim is used to simulate an analog circuit to do the fault diagnosis, and this method is verified usefully.",2010,0,
335,336,Efficient Fault-Tolerant Backbone Construction in Tmote Sky Sensor Networks,"In this study, we have investigated the effectiveness of building AFault-Tolerant BackboneA for data dissemination in Tmote Sky sensor networks. Tmote Sky sensors provide programmable and adjustable output power for data transmission. Users can control adequate transmission power for each sensor. Based on our measurements of Tmote Sky, there is a steadily transmitted distance for every power level. For certain power level, successfully-transmitted ratio was approximately 100 percent when the distance between sender and receiver was less than the steadily-transmitted distance. In accordance with the character on Tmote Sky, the ideas of fault-tolerant backbone has been made for constructing a fault-tolerant and stable system for Tmote Sky. The fault-tolerant backbone protocol builds up a connected backbone, in which nodes are endowed with a sleep/awake schedule. Practical experimental results reveal the fast fault recovery and high successfully-transmitted ratio can be fulfilled in the realistic system. The following goals in the implementation have been reached, including self-configurable fault-tolerant groups, automatic backbone construction, automatic failure recovery, and route repair.",2009,0,
336,337,Anomaly detection: A robust approach to detection of unanticipated faults,"This paper introduces a methodology to detect as early as possible with specified degree of confidence and prescribed false alarm rate an anomaly or novelty (incipient failure) associated with critical components/subsystems of an engineered system that is configured to monitor continuously its health status. Innovative features of the enabling technologies include a Bayesian estimation framework, called particle filtering, that employs features or condition indicators derived from sensor data in combination with simple models of the systempsilas degrading state to detect a deviation or discrepancy between a baseline (no-fault) distribution and its current counterpart. The scheme provides the probability of abnormal condition and the probability of false alarm. The presence of an anomaly is confirmed for a given confidence level. The efficacy of the proposed anomaly detection architecture is illustrated with test data acquired from components typically found on aircraft and monitored via a test rig appropriately instrumented.",2008,0,
337,338,Research of Remote Fault Diagnosis System Based on Multi-Agent,"A Multi-agent based Remote fault Diagnosis system is an important system for high speed and automation which can not only monitor the status of the remote device, but serve for the remote device. Remote Fault diagnosis system are vital aspects in automation process, in this sense, remote diagnosis systems should support decision-making tools, the enterprise thinking and flexibility. In this paper a kind of Remote Diagnosis System based on multi-agent is presented. This model is based on a generic framework using multi-agent systems. Specifically, this paper analyses the architecture of Remote Fault Diagnosis System and the collaboration mechanism between Agents. The method brought forward in the paper was generally applicable to a general fault diagnosis.",2010,0,
338,339,Fault diagnosis for a delta-sigma converter by a neural network,The diagnosis of faults in a first order -converter is described. The circuit behaviour of fault-free circuits and circuits containing single faults were simulated and characterized by the output bitstream patterns. The latter were compared with that of the ideal fault-free circuit. A Simplified fuzzy ARTMAP was trained with metrics derived from the bitstreams and their assigned class. A diagnostic accuracy of 93% was achieved using just two of the metrics. The technique might be useful for the diagnosis of other circuits.,2004,0,
339,340,The Impact of Link Error Modeling on the Quality of Streamed Video in Wireless Networks,The influence of channel error characteristics on higher layer protocols or methods which are considering or even exploiting the error statistics is significant especially in wireless networks where fading and interference effects result in error pattern correlation properties (error bursts). In this work we are analysing the impact of the channel properties directly on the quality of streamed video. We are focusing on the quality of transmitted H.264/AVC video streaming over UMTS DCH (Dedicated Channel) and compare the quality of the streamed video simulated over measured link error traces (the measurements performed in a live UMTS network) to simulations with a memoryless channel and to models with enhanced error characteristics. The results show that appropriate modeling of the link layer error characteristics is very important but it can also be concluded that the error correlation properties of the link- or the network-layer model do not have an impact on the quality of the video stream as long as the resulting IP packet error probability remains unchanged.,2006,0,
340,341,A portable gait analysis and correction system using a simple event detection method,"Microcontrollers are widely used in the area of portable control systems, though they are only beginning to be used for portable, unobtrusive Functional Electrical Stimulation (FES) systems. This paper describes the initial prototyping of such a portable system. This has the intended use of detecting time variant gait anomalies in patients with hemiplegia, and correcting for them. The system is described in two parts. Firstly, the portable hardware implementing two independent communicating microcontrollers for low powered parallel processing and secondly the simplified low power software. Both are designed specifically for long term, stable use and also to communicate with PC based visual software for testing and evaluation. The system operates by using bend sensors to defect the angles of the hip, knee and ankle of both legs. It computes an error signal with which to produce a stimulation wave cycle, that is synchronised and timed for the new gait cycle from that in which the error was observed. This system uses a PID controller to correct for the instability inherent with such a large time delay between observation and correction.",2002,0,
341,342,Reaction to errors in robot systems,"The paper analyzes the problem of error (failure) detection and handling in robot programming. First an overview of the subject is provided and later error detection and handling in MRROC++ are described. To facilitate system reaction to the detected failures, the errors are classified and certain suggestions are made as to how to handle those classes of errors.",2002,0,
342,343,Final Prediction Error of Autoregressive Model as a New Feature in the Analysis of Heart Rate Variability,"The aim of this study is to offer a new heart rate variability (HRV) index that increases the accuracy in the discrimination of patients with congestive heart failure (CHF) from the control group. For this purpose, final prediction errors (FPE), which shows the quality of the conformity of autoregressive (AR) model, are calculated for model degrees from 1 to 100. Although the optimal AR model order and FPE values are widely used in the literature, they have not been used as possible HRV indices. In this study, we used FPE as an HRV feature for discriminating the patients with CHF from normal subjects and made a comparison with the other common HRV indices. As a result, we showed that FPE of AR model is a possible significant HRV feature.",2007,0,
343,344,One-Dimensional Variational Retrieval of the Wet Tropospheric Correction for Altimetry in Coastal Regions,"The altimeter range is corrected for tropospheric humidity by means of microwave radiometer measurements (Envisat/MWR, Jason-1/JMR, Jason-2/AMR). Over an open ocean, the altimeter/radiometer combination is satisfactory. However, in coastal areas, radiometer measurements are contaminated by the surrounding land surfaces, and the humidity retrieval method is not appropriate anymore. In this paper, a variational assimilation technique is proposed to retrieve the wet tropospheric correction near coasts. The method is first developed on simulations using the data from a meteorological model. A performance assessment is performed, as well as a comparison with a standard algorithm. The method is then applied on actual measurements, thus evaluating its feasibility.",2010,0,
344,345,Compact Test Generation for Small-Delay Defects Using Testable-Path Information,"Testing for small-delay defects requires fault-effect propagation along the longest testable paths. However, the selection of the longest testable paths requires high CPU time and leads to large pattern counts. Dynamic test compaction for small-delay defects has remained largely unexplored thus far. We propose a path-selection scheme to accelerate ATPG based on stored testable critical-path information. A new dynamic test-compaction technique based on structural analysis is also introduced. Simulation results are presented for a set of ISCAS'89 benchmark circuits.",2009,0,
345,346,Use of fault tree analysis for evaluation of system-reliability improvements in design phase,"Traditional failure mode and effects analysis is applied as a bottom-up analytical technique to identify component failure modes and their causes and effects on the system performance, estimate their likelihood, severity and criticality or priority for mitigation. Failure modes and their causes, other than those associated with hardware, primarily electronic, remained poorly addressed or not addressed at all. Likelihood of occurrence was determined on the basis of component failure rates or by applying engineering judgement in their estimation. Resultant prioritization is consequently difficult so that only the apparent safety-related or highly critical issues were addressed. When thoroughly done, traditional FMEA or FMECA were too involved to be used as a effective tool for reliability improvement of the product design. Fault tree analysis applied to the product as a top down in view of its functionality, failure definition, architecture and stress and operational profiles provides a methodical way of following products functional flow down to the low level assemblies, components, failure modes and respective causes and their combination. Flexibility of modeling of various functional conditions and interaction such as enabling events, events with specific priority of occurrence, etc., using FTA, provides for accurate representation of their functionality interdependence. In addition to being capable of accounting for mixed reliability attributes (failure rates mixed with failure probabilities), fault trees are easy to construct and change for quick tradeoffs as roll up of unreliability values is automatic for instant evaluation of the final quantitative reliability results. Failure mode analysis using fault tree technique that is described in this paper allows for real, in-depth engineering evaluation of each individual cause of a failure mode regarding software and hardware components, their functions, stresses, operability and interactions",2000,0,
346,347,Local magnetic error estimation using action and phase jump analysis of orbit data,"It's been shown in previous conferences that action and phase jump analysis is a promising method to measure normal quadrupole components, skew quadrupole components and even normal sextupole components. In this paper, the action and phase jump analysis is evaluated using new RHIC data.",2007,0,
347,348,Mining Frequent Patterns from Software Defect Repositories for Black-Box Testing,"Software defects are usually detected by inspection, black-box testing or white-box testing. Current software defect mining work focuses on mining frequent patterns without distinguishing these different kinds of defects, and mining with respect to defect type can only give limited guidance on software development due to overly broad classification of defect type. In this paper, we present four kinds of frequent patterns from defects detected by black-box testing (called black-box defect) based on a kind of detailed classification named ODC-BD (Orthogonal Defect Classification for Blackbox Defect). The frequent patterns include the top 10 conditions (data or operation) which most easily result in defects or severe defects, the top 10 defect phenomena which most frequently occur and have a great impact on users, association rules between function modules and defect types. We aim to help project managers, black-box testers and developers improve the efficiency of software defect detection and analysis using these frequent patterns. Our study is based on 5023 defect reports from 56 large industrial projects and 2 open source projects.",2010,0,
348,349,Low error rate LDPC decoders,"Low-density parity-check (LDPC) codes have been demonstrated to perform very close to the Shannon limit when decoded iteratively. However challenges persist in building practical high-throughput decoders due to the existence of error floors at low error rate levels. We apply high-throughput hardware emulation to capture errors and error-inducing noise realizations, which allow for in-depth analysis. This method enables the design of LDPC decoders that operate without error floors down to very low bit error rate (BER) levels. Such emulation-aided studies facilitate complex systems designs.",2009,0,
349,350,Fault-tolerant and energy-efficient permutation routing protocol for wireless networks,"A wireless network (WN) is a distributed system where each node is a small hand-held commodity device called a station. Wireless sensor networks have received increasing interest in recent years due to their usage in monitoring and data collection in a wide variety of environments like remote geographic locations, industrial plants, toxic locations or even office buildings. Two of the most important issues related to a WN are their energy constraints and their potential for developing faults. A station is usually powered by a battery which cannot be recharged while on a mission. Hence, any protocol run by a WN should be energy-efficient. Moreover, it is possible that all stations deployed as part of a WN may not work perfectly. Hence, any protocol designed for a WN should work well even when some of the stations are faulty. We design a protocol which is both energy-efficient and fault-tolerant for permutation routing in a WN.",2003,0,
350,351,A novel approach to architecture of radar fault diagnosis system based on mobile agents,"In order to improve radar fault diagnosis system, a new architecture of fault diagnosis system based on mobile agents is proposed. The architecture is based on an embedded network built-in radar system. It utilizes all kinds of mobile fault diagnostic agents in embedded network to detect shortcomings of distributed subsystems in radar. In the architecture, all MFDAs can migrate in embedded network, and can be centralized a personal computer so as to be updated and retrained conveniently for different batches of radar systems. In this paper, three kinds of start-up modes of fault diagnosis are illustrated, two kinds of multi-agent cooperation diagnostic frameworks are introduced, and a kind of structure of MFDA is addressed.",2010,0,
351,352,A Flexible and efficient bit error rate simulation method for high-speed differential link analysis using time-domain interpolation and superposition,"In this paper, a flexible and efficient time-domain method for calculating the bit error rate of high-speed differential links is presented. The method applies interpolation and superposition to the step response of a channel to construct the jittery data or/and clock waveforms at the receiver. With the statistics of the actual reference-crossing points extracted from the constructed receiver waveforms, the bathtub curves can be derived and extrapolated to get the eye margin at the given bit error rate. A software has been developed and applied for high-speed differential link design using the method. Good correlation has been achieved between the simulated results using this method and the measurement data with a bit error rate tester.",2008,0,
352,353,Do Crosscutting Concerns Cause Defects?,"There is a growing consensus that crosscutting concerns harm code quality. An example of a crosscutting concern is a functional requirement whose implementation is distributed across multiple software modules. We asked the question, ""How much does the amount that a concern is crosscutting affect the number of defects in a program?"" We conducted three extensive case studies to help answer this question. All three studies revealed a moderate to strong statistically significant correlation between the degree of scattering and the number of defects. This paper describes the experimental framework we developed to conduct the studies, the metrics we adopted and developed to measure the degree of scattering, the studies we performed, the efforts we undertook to remove experimental and other biases, and the results we obtained. In the process, we have formulated a theory that explains why increased scattering might lead to increased defects.",2008,0,
353,354,Impact of Transmission Network Reinforcement on Improvement of Power System Voltage Stability and Solving the Dynamic Delayed Voltage Recovery and Motor Stalling Problem After System Faults in the Saudi Electricity in the Western Region,"The Saudi Electricity company power system in the Western Region of the Kingdom (SEC-WR) is unique in its load pattern, growth trends and type, generation recourses and network configuration. The power system of the SEC-WR faced and is facing a high load growth. The high load increase gives rise to a very high loading of the transmission system elements, mainly power transformers and cables. The Western Region load is mainly composed of air conditioner (AC) during high load season. In case of faults this nature of load induces delayed voltage recovery following fault clearing on the transmission system. The sustained low voltage following transmission line faults could cause customer interruptions and may be equipment damage. The integrity of the transmission system may also be affected. The transient stability of the system may be affected. This may also influence the stability of the generating units in the system. The existing dynamic model of SEC-WR System has been described. The response of the model to the actual faults is compared with actual records obtained from the dynamic system monitor (DSM) installed in several locations in the SEC-WR System. The solution of the delayed voltage recovery problem after system faults may be achieved by reinforcement of the system, adding static VAr compensators (SVC) to provide the dynamic reactive power support to the system, reducing the fault clearing time and by under voltage load shedding. This paper analyzes and discusses the first alternative, the system reinforcement",2006,0,
354,355,Higher-order corrections to the pi criterion for the periodic operation of chemical reactors,"The present work develops a method to determine higher-order corrections to the pi criterion, derived from basic results of Center Manifold theory. The proposed method is based on solving the Center Manifold PDE via power series. The advantage of the proposed approach is the improvement of the accuracy of the pi criterion in predicting performance under larger amplitudes. The proposed method is applied to a continuous stirred tank reactor, where the yield of the desired product must be maximized.",2009,0,
355,356,Practical Criteria for the Separability of Eddy-Current Testing Signals on Multiple Defects,"Practical quantities have been introduced by the authors to characterize the interaction among multiple defects located in a specimen under eddy-current testing (ECT). If these quantities indicate that the interaction between the pairs of defects is negligible, the signal of an ECT probe for multiple defects can be calculated as the superposition of those signals, which are obtained for each defect as if it would be a single one. Conversely, if the criteria holds, the measured ECT signal can be decomposed into signals associated with the individual defects, which essentially simplifies the solution of the inverse problem of defect reconstruction. As an application of the criteria, the design of a novel barcoding system is presented, which is developed for marking metallic parts by laser treating, and where the applicability of linear signal processing methods for the reading out of the barcode is a requirement.",2008,0,
356,357,On-Line Reconfigurable XGFT Network-on-Chip Designed for Improving the Fault-Tolerance and Manufacturability of the MPSoC Chips,"Large System-on-Chip (SoC) circuits will contain an increasing number of processors which will communicate with each other across Networks-on-Chip (NOC). The faulty processors could be replaced with faultless ones, whereas only a single defect in the NOC can make the whole chip unusable. Therefore, the fault-tolerance of the NOC is a crucial component of the fault-tolerance and manufacturability of the SoCs. This paper presents a fault-tolerant extended generalized fat tree (XGFT) NOC developed for future multi-processor SoCs (MPSoC). Its fault-tolerance is improved with a new version of fault-diagnosis-and-repair (FDAR) system, which makes it possible to diagnose and repair the NOC on-line. It detects such static, dynamic and transient faults which block packets or produce bit errors, and reconfigures the faulty switches to operate correctly. Processors can also use it for reconfiguring the faulty switch nodes after the faults are located with other test methods. Simulation and synthesis results show that slightly defected XGFTs are able to achieve good performance after they are repaired with the FDAR while the costs of the FDAR remain tolerable",2006,0,
357,358,Attitude correction algorithm using GPS measurements for flight vehicles,For flight systems with an on-board seeker the attitude error is the major factor to determine the seeker pointing error at the time of object acquisition. To achieve a desired mission it must be minimized. The proposed algorithm corrects the attitude error in the guidance computer during flight by taking its position and velocity measurements from GPS or radar. This is possible since navigator's position and velocity states are correlated with attitude state. Computer simulation is shown to prove the proposed algorithm.,2002,0,
358,359,Hardware/software optimization of error detection implementation for real-time embedded systems,"This paper presents an approach to system-level optimization of error detection implementation in the context of fault-tolerant real-time distributed embedded systems used for safety-critical applications. An application is modeled as a set of processes communicating by messages. Processes are mapped on computation nodes connected to the communication infrastructure. To provide resiliency against transient faults, efficient error detection and recovery techniques have to be employed. Our main focus in this paper is on the efficient implementation of the error detection mechanisms. We have developed techniques to optimize the hardware/software implementation of error detection, in order to minimize the global worst-case schedule length, while meeting the imposed hardware cost constraints and tolerating multiple transient faults. We present two design optimization algorithms which are able to find feasible solutions given a limited amount of resources: the first one assumes that, when implemented in hardware, error detection is deployed on static reconfigurable FPGAs, while the second one considers partial dynamic reconfiguration capabilities of the FPGAs.",2010,0,
359,360,Development of Defect Classification Algorithm for POSCO Rolling Strip Surface Inspection System,"Surface inspection system (SIS) is an integrated hardware-software system which automatically inspects the surface of the steel strip. It is equipped with several cameras and illumination over and under the steel strip roll and automatically detects and classifies defects on the surface. The performance of the inspection algorithm plays an important role in not only quality assurance of the rolled steel product, but also improvement of the strip production process control. Current implementation of POSCO SIS has good ability to detect defects, however, classification performance is not satisfactory. In this paper, we introduce POSCO SIS and suggest a new defect classification algorithm which is based on support vector machine technique. The suggested classification algorithm shows good classification ability and generalization performance",2006,0,
360,361,Design of a fault-tolerant coarse-grained,"This paper considers the possibility of implementing low-cost hardware techniques which would allow to tolerate temporary faults in the datapaths of coarse-grained reconfigurable architectures (CGRAs). Our goal was to use less hardware overhead than commonly used duplication or triplication methods. The proposed technique relies on concurrent error detection by using residue code modulo 3 and re-execution of the last operation, once an error is detected. We have chosen the DART architecture as a vehicle to study the efficiency of this approach to protect its datapaths. Simulation results have confirmed hardware savings of the proposed approach over duplication.",2010,0,
361,362,"HGRID: Fault Tolerant, Log2N Resource Management for Grids","Grid resource discovery service is currently a very important focus of research. We propose a scheme that presents essential characteristics for efficient, self-configuring and fault-tolerant resource discovery and is able to handle dynamic attributes, such as memory capacity. Our approach consists of an overlay network with a hypercube topology connecting the grid nodes and a scalable, fault-tolerant, self-configuring search algorithm. By design, the algorithm improves the probability of reaching all working nodes in the system even in the presence of failures (inaccessible, crashed or heavy loaded nodes). We analyze the static resilience of the presented approach, that is to say, how well the algorithm is able to find resources without having to update the routing tables. The results show that the presented approach has a high static resilience.",2009,0,
362,363,Adaptive Error-Resilience Transcoding and Fairness Grouping for Video Multicast Over Wireless Networks,"In this paper, we present a two-pass intra-refresh transcoder for on-the-fly enhancing error resilience of a compressed video in a three-tier streaming system. Furthermore, we consider the problem of multicasting a video to multiple clients with diverse channel conditions. We propose a MINMAX loss rate estimation scheme to determine a single intra- refresh rate for all the clients in a multicast group. For the scenario that a quality variation constraint is imposed on the users, we also propose a grouping method to partition a multicast group of heterogeneous users into a minimal number of sub-groups to minimize the channel bandwidth consumption while meeting the quality variation constraint and achieving fairness among all sub-groups. Experimental results show that the proposed method can effectively mitigate the error propagation due to packet loss as well as achieve fairness not only among all sub-groups and also clients in a multicast group.",2007,0,
363,364,Error Control for IPTV over xDSL Networks,"We discuss the necessity of error control for supporting IPTV over imperfect access networks. In particular, we consider typical DSL environments, and examine the physical-layer impairments and error-mitigation techniques. For these networks, we evaluate the performance of two different application-layer Forward Error Correction (FEC) methods. An overview of hybrid error-control methods and recent developments in standardization is also presented.",2008,0,
364,365,A new approach of halftoning based on error diffusion with rough set filtering,"The rough set filtering makes use of the concepts of indiscernibility relations and approximation spaces to define an equivalence class of neighboring pixels in a processing mask, then utilize the statistical mean of the equivalence classes to replace the gray levels of the central pixel in a processing mask. The error diffusion makes use of the correction factor composed of the weighted errors for these pixels prior to addition of the pixel to be processed to diffuse error over the neighboring pixels in a continuous tone image. Both of a system and an algorithm of implementation of halftoning on error diffusion with rough sets are introduced in the paper",2000,0,
365,366,Outage performance of mrt with unequal-power co-channel interference and channel estimation error,In this letter we investigate the outage performance of maximal ratio transmission (MRT) with unequal-power co-channel interference (CCI) and channel estimation error. The exact expression for the outage probability is presented. Our results are applicable to the MRT systems with arbitrary numbers of transmit and receive antennas.,2007,0,
366,367,Optimal placement of sensor in gearbox fault diagnosis based on VPSO,"The optimization layout of the acceleration sensor and application of particle swarm optimization (PSO) algorithm to solve the fitness problems of such optimization are discussed in this paper. Based on the gearbox finite element modeling and the result of modal analysis, use the particle swarm optimization with adaptive velocity (VPSO) algorithm, and take the two kinds of fitness function as evaluation goal, has realized the optimization and positioning of gearbox sensor layout, analyzed optimization result.",2010,0,
367,368,Study of SINS/GPS/DVL integrated navigation system's fault tolerance,This paper put forward several fault tolerant arithmetic combining engineering applications on the AUV (autonomous underwater vehicle). The arithmetic is based on traditional centralized Kalman Filter and can improve SINS/GPS/DVL integrated navigation system's precision and capability of fault tolerant. The simulation and the vehicle tests validate the arithmetic.,2005,0,
368,369,New method for current and voltage measuring offset correction in an induction motor sensorless drive,This paper presents a new algorithm for electromagnetic torque and flux estimation in a sensorless drive when uncompensated dc offset of current and/or voltage sensors are present. The novel feature of the offset error correction algorithm is an attempt not to eliminate the consequence of problem but to identify its source. The algorithm uses the first harmonic of estimated torque and dc value of estimated stator flux to identify the source and value of the current and/or voltage offset error. Identified values can be used for offset cancelation which improves estimation process.,2010,0,
369,370,The Digital Circuit Fault Diagnosis Interface Design and Realization Based on VXI,"This paper discusses in detail the development process of general interface adapter in the digital circuit fault diagnosis system based on VXI. After introducing VXI bus, the paper gives overall description of fault diagnosis system, presents a method of solving the problem about load matching and interface matching, and realizes the function of identification to read and write on the memory of interface circuit and to control chip selection. The method of identity installation in interface circuit to be selected is to give an ID number and add a memory to interface circuit to ensure the accuracy and effectiveness. The paper also describes the method of self diagnosis in the interface circuit that is the key to whole fault diagnosis system.",2008,0,
370,371,The DVB television signal transmission simulation using the forward error correction codes,The contribution deals with the simulation of the digital video signal transmission through the baseband transmission channel model. The simulation model that covers selected phenomena of DVB (digital video broadcasting) system signal processing is presented. The digital video signal is represented with the digital data of one noncompressed video frame that is channel encoded and protected against errors with the forward error correction (FEC) codes. The transmission channel model has influence on transmitted digital data and its distortion and the pertubative signals affect on the data decoding. The developed interactive simulation software (Matlab application) features are outlined too and the conclusion presents efficiency of the used FEC codes.,2003,0,
371,372,Residual Generators for Fault Diagnosis Using Computation Sequences With Mixed Causality Applied to Automotive Systems,"An essential step in the design of a model-based diagnosis system is to find a set of residual generators fulfilling stated fault detection and isolation requirements. To be able to find a good set, it is desirable that the method used for residual generation gives as many candidate residual generators as possible, given a model. This paper presents a novel residual generation method that enables simultaneous use of integral and derivative causality, i.e., mixed causality, and also handles equation sets corresponding to algebraic and differential loops in a systematic manner. The method relies on a formal framework for computing unknown variables according to a computation sequence. In this framework, mixed causality is utilized, and the analytical properties of the equations in the model, as well as the available tools for algebraic equation solving, are taken into account. The proposed method is applied to two models of automotive systems, a Scania diesel engine, and a hydraulic braking system. Significantly more residual generators are found with the proposed method in comparison with methods using solely integral or derivative causality.",2010,0,
372,373,Online drift correction in wireless sensor networks using spatio-temporal modeling,"Wireless sensor networks are deployed for the purpose of sensing and monitoring an area of interest. Sensors in the sensor network can suffer from both random and systematic bias problems. Even when the sensors are properly calibrated at the time of their deployment, they develop drift in their readings leading to erroneous inferences being made by the network. The drift in this context is defined as a slow, unidirectional, long-term change in the sensor measurements. In this paper we present a novel algorithm for detecting and correcting sensors drifts by utilising the spatio-temporal correlation between neigbouring sensors. Based on the assumption that neighbouring sensors have correlated measurements and that the instantiation of drift in a sensor is uncorrelated with other sensors, each sensor runs a support vector regression algorithm on its neigbourspsila corrected readings to obtain a predicted value for its measurements. It then uses this predicted data to self-assess its measurement and detect and correct its drift using a Kalman filter. The algorithm is run recursively and is totally decentralized. We demonstrate using real data obtained from the Intel Berkeley Laboratory that our algorithm successfully suppresses drifts developed in sensors and thereby prolongs the effective lifetime of the network.",2008,0,
373,374,Applying fault-tolerant solutions of circulant graphs to meshes and hypercubes,"Many important architectures such as rings, meshes and hypercubes can be modeled as circulant graphs. As a result, circulant graphs have received a lot of attention, and a new method was developed for designing fault-tolerant solutions for them. We review this method in this paper, and examine its applications to the design of fault-tolerant solutions for meshes and hypercubes. Our results indicate that these solutions are efficient.",2005,0,
374,375,"Vietnamese spelling detection and correction using Bi-gram, Minimum Edit Distance, SoundEx algorithms with some additional heuristics","The spelling checking problem is considered to contain two main phases: the detecting phase and the correcting phase. In this paper, we present a new approach for Vietnamese spelling checking based on Vietnamese characteristics for each phase. Our research approach includes the use of a syllable Bi-gram in combination with parts of speech (POS) to find out suspected syllables. In the correcting phase, we based on minimum edit distance, SoundEx algorithms and some heuristics to build a weight function for assessing suggestion candidates. The training corpus and the test set were collected from e-newspapers.",2008,0,
375,376,A fault-tolerant control architecture for induction motor drives in automotive applications,"This paper describes a fault-tolerant control system for a high-performance induction motor drive that propels an electrical vehicle (EV) or hybrid electric vehicle (HEV). In the proposed control scheme, the developed system takes into account the controller transition smoothness in the event of sensor failure. Moreover, due to the EV or HEV requirements for sensorless operations, a practical sensorless control scheme is developed and used within the proposed fault-tolerant control system. This requires the presence of an adaptive flux observer. The speed estimator is based on the approximation of the magnetic characteristic slope of the induction motor to the mutual inductance value. Simulation results, in terms of speed and torque responses, show the effectiveness of the proposed approach.",2004,0,
376,377,Fault classification and fault distance location of double circuit transmission lines for phase to phase faults using only one terminal data,"An accurate fault classification algorithm for double end fed parallel transmission lines based on application of artificial neural networks is presented in this paper. The proposed method uses the voltage and current available at only the local end of line. This method is virtually independent of the effects of remote end infeed and is insensitive to the variation of fault inception angle and fault location. The Simulation results show that phase-to-phase faults can be correctly detected, classified and located within one cycle after the inception of fault. Large number of faults simulations using MATLAB<sup>A</sup>7.01 have demonstrated the accuracy and effectiveness of the proposed algorithm. The proposed scheme allows the protection engineers to increase the reach setting i.e. greater portion of line length can be protected as compared to conventional techniques. The technique neither requires a communication link to retrieve the remote end data nor zero sequence current compensation for healthy phases.",2009,0,
377,378,Denoising fluorescence endoscopy - A motion compensated temporal recursive video filter with an optimal minimum mean square error parameterization,"Fluorescence endoscopy is an emerging technique for the detection of bladder cancer. A marker substance is brought into the patient's bladder which accumulates at cancer tissue. If a suitable narrow band light source is used for illumination, a red fluorescence of the marker substance is observable. Because of the low fluorescence photon count and because of the narrow band light source, only a small amount of light is detected by the camera's CCD sensor. This, in turn, leads to strong noise in the recorded video sequence. To overcome this problem, we apply a temporal recursive filter to the video sequence. The derivation of a filter function is presented, which leads to an optimal filter in the minimum mean square error sense. The algorithm is implemented as plug-in for the real-time capable clinical demonstrator platform RealTimeFrame and it is capable to process color videos with a resolution of 768times576 pixels at 50 frames per second.",2009,0,
378,379,Compact multilayer coupled stripline LTCC filter with defected ground structure,"A novel multilayer coupled stripline resonator structure is introduced to realize miniature broadband band-pass filter using low temperature co-fired ceramic (LTCC) process with defected ground structure (DGS). Wide bandwidth and good selectivity are obtained by exploiting four resonators and the filter exhibits a high rejection in stopband by adopting the tapered DGS. Moreover, an inductance feed back between the output and input is introduced to produce transmission zeros. Filter with size of <sub>0</sub> /12  <sub>0</sub> /12  h (<sub>0</sub> is the wavelength at the midband frequency; h is the substrate height) is designed, fabricated and measured. The measured responses agree well with simulation results.",2009,0,
379,380,Dynamic Fault Handling Mechanisms for Service-Oriented Applications,"Dynamic fault handling is a new approach for dealing with fault management in service-oriented applications. Fault handlers, termination handlers and compensation handlers are installed at execution time instead of being statically defined. In this paper we present this programming style and our implementation of dynamic fault handling in JOLIE, providing finally a nontrivial example of its usage.",2008,0,
380,381,Towards Software Quality Economics for Defect-Detection Techniques,"There are various ways to evaluate defect-detection techniques. However, for a comprehensive evaluation the only possibility is to reduce all influencing factors to costs. There are already some models and metrics for the cost of quality that can be used in that context. The existing metrics for the effectiveness and efficiency of defect-detection techniques and experiences with them are combined with cost metrics to allow a more fine-grained estimation of costs and a comprehensive evaluation of defect-detection techniques. The current model is most suitable for directly comparing concrete applications of different techniques",2005,0,
381,382,Fault tolerance based on the publish-subscribe paradigm for the BonjourGrid middleware,"How to federate the machines of all Boinc, Condor and XtremWeb projects? If you believe in volunteer computing and want to share more than one project then BonjourGrid may help. In previous works, we proposed a novel approach, called BonjourGrid, to orchestrate multiple instances of Institutional Desktop Grid middleware. It is our way to remove the risk of bottleneck and failure, and to guarantee the continuity of services in a distributed manner. Indeed, BonjourGrid can create a specific environment for each user based on a given computing system of his choice such as XtremWeb, Condor or Boinc. This work investigates, first, the procedure to deploy Boinc and Condor on top of BonjourGrid and, second, proposes a fault tolerant approach based on passive replication and virtualization to tolerate the crash of coordinators. The novelty resides here in an integrated environment based on Bonjour (publication-subscription mecanism) for both the coordination protocol and for the fault tolerance issues. In particular, it is not so frequent to our knowledge to describe and to implement a fault tolerant protocol according to the pub-sub paradigm. Experiments, conducted on the Grid'5000 testbed, illustrate a comparative study between Boinc (respectively Condor) on top of BonjourGrid and a centralized system using Boinc (respectively Condor) and second prove the robustness of the fault tolerant mechanism.",2010,0,
382,383,An approach to detecting domain errors using formal specification-based testing,"Domain testing, a technique for testing software or portions of software dominated by numerical processing, is intended to detect domain errors that usually arise from incorrect implementations of desired domains. This paper describes our recent work aiming to provide support for revealing domain errors using formal specifications. In our approach, formal specifications serve as a means for domain modeling. We describe a strong domain testing strategy that guide testers to select a set of test points so that the potential domain errors can be effectively detected, and apply our approach in two case studies for test cases generation.",2004,0,
383,384,A Robust Error Detection Mechanism for H.264/AVC Coded Video Sequences Based on Support Vector Machines,"Current trends in wireless communications provide fast and location-independent access to multimedia services. Due to its high compression efficiency, H.264/AVC is expected to become the dominant underlying technology in the delivery of future wireless video applications. The error resilient mechanisms adopted by this standard alleviate the problem of spatio-temporal propagation of visual artifacts caused by transmission errors by dropping and concealing all macroblocks (MBs) contained within corrupted segments, including uncorrupted MBs. Concealing these uncorrupted MBs generally causes a reduction in quality of the reconstructed video sequence.",2008,0,
384,385,Track Down HW Function Faults Using Real SW Invariants,System level functional verification by running real software stack on FPGA prototype is essential for achieving a high quality design. But it is hard to find the exact source of hardware function faults while running large closed source system software fails. This paper proposes the idea of tracking down faults through real system software control flow invariants with current trace output hardware support. It captures qualified control flow invariant trace in reference execution and test trace; and tracks down faults through comparing offline invariant trace and test trace. The approach can deal with both deterministic and nondeterministic execution. We implemented the proof of concept in full system simulator Bochs. Our experimentation with the real closed source MS Windows XP suggests that the approach is effective in tracking down hardware function faults.,2009,0,
385,386,Design of a fault-tolerant satellite cluster link establishment protocol,"The design of a protocol for satellite cluster link establishment and management that accounts for link corruption, node failures, and node re-establishment is presented in this paper. This protocol will need to manage the traffic flow between nodes in the satellite cluster, adjust routing tables due to node motion, allow for sub-networks in the cluster, and similar activities. This protocol development is in its initial stages. Preliminary results with eight nodes demonstrate its operations and potential problems that may arise when significant numbers of channel errors are present",2005,0,
386,387,A novel approach to calculate the severity and priority of bugs in software projects,"Discovering and fixing software bugs is a difficult maintenance task, and a considerable amount of effort is devoted by software developers on this issue. In the world of software one cannot get rid of the bugs, fixes, patches etc. each of them have a severity and priority associated to it. There is not yet any formal relation between these components as both of these either depends on the developer and tester or on customer and project manger to be decided on. On one hand, the priority of a component depends on the cost and the efforts associated with it. While on the other, the severity depends on the efforts required to accomplish a particular task. This research paper proposes a formula that can draw a relationship among severity and priority.",2010,0,
387,388,Reducing the soft-error rate of a high-performance microprocessor,"Single-bit upsets from transient faults have emerged as a key challenge in microprocessor design. Soft errors will be an increasing burden for microprocessor designers as the number of on-chip transistors continues to grow exponentially. Unlike traditional approaches, which focus on detecting and recovering from faults, this article introduces techniques to reduce the probability that a fault will cause a declared error. The first approach reduces the time instructions sit in vulnerable storage structures. The second avoids declaring errors on benign faults. Applying these techniques to a microprocessor instruction queue significantly reduces its error rate with only minor performance degradation",2004,0,
388,389,The Orion GN&C data-driven flight software architecture for automated sequencing and fault recovery,"The Orion Crew Exploration Vehicle (CEV) is being designed to include capabilities that allow significantly more automation than either the Space Shuttle or the International Space Station (ISS). In particular, the vehicle flight software has requirements to accommodate increasingly automated missions throughout all phases of flight. This paper presents the Guidance, Navigation & Control (GN&C) flight software architecture designed to provide evolvable automation capability that sequences through software modes and configurations. This software architecture is required to maintain flexibility to address the maturation of operational concepts over time, permit ground and crew operators to gain trust in the system, and provide capabilities for human override of the automation in `off-nominal' situations. To allow for mission flexibility, reconfigurability and reduce the recertification expense over the life of the program, a data-driven approach is used to load the mission event plan as well as the flight software artifacts associated with the GN&C subsystem. The flight software schema for automated mission sequencing is presented with a concept of operations for interactions with ground and crew members. This data is managed through a prototype database of GN&C level sequencing data, which tracks mission specific parameters to aid in the scheduling of GN&C activities. A prototype architecture for fault detection, isolation and recovery interactions with the automation software is presented as part of the upcoming design maturation to respond with appropriate GN&C and vehicle-level actions in `off-nominal' scenarios.",2010,0,
389,390,An experimental study of security vulnerabilities caused by errors,"The paper presents an experimental study which shows that, for the Intel x86 architecture, single-bit control flow errors in the authentication sections of targeted applications can result in significant security vulnerabilities. The experiment targets two well-known Internet server applications: FTP and SSH (secure shell), injecting single-bit control flow errors into user authentication sections of the applications. The injected sections constitute approximately 2-8% of the text segment of the target applications. The results show that out of all activated errors: (a) 1-2% comprised system security (create a permanent window of vulnerability); (b) 43-62% resulted in crash failures (about 8.5% of these errors create a transient window of vulnerability); and (c) 7-12% resulted in fail silence violations. A key reason for the measured security vulnerabilities is that, in the x86 architecture, conditional branch instructions are a minimum of one Hamming distance apart. The design and evaluation of a new encoding scheme that reduces or eliminates this problem is presented.",2001,0,
390,391,Defect Tolerance Based on Coding and Series Replication in Transistor-Logic Demultiplexer Circuits,"We present a family of defect tolerant transistor-logic demultiplexer circuits that can defend against both stuck-ON (short defect) and stuck-OFF (open defect) transistors. Short defects are handled by having two or more transistors in series in the circuit, controlled by the same signal. Open defects are handled by having two or more parallel branches in the circuit, controlled by the same signals, or more efficiently, by using a transistor-replication method based on coding theory. These circuits are evaluated, in comparison with an unprotected demultiplexer circuit, by: 1) modeling each circuit's ability to tolerate defects and 2) calculating the cost of the defect tolerance as each circuit's redundancy factor R, which is the relative number of transistors required by the circuit. The defect-tolerance model takes the form of a function giving the failure probability of the entire demultiplexer circuit as a function of the defect probabilities of its component transistors, for both defect types. With the advent of defect tolerance as a new design goal for the circuit designer, this new form of performance analysis has become necessary.",2007,0,
391,392,Adaptive Correction of Errors from Segmented Digital Ink Texts in Chinese Based on Context,"Digital ink texts in Chinese can neither be converted into users' desired layouts nor be recognized until their characters, lines, and paragraphs are correctly extracted. There are many errors in automatically segmented digital ink texts in Chinese because they are free forms and mixed with other languages, as well as their Chinese characters have small gaps and complex structures. Paragraphs, lines, and characters (recognizable language symbols) in digital ink may be wrongly extracted. An adaptive approach based on context is proposed to correct wrongly extracted these objects. Each extracted object is first adaptively visualized by color and shape labels according to relations between it and its neighbors. Users use simple gestures naturally and easily to merge and split wrongly extracted objects. Contexts are constructed from users' gestures and objects invoked by them, where users' intensions are identified. We have conducted experiments using real-life segmented digital ink texts in Chinese and compared the proposed approach with others. Experimental results demonstrate that the proposed approach is feasible, flexible, effective, and robust.",2010,0,
392,393,Memory Yield Improvement through Multiple Test Sequences and Application-Aware Fault Models,"In this paper, we propose a way to improve the yield of memory products by selecting the appropriate test strategy for memory Built- in Self-Test (BIST). We argue that by testing the memory through a sequence of test algorithms which differ in their fault coverage, it is possible to bin the memory into multiple yield bins and increase the yield and product revenue. Further, the test strategy must take into consideration the usage model of the memory. Thus, a number of video and audio buffers are used in sequential access mode, but are overtested using conventional memory test algorithms which model a large number of defects which do not impact the operation of the buffers. We propose a binning strategy where memory test algorithms are applied in different order of strictness such that bins have a specific defect / fault grade. Depending on the applications some of these bins need not be discarded but sold at a lower price as the functionality would never catch the fault due to its usage of memory. We introduce the notion of a test map for the on-chip memories in a SoC and provide results of yield simulation on two specific test strategies called ""Most Strict First"" and ""Least Strict First"". Our simulations indicate that significant improvements in yield are possible through the adoption of the proposed technique. We show that the BIST controller area and run-time overheads also reduce when information about the usage model of the memory, such as sequential access, is exploited.",2008,0,
393,394,Correcting asr outputs: Specific solutions to specific errors in French,"Automatic speech recognition (ASR) systems are used in a large number of applications, in spite of the inevitable recognition errors. In this study we propose a pragmatic approach to automatically repair ASR outputs by taking into account linguistic and acoustic information, using formal rules or stochastic methods. The proposed strategy consists in developing a specific correction solution for each specific kind of errors. In this paper, we apply this strategy on two case studies specific to French language. We show that it is possible, on automatic transcriptions of French broadcast news, to decrease the error rate of a specific error by 11.4% in one of two the case studies, and 86.4% in the other one. These results are encouraging and show the interest of developing more specific solutions to cover a wider set of errors in a future work.",2008,0,
394,395,The Design of Fault Diagnosis Expert System about Temperature Adjustment System Based on CLIPS,"The fault diagnosis of a certain launching unit's temperature controller is researched with the object-oriented programming method based on expert system theory, the fault-diagnosis expert-system software is designed and developed with VC++ 2008 and CLIPS. The structure of the system is firstly analyzed; The representation of knowledge, the design of database and the production rule are discussed then; lastly the diagnosis flow is studied up and a demonstration of the fault diagnosis is given.",2009,0,
395,396,A highly selective super-wide bandpass filter by cascading HMSIW with asymmetric defected ground structure,"The half mode substrate integrated waveguide (HMSIW) possesses the highpass characteristic of SIW but the size is nearly half reduced. A recently proposed asymmetric defected ground structure (ADGS), composed of two square headed slots connected with a rectangular slot transversely under a microstrip line, exhibits quasi-elliptic-function band-reject characteristics around 3 GHz with high selectivity. Based on the circuit model, the structure of the ADGS is modified to perform well at about 16 GHz. By combining the HMSIW and the modified ADGS, a super-wide bandpass filter operating at about 8-16 GHz with high selectivity at both upper and lower band is proposed. Both simulated and measured results have been presented to demonstrate the validity of the proposed wideband filter.",2010,0,
396,397,A real-time fault tolerant intra-body network,"This paper designs an intra-body network (IBN) of nodes, consisting of small sensors and processing elements (SPEs) placed at different locations within the body and a personal digital assistant placed externally but in close proximity to the body. The sensors measure specific physiological attributes such as electrophysiological and biochemical changes in the myocardium (action potentials of cells), glucose level, blood viscosity etc. and forward them to the processing element. Communication protocols for configuration and data access protocols are proposed. The privacy of the IBN data, fault tolerance and real-time data acquisition are addressed.",2002,0,
397,398,Evaluating speech recognition in the context of a spoken dialogue system: critical error rate,"Evaluating a speech recognition system is a key issue towards understanding its deficiencies and focusing potential improvements on useful aspects. When a system is designed for a given application, it is particularly relevant to have an evaluation procedure that reflects the role of the system in this application. Evaluating continuous speech recognition through word error rate is not completely appropriate when the speech recognizer is used as spoken dialogue system input. Some errors are particularly harmful, when they concern content words for example, while some others do not have any impact on the following comprehension step. The attempt is not to evaluate natural language understanding but to propose a more appropriate evaluation of speech recognition, by making use of semantic information to define the notion of critical errors.",2001,0,
398,399,Detecting VLIW Hard Errors Cost-Effectively through a Software-Based Approach,"Research indicates that as technology scales, hard errors such as wear-out errors are increasingly becoming a critical challenge for microprocessor design. While hard errors in memory structures can be efficiently detected by error correction code, detecting hard errors for functional units cost-effectively is a challenging problem. In this paper, we propose to exploit the idle cycles of the under-utilized VLIW functional units to run test instructions for detecting wear-out errors without increasing the hardware cost or significantly impacting performance. We also explore the design space of this software-based approach to balance the error detection latency and the performance for VLIW architectures. Our experimental results indicate that such a software-based approach can effectively detect hard errors with minimum impact on performance for VLIW processors, which is particularly useful for reliable embedded applications with cost constraints.",2007,0,
399,400,Evaluating the effectiveness of a software fault-tolerance technique on RISC- and CISC-based architectures,"This paper deals with a method able to provide a microprocessor-based system with safety capabilities by modifying the source code of the executed application, only. The method exploits a set of transformations which can automatically be applied, thus greatly reducing the cost of designing a safe system, and increasing the confidence in its correctness. Fault Injection experiments have been performed on a sample application using two different systems based on CISC and RISC processors. Results demonstrate that the method effectiveness is rather independent of the adopted platform",2000,0,
400,401,Experimental validation of fault injection analyses by the FLIPPER tool,The paper discusses the experimental validation of fault injection analyses accomplished with the FLIPPER tool. Validation has been accomplished through accelerated proton testing of a benchmark design provided by the European Space Agency.,2009,0,
401,402,A primary exploration of three-dimensional echocardiographic intra-cardiac virtual reality visualization of atrial septal defect: in vitro validation,"To evaluate the diagnostic value of three-dimensional echocardiography (3-DE) in congenital heart disease such as atrial septal defect (ASD) by virtual reality (VR), ten ASDs with different size and shape were created in ten fresh explained porcine hearts. HP SONOS 5500 imaging system was employed for 3-DE reconstructed and visualized by virtual reality computing techniques. The results showed that all ASDs were successfully reconstructed. The site, geometry were well appraised in its true form. The area, maximum and minimum diameter of ASD were measured on 3D reconstruction and compared with independently measured anatomic date. Good correlation was obtained (r>0.95, P<0.01). In conclusion, VR open an exciting opportunity in the field of diagnosis of 3-DE in congenital heart disease",2005,0,
402,403,A Fault-Tolerant Active Pixel Sensor for Mitigating Hot Pixel Defects,"Hot pixel defects are unavoidable in many solid-state image sensors. Affected pixels accumulate dark signal over the course of an exposure, grossly diminishing dynamic range and often rendering measurements unusable. Experiments suggest the mechanisms causing hot pixels are highly localized and the defect will be confined to a single pixel. A redundant, fault-tolerant active pixel sensor architecture that has previously been applied to other defect types is investigated for the suppression of hot pixels. A recovery scheme using minimal computational power is also described.",2007,0,
403,404,Testing for interconnect crosstalk defects using on-chip embedded processor cores,"Crosstalk effects degrade the integrity of signals traveling on long interconnects and must be addressed during production testing. External testing for crosstalk is expensive due to the need for high-speed testers. Built-in self-test, while eliminating the need for a high-speed tester, may lead to excessive test overhead as well as overly aggressive testing. To address this problem, we propose a new software-based self-test methodology for system-on-chip (SoC) devices based on embedded processors. It enables an on-chip embedded processor core to test for crosstalk in system-level interconnects by executing a self-test program in the normal operational mode of the SoC. We have demonstrated the feasibility of this method by applying it to test the interconnects of a processor-memory system. The defect coverage was evaluated using a system-level crosstalk defect simulation method.",2001,0,
404,405,Adaptive Fuzzy Prediction of Low-Cost Inertial-Based Positioning Errors,"Kalman filter (KF) is the most commonly used estimation technique for integrating signals from short-term high performance systems, like inertial navigation systems (INSs), with reference systems exhibiting long-term stability, like the global positioning system (GPS). However, KF only works well under appropriately predefined linear dynamic error models and input data that fit this model. The latter condition is rather difficult to be fulfilled by a low-cost inertial measurement unit (IMU) utilizing microelectromechanical system (MEMS) sensors due to the significance of their long- and short-term errors that are mixed with the motion dynamics. As a result, if the reference GPS signals are absent or the Kalman filter is working for a long time in prediction mode, the corresponding state estimate will quickly drift with time causing a dramatic degradation in the overall accuracy of the integrated system. An auxiliary fuzzy-based model for predicting the KF positioning error states during GPS signal outages is presented in this paper. The initial parameters of this model is developed through an offline fuzzy orthogonal-least-squares (OLS) training while the adaptive neuro-fuzzy inference system (ANFIS) is implemented for online adaptation of these initial parameters. Performance of the proposed model has been experimentally verified using low-cost inertial data collected in a land vehicle navigation test and by simulating a number of GPS signal outages. The test results indicate that the proposed fuzzy-based model can efficiently provide corrections to the standalone IMU predicted navigation states particularly position.",2007,0,
405,406,Fault tolerant PVFS2 based on data replication,"Aggregating the capacity and bandwidth of the commodity disks in the nodes of a cluster provides cost effective and high performance storage systems. Nevertheless, this strategy could be a feasible approach only if the mean time to failure of disks and nodes is faced. The number of failures increases with the nodes and it is especially important in parallel file systems, like PVFS, because having a file striped over server disks increases the probability of failures. This work proposes a strategy to include data replication in the second version of PVFS in order to provide fault tolerance. We also analyze the performance of the implementation of this approach.",2010,0,
406,407,Development of a motion correction system for respiratory-gated PET study,"A respiratory motion during whole-body imaging has been recognized as a source of image quality degradation and reduces the quantitative accuracy of positron emission tomography (PET) study. The aim of this study is to evaluate respiratory gating system and to develop a respiratory motion correction system using trigger generating device built in-house and gated-PET data acquisition mode. We utilized a commercially available laser optical sensor to detect respiratory motion during PET scanning. Each respiratory cycle is divided into 4 bins defined from average peak interval and irregular peak within the breathing motion. The acquired data within the time bins correspond to different positions within the breathing cycle and stored for the post motion correction. Motion data of diaphragm and chest wall was calculated by CT image acquisition during the normal inspiration and expiration position. In the images of a phantom, the blurring artifact due to breathing motion was reduced by our correction method. This technique improves the quantitative specific activity of the tracer which is distorted because of the respiratory motion.",2004,0,
407,408,Fault Diagnosis Method for Mobile Robots Using Multi-CMAC Neural Networks,"Multi-CMAC (cerebellar model articulation controller) neural networks based fault detection and diagnosis (FDD) method for mobile robots are proposed. Three failure types (system fault, sensor fault, and combined fault) are handled. Mobile robot system consists of several functional modules belonging to different module groups, which execute different tasks. According to the consistency among sensors information between the neighbor modules in the same module group, the method of fault diagnosis is studied. Then, multiple CMAC neural networks are used to implement the diagnosis. One CMAC neural network is set to one module group. In the neural network, the sensor information is used as the inputs and the fault signals are used as the outputs. As an example, the method is implemented on a drive system of a wheeled mobile robot. The simulation results show the effectiveness of the proposed technique.",2007,0,
408,409,Fault Evaluator: A tool for experimental investigation of effectiveness in software testing,"The specifications for many software systems, including safety-critical control systems, are often described using complex logical expressions. It is important to find effective methods to test implementations of such expressions. Analyzing the effectiveness of the testing of logical expressions manually is a tedious and error prone endeavor, thus requiring special software tools for this purpose. This paper presents Fault Evaluator, which is a new tool for experimental investigation of testing logical expressions in software. The goal of this tool is to evaluate logical expressions with various test sets that have been created according to a specific testing method and to estimate the effectiveness of the testing method for detecting specific faulty variations of the original expressions. The main functions of the tool are the generation of complete sets of faults in logical expressions for several specific types of faults; gaining expected (Oracle) values of logical expressions; testing faulty expressions and detecting whether a test set reveals a specific fault; and evaluating the effectiveness of a testing approach.",2010,0,
409,410,Calculation of correction factors to compensate for the reference electric field nonuniformity,"The inaccuracy of the reference electric field nonuniformity assessment is identified, which unnecessarily increases the measurement uncertainty of standardized systems for electric field-meters calibration, making them inadequate for calibration of modern, precision field-meters. By means of numerical field calculation, the correction factors are computed which allow compensation for the reference field nonuniformity. The uncertainty of that calculation is also estimated",2001,0,
410,411,Correction for continuous motion in small animal PET,"In small animal PET imaging experiments, animals are generally required to be anaesthetized to avoid motion artifacts. However, anaesthesia can alter biochemical pathways within the brain, thus affecting the physiological parameters under investigation. The ability to image conscious animals would overcome this problem and open up the possibility of entirely new investigational paradigms.",2008,0,
411,412,Short-circuit fault mitigation methods for interior PM synchronous machine drives using six-leg inverters,"This paper characterizes six-leg inverters to mitigate short-circuit faults for interior permanent magnet (IPM) synchronous machines. Key differences between bus structures in six-leg inverters are identified. For six-leg inverters employing two isolated DC links, it is shown that up to 75% of rated output power could be produced following a single-switch short-circuit fault. A magnet flux ing control method is proposed as a response to stator winding type short-circuit faults. This control method results in a zero-torque fault response by the motor. The important influence of the zero sequence in both the motor and inverter structure is identified and developed for this class of fault. Simulation and experimental results are presented verifying the proposed magnet flux ing control method.",2004,0,
412,413,Automatic red-eye detection and correction,"""Red-eye"" is a phenomenon that causes the eyes of flash photography subjects to appear unnaturally reddish in color. Though commercial solutions exist for red-eye correction, all of them require some measure of user intervention. A method is presented to automatically detect and correct redeye in digital images. First, faces are detected with a cascade of multi-scale classifiers. The red-eye pixels are then located with several refining masks computed over the facial region. The masks are created by thresholding per-pixel metrics, designed to detect red-eye artifacts. Once the redeye pixels have been found, the redness is attenuated with a tapered color desaturation. A detector implemented with this system corrected 95% of the red-eye artifacts in 200 tested images.",2002,0,
413,414,Implementing Probabilistic Risk Assessment with Fault Trees to support space exploration missions,This paper seeks to illustrate the implementation of a Probabilistic Risk Assessment (PRA) methodology as a foundation for space mission support risk assessment and management process. Identifying the risks to delivering expected spacecraft data services to a mission is only the first part of the risk assessment. Arriving at a quantified probability (Likelihood) of the manifestation of these risks is the desired outcome of the process.,2010,0,
414,415,Multi-View Video Coding Using Color Correction,The color variations between multi-view video sequences may degrade the inter-view prediction and result in low coding efficiency. In this paper we propose an efficient multi-view video coding scheme using dominant basic color mapping based color correction. The experimental coding results show that color correction has the potential to make multi-view video coding more efficient.,2008,0,
415,416,"We're Finding Most of the Bugs, but What are We Missing?","We compare two types of model that have been used to predict software fault-proneness in the next release of a software system. Classification models make a binary prediction that a software entity such as a file or module is likely to be either faulty or not faulty in the next release. Ranking models order the entities according to their predicted number of faults. They are generally used to establish a priority for more intensive testing of the entities that occur early in the ranking. We investigate ways of assessing both classification models and ranking models, and the extent to which metrics appropriate for one type of model are also appropriate for the other. Previous work has shown that ranking models are capable of identifying relatively small sets of files that contain 75-95% of the faults detected in the next release of large legacy systems. In our studies of the rankings produced by these models, the faults not contained in the predicted most fault prone files are nearly always distributed across many of the remaining files; i.e., a single file that is in the lower portion of the ranking virtually never contains a large number of faults.",2010,0,
416,417,Fault-Tolerant Algorithm for Distributed Primary Detection in Cognitive Radio Networks,"This paper attempts to identify the reliability of detection of licensed primary transmission based on cooperative sensing in cognitive radio networks. With a parallel fusion network model, the correlation issue of the received signals between the nodes in the worst case is derived. Leveraging the property of false sensing data due to malfunctioning or malicious software, the optimizing strategy, namely fault-tolerant algorithm for distributed detection (FTDD) is proposed, and quantitative analysis of false alarm reliability and detection probability under the scheme is presented. In particular, the tradeoff between licensed transmissions and user cooperation among nodes is discussed. Simulation experiments are also used to evaluate the fusion performance under practical settings. The model and analytic results provide useful tools for reliability analysis for other wireless decentralization-based applications (e.g., those involving robust spectrum sensing).",2009,0,
417,418,China's Research Status Quo and Development Trend of Power Grid Fault Diagnosis,"Fault diagnosis is the basic condition for smart grid to achieve the self-healing function, and it is also one of the important research topics of the intelligent dispatching decision support system. On the basis of analyzing its concept and aiming at the current status of China's studies, this paper reviewed and summarized several intelligent fault diagnosis methods, including expert system, artificial neural networks, rough set theory, data mining techniques, multi-agent technology and the entropy theory, then pointed out their application characteristics and existing problems, and finally the prospects of further development in this field were presented.",2010,0,
418,419,"Transparent, Incremental Checkpointing at Kernel Level: a Foundation for Fault Tolerance for Parallel Computers","We describe the software architecture, technical features, and performance of TICK (Transparent Incremental Checkpointer at Kernel level), a system-level checkpointer implemented as a kernel thread, specifi- cally designed to provide fault tolerance in Linux clusters. This implementation, based on the 2.6.11 Linux kernel, provides the essential functionality for transparent, highly responsive, and efficient fault tolerance based on full or incremental checkpointing at system level. TICK is completely user-transparent and does not require any changes to user code or system libraries; it is highly responsive: an interrupt, such as a timer interrupt, can trigger a checkpoint in as little as 2.5s; and it supports incremental and full checkpoints with minimal overhead-less than 6% with full checkpointing to disk performed as frequently as once per minute.",2005,0,
419,420,Algorithmic Cholesky factorization fault recovery,"Modeling and analysis of large scale scientific systems often use linear least squares regression, frequently employing Cholesky factorization to solve the resulting set of linear equations. With large matrices, this often will be performed in high performance clusters containing many processors. Assuming a constant failure rate per processor, the probability of a failure occurring during the execution increases linearly with additional processors. Fault tolerant methods attempt to reduce the expected execution time by allowing recovery from failure. This paper presents an analysis and implementation of a fault tolerant Cholesky factorization algorithm that does not require checkpointing for recovery from fail-stop failures. Rather, this algorithm uses redundant data added in an additional set of processors. This differs from previous works with algorithmic methods as it addresses fail-stop failures rather than fail-continue cases. The implementation and experimentation using ScaLAPACK demonstrates that this method has decreasing overhead in relation to overall runtime as the matrix size increases, and thus shows promise to reduce the expected runtime for Cholesky factorizations on very large matrices.",2010,0,
420,421,An automated methodology to diagnose geometric defect in the EEPROM cell,"The objective of this paper is to present an automated geometric defect diagnosis methodology for EEPROM cell (AGDE). This method focuses on speeding up the diagnosis process of geometric defects. It is based on a mathematical model generated with a ""design of simulation"" (DOS) technique. The DOS technique takes as input, simulations results of a floating gate transistor with different given geometries and produces, as output, a polynomial equation of the threshold voltage in function of the cell's geometric parameters. The diagnosis process is realized by comparing the measured threshold voltages of an EEPROM cell with the dynamically computed ones. From this comparison, the potentially defective geometric parameters are automatically extracted.",2002,0,
421,422,Multiple transient faults in logic: an issue for next generation ICs?,"In this paper, we first evaluate whether or not a multiple transient fault (multiple TF) generated by the hit of a single cosmic ray neutron can give rise to a bidirectional error at the circuit output (that is an error in which all erroneous bits are 1s rather than 0s, or vice versa, within the same word, but not both). By means of electrical level simulations, we show that this can be the case. Then, we present a software tool that we have developed in order to evaluate the likelihood of occurrence of such bidirectional errors for very deep submicron (VDSM) ICs. The application of this tool to benchmark circuits has proven that such a probability can not be neglected for several benchmark circuits. Finally, we evaluate the behavior of conventional self-checking circuits (generally designed accounting only for single TFs) with respect to such events. We show that the modifications generally introduced to their functional blocks in order to avoid output bidirectional errors due to single TFs (as required when an AUED code is implemented) can significantly reduce (up to the 40%) also the probability to have bidirectional errors because of multiple TFs.",2005,0,
422,423,Comparing fail-silence provided by process duplication versus internal error detection for DHCP server,"This paper uses fault injection to compare the ability of two fault-tolerant software architectures to protect an application from faults. These two architectures are Voltan, which uses process duplication, and Chameleon ARMORs, which use self-checking. The target application is a Dynamic Host Configuration Protocol (DHCP) server, a widely used application for managing IP addresses. NFTAPE, a software-based fault injection environment, is used to inject three classes of faults, namely random memory bit-flip, control-flow and high-level target specific faults, into each software architecture and into baseline Solaris and Linux versions",2001,0,
423,424,Quantification of PET and CT Data Misalignment Errors in Cardiac PET/CT:Clinical and Phantom Studies,"PET/CT units with high temporal resolution (particularly with 64-slice CT capability) are increasingly used as in clinical diagnosis and prognosis of cardiovascular disease. Since the CT sub-system in the combined PET/CT unit is used to perform attenuation correction of acquired PET data, misalignments between patient positioning for both scans can cause artifacts in the myocardial PET images potentially resulting in false positive artifacts. The aim of this study is to evaluate the misalignment effect (induced by spurious or physiological patient motion in-between the two modalities) on regional and global uptake values in the myocardial region. In this study, we used both phantom (RSD thorax phantom) and clinical studies (two FDG and one NH<sub>3</sub> rest/stress). Manual shifts between the CT and PET images ranging from 0 to 20 mm in six different directions were applied. Thereafter, attenuation correction was applied to the emission data using the manually shifted CT images in order to model patient motion between PET and CT. The reconstructed PET images using shifted CT images for attenuation correction were compared with the PET images corrected with the hypothetically misalignment free original CT image. The criteria and figures of merit used included VOI and linear regression analysis. The analysis was performed using 500 VOIs located within the myocardial wall in each PET dataset. The VOIs were uniformly distributed across all myocardial wall regions to assess the overall influence of PET and CT misalignment. The absolute percentage relative difference increased in all simulated movements with increasing misalignments for both phantom and clinical studies (up to 30% in some regions for the 20 mm shift). In conclusion, increasing the misalignment between PET and CT studies resulted in increased changes in the tracer uptake value within the myocardium both on a regional and global basis with respect to the reference as revealed by the various figures of meri- t used. The variation was more significant for right and down movements versus left and up directions.",2009,0,
424,425,Analysis of the Timing System Error of the Constellation Automatic Navigation,"System integrated clock (SIC) plays an important role in implementing the high-accuracy constellation automatic time synchronization and information exchange. In the establishment of SIC, error and noise are unavoidably introduced. In the paper, various error sources in the process are analyzed at first, and then an error-reduction method under the model of two-way plus common view time comparison is put forward and analyzed. Theory research and simulation experiment show that the constellation time synchronization error is below 10 seconds.",2007,0,
425,426,Formal guides for experimentally verifying complex software-implemented fault tolerance mechanisms,"Describes a framework allowing the experimental verification of complex software-implemented fault-tolerance algorithms and mechanisms (FTAMs). This framework takes into account two of the most important aspects which are increasingly required in newly-developed fault-tolerant systems: the considerations of COTS (commercial off-the-shelf) based architectures and the compliance with severe safety certification procedures. The strategy proposed shows how a rigorous FTAM specification, based on a multiple-viewpoint architectural description, may help to mechanically monitor the verification of its implementation under real conditions. The proposed strategy has been instantiated using two mechanized techniques: model checking and fault injection. The preliminary conclusions of the application of this automated approach to a small part of a commercial fault-tolerant system help us clarify its usage and its suitability for validating complex dependable systems",2001,0,
426,427,An improved fault locating system of distribution network based on fuzzy identification,"Fault locating system, which is designed for the fast power recovery, is very important in the economical operating of the distribution network. But, for the uncertainty of the fault information, the incorrect conclusion may be obtained by the traditional fault location calculation, so the most fault locating system can not be employed in the distribution network. In this paper, an improved fault locating system is proposed, which is composed of the fault signal acquisition unit and the fault location analysis center. Fuzzy identification is employed in the fault location analysis center to deal with the uncertainty of fault information. The failure and mistake rates of indicator action are used as the fuzzy parameters to calculate the fuzzy difference of the fault sequence and the standard fault set. The fault indicator is the primary device of fault information acquisition. The radio frequency and GPRS technology construct the communication channel of fault signal acquisition unit, which cuts down the construction cost and also ensures the obtaining accuracy of the fault information. The fault location system is working on the distribution network and operating well. With the accurate fault location, power supply recovers fast. The loss of power failure is reduced effectively.",2010,0,
427,428,"Pattern recognition-a technique for induction machines rotor fault detection ""eccentricity and broken bar fault""","A pattern recognition technique based on Bayes minimum error classifier is developed to detect broken rotor bar faults and static eccentricity in induction motors at the steady state. The proposed algorithm uses stator currents as input without any other sensors. First, rotor speed is estimated from stator currents, then appropriate features are extracted. The produced feature vector is normalized and fed to the trained Bayes minimum error classifier to determine if motor is healthy or has incipient faults (broken bar fault, static eccentricity or both). Only number of poles and rotor slots are needed as pre-knowledge information. Theoretical approach together with experimental results derived from a 3 hp AC induction motor show the strength of this method. In order to cover many different motor load conditions data are derived from 10% to 130% of the rated load for both a healthy induction motor and an induction motor with a rotor having 4 broken bars and/or static eccentricity.",2001,0,
428,429,Sinogram-based motion correction of PET images using optical motion tracking system and list-mode data acquisition,"A head motion during brain imaging has been recognized as a source of image degradation and introduces distortion in positron emission tomography (PET) image. There are several techniques to correct the motion artifact, but these techniques cannot correct the motion during scanning. The aim of this study is to develop a sinogram-based motion correction (SBMC) method to correct directly the head motion during PET scanning using a motion tracking system and list-mode data acquisition. This method is a rebinning procedure by which the lines of response (LOR) are geometrically transformed according to the current values of the six-dimensional motion data. Michelogram was recomposed using rebinned LOR and motion corrected sinogram was generated. In the motion corrected image, the blurring artifact due to motion was reduced by SBMC method.",2002,0,
429,430,Multi-Layer Immune Model for Fault Diagnosis,"Inspired by the multi-layer defense mechanism and incorporates the feedback mechanism in the nature immune system, the paper proposes a multi-layer immune model for fault diagnosis. In the multi-layer model, inherent immune layer direct recognition of known fault that could not cause influence to other nodes; propagation immune layer adopt the structure of the B- lymphocyte network to construct the fault propagation network for the fault localization; Adaptive immune layer learn the unknown fault pattern. Simulation results show that the multilayer immune diagnosis system has the properties of recognition, learning and memory.",2008,0,
430,431,Bit error rate of a digital radio eavesdropper on computer CRT monitors,"An eavesdropper on computer CRT (cathode ray tube) monitors can be used to intercept video information. Its anti-noise performance is analyzed in this paper. Baseband transmission models of digital signals are established according to the operating principle of the eavesdropper. The relationship between the eavesdropper's bit error rate and some parameters, such as intercept distance, superposition times and noise power, is discussed under the circumstances of ISI and ISI-freedom. Good agreement is obtained between experimental results and theoretical analysis.",2004,0,
431,432,High Continuous Availability Digital Information System Based on Stratus Fault-Tolerant Server,"With the construction of harmonious society, health improvement and the rapid development of information technology, People put forward higher requirements for the hospital. Hospital information system as an online services system requires continuous operation. Server system is the key to support hospital operations. System paralyzed accident caused by Server system failure is also not uncommon. Aiming at the problem of insufficient reliability of the traditional Cluster cluster server system, The article made a in-depth technical analysis on the performance of the Stratus fault-tolerant server. Combing with the characteristics of hospital information system, it proposed the digital hospital information system structure based on Stratus fault-tolerant server and explored and analyzed the economic and technical advantages of the program. The application effect demonstrates that the program is of the economic good and can realize continuous availability.",2010,0,
432,433,Estimation of Defects Based on Defect Decay Model: ED^{3}M,"An accurate prediction of the number of defects in a software product during system testing contributes not only to the management of the system testing process but also to the estimation of the product's required maintenance. Here, a new approach called ED<sup>3</sup>M is presented that computes an estimate of the total number of defects in an ongoing testing process. ED<sup>3</sup>M is based on estimation theory. Unlike many existing approaches the technique presented here does not depend on historical data from previous projects or any assumptions about the requirements and/or testers' productivity. It is a completely automated approach that relies only on the data collected during an ongoing testing process. This is a key advantage of the ED<sup>3</sup>M approach, as it makes it widely applicable in different testing environments. Here, the ED<sup>3</sup>M approach has been evaluated using five data sets from large industrial projects and two data sets from the literature. In addition, a performance analysis has been conducted using simulated data sets to explore its behavior using different models for the input data. The results are very promising; they indicate the ED<sup>3</sup>M approach provides accurate estimates with as fast or better convergence time in comparison to well-known alternative techniques, while only using defect data as the input.",2008,0,
433,434,A fault tolerant approach to microprocessor design,"We propose a fault-tolerant approach to reliable microprocessor design. Our approach, based on the use of an online checker component in the processor pipeline, provides significant resistance to core processor design errors and operational faults such as supply voltage noise and energetic particle strikes. We show through cycle-accurate simulation and timing analysis of a physical checker design that our approach preserves system performance while keeping area overheads and power demands low. Furthermore, analyses suggest that the checker is a fairly simple state machine that can be formally verified, scaled in performance, and reused. Further simulation analyses show virtually no performance impacts when our simple checker design is coupled with a high-performance microprocessor model. Timing analyses indicate that a fully synthesized unpipelined 4-wide checker component in 0.25 m technology is capable of checking Alpha instructions at 288 MHz. Physical analyses also confirm that costs are quite modest; our prototype checker requires less than 6% the area and 1.5% the power of an Alpha 21264 processor in the same technology. Additional improvements to the checker component are described which allow for improved detection of design, fabrication and operational faults.",2001,0,
434,435,Tolerating faults while maximizing reward,"The imprecise computation (IC) model is a general scheduling framework that is capable of expressing the precision vs. timeliness tradeoff involved in many current real-time applications. In that model, each task comprises mandatory and optional parts. While allowing greater scheduling flexibility, the mandatory parts in the IC model still have hard deadlines, and hence they must be completed before the task's deadline, even in the presence of faults. In this paper, we address fault-tolerant (FT) scheduling issues for IC tasks. First, we propose two recovery schemes, namely immediate recovery and delayed recovery. These schemes can be readily applied to provide fault tolerance to the mandatory parts by scheduling the optional parts appropriately for recovery operations. After deriving the necessary and sufficient conditions for both schemes, we consider the FT-optimality problem, i.e. generating a schedule which is FT and whose reward is maximum among all possible FT schedules. For immediate recovery, we present and prove the correctness of an efficient FT-optimal scheduling algorithm. For delayed recovery, we show that the FT-optimality problem is NP-hard, and thus is intractable",2000,0,
435,436,Incorporating fault tolerance in analog-to-digital converters (ADCs),The reliability of ADCs used in highly critical systems can be increased by applying a two-step procedure starting with sensitivity analysis followed by redesign. The sensitivity analysis is used to identify the most sensitive blocks which could then be redesigned for better reliability by incorporating fault tolerance. This paper illustrates the steps involved in incorporating fault tolerance in an ADC. Two redesign techniques to improve the reliability of a circuit are presented. Novel selective node resizing algorithms for increased tolerance against -particle induced transients are discussed.,2002,0,
436,437,Multiwave interaction analysis of a coaxial Bragg structure with a localized defect introduced in sinusoidal corrugations,"A multiwave interaction formulation is presented to investigate the effects of a localized defect on the reflective spectrum of a coaxial Bragg structure with sinusoidal corrugations. Good agreement has been achieved between the theoretical results obtained by the present formulation and those simulated by the software HFSS, which confirms the validity and the significance of the multiwave interaction formulation. It is found that, the localized defect creates defected eigenmodes within each reflective band gap of the initial standard Bragg structure, which the parameter can be controlled by the location of the localized defect.",2009,0,
437,438,Coupled field-circuit-mechanical model of an electromagnetic actuator operating in error actuated control system,"An algorithm of coupled field-circuit simulation of the dynamics of an electromagnetic linear actuator operating in error actuated control system is presented. The software consists of three main parts: (a) numerical model of the actuator dynamics which includes equations of a transient electromagnetic field in a non-linear conducting and moving medium, (b) discrete model of electric circuit and (c) optimization solver. Numerical implementation is based on the finite elements. The influence of the PID controller settings on the actuator operation is shown. In order to find optimal parameters of the system the genetic algorithm is applied. The simultaneous optimization of both: actuator structure and regulator settings has been carried out.",2008,0,
438,439,A comprehensive evaluation of capture-recapture models for estimating software defect content,"An important requirement to control the inspection of software artifacts is to be able to decide, based on more objective information, whether the inspection can stop or whether it should continue to achieve a suitable level of artifact quality. A prediction of the number of remaining defects in an inspected artifact can be used for decision making. Several studies in software engineering have considered capture-recapture models to make a prediction. However, few studies compare the actual number of remaining defects to the one predicted by a capture-recapture model on real software engineering artifacts. The authors focus on traditional inspections and estimate, based on actual inspections data, the degree of accuracy of relevant state-of-the-art capture-recapture models for which statistical estimators exist. In order to assess their robustness, we look at the impact of the number of inspectors and the number of actual defects on the estimators' accuracy based on actual inspection data. Our results show that models are strongly affected by the number of inspectors, and therefore one must consider this factor before using capture-recapture models. When the number of inspectors is too small, no model is sufficiently accurate and underestimation may be substantial. In addition, some models perform better than others in a large number of conditions and plausible reasons are discussed. Based on our analyses, we recommend using a model taking into account that defects have different probabilities of being detected and the corresponding Jackknife Estimator. Furthermore, we calibrate the prediction models based on their relative error, as previously computed on other inspections. We identified theoretical limitations to this approach which were then confirmed by the data",2000,0,
439,440,Implementing a reflective fault-tolerant CORBA system,"The use of reflection is becoming popular today for the implementation of non-functional mechanisms such as fault tolerance. The main benefits of reflection are separation of concerns between the application and the mechanisms and transparency from the application programmer point of view. Unfortunately, metaobject protocols (MOPs) available today are not satisfactory with respect to necessary features needed for implementing fault tolerance mechanisms. Previously, we proposed a specialised MOP based on Corba, well adapted for such mechanisms (M.-O. Killijian and J.C. Fabre, 1998). We deliberately focus on the implementation of this metaobject protocol using compile-time reflection and its use for implementing distributed fault tolerance. We present the design and the implementation of a fault-tolerant Corba system using this metaobject together with some preliminary experimental results. From the lessons learnt from this work, we briefly address the benefits of reflection in other layers of a system for dependability issues",2000,0,
440,441,The Optimized Combination of Fault Location Technology Based on Traveling Wave Principle,"The accuracy and the reliability of modern D-type double-ended and A-type single-ended traveling wave fault location principles used for transmission lines is comprehensively evaluated. Based on the evaluation, this paper presents the idea of optimized combination of location based on these two traveling wave principles, and successfully applies the idea in actual fault analysis of transient traveling waves. Compared with the traveling wave location schemes based on D-type or A-type principle alone, this scheme has the greatest advantages of utilizing the A-type traveling wave principle to verify and correct the location results obtained with the D-type traveling wave principle, so that both the location reliability and accuracy are enhanced. Practical applications showed that the optimized combination of traveling wave location schemes is feasible, and the location precision is improved significantly.",2009,0,
441,442,Autonomous cooperation technique to achieve fault tolerance in service oriented community system,"The advancement of mobile telecommunication and wireless technologies is required to provide local but familiar services in daily life, which has not been satisfied through the global services on the Internet. In retail business under evolving market, users request access to unknown but appropriate services based on their preference and situation, and retailers need to be aware of the current requirements of the majority of consumers in specific local trade areas. Because of the transience of the requirements of the users in their trade-areas, the services require being temporary and having the time limit. Therefore the areas of the services need to become narrower with the time. The concept of the service oriented community has been proposed to satisfy both the users and the retailers requirements. It consists of members in the specified area based on services, and they cooperate with each other in order to get mutual benefits. For realization of the service oriented community, the systems require flexibility for the effective provision of the services and fault tolerance for the stable service. In the service oriented community system, the Time Distance has been introduced as the efficient measure of the distance between the users and the retailers. The Time Distance Oriented Service System architecture has been proposed to satisfy these requirements for flexible and stable services, where the nodes are autonomously distribute services and reduce the service area based on the time distance. Here autonomous cooperation technique for achieving fault tolerance is proposed in order to satisfy the requirement of high service availability.",2002,0,
442,443,A fault-tolerant approach to network security,"Summary form only given. The increasing use of the Internet, especially for internal and business-to-business applications has resulted in the need for increased security for all networked systems to avoid unauthorized access and use. A failure of network security can effectively close the business, its availability is vital to operations. Vital functions such as firewalls and VPNs must remain in operation without loss of time for fallover, without loss of data and must be able to be placed even at remote locations where support personnel may not be readily available. Network firewalls are the first, and often are the only, line of defense against an attack. However, the firewall can be a double-edged sword. In operation, the firewall protects the network from everything from Denial of Service attacks to the entry of known viruses and unauthorized intrusion. If the firewall falls, there are generally only two options: Leave the network open to all or shut down access by anyone. The default condition is to close everything off, but this can be as disastrous as leaving the network open. Due to the importance of the firewall, most leading firewall software provides some method of establishing a form of fail-over redundancy for high availability. Yet in most cases this means some form of clustering using a secondary system as a backup with specialty software to detect and respond to a failure of the primary firewall. Such a clustered approach introduces additional complexity when establishing and configuring the firewall and additional complexity when upgrading. It also adds dramatically to the cost, not only in the hardware for the firewall, but in additional software copies and in the expertise for clustering support software required to establish and maintain the cluster. The approach we will discuss examines the creation of network security based on a hardware approach to fault tolerance. This approach will dramatically reduce the system complexity, simultaneously eliminating the need for special clustering software and special expertise for configuring the system for the kind of continuous availability that is the objective of the network security application. In addition, because the hardware approach is something that is designed in from the inception of the system, there are additional advantages. The fault tolerance is not an afterthought, but rather the purpose of the hardware, meaning that the system can be made to function very smoothly with very little administration. Failure of a part of the system is seamlessly recovered by the redundant elements, without loss of data in memory or loss of state for the system. In sum, this paper discusses the ability to create network security that reaches the standard of being continuously available, what is often referred to as the ""Holy Grail of reliability,"" 99.999% uptime",2001,0,
443,444,Research of error correction of LEO satellite orbit prediction for vehicle-borne tracking and position device,"Vehicle-borne tracking and position device is used to track LEO satellite. Because of the absence of the target which might be caused by cloud or zenith blind zone, the forecasting data will be used to acquire the target. While orbit prediction has serious errors, the target is always missed. Meanwhile, in its application to the vehicle-borne tracking and position device, due to the base of instability in the tracking process, it will result in significant difference between predicting data and tracking data, so the target will be not tracked rapidly. We applied tracking data to predict satellite orbits by improving Laplace method, and then corrected the error between Predicted data and actual measured data by interpolation method of Lagrange which improves the accuracy of prediction values. The testing data shows the accuracy of predicted data ranging from 3' to 10' for both azimuth and elevation when extrapolated satellite orbit to 7 seconds time.",2010,0,
444,445,Exploiting Mobile Agents for Structured Distributed Software-Implemented Fault Injection,"Embedded distributed real-time systems are traditionally used in safety-critical application areas such as avionics, healthcare, and the automotive sector. Assuring dependability under faulty conditions by means of fault tolerance mechanisms is a major concern in safety-critical systems. From a validation perspective, Software-Implemented Fault Injection (SWIFI) is an approved means for testing fault tolerance mechanisms. In recent work, we have introduced the concept of using mobile agents for distributed SWIFI in time-driven real-time systems. This paper presents a prototypical implementation of the agent platform for the OSEKtime real-time operating system and the FlexRay communication system. It is further shown, how to implement fault injection experiments by means of mobile agents in a structured manner following a classification of faults in terms of domain, persistence, and perception. Based on experiments conducted on ARM-based platforms, selected results are described in detail to demonstrate the potential of mobile agent based fault injection.",2006,0,
445,446,Analytical approach to internal fault simulation in power transformers based on fault-related incremental currents,"A new method for simulating faulted transformers is presented in this paper. Unlike other methods proposed in the literature, this method uses the data obtained from any sound transformer simulation to obtain the damaged condition by simply adding a set of calculated currents. These currents are obtained from the definition of the fault. The model is fully based on determining the incremental values exhibited by the currents in phases and lines from the prefault to the postfault condition. As a consequence, data obtained from simulation of the sound transformer may be readily used to define the damaged condition. The model is described for light and severe faults, introducing this latter feature as a further add-on feature to the low-level faults simulation. The technique avoids the use of complex routines and procedures devoted to specially simulate the internal fault. Of prompt application to relay testing, the proposed analytical model also gives an insight into the fault nature by means of the investigation of symmetrical components. In contrast with its low complexity, the method has shown to present large accuracy for simulating the fault performance.",2006,0,
446,447,Automated Bug Neighborhood Analysis for Identifying Incomplete Bug Fixes,"Although many static-analysis techniques have been developed for automatically detecting bugs, such as null dereferences, fewer automated approaches have been presented for analyzing whether and how such bugs are fixed. Attempted bug fixes may be incomplete in that a related manifestation of the bug remains unfixed. In this paper, we characterize the completeness of attempted bug fixes that involve the flow of invalid values from one program point to another, such as null dereferences, in Java programs. Our characterization is based on the definition of a bug neighborhood, which is a scope of flows of invalid values. We present an automated analysis that, given two versions P and P' of a program, identifies the bugs in P that have been fixed in P', and classifies each fix as complete or incomplete. We implemented our technique for null-dereference bugs and conducted empirical studies using open-source projects. Our results indicate that, for the projects we studied, many bug fixes are not complete, and thus, may cause failures in subsequent executions of the program.",2010,0,
447,448,Applying FIRMAMENT to test the SCTP communication protocol under network faults,"How to apply a fault injector to evaluate the dependability of a network protocol implementation is the main focus of this paper. In the last years, we have been developing FIRMAMENT, a tool to inject faults directly into messages that pass through the kernel protocol stack. SCTP is a promising new protocol over IP that, due its enhanced reliability, is competing with TCP where dependability has to be guaranteed. Using FIRMAMENT we evaluate the error coverage and the performance degradation of SCTP under faults. Performing a complete fault injection campaign over a third party software give us a deep insight about the additional test strategies that are needed to reach significant dependability measures.",2009,0,
448,449,Performance Evaluation of Probe-Send Fault-tolerant Network-on-chip Router,"With increasing reliability concerns for current and next generation VLSI technologies, fault-tolerance is fast becoming an integral part of system-on-chip and multi-core architectures. Another trend for such architectures is network-on-chip (NoC) becoming a standard for on-chip global communication. In an earlier work, a generic fault-tolerant routing algorithm in the context of NoCs has been presented. The proposed routing algorithm works in two phases, namely path exploration (PE) and normal communication. This paper presents fundamental insights into various novel PE approaches, their feasibility and performance trade-offs for k-ary 2-cube NoCs. The dependence of the normal communication phase on the probability of finding paths and their quality in the first phase emphasizes the PE's significance. One major contribution of this work is the investigation of application of constrained randomness to PE for optimizing the quality of paths. Another contribution is the proposed use of merging of traffic to reduce the reconfiguration time by a large amount (73.8% on an average).",2007,0,
449,450,Impact of Error Characteristics of an Indoor 802.11g WLAN on TCP Retransmissions,"In this paper we present the results of extensive measurements made over an experimental wired-to-wireless testbed, which consisted of a TCP protocol combined with a real-world indoor IEEE 802.11g WLAN. We investigated the effects of signal attenuation due to client distance from the AP on the 802.11 frame error rates (FER) and consequently on the segment loss rates and retransmission behavior of TCP at the sender in the fixed network. Specifically, we experimented with different modulation schemes belonging to the OFDM 802.11g PHY in order to gauge differences in performance between them, comparing real-world FERs calculated from actual frame captures against SNR, for both the forward and reverse WLAN channel directions. We also present real-world distributions of frame retransmissions made over the WLAN by the AP, with useful findings. Our results confirm that the reverse channel of a WLAN possesses a higher FER than the forward channel, and poses a greater threat to TCP's retransmission mechanism.",2008,0,
450,451,Statistical feature representations for automatic wood defects recognition research and applications,"In this paper, we introduce the non-negative matrix factorization (NMF) to decompose the wood images and structure the feature spaces. Local binary pattern (LBP) is used to extract the original spatial local structure features, such as curly edges, etc. and they have better luminance adaptability. Simultaneously, dual-tree complex wavelet transform (DTCWT) is used to extract the energy based statistical features from different directions and frequencies and they can maintain better time-frequency localized characteristics and finite data redundancy. We integrate the features together to choose the proper features to describe the discrepancies between sound woods and defects and then propose an automatic detection system for wood defects recognition. After many cross experiments, we received a better identification rate of more than 90% with good research values and potential applications.",2009,0,
451,452,Error concealment for stereoscopic images using mode selection,"In this paper, a novel error concealment (EC) method for compressed stereoscopic image pairs is presented, which contains a new binocular EC mode and an improved monocular EC mode. The proposed algorithm selects appropriate EC mode to conceal the error block (EB) in the stereoscopic image according to the local characteristic of the EB. The experimental results demonstrate that the proposed scheme has good subjective and objective EC performance in stereoscopic images as compared to monocular mode.",2010,0,
452,453,Fault tolerance of CNC software based on artificial neural network,"This paper proposes an efficient method for realizing the fault tolerance of CNC software with the introduction of artificial neural network (ANN) to the design filed of CNC software. In addition, function aspects (velocity, acceleration, chord error, real time, prediction accuracy) from the experiment on Non-Uniform Rational B-Spline (NURBS) interpolator based on ANN were evaluated in detail. Our experimental results showed that the NURBS interpolation based on ANN not only meet the requirements of function aspects, but also can realize fault tolerance technology, which may provide a new strategy in the improvement of the reliability of CNC software.",2010,0,
453,454,PMSG Wind Turbine Performance Analysis During Short Circuit Faults,"Due to the increasing price of fossil fuels and the security concerns of the nuclear energy, electricity generation using wind turbines has recently attracted significant attention after a period of neglect. Among different types of wind turbine generators, PM synchronous generators (PMSG) offer better performance due to higher efficiency and less maintenance since they do not have rotor current and could be used without a gear box. In addition, the utilization of a double conversion results in higher flexibility compared with other wind turbine systems. This paper develops the model of a PMSG wind turbine and then simulates short circuits to evaluate the performance of the system during short circuit fault. Since PMSG wind turbine systems use a double conversion converter, this paper develops two methods for controlling the converter, a new protection method for capacitor over voltage is also evaluated in this paper.",2007,0,
454,455,A novel fault diagnosis algorithm for K-connected distributed clusters,"In this paper, we propose an on-line two phase (TPD) fault diagnosis algorithm for distributed clusters that follows an arbitrary network topology with connectivity k. Intermediate nodes communicate heartbeat messages between different source destination pairs. The algorithm addresses a realistic fault model considering crash and value faults in the cluster nodes. The algorithm is shown to produce a time complexity of O(l) and message complexity of O(n. e) respectively. The algorithm has been simulated using discrete event simulation techniques and the results show that the algorithm is feasible for large distributed clusters.",2010,0,
455,456,The effect of the time window width of correlation method on single-ended modern traveling wave based fault location principles,"In the technique of single-ended modern traveling wave based fault location principles for transmission lines, traveling wave correlation method is a classical algorithm applied to detect the fault reflected surge. But in actual application, the lack of an effective way to choose the time window results in the limitation of correlation method - the time window width affects the correlation coefficient value which is an important indicator to show the similarity of the waveforms. This paper presents a new conception called optimal time window width of correlation method, and analyzes different factors probably affecting the width by means of EMTP-ATP and Matlab applied to simulations. Further more, the basic idea of new correlation method based on multi-time-window is proposed, which could be applied to improve the reliability of fault location technique.",2008,0,
456,457,Hierarchical application aware error detection and recovery,"Proposed is a four-tired approach to develop and integrate detection and recovery support at different levels of the system hierarchy. The proposed mechanisms exploit support provided by (i) embedded hardware, (ii) operating system, (iii) compiler, and (iv) application.",2004,0,
457,458,A Statistical Approach for Estimating the Correlation between Lightning and Faults in Power Distribution Systems,"The paper deals with the subject of the source-identification of transient voltage disturbances in distribution system buses. In particular, a statistical procedure is proposed for the evaluation of the probability that a lightning flash detected by a lightning location system (LLS) could cause a fault and, therefore, relay interventions, generally associated with voltage dips. The proposed procedure is based on the coordinated use of the information provided by the LLS and the availability of an advanced simulation tool for the accurate simulation of lightning-induced voltages on complex power systems, namely the LIOV-EMTP code. The uncertainty levels of the stroke location and of the peak current estimations provided by the LLS are discussed and their influence on the lightning-fault correlation is analyzed",2006,0,
458,459,Fault-tolerant five-phase permanent magnet motor drives,"In this paper, a control strategy that provides fault tolerance to five-phase permanent magnet motors is introduced. In this scheme, the five-phase permanent magnet (PM) motor continues operating safely under loss of up to two phases without any additional hardware connections. This feature is very important in traction and propulsion applications where high reliability is of major importance. The five-phase PM motors with sinusoidal and quasi-rectangular back-EMFs have been considered. To obtain the new set of phase currents to be applied to the motor during fault in stator phases or inverter legs, the torque producing MMF by the stator is kept constant under healthy and faulty conditions for both cases. Simulation and experimental results are provided to verify that the five-phase motor continues operating continuously and steadily under faulty conditions.",2004,0,
459,460,"Performance of Multicode DS/CDMA With Noncoherent <formula formulatype=""inline""> <img src=""/images/tex/964.gif"" alt=""M""> </formula>-ary Orthogonal Modulation in the Presence of Timing Errors","This paper derives an accurate approximation to the bit error rate (BER) of multicode direct-sequence code-division multiple access (DS/CDMA) with noncoherent <i>M</i>-ary modulation in wideband fading channels when timing errors are made at the receiver employing equal-gain combining (EGC). This reflects the practical scenario where the path delays are estimated imperfectly, leading to synchronization errors between the correlation receivers and the received signals. The analysis can be applied to any type of a fading distribution for both independent and correlated diversity branches. It is shown that the derived analytical expressions are in close agreement with the Monte Carlo system simulations, particularly in the case of small timing errors.",2008,0,
460,461,Fault tolerance as an aspect using JReplica,"Reliability and availability are very important trends in the development process of distributed systems. In order to improve these features, object replication mechanisms have been introduced. Programming replication policies for a given application is not an easy task, and this is the reason why transparency for the programmer has been one of the most important properties offered by all replication models. However, this transparency for the programmer is not always desirable. In this paper we present a replication model, JReplica, based on Aspect Oriented Programming (AOP). JReplica allows the separated specification of the replication code from the functional behaviour of objects, providing not only a high degree of transparency, as done by previous models, but also the possibility for programmers to introduce new behaviour to specify different fault tolerance requirements. Moreover, the replication aspect has been introduced at design time, and in this way, UML has been extended in order to consider replication issues separately when designing fault tolerance systems",2001,0,
461,462,A novel transmission line test and fault location methodology using pseudorandom binary sequences,"A novel pulse echo test methodology, using pseudorandom binary sequence (PRBS) excitation, is presented in this paper as an alternative to time domain reflectometry (TDR) for transmission line fault location and identification. The essential feature of this scheme is the cross correlation (CCR) of the fault response echo with the PRBS test input stimulus input which results in a unique signature for identification of the fault type, if any, or load termination present as well as its distance from the point of test stimulus injection. This fault identification method can used in a number of key industrial applications incorporating printed circuit boards, overhead transmission lines and underground cables in inaccessible locations which rely on a pathway for power transfer or signal propagation. As an improved method PRBS fault identification can be performed over several cycles at low amplitude levels online to reject normal signal traffic and extraneous noise pickup for the purpose of multiple fault coverage, resolution and identification. In this paper a high frequency co-axial transmission line model is presented for transmission line behavioural simulation with PRBS stimulus injection under known load terminations to mimic fault conditions encountered in practice for proof of concept. Simulation results, for known resistive fault terminations, with measured CCR response demonstrate the effectiveness of the PRBS test method in fault type identification and location. Key experimental test results are also presented for a co-axial cable, under laboratory controlled conditions, which substantiates the accuracy of PRBS diagnostic CCR method of fault recognition and location using a range of resistive fault terminations. The accuracy of the method is further validated through theoretical calculation via known co-axial cable parameters, fault resistance terminations and link distances in transmission line experimental testing.",2008,0,
462,463,Lightweight Fault-Tolerance Mechanism for Distributed Mobile Agent-Based Monitoring,"Thanks to asynchronous and dynamic natures of mobile agents, a certain number of mobile agent-based monitoring mechanisms have actively been developed to monitor large scale and dynamic distributed networked systems adaptively and efficiently. Among them, some mechanisms attempt to adapt to dynamic changes in various aspects such as network traffic patterns, resource addition and deletion, network topology and so on. However, failures of some domain managers are very critical to providing correct, real-time and efficient monitoring functionality in a large-scale mobile agent-based distributed monitoring system. In this paper, we present a novel fault- tolerance mechanism to have the following advantageous features appropriate for large-scale and dynamic hierarchical mobile agent-based monitoring organizations. It supports fast failure detection functionality with low failure-free overhead by each domain manager transmitting heart-beat messages to its immediate higher-level manager. Also, it minimizes the number of non-faulty monitoring managers affected by failures of domain managers. Moreover, it allows consistent failure detection actions to be performed continuously in case of agent creation, migration and termination, and is able to execute consistent takeover actions even in concurrent failures of domain managers.",2009,0,
463,464,A Superstring Galaxy Associative Memory Model with Expecting Fault-Tolerant Fields,"The synthesis problems of associative memory models are not better solved until now. Learning from the idea of superstring theory, a design method of superstring galaxy associative memory model with expecting fault-tolerant field is proposed by making the sphere mapping and giving the algorithm of galaxy covering. The method better solves a difficult synthesis problem of associative memory models. The superstring galaxy associative memory model designed by the method can have expecting fault-tolerant fields of the samples.",2009,0,
464,465,A Model-based Simulation Approach to Error Analysis of IT Services,"Utility computing environments provide on-demand IT services to customers. Such environments are dynamic in nature and continuously adapt to changes in requirements and system state. Errors are an important category of environment state changes as such environments consist of a large number of components, and hence, are subject to errors. In this paper, we design and implement a model-based simulation framework that leverages information about existing service components and their interactions, and provides concrete service behavior in presence of a variety of errors. To evaluate the framework, experiments are conducted on a virtualized blade-server based environment. Results show that the framework is effective and practical in analyzing error impacts on IT services.",2007,0,
465,466,Power quality improvement using a new structure of fault current limiter,"In this paper, power quality improvement by using a new structure of non superconducting fault current limiter (NSFCL) is discussed. This structure prevents voltage sags on Point of Common Coupling (PCC) just after fault occurrence, because of its fast operation. On the other hand, previously used structures produce harmonics on load voltage and have ac losses in normal operation. New structure has solved this problem by using dc voltage source. The proposed structure is simulated using PSCAD/EMTDC software and simulation results are presented to validate the effectiveness of this structure.",2010,0,
466,467,3 Faults Tolerant Orthogonal RAID for Large Storage,"Recently, the demand of low cost large scale storage increases. We developed VLSD (Virtual Large Scale Disks) toolkit for constructing virtual disk based distributed storages, which aggregate free spaces of individual disks. However, in order to construct large-scale storage, more than or equal to 3 fault tolerant RAID is important. In this paper, we propose MeshRAID that is 3 fault tolerant orthogonal RAID. And, we implement MeshRAID using VLSD. It is easy to implement MeshRAID using various classes in VLSD. From the viewpoint of its features, MeshRAID is addressed between RAID55 and NaryRAID.",2010,0,
467,468,Responsive Fault-Tolerant Computing in the Era of Terascale Integration State of Art Report,"Scaling in hardware integration process results in IC-process geometry reductions, lower operating voltages and increased clock speeds. This paper first surveys the reliability obstacles these developments give rise to and then points out that computing systems can no longer be safely assumed to fail only by crashing. Yet this assumption is at the core of primary-backup replication which the literature presents as the appropriate, and hence the most widely used, strategy for time-critical fault-tolerant applications. The paper then observes that building computing nodes with announced crash failure mode is a promising way forward to deal with the emerging reliability challenges. Work carried out to assure such a failure mode has also been briefly surveyed.",2008,0,
468,469,Formal Analysis of a Distributed Fault Tolerant Clock Synchronization Algorithm for Automotive Communication Systems,"A synchronized time base is indispensable for a time- triggered system since all activities in such a system are triggered by the passage of time. Distributed fault-tolerant clock synchronization algorithms are normally used to achieve the synchronized time base. As a state-of-the-art representative of the time-triggered systems for automotive applications, FlexRay uses a fault-tolerant mid-point algorithm to achieve the synchronized time base. Correctness of the algorithm plays a crucial role as most of the protocol services rely on the fact that there exists a synchronized time base in the system. Due to the distinguished characteristics of the algorithm, we propose a case-analysis based technique for the formal analysis of the algorithm. We show that the case analysis technique can greatly facilitate our formal analysis of the algorithm. Mechanical support with Isabelle/HOL, a theorem prover, is also discussed.",2008,0,
469,470,Experimental analysis of the errors induced into Linux by three fault injection techniques,"The main goal of the experimental stud), reported in this paper is to investigate to what extent distinct fault injection techniques lead to similar consequences (errors and failures). The target system we are using to carry out our investigation is the Linux kernel as it provides a representative operating system. It is featuring full controllability and observability thanks to its open source status. Three types of software-implemented fault injection techniques are considered, namely: i) provision of invalid values to the parameters of the kernel calls, ii) corruption of the parameters of the kernel calls, and iii) corruption of the input parameters of the internal functions of the kernel. The workload being used for the experiments is tailored to activate selectively each functional component. The observations encompass typical kernel failure modes (e.g., exceptions and kernel hangs) as well as a detailed analysis of the reported error codes.",2002,0,
470,471,A New Neural-Network-Based Fault Diagnosis Approach for Analog Circuits by Using Kurtosis and Entropy as a Preprocessor,"This paper presents a new fault diagnosis method for analog circuits. The proposed method extracts the original signals from the output terminals of the circuits under test (CUTs) by a data acquisition board and finds the kurtoses and entropies of the signals, which are used to measure the high-order statistics of the signals. The entropies and kurtoses are then fed to a neural network as inputs for further fault classification. The proposed method can detect and identify faulty components in an analog circuit by analyzing its output signal with high accuracy and is suitable for nonlinear circuits. Preprocessing based on the kurtosis and entropy of signals for the neural network classifier simplifies the network architecture, reduces the training time, and improves the performance of the network. The results from our examples showed that the trochoid of the entropies and kurtoses is unique when the faulty component's value varies from zero to infinity; thus, we can correctly identify the faulty components when the responses do not overlap. Applying this method for three linear and nonlinear circuits, the average accuracy of the achieved fault recognition is more than 99%, although there are some overlapping data when tolerance is considered. Moreover, all the trochoids converge to one point when the faulty component is open-circuited, and thus, the method can classify not only soft faults but also hard faults.",2010,0,
471,472,A NURBS-based error concealment technique for corrupted images from packet loss,"An error concealment using non-uniform rational B-spline (NURBS) is proposed. NURBS has been employed by many CAD/CAM systems as a fundamental geometry representation. Despite the fact that NURBS has gained tremendous popularity from the CAD/CAM and computer graphics community, its application on exploring the image problem only received little attention. On the other hand, the image contents might be corrupted or lost during transmission. Although there are quite a few existing techniques, yet developing an effective approach to conceal the error remains as one of the hottest research topics. Thus the aim of this study is to develop an image reconstruction technique using NURBS. The key idea is to use NURBS to represent the portion of image data without corruption. By accomplishing this, a single-hidden layer neural network is employed to learn the appropriate control points of NURBS. After learning, NURBS is then used to render the corrupted image data. Experimental results indicate that the proposed approach exhibits promising performance.",2002,0,
472,473,Adaptive Cancellation of a Sinusoidal Disturbance with Rapidly Varying Frequency Using an Augmented Error Algorith,This paper considers a compensator for a sinusoidal disturbance with known but rapidly varying frequency. The compensator is obtained as an adaptive feedforward cancellation algorithm using an augmented error. The system is shown to be Lyapunov stable and equivalent to a linear time-varying controller that includes an internal model of the disturbance. The stability and robustness properties of the augmented error algorithm are validated by simulation results,2005,0,
473,474,In-system partial run-time reconfiguration for fault recovery applications on spacecrafts,"This paper presents a methodology for partially reconfiguring a field programmable gate array (FPGA) device using only limited onboard resources. This paper also seeks to provide a roadmap to developing necessary tools and technologies to help design self-sufficient partial run-time reconfigurable systems for spacecraft avionic systems. To provide a vision for the technology, this paper recommends a few possible applications in spacecraft avionic systems, in fault tolerance and space-saving hardware. In addition, some previous work done on the research for reconfigurable, modular avionics are also presented at the end as an example of applications.",2005,0,
474,475,Application of Aircraft Fuel Fault Diagnostic Expert System Based on Fuzzy Neural Network,Theories of expert system and fuzzy artificial neural network (ANN) are applied to solve the problem of fault diagnosis in the aircraft fuel system. A multilayer neural network model of the aircraft fuel system is put forward and the integrated aircraft fuel fault diagnostic expert system which solves the problems of knowledge representation and knowledge acquisition of traditional expert system is realized. The hardware-in-loop simulation results show that the expert system diagnoses the fault in accessories rapidly and accurately and it is proved that the expert system is significative and helpful for further development in the aircraft fuel fault diagnosis.,2009,0,
475,476,Inverse wave field extrapolation: a different NDI approach to imaging defects,"Nondestructive inspection (NDI) based on ultrasound is widely used. A relatively recent development for industrial applications is the use of ultrasonic array technology. Here, ultrasonic beams generated by array transducers are controlled by a computer. This makes the use of arrays more flexible than conventional single-element transducers. However, the inspection techniques have principally remained unchanged. As a consequence, the properties of these techniques, as far as characterization and sizing are concerned, have not improved. For further improvement, in this paper we apply imaging theory developed for seismic exploration of oil and gas fields on the NDI application. Synthetic data obtained from finite difference simulations is used to illustrate the principle of imaging. Measured data is obtained with a 64-element linear array (4 MHz) on a 20-mm thick steel block with a bore hole to illustrate the imaging approach. Furthermore, three examples of real data are presented, representing a lack of fusion defect, a surface breaking crack, and porosity",2007,0,
476,477,A decomposition approach to the inverse problem-based fault diagnosis of liquid rocket propulsion systems,"The health monitoring of propulsion systems has being been one of the most challenging issues in space launch vehicles, particularly for the manned space missions. The development of an advanced health monitoring system involves many technical aspects, such as failure detection and fault diagnosis as well as the integration of hardware and algorithms, for improving the safety and reliability of propulsion systems. The inverse problem-based strategy provides a new solution to the design of model-based fault diagnosis methods for monitoring the health of propulsion systems. This paper presents a decomposition approach to the inverse problem-based fault diagnosis for a class of liquid rocket propulsion systems. Simulation results are provided for demonstrating the effectiveness of the proposed approach to the inverse problem-based fault diagnosis.",2004,0,
477,478,A Unified Environment for Fault Injection at Any Design Level Based on Emulation,"Sensitivity of electronic circuits to radiation effects is an increasing concern in modern designs. As technology scales down, Single Event Upsets (SEUs) are made more frequent and probable, affecting not only space applications, but also applications at earth's surface, like automotive applications. Fault injection is a method widely used to evaluate the SEU sensitivity of digital circuits. Among the existing fault injection techniques, those based on FPGA emulation have proven to be the fastest ones. In this paper a unified emulation environment which combines two fault injection techniques based on FPGA emulation is proposed. The new emulation environment provides both, a high speed tool for quick fault detection, and a medium speed tool for in-depth analysis of SEUs propagation. The experiments presented here show that the two techniques can be successfully applied in a complementary manner.",2007,0,
478,479,Enhancing Fault Tolerance And Reliability In GAIAOS Through Structured Overlay Network,"GAIAOS event manager is a distributed event service, based on CORBA event service with a centralized entry point, resulting in limited fault resilience and scalability. In this paper, we have proposed a decentralized event service for GAIAOS through the use of DHT based structured overlay network to overcome these problems. The proposed architecture provides a completely distributed event communication mechanism without any centralized entry point. Incorporation of the structured overlay network in GAIAOS results in higher degree of fault resilience and scalability",2006,0,
479,480,Application of non-parametric statistics of the parametric response for defect diagnosis,"This paper presents a method using only the rank of the measurements to separate a part's elevated response to parametric tests from its non-elevated response. The effectiveness of the proposed method is verified on the 130nm ASIC. Good die responses are correlated for same parametric tests at different conditions such as temperature, voltage and or other stress. Nonparametric correlation methods are used to calculate the intra-die correlation. When intra-die correlation is found to be low the elevated vectors that lower correlation are extracted and input to IDDQ-based diagnostic tools. Monte-Carlo simulations are described to obtain confidence bounds of the correlation for good die test response.",2009,0,
480,481,Multi-rate receiver design with IF sampling and digital timing correction,This contribution deals with a fully digital multirate radio receiver suitable for vehicular applications. Timing correction and sample rate conversion are performed by a polynomial interpolator. Three different receiver configurations are considered in terms of computational complexity and BER performance. Careful selection of the intermediate frequency turns out to play a crucial role. System parameters are provided yielding good BER performance for all considered symbol rates. Results are verified by computation of the BER degradation as compared to an analog receiver with synchronized symbol-rate sampling.,2003,0,
481,482,An improved neural network algorithm for classifying the transmission line faults,"This study introduces a new concept of artificial intelligence based algorithm for classifying the faults in power system networks. This classification identifies the exact type and zone of the fault. The algorithm is based on unique type of neural network specially developed to deal with a large set of highly dimensional input data. An improvement of the algorithm is proposed by implementing various steps of input signal preprocessing, through the selection of parameters for analog filtering, and values for the data window and sampling frequency. In addition, an advanced technique for classification of the test patterns is discussed and the main advantages compared to previously used nearest neighbor classifier are shown.",2002,0,
482,483,An error resilience scheme for layered video coding,"Layered video coding combined with prioritized transmission is widely recognized as one of the schemes for providing error resilience in video transport system. We examine the error performance of data partitioning coded MPEG-2 video bitstreams transmitted over channel subject to bit errors. While base layer errors cannot be tolerated, only a limited amount of errors in the enhancement layer is acceptable. Further improvements on bit error resilience can be achieved using the EREC coder in the enhancement layer. Our results show the PSNR gain of up to 3 dB with EREC coded enhancement layer and no errors in the base layer.",2005,0,
483,484,SIED: software implemented error detection,"This paper presents a new error detection technique called software implemented error detection (SIED). The proposed method is based on a new control check flow scheme combined with software redundancy. The distinctive advantage of the SIED approach over other fault tolerance techniques is the fault coverage. SIED is able to cope with faults affecting data and the program control flow. By-applying the proposed approach on several benchmark programs, we evaluate the error detection capabilities by means of several fault injection experiments. Experimental results underline very good error detection capabilities for the obtained hardened version of selected benchmark programs.",2003,0,
484,485,Fault detection techniques for effective line side asset monitoring,"In this paper the results of current research into the state-of-the-art in predictive fault detection and diagnosis methods for railway line-side assets is presented. Research to date has mainly focussed on point machines, track circuits and level crossing systems. It will be argued that, through the use of examples, that the most appropriate method for robust fault detection is based around generic models that are tuned for a particular instance of an asset. Furthermore, once a fault has been detected, it is necessary to have an a priori knowledge of the symptoms that are observable under fault conditions to reliably diagnose faults.",2005,0,
485,486,A Family of Electronic Ballasts Integrating Power Factor Correction and Power Control Stages to Supply HPS Lamps,"This paper presents a family of high power factor electronic ballasts applied to the public lighting system. Flyback, buck-boost, boost or SEPIC converter is employed in the power factor correction stage, integrated to the power control stage through a single active switch. The use of a half-bridge inverter, to supply the lamp, becomes possible through the employment of a flyback converter in the lamp power control stage. The lamp is supplied in a low frequency voltage square waveform in order to guarantee the safe lamp operation, regarding to the acoustic resonance phenomenon. The presented solutions to supply HPS lamps take the advantage of low cost and simplicity. The shared switch characteristics are analyzed and discussed during this work. A comparative analysis among the presented electronic ballasts is performed",2006,0,
486,487,Respiratory motion correction of PET using motion parameters from MR,"Respiratory motion during PET acquisition from the chest/abdomen leads to significant image degradation. Combined PET/MR scanners open up the opportunity to correct motion using MR data acquired simultaneously with PET. As simultaneous human chest/abdomen PET/MR images are currently unobtainable, in this preliminary study we determined motion parameters from respiratory-gated MR and then used these to correct pseudo-PET images generated from the MR. The gated MR images were segmented to typical organ FDG SUV values, smoothed to mimic PET resolution, forward projected into the GE advance geometry and reconstructed separately using OSEM. The MR images were registered using a combined affine and non-rigid B-splines algorithm, with mutual information used as the cost function in a multi-resolution approach. Motion corrected images from both post-reconstruction registration and 4D image reconstruction are shown to be superior to those without motion compensation for most organs.",2009,0,
487,488,Assessing and improving the effectiveness of logs for the analysis of software faults,"Event logs are the primary source of data to characterize the dependability behavior of a computing system during the operational phase. However, they are inadequate to provide evidence of software faults, which are nowadays among the main causes of system outages. This paper proposes an approach based on software fault injection to assess the effectiveness of logs to keep track of software faults triggered in the field. Injection results are used to provide guidelines to improve the ability of logging mechanisms to report the effects of software faults. The benefits of the approach are shown by means of experimental results on three widely used software systems.",2010,0,
488,489,A novel approach to fault diagnostics and prognostics,A novel fault diagnostics and prognostics algorithm based on hidden Markov model (HMM) is proposed. The algorithm combines fault diagnostics and prognostics in a unified framework. The algorithm has been fully tested by using experimental data from a rotating shift testbed in our laboratory.,2003,0,
489,490,Best ANN structures for fault location in single-and double-circuit transmission lines,"The great development in computing power has allowed the implementation of artificial neural networks (ANNs) in the most diverse fields of technology. This paper shows how diverse ANN structures can be applied to the processes of fault classification and fault location in overhead two-terminal transmission lines, with single and double circuit. The existence of a large group of valid ANN structures guarantees the applicability of ANNs in the fault classification and location processes. The selection of the best ANN structures for each process has been carried out by means of a software tool called SARENEUR.",2005,0,
490,491,Defect Data Analysis Based on Extended Association Rule Mining,"This paper describes an empirical study to reveal rules associated with defect correction effort. We defined defect correction effort as a quantitative (ratio scale) variable, and extended conventional (nominal scale based) association rule mining to directly handle such quantitative variables. An extended rule describes the statistical characteristic of a ratio or interval scale variable in the consequent part of the rule by its mean value and standard deviation so that conditions producing distinctive statistics can be discovered As an analysis target, we collected various attributes of about 1,200 defects found in a typical medium-scale, multi-vendor (distance development) information system development project in Japan. Our findings based on extracted rules include: (l)Defects detected in coding/unit testing were easily corrected (less than 7% of mean effort) when they are related to data output or validation of input data. (2)Nevertheless, they sometimes required much more effort (lift of standard deviation was 5.845) in case of low reproducibility, (i)Defects introduced in coding/unit testing often required large correction effort (mean was 12.596 staff-hours and standard deviation was 25.716) when they were related to data handing. From these findings, we confirmed that we need to pay attention to types of defects having large mean effort as well as those having large standard deviation of effort since such defects sometimes cause excess effort.",2007,0,
491,492,A Hybrid Fault-Tolerant Algorithm for MPLS Networks,In this paper we present a novel fault-tolerant algorithm for use in MPLS based networks. The algorithm is employing both protection switching and path rerouting techniques and satisfies four selected performance criteria,2006,0,
492,493,Using software implemented fault inserter in dependability analysis,We investigate program susceptibility to hardware faults in Win32 environment. For this purpose we use the software implemented fault injector FITS. We analyze natural fault resistivity of COTS systems and the effectiveness of various software techniques improving system dependability. The problems of experiment tuning and result interpretation are discussed in context of a wide spectrum of applications.,2002,0,
493,494,Fault-Tolerant Discrete Dynamical Systems Over Finite Ring,"It is worked out some general method of fault- tolerant synthesis for implementations of information-looseness dynamical systems over finite ring, based on application of error control codes. Corresponding self-checking systems are designed complexity and some basic characteristics of designed implementations is characterized.",2007,0,
494,495,A Fault-Tolerant Scheme for Complex Transaction Patterns in J2EE,"End-to-end reliability is an important issue in building large-scale distributed enterprise applications based on multi-tier architecture, but the support of reliability as adopted in conventional replication or transactions mechanisms is not enough due to their distinct objectives - replication guarantees the liveness of computational operations by using forward error recovery, while transactions guarantee the safety of application data by using backward error recovery. Combining the two mechanisms for stronger reliability is a challenging task Current solutions, however, are typically on the assumption of simple transaction pattern where a request from a single client executes in the context of exactly one transaction at the middle-tier application server, and seldom think about some complex patterns, such as client transaction enclosing multiple client requests or nested transactions. In this paper, we first identify four transaction pattern classes, and then propose a fault-tolerant scheme that can uniformly provide exactly-once semantic reliability support for these patterns. In this scheme, application servers are passively replicated to endow business logics with high reliability and high availability. In addition, by replicating transaction coordinator, the blocking problem of 2PC protocol during distributed transactions processing is eliminated. We have implemented this approach and integrated it into our own J2EE application server, OnceAS. Also, its effectiveness is discussed in different transaction patterns and the corresponding performance is evaluated",2006,0,
495,496,Tracking the elusive online help topic. Organizing the review process with defect-tracking software,"Online help systems consist of hundreds of help topics. Keeping track of reviewers, comments about each help topic requires a database to do the job effectively. Rather than develop such a database from scratch, it may be possible to adapt the defect-tracking software already in use in the QA department to this task. This paper describes how technical writers can adapt defect-tracking software to organize the online help review process",2001,0,
496,497,Modeling Defect Enhanced Detection at 1550 nm in Integrated Silicon Waveguide Photodetectors,"Recent attention has been attracted by photo-detectors integrated onto silicon-on-insulator (SOI) waveguides that exploit the enhanced sensitivity to subbandgap wavelengths resulting from absorption via point defects introduced by ion implantation. In this paper, we present the first model to describe the carrier generation process of such detectors, based upon modified Shockley-Read-Hall generation/recombination, and, thus, determine the influence of the device design on detection efficiency. We further describe how the model may be incorporated into commercial software, which then simulates the performance of previously reported devices by assuming a single midgap defect level (with properties commensurate with the single negatively charged divacancy). We describe the ability of the model to highlight the major limitations to responsivity, and thus suggest improvements which diminish the impact of such limitations.",2009,0,
497,498,Fault diagnosis technology based on wavelet analysis and resonance demodulation,"The impulse signal is contained in the fault signals of some pivotal components such as gears and axletrees. Extracting weensy impact information is an important method to diagnose equipment. A mathematical model on the technology of resonant demodulation is put forward in this paper. The model provides the theoretical basis on how to use the technology to extract the weensy impulse signal from the normal low-frequency vibrating signal, at the same time, another method that wavelet analysis is used to extract the weensy impulse information is introduced too. Simulation and practical application manifest that both wavelet analysis and demodulation have good effect on extracting the weensy impulse from the mechanical fault caused by gear and axletree.",2004,0,
498,499,Cluster-Based Error Messages Detecting and Processing for Wireless Sensor Networks,"Wireless sensor networks (WSNs) have emerged as a new technology about acquiring and processing messages for a variety of applications. Faults occurring to sensor nodes are common due to lack of power or environmental interference. In order to guarantee the network reliability of service, it is necessary for the WSN to be able to detect and processes the faults and take appropriate actions. In this paper, we propose a novel approach to distinguish and filter the error messages for cluter-based WSNs. The simulation results show that the proposed method not only can avoid frequent re-clustering but also can save the energy of sensor nodes, thus prolong the lifetime of sensor network.",2008,0,
499,500,Error-Correcting Codes Based on Quasigroups,"Error-correcting codes based on quasigroup transformations are proposed. For the proposed codes, similar to recursive convolutional codes, the correlation exists between any two bits of a codeword, which can have infinite length, theoretically. However, in contrast to convolutional codes, the proposed codes are nonlinear and almost random: for codewords with large enough length, the distribution of the letters, pair of letters, triple of letters, and so on, is uniform. Simulation results of bit-error probability for several codes in binary symmetric channels are presented.",2007,0,
500,501,Multimedia processor-based implementation of an error-diffusion halftoning algorithm exploiting subword parallelism,"Multimedia processor-based implementations of digital image processing algorithms have become important since several multimedia processors are now available and can replace special-purpose hardware-based systems because of their flexibility. Multimedia processors increase throughput by processing multiple pixels simultaneously using a subword-parallel arithmetic and logic unit architecture. The error-diffusion halftoning algorithm employs feedback of quantized output signals to faithfully convert a multi-level image to a binary image or to one with fewer levels of quantization. This makes it difficult to achieve speedup by utilizing the multimedia extension. In this study, the error-diffusion halftoning algorithm is implemented for a multimedia processor using three methods: single-pixel, single-line, and multiple-line processing. The single-pixel approach is the closest to conventional implementations, but the multimedia extension is used only in the filter kernel. The single-line approach computes multiple pixels in one scan-line simultaneously, but requires a complex algorithm transformation to remove dependencies between pixels. The multiple-line method exploits parallelism by employing a skewed data structure and processing multiple pixels in different scan-lines. The Pentium MMX instruction set is used for quantitative performance evaluation including run-time overheads and misaligned memory accesses. A speedup of more than ten times is achieved compared to the software (integer C) implementation on a conventional processor for the structurally sequential error-diffusion halftoning algorithm",2001,0,
501,502,Fault Diagnosis Implementation of Induction Machines based on Advanced Digital Signal Processing Techniques,"In this paper, a comprehensive cross correlation-based fault diagnostic method is proposed for real time DSP implementation. It covers both fault monitoring and decision making stages. In practice, a motor driven by an adjustable speed drive is run at various operating points where the frequency, amplitude and phase of the fault signatures varies with time. These dynamic changes are considered as one of the common factor that yields erroneous fault tracking and unstable fault detection. In this paper, the proposed algorithms deals with the operating point dependent ambiguities and threshold issues. It is theoretically and experimentally verified that the motor fault can continuously be tracked when the operating point changes within a limited range.",2009,0,
502,503,Positive Switching Impulse Discharge Performance and Voltage Correction of Rod-Plane Air Gap Based on Tests at High-Altitude Sites,"The Qinghai-Tibet Railway is the highest railway in the world. Up to now, there were no test and service data for the external insulation of the power-supply project of the railway system above 4000 m above sea level (a.s.l.). The ldquogrdquo parameter method recommended by IEC Publication 60.1 (1989) has a limited applicable range. Therefore, based on the former tests carried on the artificial climate chamber (ACC), in this paper, a series of test investigations is conducted on the positive switching impulse (PSI) discharge performance of rod-plane air gaps with gap spacing of 0.25 to 3.0 m at the six high-altitude sites along the Qinghai-Tibet Railway with altitudes of 2820 to 5050 m. With analyses of the mathematical optimization method on the test results, the new correction method of discharge voltage is proposed. They are also checked and compared with the test results obtained from the simulation tests carried out in the ACC. It is indicated that the 50% PSI discharge voltage <i>U</i> <sub>50</sub> of the air gap at high altitude is a power function of gap spacing <i>d</i>, also a power function of relative pressure of dry air and absolute humidity. The influence law of atmospheric parameters on <i>U</i> <sub>50</sub> obtained at high-altitude sites is the same as that obtained in the ACC. <i>U</i> <sub>50</sub> . obtained in the ACC, is about 8.15% higher than that obtained at high-altitude sites due to the influence of nonsimulated factors, such as ultraviolet ray and cosmic radiation. The ldquogrdquo parameter method is not applicable to the regions with an altitude above 2800 m.",2009,0,
503,504,A randomized error recovery algorithm for reliable multicast,"An efficient error recovery algorithm is essential for a liable multicast in large groups. Tree-based protocols (RMTP, TMTP, LBRRM) group receivers into local regions and select a repair server for performing error recovery in each region. Hence a single server bears the entire responsibility of error recovery for a region. In addition, the deployment of repair servers requires topological information of the underlying multicast tree, which is generally not available at the transport layer. This paper presents RRMP, a randomized reliable multicast protocol which improves the robustness of tree-based protocols by diffusing the responsibility of error recovery among all members in a group. The protocol works well within the existing IP multicast framework and does not require additional support from routers. Both analysis and simulation results show that the performance penalty due to randomization is low and can be tuned according to application requirements",2001,0,
504,505,Combined Use of Fuzzy Set-Covering Theory and Mode Identification Technique for Fault Diagnosis in Power Systems,"After a fault occurs in a power system, generally some operating information of protective relays and circuit breakers could be obtained. Because protective relays and circuit breakers might improperly operate or fail to operate, and some errors and distortion may exist in data acquisition and communication, as a result uncertainties could be involved in the received information. A fault diagnosis model based on fuzzy set- covering theory and mode identification technique is proposed in this paper. With the fuzzy technology, the above mentioned uncertainties could be dealt with very well. Meanwhile, as the protective relays and circuit breakers may fail to operate in some cases, there are several different operating modes in protective relays and circuit breakers even for a same electrical device failure. Based on the received information, the proposed model could identify the most possible operating mode, and then the information corresponding to a fault hypothesis could be obtained. In the proposed model, the proposed fault diagnosis problem is described as a 0-1 integer programming one, and thus could be solved by the widely employed search technology, i.e., the well-known Tabu search method. The feasibility and efficiency of the proposed model is demonstrated by a sample power system.",2007,0,
505,506,H<sub></sub> Dynamic observer design with application in fault diagnosis,"Most observer-based methods applied in fault detection and diagnosis (FDD) schemes use the classical two-degrees of freedom observer structure in which a constant matrix is used to stabilize the error dynamics while a post filter helps to achieve some desired properties for the residual signal. In this paper, we consider the use of a more general framework which is the dynamic observer structure in which an observer gain is seen as a filter designed so that the error dynamics has some desirable frequency domain characteristics. This structure offers extra degrees of freedom and we show how it can be used for the sensor faults diagnosis problem achieving detection and estimation at the same time. The use of weightings to transform this problem into a standard H<inf></inf>problem is also demonstrated.",2005,0,
506,507,Practical considerations in making CORBA services fault-tolerant,"This paper examines the CORBA Naming, Event, Notification, Trading, Time and Security Services, with the objective of identifying the issues that must be addressed in order make these services fault-tolerant. The reliability considerations for each of these services involves strategies for replicating the service objects, and for keeping the states of the replicas consistent. Of particular interest are the sources of non-determinism in each of these services, along with the means for addressing the non-deterministic behavior in the interests of ensuring strong fault tolerance",2002,0,
507,508,Analysis - The terrors and the errors [IT Change Management],"According to last month??s Sophos `Security Threat Report??, concern is increasing that computer applications running critical national infrastructures are vulnerable to malevolent hacks. Such hacks could in theory switch control of power and gas supplies, say, to the keyboards of hostile entities, enabling them to wreak damage and disruption. Similar threats face crucial financial computer platforms that underpin national economies, and even emergency services communication channels.",2010,0,
508,509,Innovative airborne inventory and inspection technology for electric power line condition assessments and defect reporting,"A cost-effective and innovative airborne inventory and inspection patrol system for distributed assets such as transmission lines, pipelines, and roadways has been developed and evaluated. Results show that aerial high-resolution digital visual and spectral images tagged by Global Positioning Satellite (GPS) coordinates can be successfully used to cost-effectively identify the majority of conditions/defects on electric power lines. Experiments show that the condition and defect detection rate of the airborne inventory and inspection system is significantly higher than rates derived from traditional patrols and comparable to values achieved from driving patrols. Geographic information systems (GIS) based mapping tools can be used to quickly and efficiently interpret digital images collected from aerial platforms. Digital images provide an archival record of the condition of the distributed assets to estimate the long-term performance of the assets and to define cost-effective maintenance and replacement schedules",2000,0,
509,510,Advanced Cu CMP defect excursion control for leading edge micro-processor manufacturing,"The introduction of yield sensitive, advanced interconnect technology coupled with the requirement for accelerating yield ramp in today's state-of-the-art semiconductor manufacturing facilities, are driving tool monitoring requirements for fast and accurate defect excursion control. In the Copper CMP module the challenge is accentuated by the relative immaturity of this process, the dominance of single wafer excursions and a high count of nuisance defect types relative to the critical yield-limiting defect types. A manufacturing-worthy Copper CMP tool monitor methodology is described here that improves excursion control through detection and tracking of critical, yield-limiting defect types, independent of non-yield-critical nuisance defect types. High-resolution automatic defect review and classification, a critical component of the methodology, is limited to wafers with high critical-defect counts, reducing monitoring cost and time-to-results. A new trigger sampling feature and intelligent image sampling reduces monitoring cost and time-to-results through minimizing defect review overhead. Integration of such a solution into the manufacturing environment is presented in detail and contrasted next to existing traditional defect excursion control model. Ease-of-use considerations are highlighted with use case examples. The paper will approximate the cost savings to manufacturing such as reducing existing levels of false excursion due to nuisance defects and improving the cycle time in the Cu CMP module. Benefits are achieved by integrating functionality into existing inspection hardware. No additional capital equipment was required.",2002,0,
510,511,Hybrid fault-tolerant control of aerospace vehicles,"We describe our recent results (2001) related to the design of hybrid online failure detection and identification and adaptive reconfigurable control algorithms for aerial and space vehicles. Our approach is based on the multiple models, switching and tuning methodology and its extensions, and has been demonstrated as an efficient tool for hybrid fault tolerant control under subsystem and component failures and structural damage",2001,0,
511,512,Active error recovery for reliable multicast,"An error recovery scheme is essential for large-scale reliable multicast. We design, implement, and evaluate an improved active error recovery scheme for reliable multicast (AERM). The AERM uses soft-state storage to facilitate fast error recovery. It has the following features: a simple NAK suppression and aggregation mechanism, an efficient hierarchical RTT measurement mechanism, an effective local recovery and scoped retransmission mechanism, and a periodical ACK mechanism. We implement the AERM and study its characteristics in NS2. We also compare performance with ARM and AER/NCA, both of which are representative active reliable multicast protocols. The results indicate that AERM can achieve considerable performance improvement with limited support from routers. Our work also confirms that active networks can benefit some applications and become a promising network computing platform in the future",2001,0,
512,513,An Error Concealment Scheme for Entire Frame Losses for H.264/AVC,"In this paper, an error concealment scheme is proposed to conceal an entirely lost frame in a compressed video bitstream due to errors introduced during transmission. The proposed scheme targets low bit rate video transmission applications using H.264/AVC. The motion field of the lost frame is first reconstructed by copying the co-located motion vectors and reference indices from the last decoded reference frame. After the motion field estimation of the missing frame, motion compensation is performed to reconstruct the frame. This technique reuses existing modules of the video decoder and it does not incur extra complexity compared to decoding a normal frame. It has also been adopted as a non-normative decoder option to the JM reference software at the JVT meeting in Poznan, Poland in July 2005 [1] and has been incorporated into the SA4 video ad hoc group's toolkit at the 3GPP meeting at Paris [2] in September 2005. Simulation results will show its improved performance over other simple error concealment schemes such as ""frame copy,"" both subjectively and objectively, without significant complexity overhead.",2006,0,
513,514,A new textual/non-textual classifier for document skew correction,A robust approach is proposed for document skew detection. We use Fourier analysis and SVM to classify textual areas from non-textual areas of documents. We also propose a robust method to determine the skew angle from textual areas. Our approach achieves good performance on documents with large area of non-textual contents.,2002,0,
514,515,Evolutionary design and adaptation of digital filters within an embedded fault tolerant hardware platform,"Finite impulse response filters (FIRs) are crucial device for robust data communication and manipulation. Multiplierless filters have been shown to produce high performance systems with fast signal processing and reduced area. Furthermore, the distributed architecture inherent in multiplierless filters makes it a suitable candidate for fault tolerant design. Alternative approaches to the design of fault tolerant systems have been proposed using evolutionary algorithms (EAs) and the concept of evolvable hardware (EHW). This paper presents an evolvable hardware platform for the automated design and adaptation of multiplierless digital filters. Filters are realised within a dedicated programmable logic array (PLA). The platform employs a genetic algorithm to autonomously configure the PLA for a give set of coefficients. The ability of the platform to adapt to increasing numbers of faults was investigated through the evolution of a 31-tap low-pass FIR filter. Results show that the functionality of filters evolved on the PLA was maintained despite an increasing number of faults covering up to 25% of the PLA area. Additionally, three PLA initialisation methods were investigated to ascertain which produced the fastest fault recovery times. It was shown that seeding a population of random configuration-strings with the best configuration currently obtained resulted in a 6 fold increase in fault recovery speed over other methods investigated",2001,0,
515,516,Defect Identification in Large Area Electronic Backplanes,We describe a rapid testing system for active matrix thin-film transistor (TFT) backplanes which enables the identification of many common processing defects. The technique spatially maps the charge feedthrough from TFTs in the pixel and is suited for pixels with switched-capacitor architecture.,2009,0,
516,517,Design and analysis of a fault-tolerant mechanism for a server-less video-on-demand system,"Video-on-demand (VoD) systems have traditionally been built on the client-server architecture, where a video server stores, retrieves, and transmits video data to video clients for playback This paper investigates a radically different approach to building VoD systems, one where the server, and hence the primary bottleneck, is completely eliminated. This server-less architecture comprises homogeneous hosts, called nodes, which serve both as client and as mini-server. Video data are distributed over all nodes and these nodes cooperatively stream video data to one another for playback. However, unlike traditional video server that runs on high-end server hardware in a carefully controlled and protected data centre, a node in a server less system is likely to be far more unreliable. Therefore it is essential that sufficient data and capacity redundancies are incorporated to maintain an acceptable set-vice reliability. This paper presents and analyzes a fault tolerant mechanism based on inter-node striping and erasure correction codes to tackle this challenge. By formulating the system's reliability as a Markov chain model, we obtain insights into the feasible operating region of the system, such as the amount of redundancy required and the node-level reliability that can be tolerated. Numerical results show that a server-less VoD system of 200 nodes can achieve reliability surpassing that of dedicated video server using a redundancy overhead of only 21.2% even though individual nodes are highly unreliable.",2002,0,
517,518,Performance analysis and improvements for a simulation-based fault injection platform,"In this paper, we study and present two techniques to improve the performance of a simulation-based fault injection platform that inserts bit flips in order to model soft errors on digital circuits. The platform is based on the ESA Data Systems Divisionpsilas SEE simulation tool. In contrast with methods based on emulation, the proposed approach reduces the complexity and costs, supplying a test environment with the same reliability as emulation systems. Only one disadvantage appears when comparing both methodologies: the lower performance of the simulation in cases where the fault injection campaigns are very large. Two proposals have been developed in order to address this drawback: the first one is based on software (through checkpoints) and the second one uses parallel computation.",2008,0,
518,519,Autonomous Decentralized VoD System Architecture and Fault-Tolerant Technology to Assure Continuous Service,"In distributed and ubiquitous computing systems, not only the composing subsystems and their functions, but also the system structure would be changed constantly under the evolving situation. With the advances of compression, storage and network technologies, Video on Demand (VoD) service is becoming more and more popular. However, it is difficult for conventional systems to meet the continuous and heterogeneous requirements from service providers and users simultaneously. This paper introduces an autonomous decentralized VoD system sustained by mobile agents for information service provision and utilization. Under the proposed architecture, autonomous fault detection and recovery technologies are proposed to assure continuous service. The effectiveness of the proposed technology is proved by the simulation. The results show that an average of 30% improvement in recovery time and users' video service can be recovered without stopping compared with the conventional system.",2009,0,
519,520,A novel framework for robust video streaming based on H.264/AVC MGS coding and unequal error protection,"We present a novel framework to provide robust video streaming service over time-varying error-prone network. The scheme is based on the medium granularity scalability (MGS) video coding of the H.264/AVC standard, which adopts a hierarchical prediction structure for the group-of-pictures (GOP). We determine the optimal allocation of protection strength for different network abstraction layer (NAL) units according to their individual importance to the end-to-end video quality. To analyse the importance of the NAL units, we emulate the error concealment if one frame is considered as lost and take into account the propagation distortion within the GOP. An efficient algorithm is proposed to account for the non-convex rate-distortion characteristics associated with the NAL units in the hierarchical GOP. With this framework, we can provide robust video streaming for the range of packet loss rates from 0% to 40% with about 30% additional channel bit-rate for the channel coding. The simulation results demonstrate high flexibility and efficiency of the proposed framework, which can effectively prevent frequent loss of frames.",2009,0,
520,521,Application of an automated PD failure identification system for EMC-assessment strategies of multiple PD defects at HV-insulators,"EMC-assessment of field emission of high voltage insulators can be performed using phase-angle-resolved partial discharge diagnosis. However, with conventional PD-detection systems no satisfying statements about multiple PD defects are possible because of the highly dynamic apparent charge values for different discharge phenomena. This measurement problem can be solved by a new approach. For this purpose phase resolved pulse sequence analysis methods are suitable diagnosis tools without using the apparent charge as a dominating influence. A recently developed feature extraction method based on consecutive u/ values shows good classification results. The problem of multiple PD defects, which occur at the same time, is a new challenge for PD diagnosis systems. For the investigation two reference databases are generated. With the database, which takes these multiple PD defects into account, the diagnosis system WinTED of the University of Wuppertal is able to identify with high reliability actual measurements made by the University of Dortmund",2000,0,
521,522,Design aspects and pattern prediction for phased arrays with subarray position errors,"In modern array design, the antenna elements are often grouped into mechanical units such as printed antenna boards and mechanical subarrays/multipacks. This contributes to a more cost efficient manufacturing process and facilitates integration, handling, reuse and exchange of units, but it also makes the antenna element position errors correlated. Classical papers predict the statistical sidelobe level based on the assumption of uncorrelated errors, but using this for the general case, the statistical sidelobe level is under estimated. In this paper, the statistical sidelobe level for arrays with correlated position errors is predicted. Furthermore, rules of thumb relating antenna element position tolerances and mechanical array design to antenna array performance (sidelobe level) are given. Finally, array design aspects are discussed.",2010,0,
522,523,Double Redundant Fault-Tolerance Service Routing Model in ESB,"With the development of the Service Oriented Architecture (SOA), the Enterprise Service Bus (ESB) is becoming more and more important in the management of mass services. The main function of it is service routing which focuses on delivery of message among different services. At present, some routing patterns have been implemented to finish the messaging, but they are all static configuration service routing. Once one service fails in its operation, the whole service system will not be able to detect such fault, so the whole business function will also fail finally. In order to solve this problem, we present a double redundant fault tolerant service routing model. This model has its own double redundant fault tolerant mechanism and algorithm to guarantee that if the original service fails, another replica service that has the same function will return the response message instead automatically. The service requester will receive the response message transparently without taking care where it comes from. Besides, the state of failed service will be recorded for service management. At the end of this article, we evaluated the performance of double redundant fault tolerant service routing model. Our analysis shows that, by importing double redundant fault tolerance, we can improve the fault-tolerant capability of the services routing apparently. It will solve the limitation of existent static service routing and ensure the reliability of messaging in SOA.",2009,0,
523,524,Stator Current and Motor Efficiency as Indicators for Different Types of Bearing Faults in Induction Motors,"This paper proposes a new approach to use stator current and efficiency of induction motors as indicators of rolling-bearing faults. After a presentation of the state of the art about condition monitoring of vibration and motor current for the diagnostics of bearings, this paper illustrates the experimental results on four different types of bearing defects: crack in the outer race, hole in the outer race, deformation of the seal, and corrosion. The first and third faults have not been previously considered in the literature, with the latter being analyzed in other research works, even if obtained in a different way. Another novelty introduced by this paper is the analysis of the decrease in efficiency of the motor with a double purpose: as alarm of incipient faults and as evaluation of the extent of energy waste resulting from the lasting of the fault condition before the breakdown of the machine.",2010,0,
524,525,Noise-Related Radiometric Correction in the TerraSAR-X Multimode SAR Processor,"Synthetic aperture radar (SAR) image intensity is disturbed by additive system noise. During SAR focusing, pattern corrections that are adapted to the characteristics of the wanted signal, but not to the characteristics of the noise, influence the spatial distribution of the noise power. Particularly in the case of ScanSAR, a distinct residual noise pattern in low backscatter areas results. This necessitates a noise-adapted radiometric correction of the focused image for almost all applications except interferometry. In this paper, we thoroughly investigate this topic. Based on signal theoretical and stochastic considerations, we develop a radiometric correction scheme. Simulations and the application of the algorithm to TerraSAR-X datatakes support the theoretical results.",2010,0,
525,526,Robot fault-tolerance using an embryonic array,"Fault-tolerance, complex structure management and reconfiguration are seen as valuable characteristics. Embryonic arrays represent one novel approach that takes inspiration from nature to improve upon standard techniques. An existing BAE SYSTEMS RASCALTM robot has been augmented so as to improve the motor control system reliability through two biologically-inspired systems: an embryonic array and an artificial immune system. This paper is concerned with the embryonic array; this is novel in that it supports datapath-wide arithmetic and logic functions. The array is configured to provide an autonomous self-repairing hardware motor controller and is realized using a standard Xilinx Virtex FPGA. As with previous embryonic systems, the logic requirement of the array is greater than that of a conventional FPGA or standard modular-redundancy approach. However, the array offers the advantages of both conventional FPGAs and modular-redundancy techniques. It is a reconfigurable computing platform that provides inherent fault-tolerance through its distributed self-repair mechanism.",2003,0,
526,527,Comparing code reading techniques applied to object-oriented software frameworks with regard to effectiveness and defect detection rate,"This paper first reasons on understanding software frameworks for defect detection, and then presents an experimental research for comparing the effectiveness and defect detection rate of code-reading techniques, once applied to C++ coded object-oriented frameworks. We present and discuss the functionality-based approach to framework understanding. Then, we present an experiment that compared three reading techniques for inspection of software frameworks. Two of those reading techniques, namely checklist-based reading, and systematic order-based reading, were adopted from scientific literature, while the third one, namely functionality-based reading, was derived from the functionality-based approach. The results of the experiment are that (1) functionality-based reading is much more effective and efficient than checklist based reading. (2) Functionality-based Reading is significantly more effective and efficient than systematic order-based reading. (3) Systematic order-based reading performs significantly better than checklist based reading for what concerns defect detection rate. However, because we used checklist-based reading and systematic order-based reading quite as they are, with limited adaptation to frameworks, it is too early to draw strong conclusions from the experiment results and improving and replicating this study is strongly recommended.",2004,0,
527,528,Fault detection in IP-based process control networks using data mining,"Industrial process control IP networks support communications between process control applications and devices. Communication faults in any stage of these control networks can cause delays or even shutdown of the entire manufacturing process. The current process of detecting and diagnosing communication faults is mostly manual, cumbersome, and inefficient. Detecting early symptoms of potential problems is very important but automated solutions do not yet exist. Our research goal is to automate the process of detecting and diagnosing the communication faults as well as to prevent problems by detecting early symptoms of potential problems. To achieve our goal, we have first investigated real-world fault cases and summarized control network failures. We have also defined network metrics and their alarm conditions to detect early symptoms for communication failures between process control servers and devices. In particular, we leverage data mining techniques to train the system to learn the rules of network faults in control networks and our testing results show that these rules are very effective. In our earlier work, we presented a design of a process control network monitoring and fault diagnosis system. In this paper, we focus on how the fault detection part of this system can be improved using data mining techniques.",2009,0,
528,529,Investigating effects of neutral wire and grounding in distribution systems with faults,"In some applications, like fault analysis, fault location, power quality studies, safety analysis, loss analysis, etc., knowing the neutral wire and ground currents and voltages could be of particular interest. In order to investigate the effects of neutrals and system grounding on the operation of distribution feeders with faults, a hybrid short circuit algorithm is generalized. In this novel use of the technique, the neutral wire and assumed ground conductor are explicitly represented. Results obtained from several case studies using the IEEE 34-node test network are presented and discussed.",2004,0,
529,530,On Line Fault Detection and an Adaptive Algorithm to Fast Distance Relaying,"This paper presents the design of an hybrid scheme of wavelet transforms and an adaptive Fourier filtering technique for on line fault detection and phasor estimation to fast distance protection of transmission lines. The wavelet transform is used as a signal processing tool. The sampled voltage and current signals at the relay location are decomposed using wavelet transform-Multi Resolution Analysis (MRA). The decomposed signals are used for the fault detection and as input to the phasor estimation algorithm. The phasor estimation algorithm possesses the advantage of recursive computing and a decaying dc offset component is removed from fault signals by using an adaptive compensation method. Fault detection index and a variable data window scheme are embedded in the algorithm. The proposed scheme provides capability for fast tripping decision, taking accuracy into account. Extensive simulation tests and comparative evaluation presented prove the efficacy of the proposed scheme in distance protection.",2008,0,
530,531,Parametric fault trees with dynamic gates and repair boxes,"A new approach is proposed to include s-dependencies in fault tree (FT) models. With respect to previous techniques, the approach presented in this paper is based on two peculiar powerful features. First, adopting a parameterization technique, referred to as parametric FT (PFT), to fold equal subtrees (or basic events) in order to resort to a more compact FT representation. It is shown that parameterization can be conveniently adopted as well for dynamic gates. Second, PFT can be modularized and each module translated into a high level colored Petri net in the form of a stochastic well-formed net (SWN). SWN generate a lumped Markov chain and the saving in the dimension of the state space can be very substantial with respect to standard (non colored) Petri nets. Translation of PFT modules into SWN has proved to be very flexible, and various kinds of new dependencies can be easily accommodated. In order to exploit this flexibility a new primitive, called repair box, is introduced. A repair box, attached to an event, causes the starting of a repair activity of all the components that failed as the event occurs. In contrast to all the previous FT based models, the addition of repair boxes enables the approach to model cyclic behaviors. The proposed approach as dynamic repairable PFT (DRPFT) was referred to. A tool supporting DRPFT is briefly described and the tool is validated by analyzing a benchmark proposed recently in the literature for quantitative comparison [H. Zhu et al., 2001].",2004,0,
531,532,An Enhanced Fault-Tolerant Routing Algorithm for Mesh Network-on-Chip,"Fault-tolerant routing is the ability to survive failure of individual components and usually uses several virtual channels (VCs) to overcome faulty nodes or links. A well-known wormhole-switched routing algorithm for 2-D mesh interconnection network called f-cube3 uses three virtual channels to pass faulty regions, while only one virtual channel is used when a message does not encounter any fault. One of the integral stages of designing network-on-chips (NoCs) is the development of an efficient communication system in order to provide low latency networks. We have proposed a new fault-tolerant routing algorithm based on f-cube3 as a solution to reduce the delay of network packets which uses less number of VCs in comparison with f-cube3. Moreover, in this method we have improved the use of VCs per each physical link by reducing required channels to two. Furthermore, simulations of both f-cube3 and our algorithm based on same conditions have been presented.",2009,0,
532,533,Path diversity with forward error correction (PDF) system for packet switched networks,"Packet loss and end-to-end delay limit delay sensitive applications over the best effort packet switched networks such as the Internet. In our previous work, we have shown that substantial reduction in packet loss can be achieved by sending packets at appropriate sending rates to a receiver from multiple senders, using disjoint paths, and by protecting packets with forward error correction. In this paper, we propose a path diversity with forward error correction (PDF) system for delay sensitive applications over the Internet in which, disjoint paths from a sender to a receiver are created using a collection of relay nodes. We propose a scalable, heuristic scheme for selecting a redundant path between a sender and a receiver, and show that substantial reduction in packet loss can be achieved by dividing packets between the default path and the redundant path. NS simulations are used to verify the effectiveness of PDF system.",2003,0,
533,534,New Approach for Defect Inspection on Large Area Masks,"Besides the mask market for IC manufacturing, which mainly uses 6 inch sized masks, the market for the so called large area masks is growing very rapidly. Typical applications of these masks are mainly wafer bumping for current packaging processes, color filters on TFTs, and Flip Chip manufacturing. To expose e.g. bumps and similar features on 200 mm wafers under proximity exposure conditions 9 inch masks are used, while in 300 mm wafer bumping processes 14 inch masks are handled. Flip Chip manufacturing needs masks up to 28 by 32 inch. This current maximum mask dimension is expected to hold for the next 5 years in industrial production. On the other hand shrinking feature sizes, just as in case of the IC masks, demand enhanced sensitivity of the inspection tools. A defect inspection tool for those masks is valuable for both the mask maker, who has to deliver a defect free mask to his customer, and for the mask user to supervise the mask behavior conditions during its lifetime. This is necessary because large area masks are mainly used for proximity exposures. During this process itself the mask is vulnerable by contacting the resist on top of the wafers. Therefore a regular inspection of the mask after 25, 50, or 100 exposures has to be done during its whole lifetime. Thus critical resist contamination and other defects, which lead to yield losses, can be recognized early. In the future shrinking feature dimensions will require even more sensitive and reliable defect inspection methods than they do presently. Besides the sole inspection capability the tools should also provide highly precise measurement capabilities and extended review options.",2007,0,
534,535,On the bit-error probability of differentially encoded QPSK and offset QPSK in the presence of carrier synchronization,"We investigate the differences between allowable differential encoding strategies and their associated bit-error probability performances for quadrature phase-shift keying (QPSK) and offset QPSK modulations when the carrier demodulation reference signals are supplied by the optimum (motivated by maximum a posteriori estimation of carrier phase) carrier-tracking loop suitable for that modulation. In particular, we show that in the presence of carrier-synchronization phase ambiguity but an otherwise ideal loop, both the symbol and bit-error probabilities in the presence of differential encoding are identical for the two modulations. On the other hand, when in addition the phase error introduced by the loop's finite signal-to-noise ratio is taken into account, it is shown that the two differentially encoded modulations behave differently, and their performances are no longer equivalent. A similar statement has previously been demonstrated for the same modulations when the phase ambiguity was assumed to have been perfectly resolved by means other than differential encoding.",2006,0,
535,536,Robust sensor fault estimation for tolerant control of a civil aircraft using sliding modes,This paper proposes a sensor fault tolerant control scheme for a large civil aircraft. It is based on the application of a robust method for sensor fault reconstruction using sliding mode theory. The novelty lies in the application of the sensor fault reconstruction scheme to correct the corrupted measured signals before they are used by the controller and therefore the controller does not need to be reconfigured to adapt to sensor faults,2006,0,
536,537,High level net models: a tool for permutation mapping and fault detection in multistage interconnection network,"This paper aims at structurising the detection of different types of stuck-at faults for a wide range of multistage interconnection networks (MINs). The results reported so far in this respect have been mainly based on direct combinatorial analysis of the concerned networks with very little consideration towards the modelling aspects. Graphical representation coupled with well-defined semantics allowing formal analysis has already established the Petri net as an effective tool for modelling dynamic systems. However, the existing variants of high level nets had certain limitations in modelling the dynamic behaviour of mapping a permutation through the MIN and further analysis of the same. This has inspired the authors to propose a couple of new high level net models, called MP-net and S-net in their earlier works. The S-net model uses tokens to hold and propagate information apart from controlling the firing of events. It uses two different types of places and transitions each as has been defied subsequently. In this paper, we have concentrated on the detection of faults in MINs using this S-net model",2000,0,
537,538,From Fireflies to Fault-Tolerant Swarms of Robots,"One of the essential benefits of swarm robotic systems is redundancy. In case one robot breaks down, another robot can take steps to repair the failed robot or take over the failed robot's task. Although fault tolerance and robustness to individual failures have often been central arguments in favor of swarm robotic systems, few studies have been dedicated to the subject. In this paper, we take inspiration from the synchronized flashing behavior observed in some species of fireflies. We derive a completely decentralized algorithm to detect non-operational robots in a swarm robotic system. Each robot flashes by lighting up its on-board light-emitting diodes (LEDs), and neighboring robots are driven to flash in synchrony. Since robots that are suffering catastrophic failures do not flash periodically, they can be detected by operational robots. We explore the performance of the proposed algorithm both on a real-world swarm robotic system and in simulation. We show that failed robots are detected correctly and in a timely manner, and we show that a system composed of robots with simulated self-repair capabilities can survive relatively high failure rates.",2009,0,
538,539,Implications of Rent's Rule for NoC Design and Its Fault-Tolerance,"Rent's rule is a powerful tool for exploring VLSI design and technology scaling issues. This paper applies the principles of Rent's rule to the analysis of networks-on-chip (NoC). In particular, a bandwidth-version of Rent's rule is derived, and its implications for future NoC scaling examined. Hop-length distributions for Rent's and other traffic models are then applied to analyse NoC router activity. For fault-tolerant design, a new type of router is proposed based on this analysis, and it is evaluated for mutability and its impact on congestion by further use of the hop-length distributions. It is shown that the choice of traffic model has a significant impact on scaling behaviour, design and fault-tolerant analysis",2007,0,
539,540,Analysis on fault voltage and secondary arc current of single phase refusing-shut of the 500kV extra high voltage transmission line,"It is common knowledge that 500 kV extra high voltage and long distant transmission line join a shunt reactor and a neutral grounding via small reactor; This paper analysis systematically an possible condition of the frequency-regulating resonance over-voltage on single phase cut fault to refusing-shut of the 500 kV extra high voltage transmission line which join a shunt reactor, the system compose an complex series resonance circuits, and present a rational mode of reactive compensation. This paper also build rational mathematic mode on systemic parameter of 500 kV ci-yong transmission line, and resolute detailedly its power frequency component, low frequency component and its DC component of single phase cut fault voltage and secondary arc current by the mean of Laplacian transformation ruling formula. All the this is to offer an farther analysis on switching over-voltage and secondary arc current interrupter of long distant transmission line. In the end, this system also implemented using MATLAB software, compute the transient process on single phase cut fault voltage and secondary arc current.",2009,0,
540,541,BOAs: backoff adaptive scheme for task allocation with fault tolerance and uncertainty management,"We propose the backoff adaptive scheme (BOAs) as a new technique for the automatic allocation of tasks amongst a team of heterogeneous mobile robots. It is an optimal, decentralized decision making scheme that utilizes explicit communication between the agents. A structured and unified framework is also proposed for task specification. This scheme is fault tolerant (to robot malfunctions) and allows for uncertainty in the nature of task specification in terms of the actual number of robots required. Team demography may change without the need for the respecification of tasks. The adaptive feature in BOAs further improves the flexibility of the team. Realistic simulations are carried out to verify the effectiveness of the scheme.",2004,0,
541,542,Fault-tolerant vibration control in a networked and embedded rocket fairing system,"Active vibration control using piezoelectric actuators in a networked and embedded environment has been widely applied to solve the rocket fairing vibration problem. However, actuator failures may lead to performance deterioration or system dysfunction. To guarantee the desired system performance, the remaining actuators should be able to coordinate with each other to compensate for the damaging effects caused by the failed actuator in a timely manner. Further, in the networked control environment, timing issues such as sampling jitter and network-induced delay should be considered in the controller design. In this study, a timing compensation approach is implemented in an adaptive actuator failure compensation controller to maintain the fairing system performance by also considering the detrimental effects from real-time constraints. In addition, time-delay compensation in the networked control system is discussed, which is able to reduce damaging effects of network-induced delays.",2004,0,
542,543,Techniques to enable FPGA based reconfigurable fault tolerant space computing,Reconfigurable computing using field programmable gate arrays (FPGAs) offer significant performance improvements over traditional space based processing solutions. The application of commercial-off-the-shelf (COTS) FPGA processing components requires radiation-effect detection and mitigation strategy to compensate for the FPGAs' susceptibility to single event upsets (SEUs) and single event functional interrupts (SEFIs). A reconfigurable computing architecture that uses external triple modular redundancy (TMR) via a radiation-hardened ASIC provides the most robust approach to SEU and SEFI detection and mitigation. Honeywell has designed a TMR Voter ASIC with an integrated FPGA configuration manager that can automatically reconfigure an upset FPGA upon TMR error detection. The automatic configuration manager also has features to support resynchronizing the upset FPGA with the remaining two FPGAs operating in a self checking pair (SCP) mode. Automating and minimizing reconfiguration times and re synchronization times enables high performance FPGA-based processors to provide high system availability with minimal software/system controller intervention,2006,0,
543,544,A Predictive Method for Providing Fault Tolerance in Multi-agent Systems,"The growing importance of multi-agent applications and the need for a higher quality of service in these systems justify the increasing interest in fault-tolerant multi-agent systems. In this article, we propose an original method for providing dependability in multi- agent systems through replication. Our method is different from other works because our research focuses on building an automatic, adaptive and predictive replication policy where critical agents are replicated to avoid failures. This policy is determined by taking into account the criticality of the plans of the agents, which contain the collective and individual behaviors of the agents in the application. The set of replication strategies applied at a given moment to an agent is then fine-tuned gradually by the replication system so as to reflect the dynamicity of the multi-agent system. We report on experiments assessing the efficiency of our approach.",2006,0,
544,545,Design of a fault-tolerant voter for safety related analog inputs,"This paper introduces a voting scheme for safety-related analog input module to arbitrate between the results of redundant channels in fault-tolerant system. The design approach is a distributed system using a sophisticated form of duplication. For each running process, there is a backup process running on a different CPU. The voter is responsible for checkpointing its state to duplex CPUs. In order to increase the dependability for safety-related controllers, the I/O modules use redundancy to reduce the risk associated with relying upon any single component operating flawlessly. The 1oo2D voting principle is commonly used in fault tolerant I/O modules to provide passive redundancy for masking runtime faults at hardware and software levels, respectively. A dual architecture (1oo2D) which provides high safety integrity to a rating of SIL 3 is presented. The outputs from two identical channels operating in parallel with the same inputs are supplied to a voting unit that arbitrates between them to produce an overall output. Based on the hardware logic model and FPGA technique, the study adopts the hardware voter which has much more advantage in the velocity and reliability. Finally, using modelsim simulations, we verify the effectiveness of the proposed voter design in preserving the hazard-free property of the response of an analog inputs module.",2010,0,
545,546,Research on Multi-agent System Model of Diesel Engine Fault Diagnosis by Case-Based Reasoning,"Oil monitoring technology is a useful method in condition monitoring and fault diagnosis for the machine, especially for low-speed, heavy-load, reciprocated and lubricated diesel engine equipment. But it is difficult to implement intelligent diagnosis because monitored information lacks logical relationship in oil monitoring. To solve this problem, the theory and method of case-based reasoning is adopted for the data processing and fault analysis in oil monitoring with a multi-agent system structure. Detailed definitions of agents in the system were proposed, and the multi-agent system framework was established finally. Multi-agent mechanism brings flexible for case based reasoning. It enhances the capability of solving complicated question in new system, and overcomes the shortcoming of the fault knowledge difficult to update in traditional systems",2006,0,
546,547,Predicting and controlling FPGA Device Heat using System monitor and IBERT (internal bit error ratio tester),The aim of this paper is to present a new methodology and the tools used to predict and control the FPGA Device Heat before starting the design. Knowing that the FPGA silicon heat is crucial as they all have a temperature above and under which their functionalities is not longer guaranteed. The silicon temperature is linked to the different options and strategies used to implement the design. Many tools such ldquouse Xpowerrdquo from Xilinx allows the user to have an estimation of the power consumption. This paper will present a primitive called System monitor which is present in every Virtex 5 to monitor the environment around the FPGA. Monitoring the device environment maximises the probability of getting the FPGA work after implementing required design.,2009,0,
547,548,Traveling Wave Fault Location for Power Cables Based on Wavelet Transform,"In this paper, traveling wave fault location equipment for power cables is designed, and the characteristic waveforms of cable fault point broken down and not broken down are simulated respectively. Then a new traveling wave fault location method based on wavelet transform is presented. Wavelet transform have good performance in denoising and singularity detection, which well solved the difficulty in identifying the initial point of the reflected traveling wave because of the local time-frequency characteristic. The fault distance can be calculated by the round-trip times which the traveling wave spends in the cable. The only required parameter is the length of cable. With the method, the result of fault location is not influenced by the change of propagation velocity of traveling wave. The correctness and effectiveness of this method are analyzed by computer simulation. The obtained results show an acceptable degree of accuracy for fault location.",2007,0,
548,549,"A note on inconsistent axioms in Rushby's ""systematic formal verification for fault-tolerant time-triggered algorithms""",We describe some inconsistencies in John Rushby's axiomatization of time-triggered algorithms that he presented in these transactions and that he formally specifies and verifies in the mechanical theorem-prover PVS. We present corrections for these inconsistencies that have been checked for consistency in PVS,2006,0,
549,550,Fault Tolerant Service Composition in Service Overlay Networks,"In a service overlay network, the services provided by different service providers might span multiple Internet domains. A service provider failure may cause significant performance deterioration. Thus, it is desirable to provide fault tolerant service composition solutions such that the service composition can be switched to the backup service composition solution in case of a service provider failure. To provide 100% protection against a single service provider failure, fault tolerant service composition essentially requires to partition service providers into two disjoint sets, each of them can provide a service composition solution. We study a generalized fault tolerant service composition which aims to find two service composition solutions for each request to minimize the number of shared service providers. Subject to such a primary objective, we also aim to minimize the total service composition cost. We firstly prove that the problem is NP-Complete, and formulate the problem as an integer linear program. We then propose heuristic algorithms to efficiently solve the problem. Simulation results demonstrate the effectiveness of the proposed heuristic algorithms.",2008,0,
550,551,Application of Taguchi technique to reduce positional error in two degree of freedom rotary-rotary planar robotic arm,"In present work, positional accuracy of robotic arm has been discussed. The factors considered in the experiment were the length of links, the mass of both links, the velocity of end point and torque on both links. A considerable reduction in performance variation can be obtained by Taguchi technique. Through simple multifactorial experiments on manipulator, controlled factors can be isolated to provide centering and variance control for a process variable. The primary objective in present work is to investigate the effect of process parameter on performance variation to improve positional accuracy. An attempt has been made to introduce a small variation to current approaches broadly called Taguchi parametric design method. In these methods, there are two broad categories of problems associated with simultaneously minimizing performance variations and bringing the mean on target, viz. Type 1- minimizing variations in performance caused by variations in noise factors (uncontrolled parameters); Type 2-minimizing variations in performance caused by variations in control factors (design variables).",2007,0,
551,552,Fault Tolerant Permanent Magnet Motor Drive Topologies for Automotive X-By-Wire Systems,"Future automobiles will be equipped with by-wire systems to improve reliability, safety and performance. The fault tolerant capability of these systems is crucial due to their safety critical nature. Three fault tolerant inverter topologies for permanent magnet brushless dc motor drives suitable for automotive x-by-wire systems are analyzed. A figure of merit taking into account both cost and post-fault performance is developed for these drives. Simulation results of the two most promising topologies for various inverter faults are presented. The drive topology with the highest post-fault performance and cost effectiveness is built and evaluated experimentally.",2008,0,
552,553,Using memory errors to attack a virtual machine,"We present an experimental study showing that soft memory errors can lead to serious security vulnerabilities in Java and .NET virtual machines, or in any system that relies on type-checking of untrusted programs as a protection mechanism. Our attack works by sending to the JVM a Java program that is designed so that almost any memory error in its address space will allow it to take control of the JVM. All conventional Java and .NET virtual machines are vulnerable to this attack. The technique of the attack is broadly applicable against other language-based security schemes such as proof-carrying code. We measured the attack on two commercial Java virtual machines: Sun's and IBM's. We show that a single-bit error in the Java program's data space can be exploited to execute arbitrary code with a probability of about 70%, and multiple-bit errors with a lower probability. Our attack is particularly relevant against smart cards or tamper-resistant computers, where the user has physical access (to the outside of the computer) and can use various means to induce faults; we have successfully used heat. Fortunately, there are some straightforward defenses against this attack.",2003,0,
553,554,Error propagation of the robotic system for liver cancer coagulation therapy,"The goal of this paper is to establish the error propagation model of the ultrasound-guided robot for liver cancer coagulation therapy, which consists of ultrasound machine, image-guided software subsystem, position tracking unit and needle-driven robot. The target of tumor is transformed to robot coordinate frame to let the robot move to the target. The transformation includes three dimension ultrasound construction, registration between pre-operative model and intra-operative physical body, coordinate transformation from position tracking unit to robot. The factors affecting the system accuracy can be expressed by the sum of target mapping error and robot positioning error. Then, the propagation model of target mapping error on the Euclidean motion group is established. At last, the simulations of the propagation model of target mapping error and the experiment of the system accuracy are carried out and the results show our proposed error propagation model is efficient and the system accuracy can satisfy the need of coagulation therapy for liver cancer.",2009,0,
554,555,Evaluating the Performance of Adaptive Fault-Tolerant Routing Algorithms for Wormhole-Switched Mesh Interconnect Networks,"One of the fundamental problems in parallel computing is how to efficiently perform routing in a faulty network each component of which fails with some probability. This paper presents a comparative performance study of ten prominent adaptive fault-tolerant routing algorithms in wormhole-switched 2D mesh interconnect networks. These networks carry a routing scheme suggested by Boppana and Chalasani as an instance of a fault-tolerant method. The suggested scheme is widely used in the literature to achieve high adaptivity and support inter-processor communications in parallel computer systems due to its ability to preserve both communication performance and fault-tolerant demands in these networks. The performance measures studied are the throughput, average message latency and average usage of virtual channels per node. Results obtained through simulation suggest two classes of presented routing schemes as high performance candidate in most faulty networks.",2007,0,
555,556,Analysis of Single-Phase-to-Ground Fault Generated Initial Traveling Waves,"Analysis of fault generated traveling waves is the base to implement traveling waves based protection and fault location. However, the structure at fault point is not asymmetrical under single-phase to ground fault condition in multi-phase power system, so that traveling waves analysis method of single circuit can not be applied. The paper at first analyzes initial traveling wave at fault point generated by the fault through resistance, according to superimposed theory and using phase-to-module transformation method, then considers the fault generated traveling waves' characteristics at the relay point. At last, EMTP is implemented to verify the correctness of analysis of single-phase-to-ground fault generated initial traveling waves",2005,0,
556,557,Real-time correction of distortion image based on FPGA,"Correcting infrared camera distortion is necessary in target tracking and object recognition system. The existent FPGA algorithm didn't utilize sufficiently the advantage of the parallel processing and leaded to the results that a great deal of the system resources were consumed and the running speed was slowed down. The paper analyzed the existing problems such as serial structure in other algorithms, proposed a new parallel algorithm and realized it with the lowest resources. The experiments carried on the chip-virtex5 produced by Xilinx company show that the proposed algorithm has a good real-time performance, use less resource than previous structure and realize the correction of distortion on FPGA on line.",2010,0,
557,558,Executable assertions for detecting data errors in embedded control systems,"In order to be able to tolerate the effects of faults, we must first detect the symptoms of faults, i.e. the errors. This paper evaluates the error detection properties of an error detection scheme based on the concept of executable assertions aiming to detect data errors in internal signals. The mechanisms are evaluated using error injection experiments in an embedded control system. The results show that using the mechanisms allows one to obtain a fairly high detection probability for errors in the areas monitored by the mechanisms. The overall detection probability for errors injected to the monitored signals was 74%, and if only errors causing failure are taken into account we have a detection probability of over 99%. When subjecting the target system to random error injections in the memory areas of the application, i.e., not only the monitored signals, the detection probability for errors that cause failure was 81%",2000,0,
558,559,Simultaneous optimization for wind derivatives based on prediction errors,"Wind power energy has been paid much attention recently for various reasons, and the production of electricity with wind energy has been increasing rapidly for a few decades. In this work, we will propose a new type of weather derivatives based on the prediction errors for wind speeds, and estimate their hedge effect on wind power energy businesses. At first, we will investigate the correlation of prediction errors between the power output and the wind speed in a Japanese wind farm. Then we will develop a methodology that will optimally construct a wind derivative based on the prediction errors using nonparametric regressions. A simultaneous optimization technique of the loss and payoff functions for wind derivatives is demonstrated based on the empirical data.",2008,0,
559,560,An empirical study of fault localization for end-user programmers,"End users develop more software than any other group of programmers, using software authoring devices such as e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been little research on finding ways to help these programmers with the dependability of their software. We have been addressing this problem in several ways, one of which includes supporting end-user debugging activities through fault localization techniques. This paper presents the results of an empirical study conducted in an end-user programming environment to examine the impact of two separate factors in fault localization techniques that affect technique effectiveness. Our results shed new insights into fault localization techniques for end-user programmers and the factors that affect them, with significant implications for the evaluation of those techniques.",2005,0,
560,561,A stochastic model of fault introduction and removal during software development,"Two broad categories of human error occur during software development: (1) development errors made during requirements analysis, design, and coding activities; (2) debugging errors made during attempts to remove faults identified during software inspections and dynamic testing. This paper describes a stochastic model that relates the software failure intensity function to development and debugging error occurrence throughout all software life-cycle phases. Software failure intensity is related to development and debugging errors because data on development and debugging errors are available early in the software life-cycle and can be used to create early predictions of software reliability. Software reliability then becomes a variable which can be controlled up front, viz, as early as possible in the software development life-cycle. The model parameters were derived based on data reported in the open literature. A procedure to account for the impact of influencing factors (e.g., experience, schedule pressure) on the parameters of this stochastic model is suggested. This procedure is based on the success likelihood methodology (SLIM). The stochastic model is then used to study the introduction and removal of faults and to calculate the consequent failure intensity value of a small-software developed using a waterfall software development",2001,0,
561,562,Detecting faults in technical indicator computations for financial market analysis,"Many financial trading and charting software packages provide users with technical indicators to analyze and predict price movements in financial markets. Any computation fault in technical indicator may lead to wrong trading decisions and cause substantial financial losses. Testing is a major software engineering activity to detect computation faults in software. However, there are two problems in testing technical indicators in these software packages. Firstly, the indicator values are updated with real-time market data that cannot be generated arbitrarily. Secondly, technical indicators are computed based on a large amount of market data. Thus, it is extremely difficult, if not impossible, to derive the expected indicator values to check the correctness of the computed indicator values. In this paper, we address the above problems by proposing a new testing technique to detect faults in computation of technical indicators. We show that the proposed technique is effective in detecting computation faults in faulty technical indicators on the MetaTrader 4 Client Terminal.",2010,0,
562,563,Dynamic node management and measure estimation in a state-driven fault injector,"The following topics were dealt with: visual querying and data exploration; graphs and hierarchies; taxonomies, frameworks and methodology; document visualization and collaborative visualization; algorithm visualization; and 3D navigation",2000,0,
563,564,Very high-resistance fault on a 525 kV transmission line - Case study,"This paper analyzes a 300 ohm primary ground fault, which is an unusually high value for a 525 kV transmission line in southeastern Brazil. This case study emphasizes the techniques used by the analysts. Considering that the fault impedance was larger than those usually observed in single-phase faults on extra-high-voltage (EHV) lines, this paper discusses the probable cause of the fault and mentions an analysis technique to evaluate such faults. The protective relaying community lacks information regarding the causes and values of fault resistances to ground on high-voltage (HV) and EHV transmission lines. The objectives of this paper are to stimulate research and contribute to the collection of very high-resistance fault information. The analysis techniques are presented using symmetrical components and fault calculations to arrive at fault parameter values that are very close to the ones provided by protective relays. The performance of the line protection is evaluated for the specific fault conditions, with calculation of the observed impedances and currents. The importance of the ground over- current directional protection on a pilot directional comparison scheme is shown. Speculation on the widespread use of differential protection for transmission lines should stimulate discussions of line protection philosophies and applications. The criteria for the resistive reach setting of the quadrilateral ground distance characteristic are presented to show an evolution of past criteria and to open discussion about the setting limits. The conclusions of this paper highlight the importance of present event report analysis techniques regarding fault calculation software and the need for appropriate settings criteria for the resistive ground distance element threshold. This paper also supports the use of ground directional overcurrent protection with a pilot scheme for HV and EHV transmission line protection, while proposing the widespread use of differential functions f- r transmission lines, even for the most extensive cases.",2009,0,
564,565,Circuit-level modeling of soft errors in integrated circuits,"This paper describes the steps necessary to develop a soft-error methodology that can be used at the circuit-simulation level for accurate nominal soft-error prediction. It addresses the role of device simulations, statistical simulation, analytical soft-error rate (SER) model development, and SER-model calibration. The resulting approach is easily automated and generic enough to be applied to any type of circuit for estimation of the nominal SER.",2005,0,
565,566,Development of a technique for calculation of the influence of generator design on power system balanced fault behaviour,"This paper presents the development of a method for quantitatively determining the potential impact that the design of a single generator may have upon the performance of power system under fault conditions. Initially it is illustrated that the impact that a single generator may have on network fault behaviour is limited by the configuration of the existing network to which the new generator is connected. These constraints are then used to develop a quantitative measure of variability in network-wide fault currents and the subsequent voltage disturbances that can be produced under balanced fault conditions by changing the design of a new generator, irrespective of its point of connection. Finally comparisons with the observed variation in network fault behaviour obtained from the simulation in PSS/E of a realistic 600-bus transmission network are used to demonstrate the technique's apparent effectiveness.",2002,0,
566,567,Fast Enhancement of Validation Test Sets for Improving the Stuck-at Fault Coverage of RTL Circuits,"A digital circuit usually comprises a controller and datapath. The time spent for determining a valid controller behavior to detect a fault usually dominates test generation time. A validation test set is used to verify controller behavior and, hence, it activates various controller behaviors. In this paper, we present a novel methodology wherein the controller behaviors exercised by test sequences in a validation test set are reused for detecting faults in the datapath. A heuristic is used to identify controller behaviors that can justify/propagate pre-computed test vectors/responses of datapath register-transfer level (RTL) modules. Such controller behaviors are said to be <i>compatible</i> with the corresponding precomputed test vectors/responses. The heuristic is fairly accurate, resulting in the detection of a majority of stuck-at faults in the datapath RTL modules. Also, since test generation is performed at the RTL and the controller behavior is predetermined, test generation time is reduced. For microprocessors, if the validation test set consists of instruction sequences then the proposed methodology also generates instruction-level test sequences.",2009,0,
567,568,Fault tolerant error coding and detection using reversible gates,"In recent years, reversible logic has emerged as one of the most important approaches for power optimization with its application in low power CMOS, quantum computing and nanotechnology. Low power circuits implemented using reversible logic that provides single error correction - double error detection (SEC-DED) is proposed in this paper. The design is done using a new 4times4 reversible gate called 'HCG' for implementing hamming error coding and detection circuits. A parity preserving HCG (PPHCG) that preserves the input parity at the output bits is used for achieving fault tolerance for the hamming error coding and detection circuits.",2007,0,
568,569,Design of fault simulation training system for a certain tank,"It is necessary to carry out simulation driving training before forces trainees conduct real vehicle driving training for tanks, which can save training funds and raise the level of military modernization. A fault simulation training system for a certain tank is designed and its hardware and software platform is introduced. The dynamics model of tank in linear motion is derived. The simulation shows that the system has good interaction between the trainees on the tank driving platform and the instructors on the ground master platform. The functions of driving simulation and fault exclusion have been realized initially.",2010,0,
569,570,On errors-in-variables regression with arbitrary covariance and its application to optical flow estimation,"Linear inverse problems in computer vision, including motion estimation, shape fitting and image reconstruction, give rise to parameter estimation problems with highly correlated errors in variables. Established total least squares methods estimate the most likely corrections Acirc and bcirc to a given data matrix [A, b] perturbed by additive Gaussian noise, such that there exists a solution y with [A + Acirc, b +bcirc]y = 0. In practice, regression imposes a more restrictive constraint namely the existence of a solution x with [A + Acirc]x = [b + bcirc]. In addition, more complicated correlations arise canonically from the use of linear filters. We, therefore, propose a maximum likelihood estimator for regression in the general case of arbitrary positive definite covariance matrices. We show that Acirc, bcirc and x can be found simultaneously by the unconstrained minimization of a multivariate polynomial which can, in principle, be carried out by means of a Grobner basis. Results for plane fitting and optical flow computation indicate the superiority of the proposed method.",2008,0,
570,571,Vehicle localization in outdoor woodland environments with sensor fault detection,"This paper describes a 2D localization method for a differential drive mobile vehicle on real forested paths. The mobile vehicle is equipped with two rotary encoders, Crossbow's NAV420CA inertial measurement unit (IMU) and a NAVCOM SF-2050M GPS receiver (used in StarFire-DGPS dual mode). Loosely-coupled multisensor fusion and sensor fault detection issues are discussed as well. An extended Kalman filter (EKF) is used for sensor fusion estimation where a GPS noise pre-filter is used to avoid introducing biased GPS data (affected by multi-path). Normalized innovation squared (NIS) tests are performed when a GPS measurement is incorporated to reject GPS data outliers and keep the consistency of the filter. Finally, experimental results show the performance of the localization system compared to a previously measured ground truth.",2008,0,
571,572,The design and implementation of a microcontroller-based single phase on- line uninterrupted power supply with power factor correction,"In this study, the design and implementation of a microcontroller-based single phase on-line UPS (Uninterrupted Power Supply) with PFC (Power Factor Correction) were made practically. SP-320-24 SMPS (Switch Mode Power Supply) module was used to correct the input power factor. Input power factor value was held at the desired value in uninterrupted power supply topologies. In the realized system, two PIC16F876 were used as microcontroller. One of them was used to generate sinusoidal PWM (Pulse Width Modulation) signals that are used to drive n-channel MOSFETs in push pull inverter and to assure feedback control. Other one was used to control and display units. Harmonics were eliminated and output filter was simplified by using sinusoidal PWM technology.",2009,0,
572,573,Fault-recovery Non-FPGA-based Adaptable Computing System Design,"Reconfigurability with fault-tolerance is one of the most desirable hardware combinations for space computing systems. This paper introduces an adaptable computing architecture that includes random and delay- fault recovering capability for avionics and space applications. A micro-architecture level fault handling and recovering scheme that can immunize random/delay errors is presented as a means of overcoming the limitations of gate-level fault tolerance. The fault- recovery flexible architecture was developed based on a pure-ASIC-based retargetable computing system. The retargetable system also offers sufficient flexibility without employing programmable devices. This adaptable system reasserts different signal patterns for random/delay faults by rerouting micro-operations of the operation that caused the faults. Different sequences of bit-pattern generated by the retargetable system avoid the same faulty situation in high-speed VLSI circuits, while continuously supporting seamless modification and migration of underlying hardware and software after fabrication of retargetable systems.",2007,0,
573,574,Soft errors in Flash-based FPGAs: Analysis methodologies and first results,"The paper presents the development of three different analysis methodologies in order to evaluate soft errors effects in flash-based FPGAs. They are complementary and can be used in different design stages, from the device characterization up to the design sensitiveness estimation. First results are very promising, proving that such methodologies are valid and open new ways of investigation. In particular, we are going to upgrade the experimental setup in order to support higher frequencies (up to 250 MHz) for further characterizing SEE effects. Moreover, a benchmark circuit should be defined in order to correctly predict the expected number of SETs for real circuits, taking into account other side effects, like broadening and logical masking. We expect that from the analysis results we will able to delight suitable hardening techniques that will undergo to both radiation test and prediction analysis.",2009,0,
574,575,"Reliable 3D surface acquisition, registration and validation using statistical error models","We present a complete data acquisition and processing chain for the reliable inspection of industrial parts considering anisotropic noise. Data acquisition is performed with a stripe projection system that was modeled and calibrated using photogrammetric techniques. Covariance matrices are attached individually to points during 3D coordinate computation. Different datasets are registered using a new multi-view registration technique. In the validation step, the registered datasets are compared with the CAD model to verify that the measured part meets its specification. While previous methods have only considered the geometrical discrepancies between the sensed part and its CAD model, we also consider statistical information to decide whether the differences are significant",2001,0,
575,576,Measuring application error rates for network processors,"Faults in computer systems can occur due to a variety of reasons. In many systems, an error has a binary effect, i.e. the output is either correct or it is incorrect. However, networking applications exhibit different properties. For example, although a portion of the code behaves incorrectly due to a fault, the application can still work correctly. Integrity of a network system is often unchanged during faults. Therefore, measuring the effects of faults on the network processor applications require new measurement metrics to be developed. In this paper, we highlight essential application properties and data structures that can be used to measure the error behavior of network processors. Using these metrics, we study the error behavior of seven representative networking applications under different cache access fault probabilities.",2004,0,
576,577,Study on the Features of Loudspeaker Sound Faults,"In this paper, the short-time Fourier transformation (STFT) is adopted to transform the loudspeaker sound signal. By STFT, the one-dimensional loudspeaker response signal is converted into two-dimensional time-frequency figure. Then, the figure is decomposed into a number of areas according to its harmonics distribution. The peak and mean values of every area are computed. Through observation and calculation, the features of loudspeaker defects are found. According to the experiment, this method is very effective and universal for different types of loudspeakers.",2009,0,
577,578,A Cellular Approach to Fault Detection and Recovery in Wireless Sensor Networks,"In the past few years wireless sensor networks have received a greater interest in application such as disaster management, border protection, combat field reconnaissance and security surveillance. Sensor nodes are expected to operate autonomously in unattended environments and potentially in large numbers. Failures are inevitable in wireless sensor networks due to inhospitable environment and unattended deployment. The data communication and various network operations cause energy depletion in sensor nodes and therefore, it is common for sensor nodes to exhaust its energy completely and stop operating. This may cause connectivity and data loss. Therefore, it is necessary that network failures are detected in advance and appropriate measures are taken to sustain network operation. In this paper we extend our cellular architecture and proposed a new mechanism to sustain network operation in the event of failure cause of energy-drained nodes. In our solution the network is partitioned into a virtual grid of cells to perform fault detection and recovery locally with minimum energy consumption. Specifically, the grid based architecture permits the implementation of fault detection and recovery in a distributed manner and allows the failure report to be forwarded across cells. The proposed failure detection and recovery algorithm has been compared with some existing related work and proven to be more energy efficient.",2009,0,
578,579,Engineering knowledge-based condition analyzers for on-board intelligent fault classification: A case study,"In this paper we describe the design of a knowledge-based condition analyzer that performs on-board intelligent fault classification. The system is designed to be deployed as a prototype on E414 locomotives, a series of downgraded highspeed vehicles that are currently employed in standard passenger service. Our goal is to satisfy the requirements of a development scenario in the Integrail project for a condition analyzer that leverages an ontology-based description of some critical E414 subsystems in order to classify faults considering mission and safety related aspects.",2008,0,
579,580,The Dangers of Failure Masking in Fault-Tolerant Software: Aspects of a Recent In-Flight Upset Event,"On 1 August 2005, a Boeing Company 777-200 aircraft, operating on an international passenger flight from Australia to Malaysia, was involved in a significant upset event while flying on autopilot. The Australian Transport Safety Bureau's investigation into the event discovered that ""an anomaly existed in the component software hierarchy that allowed inputs from a known faulty accelerometer to be processed by the air data inertial reference unit (ADIRU) and used by the primary flight computer, autopilot and other aircraft systems."" This anomaly had existed in original ADIRU software, and had not been detected in the testing and certification process for the unit. This paper describes the software aspects of the incident in detail, and suggests possible implications concerning complex, safety- critical, fault-tolerant software.",2007,0,
580,581,A fault line selection algorithm in non-solidly earthed network based on holospectrum,"The methods of line selection today focus on one target of the signal such as amplitude, frequency or phase. A novel method based on holospectrum algorithm to detect single-phase faults in distribution systems is proposed in this paper. After structuring analytic signals of zero sequence current and voltage, the holospectrum algorithm is applied. Thus the analysis of combined signal of amplitude, frequency and phase is realized. Compared with the use of single amplitude, frequency or phase, combined signal carries more details and information of transient signal. Theoretical analysis and simulation based on Simulink of MATLAB show that the presented method can exactly and effectively choose the faulty line in single-phase-to-ground fault.",2010,0,
581,582,Design of Integrated Fault Diagnostic System (FDS),"Early diagnosis of plant faults/deviations is a critical factor for optimized and safe plant operation. Although smart controllers and diagnosis systems are available and widely used in chemical plants, however, some faults couldn't be detected. Major reason is the lack of learning techniques that can learn from operational running data and previous abnormal cases. In addition, operator and maintenance engineer opinions and observations are not well used, and useful diagnosis knowledge is ignored. Providing link between operation management, maintenance management and fault diagnostic and monitoring systems will enable closing such gap where diagnostic and monitoring results can be used more effectively for real time operation support, and optimized plant maintenance. In addition, operation and maintenance findings and discovered knowledge can be used effectively for plant condition monitoring. This research work presents the framework and mechanism for such integrated fault diagnostic system, which is called FDS. The proposed idea will support operation and maintenance planning as well as overall plant safety",2006,0,
582,583,Develop on feed-forward real time compensation control system for movement error in CNC machining,"A theory model of feed-forward compensation controlling system is constructed by the method of precision compensation. A feed-forward compensation hardware control system is designed to MCS51CPU as the core and structure of compensation data processing program. Established components of linear contour error mathematical model, thus determine the amount of feed-forward compensation algorithm. CNC x-y experiment platform simulation results indicate that this design can effectively eliminate the phase lag and amplitude errors of the computer numerical control (CNC) system, and improve the general CNC machining accuracy on the part contour.",2010,0,
583,584,Non-inductive variable reactor design and computer simulation of rectifier type superconducting fault current limiter,"A rectifier type superconducting fault current limiter with noninductive reactor has been proposed by the authors. The concept behind this SFCL is that the high impedance generated during superconducting to normal state of the trigger coil limits the fault current. In the hybrid bridge circuit of the SFCL, two superconducting coils: a trigger coil and a limiting coil are connected in anti-parallel. Both the coils are magnetically coupled with each other and could have the same value of self inductance so that they can share the line current equally. At fault time when the trigger coil current reaches a certain level, the trigger coil changes from superconducting state to normal state. This super to normal transition of the trigger coil changes the current ratio of the coils and therefore the flux inside the reactor is no longer zero. So, the equivalent impedance of both the coils is increased and limits the fault current. We have carried out computer simulation using PSCAD/EMTDC and observed the results. Both the simulation and preliminary experiment shows good results. The advantage of using hybrid bridge circuit is that the SFCL can also be used as circuit breaker.",2005,0,
584,585,An FMO based error resilience method in H.264/AVC and its UEP application in DVB-H link layer,"Flexible Macroblock Ordering (FMO) is one of the new error resilience tools introduced in H264/AVC. Several slice grouping methods have been studied for improving error robustness using FMO. In this paper, a simple and fast slice grouping method for inter frames is introduced. Fast mode decision and early Skip Mode decision are applied for the first encoding pass, and only the features that are available at the stage of early Skip Mode decision are used for the classification. The computation time cost can be reduced by about 50% on average compared to traditional methods. The proposed scheme is tested under the proposed Unequal Error Protection scheme at the DVB-H link layer. The results are compared to the standard MPE-FEC EEP scheme using traditional FMO type `interleaved' at the DVB-H link layer. It is shown that the proposed scheme can provide improved error robustness for high error rate channels in a DVB-H system.",2010,0,
585,586,A PH complex control system built-in correction factor,"Besides the pH deployment process's non-linear, large hysteretic nature, the system's requirement of real-time and accuracy, the traditional control methods can not get to the high quality control results. The fuzzy control does not rely on a mathematical model of the object. It is very difficult to eliminate the steady-state deviation from the root. Because PI control has a very good scavenging effect of the steady-state, therefore the system uses a built-correction factor of the Fuzzy-PI composite control strategy.",2010,0,
586,587,Methodology to support laser-localized soft defects on analog and mixed-mode advanced ICs,"The soft defect localization on analog or mixed-mode ICs is becoming more and more challenging due to their increasing complexity and integration. New techniques based on dynamic laser stimulation are promising for analog and mixedmode ICs. Unfortunately, the considerable intrinsic sensitivity of this kind of devices under laser stimulation makes the defect localization results complex to analyze. As a matter of fact, the laser sensitivity mapping contains not only abnormal sensitive regions but also naturally sensitive ones. In order to overcome this issue by extracting the abnormal spots and therefore localize the defect, we propose in this paper a methodology that can improve the FA efficiency and accuracy. It consists on combining the mapping results with the electrical simulation of laser stimulation impact on the device. First, we will present the concept of the methodology. Then, we will show one case study on a mixed-mode IC illustrating the soft defect localization by using laser mapping technique & standard electrical simulations. Furthermore, we will argument the interest of a new methodology and we will show two simple examples from our experiments to validate it.",2009,0,
587,588,Empirical evaluation of the fault-detection effectiveness of smoke regression test cases for GUI-based software,"Daily builds and smoke regression tests have become popular quality assurance mechanisms to detect defects early during software development and maintenance. In previous work, we addressed a major weakness of current smoke regression testing techniques, i.e., their lack of ability to automatically (re)test graphical user interface (GUI) event interactions - we presented a GUI smoke regression testing process called daily automated regression tester (DART). We have deployed DART and have found several interesting characteristics of GUI smoke tests that we empirically demonstrate in this paper. We also combine smoke tests with different types of test oracles and present guidelines for practitioners to help them generate and execute the most effective combinations of test-case length and test oracle complexity. Our experimental subjects consist of four GUI-based applications. We generate 5000-8000 smoke tests (enough to be run in one night) for each application. Our results show that: (1) short GUI smoke tests with certain test oracles are effective at detecting a large number of faults; (2) there are classes of faults that our smoke test cannot detect; (3) short smoke tests execute a large percentage of code; and (4) the entire smoke testing process is feasible to do in terms of execution time and storage space.",2004,0,
588,589,Automatically translating dynamic fault trees into dynamic Bayesian networks by means of a software tool,"This paper presents a software tool allowing the automatic analysis of a dynamic fault tree (DFT) exploiting its conversion to a dynamic Bayesian network (DBN). First, the architecture of the tool is described, together with the rules implemented in the tool, to convert dynamic gates in DBNs. Then, the tool is tested on a case of system: its DFT model and the corresponding DBN are provided and analyzed by means of the tool. The obtained unreliability results are compared with those returned by other tools, in order to verify their correctness.",2006,0,
589,590,Average Error Performance of M-ary Modulation Schemes in Nakagami-q (Hoyt) Fading Channels,"Presented are exact-form expressions for the average error performance of various coherent, differentially coherent, and noncoherent modulation schemes in Nakagami-q (Hoyt) fading channels. The expressions are given in terms of the Lauricella hypergeometric function, F<sub>D</sub> <sup>(n)</sup>; for nges1, which can be evaluated numerically using its integral or converging series representation. It is shown that the derived expressions reduce to some existing results for Rayleigh fading as special cases",2007,0,
590,591,Analysis of the soft error effects on CAN network controller,"In this article, the effects of the single event upset on a Controller Area Network (CAN) controller and its effects on the network is being evaluated. The experiment is done using SINJECT fault injection tool in a simulation based environment. Three mail modules of the controller are used in three independent set of experiments in one of the CAN controllers of the network. The results show that the main cause of the network failure is the bit stream processor. 6.7% of the injected faults in the bit stream processor led to the network failure. On the other hand, the registers sub-module of the controller showed to be most fault tolerant. The experiment showed that 0.3% of the faults in the registers module results in network failure, and the bit timing module is responsible for the failure of the whole network in 3.2% of the injected single event upset faults.",2010,0,
591,592,Modeling of cable fault system,"Modeling is the essential part of implementing the prediction and location of three-phase cable fault. To predict and locate cable fault, a model of three-phase cable fault system is constructed based on a great deal of measured validation data by choosing BP neural network that has nonlinear characteristic and using the unproved BP algorithm, Levenberg-Marquardt data-optimized method. It is shown by the simulation using MATLAB software that the parameters of the model converge rapidly, and the simulated output of the neural network model and the measured output of cable fault system are approximately equal, and the mean value of the relatively predictive error of the fault distance is smaller than 0.3%, so that the model quality is reliable.",2004,0,
592,593,Asymmetries in soft-error rates in a large cluster system,"Early in the deployment of the ASC Q cluster supercomputer system, an unexpectedly high rate of soft errors were observed in the board-level cache subsystems of the constituent AlphaServer ES45 systems that make up the compute component of this large cluster. A series of tests and experiments was undertaken to validate the hypothesis that this frequency was consistent with the high level of terrestrial secondary cosmic-ray neutron flux resulting from the high elevation of its installation site. The overall success of this effort is reported elsewhere in this issue. This paper reports on three secondary phenomena that were observed during these tests and experiments: Error logs were collected from all servers during a representative period and examined for nonrandom event rates, which would indicate a systematic cause. The only significant result of this exploration was the discovery of a latent soft-error discovery effect, and a self-shielding effect, whereby the servers positioned physically higher in their racks suffered disproportionately higher soft-error rates. This excess was examined and found to be consistent with established shielding effect of the high-Z composition of the constituents of the overlying systems. Experiments with individual ES45 systems in an artificial neutron beam at the Los Alamos Neutron Science Center facility have established that the soft-error rates observed in the SRAM parts is significantly dependent on the incident direction of the neutrons in the beam. These asymmetries could be exploited as part of a strategy for mitigating the frequency of soft errors in future computer systems.",2005,0,
593,594,Sandra - A New Concept for Management of Fault Isolation in Aircraft Systems,"The embedded Fault Isolation functionality in the Saab JAS39 Gripen aircraft has been designed to accurately and reliably provide the technician with proposed maintenance procedures. A previously identified drawback and built in limitation has been the significant lead time for Fault Isolation functional changes based on aircraft operational statistics and line experience. With the Fault Isolation executing as compiled source code, changes and corrections require adaptation of the regular onboard systems computer software and careful planning of code and documentation releases, implying not only significant delays, but also high costs for necessary updates. The ""Sandra"" project aims at even further refine - and to introduce a state of the art - fault isolation maintenance concept for the Saab JAS39 Gripen aircraft. Based on an easy-to-use PC based graphical tool, Fault Isolation on dedicated aircraft monitoring and safety check result data is specified. Output in the form of design documentation artifacts, such as flowcharts and technical publications, is generated. The contained Fault Isolation object data is updated in parallel with the regular onboard computer software development process and the corresponding Loadable Data File will be delivered when convenient. The PC application constitutes the maintenance engineer's primary Fault Isolation design tool. The tool enables the maintenance engineer to select dedicated settings via a graphical user interface and use logical expressions to propose detailed and specific maintenance actions to be performed by the aircraft technician. The tool is capable of verifying a complete set of design documents towards the content of a generated loadable file. Thus, a generated output file with a minimum of additional verification can be delivered to be loaded into the aircraft. This new approach implies that the lead time for a Fault Isolation functional change can be reduced by as much as 80 %. The cost for the corresponding- functional change will decrease by more than 50 %.",2007,0,
594,595,Development of simulation model based on directed fault propagation graph,"A new method of simulation model is presented in this paper in order to deal with system based fault mode and effect analysis model in modern complex system with large structure. Directed fault propagation graph model based on fault influence degree is proposed and fault propagation model is put forward. With the definition of direct fault propagation influence degree and indirect fault propagation influence degree is introduced, the algorithm of propagation and search method for fault propagation model is discussed. Visualization simulation system based on directed fault propagation graph is developed with object oriented method according to the proposed fault analysis model. The Simulation system can used for fault propagation analysis and fault influence of exist complex system, simulation result can be validated and verified by control area network platform, the method is useful for fault diagnosis and analysis model in modern large complex system.",2010,0,
595,596,Research on Analyse and Compensation Approach Aimed at CNC Machine Geometrical and Kinematic Errors,"The geometrical error and kinematic error are regarded as the prime reasons to bring about CNC machine contour error, which confine the improvement of machining precision further. In the paper, the main error sources that produce geometrical error and kinematic error in CNC machine are researched in detail, and an analyse approach aimed at the main error sources is put forward which adopts the ""arc interpolation motion - arc image method"". Furthermore, the compensation approach aimed at error resources such as the mismatch of position loop servo gains and orthogonal axes out of the vertical is developed in CNC software. Finally, the analyse and compensation approach is tested on a CNC experiment table. The experimentation result reveals that the developed analyse and compensation approach aimed at the main error sources can enhance machine contour precision greatly. Consequently, the research is helpful to improve and keep CNC machine high precision long-time.",2009,0,
596,597,Hybrid Error Concealment with Automatic Error Detection for Transmitted MPEG-2 Video Streams over Wireless Communication Network,"This work presents a complete error concealment system, for overcoming visible distortions in video sequences which are transmitted over a lossy communication network. The system we propose provides an error concealment solution from the point of receiving the transmitted sequence by the decoder, until it is presented to viewers, without human interference. The system is composed of an automatic error detection algorithm, and a decision tree error concealment algorithm. The performance of the detection algorithm is estimated, along with a performance evaluation of the decision tree algorithm by comparing it to the other three error concealment methods. The results are evaluated using two quality measures. We show that our error concealment method achieves the highest quality compared to the other methods for most of the conducted tests.",2006,0,
597,598,On the Distribution of Software Faults,"The Pareto principle is often used to describe how faults in large software systems are distributed over modules. A recent paper by Andersson and Runeson again confirmed the Pareto principle of fault distribution. In this paper, we show that the distribution of software faults can be more precisely described as the Weibull distribution.",2008,0,
598,599,Accelerating learning from experience: avoiding defects faster,"All programmers learn from experience. A few are rather fast at it and learn to avoid repeating mistakes after once or twice. Others are slower and repeat mistakes hundreds of times. Most programmers' behavior falls somewhere in between: They reliably learn from their mistakes, but the process is slow and tedious. The probability of making a structurally similar mistake again decreases slightly during each of some dozen repetitions. Because of this a programmer often takes years to learn a certain rule-positive or negative-about his or her behavior. As a result, programmers might turn to the personal software process (PSP) to help decrease mistakes. We show how to accelerate this process of learning from mistakes for an individual programmer, no matter whether learning is currently fast, slow, or very slow, through defect logging and defect data analysis (DLDA) techniques",2001,0,
599,600,Towards high-precision lens distortion correction,"This paper points out and attempts to remedy a serious discrepancy in results obtained by global calibration methods: The re-projection error can be rendered very small by these methods, but we show that the optical distortion correction is far less accurate. This discrepancy can only be explained by internal error compensations in the global methods that leave undetected the inadequacy of the distortion model. This fact led us to design a model-free distortion correction method where the distortion can be any image domain diffeomorphism. The obtained precision compares favorably to the distortion given by state of the art global calibration and reaches a RMSE of 0.08 pixels. Nonetheless, we also show that this accuracy can still be improved.",2010,0,
600,601,Research on code pattern automata-based code error pattern automatic detection technique,"Nowadays, many defects, e.g., obscure error generation-scenario and lacking of formalization which is the basis for the automatic error detection, exist in field of code error research. Furthermore, the automation of error detection will greatly affect the quality and efficiency of software testing. Therefore, more deeply research on code errors need to be done. At first, this paper presents the definition of code error pattern based on definition of pattern. Secondly, it investigates the formalization description of code error pattern. Then, it studies the automatic error pattern detecting technique based on non-determinate finite state automata and treats the matching technique of error pattern as the key problem. Finally, some case studies are given. The preliminary results show the rationality of code error pattern definition and the effectiveness of error pattern formalization description and error pattern matching technique.",2009,0,
601,602,Multichamber Tunable Liquid Microlenses with Active Aberration Correction,"A design approach and new manufacturing technique for a novel type of stacked fluidic multi-chamber tunable lenses is presented. The design offers flexibility and extensibility, leading to fully functional miniature tunable optical lens systems with the ability for low order aberration control.",2009,0,
602,603,A New Fault-Information Model for Adaptive & Minimal Routing in 3-D Meshes,"In this paper, we rewrite the minimal-connected-component (MCC) model in 2-D meshes in a fully-distributed manner without using global information so that not only can the existence of a Manhattan-distance-path be ensured at the source, but also such a path can be formed by routing-decisions made at intermediate nodes along the path. We propose the MCC model in 3-D meshes, and extend the corresponding routing in 2-D meshes to 3-D meshes. We consider the positions of source & destination when the new faulty components are constructed. Specifically, all faulty nodes will be contained in some disjoint fault-components, and a healthy node will be included in a faulty component only if using it in the routing will definitely cause a non-minimal routing-path. A distributed process is provided to collect & distribute MCC information to a limited number of nodes along so-called boundaries. Moreover, a sufficient & necessary condition is provided for the existence of a Manhattan-distance-path in the presence of our faulty components. As a result, only the routing having a Manhattan-distance-path will be activated at the source, and its success can be guaranteed by using the information of boundary in routing-decisions at the intermediate nodes. The results of our Monte-Carlo-estimate show substantial improvement of the new fault-information model in the percentage of successful Manhattan-routing conducted in 3-D meshes.",2008,0,
603,604,Current fault management trends in NASA's planetary spacecraft,"Fault management for today's space missions is a complex problem, going well beyond the typical safing requirements of simpler missions. Recent missions have experienced technical issues late in the project lifecycle, associated with the development and test of fault management capabilities, resulting in both project schedule delays and cost overruns. Symptoms seem to become exaggerated in the context of deep space and planetary missions, most likely due to the need for increased autonomy and the limited communications opportunities with Earth-bound operators. These issues are expected to cause increasing challenges as the spacecraft envisioned for future missions become more capable and complex. In recognition of the importance of addressing this problem, the Discovery and New Frontiers Program Office hosted a Fault Management Workshop on behalf of NASA's Science Mission Directorate, Planetary Science Division, to bring together experts in fault management from across NASA, DoD, industry and academia. The scope of the workshop was focused on deep space and planetary robotic missions, with full recognition of the relevance of, and subsequent benefit to, Earth-orbiting missions. Three workshop breakout sessions focused the discussions to target three topics: 1) fault management architectures, 2) fault management verification and validation, and 3) fault management development practices, processes and tools. The key product of this three-day workshop is a NASA White Paper that documents lessons learned from previous missions, recommended best practices, and future opportunities for investments in the fault management domain. This paper summarizes the findings and recommendations that are captured in the white paper.",2009,0,
604,605,Extraction of Tectonic Faults of Longmen Mountain Based on DEM,"According to the analysis of the tectonic characteristics of thrust belt in the Longmen Mountain, the present study aims to build a methodology to extract liner fault structures in the study area. The methodology is an approach which includes automatic extraction of major faults based on combined calculation of landform factors from the SRTM-DEM and revision of the automatic extraction result according to remote sensing images and geologic data. Therein, these landform factors including elevation, slope, aspect and variation of aspect, slope of slope (SOS) and slope of aspect (SOA). The compound method, including the spatial analysis techniques based on SRTM-DEM, interpretation of remote sensing images, and some geosciences' researches, provides strong technical support to achieve the quantization of the morphotectonics research.",2009,0,
605,606,"A Comparison of Cascading Horizontal and Vertical Menus with Overlapping and Traditional Designs in Terms of Effectiveness, Error Rate and user Satisfaction","In this study, effectiveness, efficiency and user satisfaction of different menu designs were investigated. 24 graduate students voluntarily participated to the study. The results indicate that horizontal menus are more effective than vertical menus in terms of selecting sub menu items, overall task completion time is not related to menu design, horizontal overlapping menu design is the most effective one in terms of preventing user errors. Lastly, user satisfaction doesn't vary according to menu designs.",2007,0,
606,607,A new method in reducing the overcurrent protection response times at high fault currents to protect equipment from extended stress,"This paper describes a new method for protecting power equipment from extended stresses during high fault current conditions. This was achieved using a universal protection device with a software platform that can facilitate designing time-current characteristic (TCC) curves of different shapes, all in the same hardware. When combined, recloser control and relay response times are faster and more accurate than with conventional means. Reduced device response times are achieved by combining different overcurrent TCCs. A coordination example is presented for a typical distribution system loop scheme containing new multifunction relays and reclosers. The advantages of using integrated device functions over standard overcurrent relays and recloser controls are illustrated. A comparative analysis is presented to quantify the reduction in let-thru I<sup>2</sup>t values and equipment stress that can be realized using this method during high fault current conditions",2001,0,
607,608,Fault-tolerance for exascale systems,"Periodic, coordinated, checkpointing to disk is the most prevalent fault tolerance method used in modern large-scale, capability class, high-performance computing (HPC) systems. Previous work has shown that as the system grows in size, the inherent synchronization of coordinated checkpoint/restart (CR) limits application scalability; at large node counts the application spends most of its time checkpointing instead of executing useful work. Furthermore, a single component failure forces an application restart from the last correct checkpoint. Suggested alternatives to coordinated CR include uncoordinated CR with message logging, redundant computation, and RAID-inspired, in-memory distributed checkpointing schemes. Each of these alternatives have differing overheads that are dependent on both the scale and communication characteristics of the application. In this work, using the Structural Simulation Toolkit (SST) simulator, we compare the performance characteristics of each of these resilience methods for a number of HPC application patterns on a number of proposed exascale machines. The result of this work provides valuable guidance on the most efficient resilience methods for exascale systems.",2010,0,
608,609,Detection and correction of abnormal pixels in Hyperion images,"Hyperion images are currently processed to level 1a (from level 0 or raw data). These level 1a images are files of radiometrically corrected data in units of either watts/(sr  micron  m<sup>2</sup>)  40 for VNIR bands or watts/(sr  micron  m<sup>2</sup>)  80 for SWIR bands. Each distributed Hyperion level 1a image tape contains a log file, called ""(EO-1 identifier).fix.log"", that reports the bad or corrupted pixels (called known bad pixels) found during the pre-flight checking, and details how they were fixed. All bad pixels should be corrected in a level 1a image. However, bad pixels are still evident. In addition, there are dark vertical stripes in the image that are not reported in the log file. In this paper, we introduce a method to detect and correct the bad pixels and vertical stripes (we will refer to these occurrences as abnormal pixels). Images from the Greater Victoria Watershed and other EVEOSD test sites are used to determine how stationary the locations of the abnormal pixels are. After abnormal pixel correction a Hyperion image is ready for geometric correction, atmospheric correction, and further analysis.",2002,0,
609,610,Fault diagnosis for transformer based on fuzzy entropy,"Power transformers are one of the key equipments of the power system, so it is valuable to discover the incipient fault timely and truly. Code deficiency exists in the gas ratio method by the IEC/DEEE standard and fault diagnosis for power transformers. A model based on fuzzy entropy for power transformer faults diagnosis is put forward, which expand coding bound of original IEC three-ratio. At the same time, the method has some contain fault ability in a certain degree. It also shows the probability and disposes lost or false power transformer fault symptoms. That shows the validity of the method for power transformer fault diagnosis by dissolved gas-in-oil analysis.",2007,0,
610,611,Enhanced Fault Ride-Through Method for Wind Farms Connected to the Grid Through VSC-Based HVDC Transmission,"This paper describes a new control approach for secure fault-ride through of wind farms connected to the grid through a voltage source converter-based high voltage DC transmission. On fault occurrence in the high voltage grid, the proposed control initiates a controlled voltage drop in the wind farm grid to achieve a fast power reduction. In this way overvoltages in the DC transmission link can be avoided. It uses controlled demagnetization to achieve a fast voltage reduction without producing the typical generator short circuit currents and the related electrical and mechanical stress to the wind turbines and the converter. The method is compared to other recent FRT methods for HVDC systems and its superior performance is demonstrated by simulation results.",2009,0,
611,612,Error Resilient Video Coding Using B Pictures in H.264,"Since the quality of compressed video is vulnerable to errors, video transmission over unreliable Internet is very challenging today. Multi-hypothesis motion-compensated prediction (MHMCP) has been shown to have error resilience capability for video transmission, where each macroblock is predicted by a linear combination of multiple signals (hypotheses). B picture prediction is a special case of MHMCP. In H.264/AVC, the prediction of B pictures is generalized such that both of the two predictions can be selected from the past pictures or from the subsequent pictures. The multiple reference picture framework in H.264/AVC also allows previously decoded B pictures to be used as references for B picture coding. In this paper, we will discuss the error resilience characteristics of the generalized B pictures in H.264/AVC. Three prediction patterns of B pictures are analyzed in terms of their error-suppressing abilities. Both theoretical models (picture level error propagation) and simulation results are given for the comparison.",2009,0,
612,613,Managing Post-Development Fault Removal,"In this paper, we manage fault removal by classifying and prioritizing fault warnings reported by a static analysis tool. We present our findings from analyzing three cross-platform industrial code bases at Yahoo! totaling approximately 3.6+ MLOC. The tool found 1.2K potential fault warnings as follows: 52.29% true faults and 47.71% false/noise. The 52.29% correctly reported faults were prioritized based on severity. Additionally, we connected the tool classification to a standard software weakness schema, Common Weakness Enumeration (CWE) to standardized discourse. The results from creating a management system for post-development fault removal are intended to be shifted back into earlier stages of software development.",2009,0,
613,614,A Probabilistic Method for Aligning and Merging Range Images with Anisotropic Error Distribution,"This paper describes a probabilistic method of aligning and merging range images. We formulate these issues as problems of estimating the maximum likelihood. By examining the error distribution of a range finder, we model it as a normal distribution along the line of sight. To align range images, our method estimates the parameters based on the expectation maximization (EM) approach. By assuming the error model, the algorithm is implemented as an extension of the iterative closest point (ICP) method. For merging range images, our method computes the signed distances by finding the distances of maximum likelihood. Since our proposed method uses multiple correspondences for each vertex of the range images, errors after aligning and merging range images are less than those of earlier methods that use one-to-one correspondences. Finally, we tested and validated the efficiency of our method by simulation and on real range images.",2006,0,
614,615,New resonance type Fault Current Limiter,"This paper proposes a new parallel LC resonance type Fault Current Limiter (FCL). This structure has low cast because of using dry capacitor and non-superconducting inductor and fast operation. The proposed FCL is able to limit fault current in constant value near to pre-fault condition value against series resonance type FCL. In this way, the voltage of point of common coupling (PCC) will not change during fault. Analytical analysis is presented in detail and simulation results are involved to validate the effectiveness of this structure.",2010,0,
615,616,ConvexFit: an optimal minimum-error convex fitting and smoothing algorithm with application to gate-sizing,"Convex optimization has gained popularity due to its capability to reach global optimum in a reasonable amount of time. Convexity is often ensured by fitting the table data into analytically convex forms such as posynomials. However, fitting the look-up tables into the posynomial forms with minimum error itself may not be a convex optimization problem and hence excessive fitting errors may be introduced. In this paper, we propose to directly adjust the look-up table values into a numerically convex look-up table without explicit analytical form. We show that numerically ""convexifying"" the table data with minimum perturbation can be formulated as a convex semidefinite optimization problem and hence optimality can be reached in polynomial time. Without an explicit form limitation, we find that the fitting error is significantly reduced while the convexity is still ensured. As a result, convex optimization algorithms can still be applied. Furthermore, we also develop a ""smoothing"" algorithm to make the table data smooth and convex to facilitate the optimization process. Results from extensive experiments on industrial cell libraries demonstrate that our method reduces 30 fitting error over a well-developed posynomial fitting algorithm. Its application to circuit tuning is also presented.",2005,0,
616,617,Stereoscopic video error concealment for missing frame recovery using disparity-based frame difference projection,"At low bit-rate video communications, packet loss may easily cause whole-frame loss that, in return, leads to annoying frame drop phenomenon. In this paper, a novel error concealment algorithm is specifically developed for stereoscopic video, called the disparity-based frame difference projection (DFDP), to recover the lost frames at the decoder. The proposed DFDP contains three key components: 1) change detection; 2) disparity estimation; and 3) frame difference projection, which exploits both the intra-view frame difference from one view and interview correlation to estimate the lost frame in another view. The change region computed on the correctly received frame will be used to predict the change region between current missing frame and its previous frame through the estimated disparity, which is the summation of the estimated global disparity and the estimated local disparity. Experimental results have shown that the proposed stereoscopic video error concealment method can effectively restore the lost frames at the decoder and deliver attractive performance, in terms of objective measurement (in peak signal-to-noise ratio) and subjective visual quality.",2009,0,
617,618,Fault-tolerant static scheduling for grids,"While fault-tolerance is desirable for grid applications because of the distributed and dynamic nature of grid resources, it has seldom been considered in static scheduling. We present a fault-tolerant static scheduler for grid applications that uses task duplication and combines the advantages of static scheduling, namely no overhead for the fault-free case, and of dynamic scheduling, namely low overhead in case of a fault. We also give preliminary experimental results on our scheme.",2008,0,
618,619,Charge sharing and interaction depth corrections in a wide energy range for small pixel pitch CZT detectors,"The CSTD project aims at developing a high resolution pixel gamma detector based on CdZnTe for Compton imaging applications. Our research group has been working recently on the design and characterization of a new pixel detector with specifications focused at high energy SPECT for medical imaging applications. The detector pitch, 0.3 mm, and its thickness, 5 mm, allows to reach high spatial resolution and high detector efficiency. Non-ideal performance appears with more strength in small pixel pitch CdZnTe detectors, below 1 mm, affecting at the spectroscopic results. In order to recuperate the shared charge, the customized ASIC simultaneously collects the charge in the triggering pixel and its eight neighboring pixels per event. The detector design, readout electronics, acquisition software and data analysis have been completed at CIEMAT. Data has been taken by irradiating the CdZnTe detector with high and low energy gamma-ray sources. The high energy events of the <sup>137</sup>Cs source suffer from a great proportion of charge sharing in the neighboring pixels. Two <sup>137</sup>Cs spectra, with and without energy correction, are shown and compared. To obtain the corrected spectra offline, the collected charge at the neighboring pixels is added to the trigger pixel collected charge. The corrected spectra show that the 662 keV photopeak is reconstructed. Interaction depth correction follows to improve the energy resolution by data segmentation of the 662 keV energy peak according to fifty cathode to pixel ratios. The computed interaction depth correction profile is the inverse of the charge collection efficiency. Energy resolution can be improved discarding the segmented data which do not achieve an acceptable energy resolution. Several interaction depth correction profiles at 81, 356 and 662 keV are shown and reveal a second correlation between the charge collecting efficiency and the collecting energy.",2010,0,
619,620,Automatic Diagnosis of Defects of Rolling Element Bearings Based on Computational Intelligence Techniques,"This paper presents a method, based on classification techniques, for automatic detection and diagnosis of defects of rolling element bearings. We used vibration signals recorded by four accelerometers on a mechanical device including rolling element bearings: the signals were collected both with all faultless bearings and after substituting one faultless bearing with an artificially damaged one. We considered four defects and, for one of them, three severity levels. In all the experiments performed on the vibration signals represented in the frequency domain we achieved a classification accuracy higher than 99%, thus proving the high sensitivity of our method to different types of defects and to different degrees of fault severity. We also assessed the degree of robustness of our method to noise by analyzing how the classification performance varies on variation of the signal-to-noise ratio and using statistical classifiers and neural networks. We achieved very good levels of robustness.",2009,0,
620,621,Cloud Model-Based Security-Aware and Fault-Tolerant Job Scheduling for Computing Grid,"The uncertainties of grid nodes security are main hurdle to make the job scheduling secure, reliable and fault-tolerant. The fixed fault-tolerant strategy in jobs scheduling may utilize excessive resources. In this paper, the job scheduling decides which kinds of fault-tolerance strategy will be applied to each individual job for more reliable computation and shorter makespan. And we discuss the fuzziness or uncertainties between TL and SD attributes by the subjective judgment of human beings. Cloud model is a model of the uncertain transition between qualitative concept and its quantitative representation. Based on the cloud model, We propose a security-aware and fault-tolerant jobs scheduling strategy for grid (SAFT), which makes the assess of SD and SL to become more flexible and more reliable. Meanwhile, the different fault-tolerant strategy has been applied in grid job scheduling algorithm by the SD and job workload. Moreover, much more important, we are able to set up some rules and active each qualitative rule to select a suitable fault-tolerant strategy for a scheduling job by input value (the SD and job workload) to realize the uncertainty reasoning. The results demonstrate that our algorithm has shorter makespan and more excellent efficiencies on improving the job failure rate than the fixed fault-tolerant strategy selection.",2010,0,
621,622,SWIFT: software implemented fault tolerance,"To improve performance and reduce power, processor designers employ advances that shrink feature sizes, lower voltage levels, reduce noise margins, and increase clock rates. However, these advances make processors more susceptible to transient faults that can affect correctness. While reliable systems typically employ hardware techniques to address soft-errors, software techniques can provide a lower-cost and more flexible alternative. This paper presents a novel, software-only, transient-fault-detection technique, called SWIFT. SWIFT efficiently manages redundancy by reclaiming unused instruction-level resources present during the execution of most programs. SWIFT also provides a high level of protection and performance with an enhanced control-flow checking mechanism. We evaluate an implementation of SWIFT on an Itanium 2 which demonstrates exceptional fault coverage with a reasonable performance cost. Compared to the best known single-threaded approach utilizing an ECC memory system, SWIFT demonstrates a 51% average speedup.",2005,0,
622,623,On-line diagnosis of interconnect faults in FPGA-based systems,"This paper presents an on-line diagnosis approach for locating the interconnect faults in field programmable gate arrays (FPGAs)-based systems. The diagnosis proposed approach consists of two phases. Phase one is locating the faulty tile through partitioning the FPGA-based system into self-checking tiles. The faulty tile can be detected concurrently with the normal system operation. This operation is performed prior to scheduling and allocating the circuit. The proposed partitioning approach was applied on certain circuits as a case study, and has been implemented using Xilinx foundation CAD tool with FPGA chip XC4010. The simulation study proved that our partitioning scheme reduces the test complexity and produces lower overheads. Upon locating a faulty tile and by the aid of a proposed path-list file per tile created during the routing process, the second phase of the diagnosis approach is applied only on the utilized interconnection of that tile for locating the faulty wires and switches. Therefore, the diagnosis approach is considered to be simplified.",2004,0,
623,624,Application of Empirical Bayes Estimation in Error Model Identification of Two Orthometric Accelerometers,"The high accuracy accelerometer can be demarcated in multiposition tumbling experiment under 1g gravitational field. The g<sup>2</sup> observation model of Two Orthometric Accelerometers can eliminate the corner error. there is a serious multicollinearity exit in this system because some model coefficient mix together. In view of above question, this article has given the arithmetic of Empirical Bayes Estimation(EB), and applied this method in model which is mentioned above. The result of the simulation and the experiment shows that compared with the conventional least squares method and the generalized diagonal ridge estimation, the Empirical Bayes Estimation can overcome the influence of the multicollinearity and can separate two coefficients which are The Second-Order Terms and the cross-coupling terms.",2010,0,
624,625,Multi Gigabit Transceiver Configuration RAM Fault Injection Response,"High performance processing and memory systems require enormous amounts of I/O bandwidth. Wide parallel bus architectures have reached their practical limits for high bandwidth transport. High speed serial interfaces that support 10's of Gbps are now displacing wide shared bus architectures for many systems. Xilinx FPGAs serial links support this transition by providing more than 10 Gbps in their multi gigabit transceiver (MGT) I/Os. For space applications, these links are susceptible to single event effects (SEE). Many of these effects are due to upsets in the FPGAs configuration RAM that control the many features and functions of the I/O. This paper details the functional effects of configuration RAM upsets in Xilinx MGTs. These effects are realized by injecting upsets in the FPGA configuration RAM while monitoring MGT functional operation. Configuration RAM upset effects are described and functional upset rates due to configuration RAM upsets are calculated for an example orbit. The results of this work provide insight into the on-orbit upset rate and effects of Xilinx multigigabit transceivers",2005,0,
625,626,Statistical software debugging: From bug predictors to the main causes of failure,"Detecting latent errors is a key challenging issue in the software testing process. Latent errors could be best detected by bug predictors. A bug predictor manifests the effect of a bug on the program execution state. The aim has been to find the smallest reasonable subset of the bug predictors, manifesting all possible bugs within a program. In this paper, a new algorithm for finding the smallest subset of bug predictors is presented. The algorithm, firstly, applies a LASSO method to detect program predicates which have relatively higher effect on the termination status of the program. Then, a ridge regression method is applied to select a subset of the detected predicates as independent representatives of all the program predicates. Program control and data dependency graphs can be best applied to find the causes of bugs represented by the selected bug predictors. Our proposed approach has been evaluated on two well-known test suites. The experimental results demonstrate the effectiveness and accuracy of the proposed approach.",2009,0,
626,627,ERCOT's experience in identifying parameter and topology errors using State Estimator,"The State Estimator is an important tool that ERCOT relies on to monitor the real time state of power grid. As the parameter and topology errors are critical to the quality of state estimator results, operations engineers in ERCOT are using multiple tools to detect and identify the topology and parameter errors in ERCOT EMS network model. This paper will present ERCOT experience in detecting and identifying the topology and parameter errors using state estimator monitoring tools and by analyzing SE results.",2010,0,
627,628,Performance of fault-tolerant distributed shared memory on broadcast- and switch-based architectures,"This paper presents a set of distributed-shared-memory protocols that provide fault tolerance on broadcast-based and switch-based architectures with no decrease in performance. These augmented DSM protocols combine the data duplication required by fault tolerance with the data duplication that naturally results in distributed-shared-memory implementations. The recovery memory at each backup node is continuously maintained consistent and is accessible by all processes executing at the backup node. Simulation results show that the additional data duplication necessary to create fault-tolerant DSM causes no reduction in system performance during normal operation and eliminates most of the overhead at checkpoint creation. Data blocks which are duplicated to maintain the recovery memory are also utilized by the DSM protocol, reducing network traffic, and increasing the processor utilization significantly. We use simulation and multiprocessor address trace files to compare the performance of a broadcast architecture called the SOME-Bus to the performance of two representative switch architectures.",2005,0,
628,629,"Image steganalysis based on moments of characteristic functions using wavelet decomposition, prediction-error image, and neural network","In this paper, a general blind image steganalysis system is proposed, in which the statistical moments of characteristic functions of the prediction-error image, the test image, and their wavelet subbands are selected as features. Artificial neural network is utilized as the classifier. The performance of the proposed steganalysis system is significantly superior to the prior arts.",2005,0,
629,630,Supporting fault tolerance in a data-intensive computing middleware,"Over the last 2-3 years, the importance of data-intensive computing has increasingly been recognized, closely coupled with the emergence and popularity of map-reduce for developing this class of applications. Besides programmability and ease of parallelization, fault tolerance is clearly important for data-intensive applications, because of their long running nature, and because of the potential for using a large number of nodes for processing massive amounts of data. Fault-tolerance has been an important attribute of map-reduce as well in its Hadoop implementation, where it is based on replication of data in the file system. Two important goals in supporting fault-tolerance are low overheads and efficient recovery. With these goals, this paper describes a different approach for enabling data-intensive computing with fault-tolerance. Our approach is based on an API for developing data-intensive computations that is a variation of map-reduce, and it involves an explicit programmer-declared reduction object. We show how more efficient fault-tolerance support can be developed using this API. Particularly, as the reduction object represents the state of the computation on a node, we can periodically cache the reduction object from every node at another location and use it to support failure-recovery. We have extensively evaluated our approach using two data-intensive applications. Our results show that the overheads of our scheme are extremely low, and our system outperforms Hadoop both in absence and presence of failures.",2010,0,
630,631,Enhancing Motion Picture Lens Performance by Digital Calibration and Correction,"To some degree, all lenses used by the motion picture industry exhibit certain distortions which can detract from the ideal viewing experience. This paper presents a lens calibration and correction system which enables these problems to be resolved digitally in post production.  For some time lenses such as the Cook 4i and now 5i provide the necessary metadata on focus and aperture settings to enable digital post corrections to be applied. Cameras such the Alexa and RED are capable of capturing this metadata. Many other lenses however may be adapted to provide the necessary metadata by means of a simple encoder. The paper presents how this is achieved followed by a presentation of the digital correction system for enhancing such scenarios as extreme focus pulls. Using digital high definition camera systems the calibration of each individual lens is presented along with the automatic derivation of the required lens database. The full post production work flow through to final image generation is presented.",2010,0,
631,632,The study of fault diagnosis in rotating machinery,"This project presents a detail review of the subject fault diagnosis; feature extraction, dimensionality reduction and fault classification are being discussed. This project focuses on the faulty bearing which mainly caused by mass imbalance and axis misalignment. By analyzing the vibration signal obtained from the test rigs (rigs that are built to demonstrate the effect of faults in rotating machinery), it gives solid information concerning any faults within the rotating machinery.",2009,0,
632,633,Multiobjective Optimization for HTS Fault-Current Limiters Based on Normalized Simulated Annealing,"This paper presents an improved simulated annealing (SA) algorithm for multiobjective optimization, which is a positive approach in the design of high-temperature superconducting (HTS) fault-current limiters (SFCLs).The main goal of this paper is to achieve an effective and feasible approach in the structural design of HTS FCLs by means of multiobjective decision-making techniques, based on normalized SA. The combination of electrical and thermal models of a purpose-designed resistive-type HTS FCL is defined as a component in PSCAD/EMTDC simulations from which the proposed method will be used to optimize the selective parameters of the SFCL. The above requires the need of advanced numerical techniques for simulation studies by PSCAD on a sample distribution system for determining a global optimum HTS FCL, by considering individual parameters and accounting for the constraints, which is the main motivation for initiating this paper.",2009,0,
633,634,A Multi-agent System for Complex Vehicle Fault Diagnostics and Health Monitoring,"This paper presents a multi-agent system(MAS_VFD&HM) developed for complex vehicle fault diagnosis and health monitoring. The MAS_VFD&HM consists of signal diagnostic agents, special case agents, and a vehicle diagnostic/monitoring agent. A signal agent is responsible for the fault diagnosis or monitoring of one particular signal using either a single signal or multiple signals depending on the complexity of signal faults. Special case agents are those trained to detect specific component faults. All these agents are autonomous and report their results to the Vehicle System Agent. A computational framework is presented for agent learning and agent operation. The proposed MAS_VFD&HM is scalable, versatile, and has the capability of dealing complex problems such as multiple faults in a vehicle system. Although our focus was on the automotive diagnostics, the proposed MAS_VFD&HM is applicable to complex engineering diagnostic problems beyond vehicles.",2010,0,
634,635,Channel capacity and average error rates in generalised-K fading channels,"In the present study, the performance of digital communication systems operating over a composite fading channel modelled by the generalised-<i>K</i> distribution is analysed and evaluated. Novel closed-form expressions for the outage performance, the average bit error probabilities of several modulation schemes and the channel capacity under four different adaptive transmission schemes are derived. The analytical expressions are used to investigate the impact of different fading parameters of this composite fading channel model on the average bit error rate performance for a variety of digital modulation schemes and the spectral efficiency of different adaptive transmission policies.",2010,0,
635,636,Stochastic change detection based on an active fault diagnosis approach,The focus in this paper is on stochastic change detection applied in connection with active fault diagnosis (AFD). An auxiliary input signal is applied in AFD. This signal injection in the system will in general allow to obtain a fast change detection/isolation by considering the output or an error output from the system. The classical CUSUM (cumulative sum) method will be modified such that it will be able to detect change in the signature from the auxiliary input signal in the (error) output signal. It will be shown how it is possible to apply both the gain as well as the phase change of the output vector in the CUSUM test.,2007,0,
636,637,Identification of faulted section in TCSC transmission line based on DC component measurement,"This paper presents an analysis of possibility of detection of a fault position with respect to the compensating bank in a series compensating transmission line. The algorithm designed for this purpose is based on determining the contents of dc components in the distance relay input currents. Fuzzy logic technique is applied for making the decision whether a fault is in front of the compensating bank or behind it. The delivered algorithm has been tested and evaluated with use of the fault data obtained from versatile ATP-EMTP simulations of faults in the test power network containing the 400 kV, 300 km transmission line, compensated with the aid of TCSC (Thyristor Controlled Series Capacitor) bank installed at mid-line. The results of the evaluation are reported and discussed.",2009,0,
637,638,Research on Web-Based Multi-Agent System for Aeroengine Fault Diagnosis,"On the analysis of current state of aeroengine remote diagnosis, collaborative mechanism based on multi-agent was introduced to overcome the obstacles of conventional remote fault diagnosis. The model of aeroengine remote collaborative diagnosis based on multi-agent was put forward on analysis of the positional relationship of all agents in the collaborative environment and the relationship between collaborative agents and roles in the course of collaboration. Some key technologies such as coordination mechanism, task assignment mechanism, agent interaction mechanism, case-based reasoning (CBR) in treatment agent, and the analytic hierarchy process (AHP) in decision analysis were discussed and specific methods of realization were given concretely. Based on these, a Web-based prototype system for aeroengine fault diagnosis was developed on the JADE (Java Agent DEvelopment Framework) platform. The process of system implementation and a case example of fault diagnosis were presented to illustrate and prove the proposed system's applicability. Running results show the feasibility and reliability of the framework, which will be helpful to integrate the aeroengine diagnosis knowledge, improve the diagnosis efficiency effectively and decrease the aeroengine diagnosis cost remarkably.",2008,0,
638,639,A perceptual Sensitivity Based Redundant Slices Coding Scheme for Error-Resilient Transmission H.264/AVC Video,"In this paper, redundant slices feature of the H.264/AVC codec is evaluated. In order to trade off compression efficiency and error robustness of a H.264/AVC codec with redundant slice capability, we propose a novel perceptual sensitivity based redundant slices coding scheme. The perceptually sensitive regions are determined by using a simple yet effective perceptual sensitivity analysis technique, which analyzes both the motion and the texture structures in the original video sequence. The experimental results show that our proposed algorithm can remarkably improve the reconstructed video quality in the packet lossy network",2006,0,
639,640,Application of methods of 3D surface reconstruction for characterization of pitting defects,In this paper a possibility of application of two different methods of pitting visualization is discussed.,2009,0,
640,641,A State Machine for Detecting C/C++ Memory Faults,"Memory faults are major forms of software bugs that severely threaten system availability and security in C/C++ program. Many tools and techniques are available to check memory faults, but few provide systematic full-scale research and quantitative analysis. Furthermore, most of them produce high noise ratio of warning messages that require many human hours to review and eliminate false-positive alarms. And thus, they cannot locate the root causes of memory faults precisely. This paper provides an innovative state machine to check memory faults, which has three main contributions. Firstly, five concise formulas describing memory faults are given to make the mechanism of the state machine simple and flexible. Secondly, the state machine has the ability to locate the cause roots of the memory faults. Finally, a case study applying to an embedded software, which is written in 50 thousand lines of C codes, shows it can provide useful data to evaluate the reliability and quality of software",2005,0,
641,642,A different view of fault prediction,"We investigated a different mode of using the prediction model to identify the files associated with a fixed percentage of the faults. The tester could ask the tool to identify which files are likely to contain the bulks of faults, with the tester selecting any desired percentage of faults. Again the tool would return a list ordered in decreasing order of the predicted numbers of faults in the files the model expects to be most problematic. If the number of files identified is too large, the tester could reselect a smaller percentage of faults. This would make the number of files requiring particular scrutiny manageable. We expect both modes to be valuable to professional software testers and developers.",2005,0,
642,643,Insulation fault detection in a PWM controlled induction motor-experimental design and preliminary results,"To investigate feature extraction methods for early detection of insulation degradation in low voltage (under 600 V), 3-phase, PWM controlled induction motors, a series of seeded fault tests was planned on a 50 HP, 440 V motor. In this paper, the background and rationale for the test plan are described. The instrumentation and test plan are then detailed. Finally, preliminary test experiences are related",2000,0,
643,644,Clinic: A Service Oriented Approach for Fault Tolerance in Wireless Sensor Networks,"With the size and complexity of modern Wireless Sensor Networks (WSNs) systems, a system's ability to recover from faults is becoming more important. A self-healing system is one that has the capability to recover from faults without human intervention during execution. Since WSNs are inherently fault-prone and since their on-site maintenance is infeasible, scalable self-healing is crucial for enabling the deployment of large-scale sensor network applications. Previous work has typically dealt with single faults in isolation, has imposed constraints on systems, or required new protocol elements. In this paper, we attempt to solve some of these problems through the use of service-oriented architecture. We propose a service-oriented self-healing approach, called Clinic, that works with existing network components, e.g. routing protocols, and resources without adding extra overhead on the network. In Clinic, different network capabilities are viewed as services of the network instead of being isolated capabilities of individual nodes. This view of the network promotes collaboration among nodes and information reuse by sharing information collected by one service with other network services. Preliminary evaluation showed that Clinic achieved fault tolerance while keeping low communication overhead by reusing only the information collected by other network services to heal from faults.",2010,0,
644,645,Developing Fault Injection Environment for Complex Experiments,"The paper addresses the problem of creating a comprehensive fault injection environment, which integrates and improves various simulation and supplementary functions. This is illustrated with experimental results.",2008,0,
645,646,Neural network methods for error canceling in human-machine manipulation,"A neural network technique is employed to cancel hand motion error during microsurgery. A cascade-correlation neural network trained via extended Kalman filtering was tested on 15 recordings of hand movement collected from 4 surgeons. The neural network was trained to output the surgeon's desired motion, suppressing erroneous components. In experiments this technique reduced the root mean square error (rmse) of the erroneous motion by an average of 39.5%. This was 9.6% greater than the reduction achieved in earlier work, which followed the complementary approach of estimating the error rather than the desired component. Preliminary results are also presented from tests in which training and testing data were taken from different surgeons.",2001,0,
646,647,Exploration of beam fault scenarios for the Spallation Neutron Source target,"The Spallation Neutron Source (SNS) accelerator systems will provide a 1 GeV, 1.44 MW proton beam to a liquid mercury target for neutron production. In order to ensure adequate lifetime of the target system components, requirements on several beam parameters must be maintained. A series of error studies was performed to explore credible fault scenarios which can potentially violate the various beam-on-target parameters. The response of the beam-on-target parameters to errors associated with the phase-space painting process in the ring and field setpoint errors in all the ring-to-target beam transport line elements were explored and will be presented. The plan for ensuring beam-on-target parameters will also be described.",2003,0,
647,648,On undetectable faults in partial scan circuits,We provide a definition of undetectable faults in partial scan circuits under a test application scheme where a test consists of primary input vectors applied at-speed between scan operations. We also provide sufficient conditions for a fault to be undetectable under this test application scheme. We present experimental results on finite-state machine benchmarks to demonstrate the effectiveness of these conditions in identifying undetectable faults.,2002,0,
648,649,Analytical Modeling Approach to Detect Magnet Defects in Permanent-Magnet Brushless Motors,The paper presents a novel approach to detect magnet faults such as local demagnetization in brushless permanent-magnet motors. We have developed a new form of analytical model that solves the Laplacian/quasi-Poissonian field equations in the machine's air-gap and magnet element regions. We verified the model by using finite-element software in which demagnetization faults were simulated and electromotive force was calculated as a function of rotor position. We then introduced the numerical data of electromotive force into a gradient-based algorithm that uses the analytical model to locate demagnetized regions in the magnet as simulated in the finite-element package. The fast and accurate convergence of the algorithm makes the model useful in magnet fault diagnostics.,2008,0,
649,650,Error-rate analysis for multirate DS-CDMA transmission schemes,"We analyze and compare the error performance of a dual-rate direct-sequence code-division multiple-access (DS-CDMA) system using multicode (MCD) and variable-spreading gain (VSG) transmission in the uplink. Specifically, we present two sets of results. First, we consider an ideal additive white Gaussian noise channel. We show that the bit-error rate (BER) of VSG users is slightly lower than that of MCD users if the number of low-rate interferers is smaller than a specific threshold. Otherwise, they exhibit similar error performance. Second, we look at multipath fading channels. We show that with diversity RAKE reception, the VSG user suffers from a larger interference power than the MCD user if the channel delay spread is small. The reverse is true for a large delay spread. However, a larger interference power in this case does not necessarily lead to higher error probability. Essentially, our results for both cases show that: 1) in addition to the signal-to-interference ratio (SIR), the difference in error performance between the two systems strongly depends on the distributions of multiple-access and multipath interference; 2) for practical cellular communications, performances for both systems are expected to be similar most of the time.",2003,0,
650,651,An AS-DSP for forward error correction applications,"An application specific digital signal processor for channel coding is presented. The vector operations can improve both the performance of memory accesses and program code density. The special function units and datapaths for channel decoding accelerate the decoding speed and facilitate algorithm implementation. The processor had been fabricated in a 0.18 m CMOS 1P6M technology. The chip size is 7.73 mm<sup>2</sup> including 18k bits embedded memory, and the power consumption is 141 mW while decoding Reed-Solomon code and convolutional code. In contrast with general purpose processor designs, the results show this chip has at least 50% improvement in code density and 66% data rate enhancement.",2005,0,
651,652,Compensation of inertia error in brake dynamometer testing,"Loss in terms of windage and bearing friction is an important origin of inertia error to be compensated in brake dynamometer testing, acquisition of which has always been a troublesome problem. An indirect method of loss measurement using speed data under null pipeline pressure is described in this paper. Mathematical model of resistance torque or energy loss is calculated by regression of collected speed data using SPSS software. Error compensation of two inertia simulating methods, torque control method and energy compensation method, is discussed. Experiments of the former are conducted on NT11 brake dynamometer, which proves it to be effective in eliminating inertia error.",2009,0,
652,653,Rate-Distortion Optimal Video Transport Over IP with Bit Errors,"In this paper we propose a method for video delivery over bit error channels. In particular, we propose a rate distortion optimal method for slicing and unequal error protection (UEP) of packets over bit error channels. The proposed method performs full frame based search using a novel dynamic programming approach to determine the optimal slicing configuration in a practically short time. Also we propose a rate and distortion estimation technique that decreases the time to evaluate the objective function for a slice configuration. The proposed method can perform rate-distortion UEP that can be used over forward error correction (FEC) capable channels. We show that the proposed method successfully exploit the local dynamics of a video frame and perform more than 1 dB better than common methods.",2006,0,
653,654,Fault-Tolerant Overlay Protocol Network,"Voice over Internet Protocol (VoIP) and other time critical communications require a level of availability much higher than the typical transport network supporting traditional data communications. These critical command and control channels must continue to operate and remain available in the presence of an attack or other network disruption. Even disruptions of short duration can severely damage, degrade, or drop a VoIP connection. Routing protocols in use today can dynamically adjust for a changing network topology. However, they generally cannot converge quickly enough to continue an existing voice connection. As packet switching technologies continue to erode traditional circuit switching applications, some methodology or protocol must be developed that can support these traditional requirements over a packet-based infrastructure. We propose the use of a modified overlay tunneling network and associated routing protocols called the fault tolerant overlay protocol (FTOP) network. This network is entirely logical; the supporting routing protocol may be greatly simplified due to the overlays's ability to appear fully connected. Therefore, ensuring confidentiality and availability are much simpler using traditional cryptographic isolation and VPN technologies. Empirical results show for substrate networks, convergence time may be as high as six to ten minutes. However, the FTOP overlay network has been shown to converge in a fraction of a second, yielding an observed two order of magnitude convergence time improvement. This unique ability enhances availability of critical network services allowing operation in the face of substrate network disruption caused by malicious attack or other failure",2006,0,
654,655,Adaptive error protection for Scalable Video Coding extension of H.264/AVC,"This paper presents an adaptive error protection method which provides different packet correction capacities by using only one Reed-Solomon code. The proposed method can be applied separately for each data part in a bit stream. The adaption of the error correction capacity works on-the-fly and only based on the way of data interleaving. In this work, the error protection is applied unequally to data units in the Network Abstraction Layer (NAL) of the Scalable Video Coding (SVC) extension of H.264/AVC. Simulation results show that the video quality increases 6 dB in average with the total overhead of ca. 9%. The advantage of our method is the simpleness and flexibility to apply. Therefore, it is suitable for real-time streaming applications.",2008,0,
655,656,Power factor correction and efficiency investigation of AC-DC converters using forced commutation techniques,"In this paper the power factor and the efficiency of a suggested AC-DC converter topology is studied via Mathlab/Simulink simulation. This converter topology consists of four MOSFET elements in bridge form and: a) two antiparallel IGBT elements between the bridge and the AC grid, b) one MOSFET element between the bridge and the DC load. These switching elements control the conduction time intervals of the bridge by a hysteresis current controller in order to achieve an AC current waveform in phase with the AC voltage as well as a very low content of higher harmonics. This way the values of the power factor and the efficiency become very high (e.g. 0,98... 0,99).",2005,0,
656,657,The Application of Safety Simulation Technology in the Fault Diagnosis of the Chemical Process,"With the development of information and computational technology, the safety simulation technique is becoming more and more useful in the chemical process hazard assessment, hazard identification, and safety control system design and operating personnel training etc.The fault diagnosis of the gravity water tank is studied by using dynamic simulation of HYSYS (Hyprotech System for Engineers). The simulation results presents the method need not design problem-specific observer to estimate unmeasured state variables, and can identification and diagnosis faults simultaneously as well. The parameters of the chemical process are updated via on-line correction.",2008,0,
657,658,On the relation between design contracts and errors: a software development strategy,"When designing a software module or system, a systems engineer must consider and differentiate between how the system responds to external and internal errors. External errors cannot be eliminated and must be tolerated by the system, while the number of internal errors should be minimized and the resulting faults should be detected and removed. This paper presents a development strategy based on design contracts and a case study of an industrial project in which the strategy was successfully applied. The goal of the strategy is to minimize the number of internal errors during the development of a software system while accommodating external errors. A distinction is made between weak and strong contracts. These two types of contracts are applicable to external and internal errors, respectively. According to the strategy, strong contracts should be applied initially to promote the correctness of the system. Before releasing, the contracts governing external interfaces should be weakened and error management of external errors enabled. This transformation of a strong contract to a weak one is harmless to client modules",2002,0,
658,659,Defect control methods for SIMOX SOI wafer manufacture and processing,"The layered structure of thin film silicon-on-insulator (SOI) wafers introduces new considerations for defect detection, particularly for optical metrology tools used to characterize and control SOI wafer processing. Multi-layer interference, as well as subsurface features of the material, can complicate the detection of surface defects. Non-particle defect types which scatter light, such as mounds, pits (including so-called HF defects), and slip lines, can be efficiently detected and classified with advanced operating modes of state-of-the art optical metrology tools. Such capabilities facilitate improvements in the wafer manufacturing process, and result in improved defect detection capabilities and material quality. This work describes defect characterization of SIMOX-SOI wafers using the KLA-Tencor Surfscan 6420 and SP1<sup>TBI</sup>",2000,0,
659,660,Usage of Weibull and other models for software faults prediction in AXE,There are several families for software quality prediction techniques in development projects. All of them can be classified in several subfamilies. Each of these techniques has its own distinctive feature and it may not give correct prediction of quality for a scenario different from the one for which the technique was designed. All these techniques for software quality prediction are dispersed. One of them is statistical and probabilistic technique. The paper deals with software quality prediction techniques in development projects. Four different models based on statistical and probabilistic approach is presented and evaluated for prediction of software faults in very large development projects.,2008,0,
660,661,Fault Emulation for Dependability Evaluation of VLSI Systems,"Advances in semiconductor technologies are greatly increasing the likelihood of fault occurrence in deep-submicrometer manufactured VLSI systems. The dependability assessment of VLSI critical systems is a hot topic that requires further research. Field-programmable gate arrays (FPGAs) have been recently pro posed as a means for speeding-up the fault injection process in VLSI systems models (fault emulation) and for reducing the cost of fixing any error due to their applicability in the first steps of the development cycle. However, only a reduced set of fault models, mainly stuck-at and bit-flip, have been considered in fault emulation approaches. This paper describes the procedures to inject a wide set of faults representative of deep-submicrometer technology, like stuck-at, bit-flip, pulse, indetermination, stuck-open, delay, short, open-line, and bridging, using the best suitable FPGA- based technique. This paper also sets some basic guidelines for comparing VLSI systems in terms of their availability and safety, which is mandatory in mission and safety critical application contexts. This represents a step forward in the dependability benchmarking of VLSI systems and towards the definition of a framework for their evaluation and comparison in terms of performance, power consumption, and dependability.",2008,0,
661,662,Delay Constraint Error Control Protocol for Real-Time Video Communication,"Real-time video communication over wireless channels is subject to information loss since wireless links are error-prone and susceptible to noise. Popular wireless link-layer protocols, such as retransmission (ARQ) based 802.11 and hybrid ARQ methods provide some level of reliability while largely ignoring the latency issue which is critical for real-time applications. Therefore, they suffer from low throughput (under high-error rates) and large waiting-times leading to serious degradation of video playback quality. In this paper, we develop an analytical framework for video communication which captures the behavior of real-time video traffic at the wireless link-layer while taking into consideration both reliability and latency conditions. Using this framework, we introduce a delay constraint packet embedded error control (DC-PEEC) protocol for wireless link-layer. DC-PEEC ensures reliable and rapid delivery of video packets by employing various channel codes to minimize fluctuations in throughput and provide timely arrival of video. In addition to theoretically analyzing DC-PEEC, the performance of the proposed scheme is analyzed by simulating real-time video communication over ldquorealrdquo channel traces collected on 802.11 b WLANs using H.264/AVC JM14.0 video codec. The experimental results demonstrate performance gains of 5-10 dB for different real-time video scenarios.",2009,0,
662,663,Evolutionary design of lifting scheme wavelet-packet adaptive filters for elevator fault detection,"An evolutionary-based procedure for designing adaptive filters based on second-generation wavelet (lifting scheme) packet decomposition for industrial fault detection is presented. The proposed procedure is validated by an experimental case study for induction motor fault diagnosis in an elevator system. Preliminary results on two typologies of faults, broken rotor bars and static air gap eccentricity, are discussed by showing encouraging performance.",2010,0,
663,664,Job Migration and Fault Tolerance in SLA-Aware Resource Management Systems,"Contractually fixed service quality levels are mandatory prerequisites for attracting the commercial user to Grid environments. Service level agreements (SLAs) are powerful instruments for describing obligations and expectations in such a business relationship. At the level of local resource management systems, checkpointing and restart is an important instrument for realizing fault tolerance and SLA- awareness. This paper highlights the concepts of migrating such checkpoint datasets to achieve the goal of SLA- compliant job execution.",2008,0,
664,665,A particle swarm optimization approach for automatic diagnosis of PMSM stator fault,"Permanent magnet synchronous motors (PMSM) are frequently used to high performance applications. Accurate diagnosis of small faults can significantly improve system availability and reliability. This paper proposes a new scheme for the automatic diagnosis of interturn short circuit faults in PMSM stator windings. Both the fault location and fault severity are identified using a particle swarm optimization (PSO) algorithm. The performance of the motor under the fault conditions is simulated through lumped-parameter models. Waveforms of the machine phase currents are monitored, based on which a fitness function is formulated and PSO is used to identify the fault location and fault size. The proposed method is simulated in MATLAB environment. Simulation results provide preliminary verification of the diagnosis scheme",2006,0,
665,666,Efficient techniques for reducing error latency in on-line periodic BIST,"With transient and intermittent operational faults becoming a dominant failure mode in modern digital systems, the deployment of on-line test technology is becoming a major design objective. On-line periodic BIST is a testing method for the detection of operational faults in digital systems. The method applies a near-minimal deterministic test sequence periodically to the circuit under test and checks the circuit responses to detect the existence of operational faults. On-line periodic BIST is characterized by full error coverage, bounded error latency, moderate space and time redundancy. In this paper, we present various techniques to minimize the error latency without sacrificing the full error coverage. These techniques are primarily based on the reordering the test vectors or the selective repetition of test vectors. Our analytical and preliminary experimental results demonstrate that our techniques lead to a significant reduction in the error latency.",2009,0,
666,667,A multi-path routing protocol with fault tolerance in mobile ad hoc networks,"In recent years many researches have focused on ad-hoc networks, mainly because of their independence to any specific structure. These networks suffers from frequent and rapid topology changes that cause many challenges in their routing. Most of the routing protocols try to find a path between source and destination nodes because any path will expire, offer a short period, the path reconstruction may cause the network inefficiency. The proposed protocol build two paths between source and destination and create backup paths during the route reply process, route maintenance process and local recovery process in order to improve the data transfer and the fault tolerance. The protocol performance is demonstrated by using the simulation results obtain from the global mobile simulation software(Glomosim). The experimental results show that this protocol can decrease the packet loss ratio rather than DSR and SMR and it is useful for the applications that need a high level of reliability.",2009,0,
667,668,Cleansing Test Suites from Coincidental Correctness to Enhance Fault-Localization,"Researchers have argued that for failure to be observed the following three conditions must be met: 1) the defect is executed, 2) the program has transitioned into an infectious state, and 3) the infection has propagated to the output. Coincidental correctness arises when the program produces the correct output, while conditions 1) and 2) are met but not 3). In previous work, we showed that coincidental correctness is prevalent and demonstrated that it is a safety reducing factor for coverage-based fault localization. This work aims at cleansing test suites from coincidental correctness to enhance fault localization. Specifically, given a test suite in which each test has been classified as failing or passing, we present three variations of a technique that identify the subset of passing tests that are likely to be coincidentally correct. We evaluated the effectiveness of our techniques by empirically quantifying the following: 1) how accurately did they identify the coincidentally correct tests, 2) how much did they improve the effectiveness of coverage-based fault localization, and 3) how much did coverage decrease as a result of applying them. Using our better performing technique and configuration, the safety and precision of fault-localization was improved for 88% and 61% of the programs, respectively.",2010,0,
668,669,Videoendoscopic distortion correction and its application to virtual guidance of endoscopy,"Modern video based endoscopes offer physicians a wide-angle field of view (FOV) for minimally invasive procedures, Unfortunately, inherent barrel distortion prevents accurate perception of range. This makes measurement and distance judgment difficult and causes difficulties in emerging applications, such as virtual guidance of endoscopic procedures. Such distortion also arises in other wide FOV camera circumstances. This paper presents a distortion correction technique that can automatically calculate correction parameters, without precise knowledge of horizontal and vertical orientation. The method is applicable to any camera-distortion correction situation. Based on a least-squares estimation, the authors' proposed algorithm considers line fits in both FOV directions and gives a globally consistent set of expansion coefficients and an optimal image center. The method is insensitive to the initial orientation of the endoscope and provides more exhaustive FOV correction than previously proposed algorithms. The distortion-correction procedure is demonstrated for endoscopic video images of a calibration test pattern, a rubber bronchial training device, and real human circumstances. The distortion correction is also shown as a necessary component of an image-guided virtual-endoscopy system that matches endoscope images to corresponding rendered three-dimensional computed tomography views.",2001,0,
669,670,Test Generation and Diagnostic Test Generation for Open Faults with Considering Adjacent Lines,"In order to ensure high quality of DSM circuits, testing for the open defect in the circuits is necessary. However, the modeling and techniques for test generation for open faults have not been established yet. In this paper, we propose a method for generating tests and diagnostic tests based on a new open fault model. Firstly, we show a new open fault model with considering adjacent lines [9]. Under the open fault model, we reveal more about the conditions to excite the open fault. Next we propose a method for generating tests for open faults by using a stuck-at fault test with don't cares. We also propose a method for generating a diagnostic test that can distinguish the pair of open faults. Finally, experimental results show that (1) the proposed method is able to achieve 100% fault coverages for almost all benchmark circuits and (2) the proposed method is able to reduce the number of indistinguished open fault pairs.",2007,0,
670,671,Minimum Zone Evaluation of Sphericity Error Based on Ant Colony Algorithm,"In this paper, based on the analysis of existent evaluation methods for sphericity errors, an intelligent evaluation method is provided. The evolutional optimum model and the calculation process are introduced in detail. According to characteristics of sphericity error evaluation, ant colony optimization (ACO) algorithm is proposed to evaluate the minimum zone error. Compared with conventional optimum evaluation methods such as simplex search and Powell method, it can find the global optimal solution, and the precision of calculating result is very high. Then, the objective function calculation approaches for using the ACO algorithm to evaluate minimum zone error are formulated. Finally, the control experiment results evaluated by different method such as the least square, simplex search, Powell optimum methods and GA, indicate that the proposed method can provide better accuracy on sphericity error evaluation, and it has fast convergent speed as well as using computer expediently and popularizing application easily.",2007,0,
671,672,Development and evaluation of a model of programming errors,"Models of programming and debugging suggest many causes of errors, and many classifications of error types exist. Yet, there has been no attempt to link causes of errors to these classifications, nor is there a common vocabulary for reasoning about such causal links. This makes it difficult to compare the abilities of programming styles, languages, and environments to prevent errors. To address this issue, this paper presents a model of programming errors based on past studies of errors. The model was evaluated with two observational of Alice, an event-based programming system, revealing that most errors were due to attentional and strategic problems in implementing algorithms, language constructs, and uses of libraries. In general, the model can support theoretical, design, and educational programming research.",2003,0,
672,673,A Comparative Study of Voice Over Wireless Networks Using NS-2 Simulation with an Integrated Error Model,"Wireless communication is the fastest growing field and with the emergence of IEEE 802.11 based devices, wireless access is becoming more popular. Many multimedia applications for IP networks have been developed and thus the demand for quality of service (QoS) has increased. In this paper our primary objective is to evaluate 802.11e EDCF framework for video, voice and background traffic all at the same time. Our assessment is based on an error model called E-model, MOS for VoIP and PSNR for video. We also studied the effects of random uniform error model on various types of traffic. As expected, wireless networks are more prone to errors than wired networks",2006,0,
673,674,Adaptive Causal Models for Fault Diagnosis and Recovery in Multi-Robot Teams,"This paper presents an adaptive causal model method (adaptive CMM) for fault diagnosis and recovery in complex multi-robot teams. We claim that a causal model approach is effective for anticipating and recovering from many types of robot team errors, presenting extensive experimental results to support this claim. To our knowledge, these results show the first, full implementation of a CMM on a large multi-robot team. However, because of the significant number of possible failure modes in a complex multi-robot application, and the difficulty in anticipating all possible failures in advance, our empirical results show that one cannot guarantee the generation of a complete a priori causal model that identifies and specifies all faults that may occur in the system. Instead, an adaptive method is needed to enable the robot team to use its experience to update and extend its causal model to enable the team, over time, to better recover from faults when they occur. We present our case-based learning approach, called LeaF (for learning-based fault diagnosis), that enables robot team members to adapt their causal models, thereby improving their ability to diagnose and recover from these faults over time",2006,0,
674,675,Effects of clipping on the error performance of OFDM in frequency selective fading channels,"Previous studies on the effect of the clipping noise on the error performance of orthogonal frequency-division multiplexing (OFDM) systems in frequency selective fading channels provide pessimistic results. They do not consider the effect of channel fading on the clipping noise. The clipping noise is added at the transmitter and hence fades with the signal. Here, the authors show that the ""bad"" subcarriers that dominate the error performance of the OFDM system are least affected by the clipping noise and, as a result, the degradation in the error performance of OFDM system in fading channels is very small.",2004,0,
675,676,Master Defect Record Retrieval Using Network-Based Feature Association,"As electronic records (e.g., medical records and technical defect records) accumulate, the retrieval of a record from a past instance with the same or similar circumstances, has become extremely valuable. This is because a past record may contain the correct diagnosis or correct solution to the current circumstance. We refer to the two records of the same or similar circumstances as <i>master</i> and <i>duplicate</i> records. Current record retrieval techniques are lacking when applied to this special master defect record retrieval problem. In this study, we propose a new paradigm for master defect record retrieval using network-based feature association (NBFA). We train the master record retrieval process by constructing feature associations to limit the search space. The retrieval paradigm was employed and tested on a real-world large-scale defect record database from a telecommunications company. The empirical results suggest that the NBFA was able to significantly improve the performance of master record retrieval, and should be implemented in practice. This paper presents an overview of technical aspects of the master defect record retrieval problem, describes general methodologies for retrieval of master defect records, proposes a new feature association paradigm, provides performance assessments on real data from a telecommunications company, and highlights difficulties and challenges in this line of research that should be addressed in the future.",2010,0,
676,677,Fault-tolerant scheduling in distributed real-time systems,"In distributed systems, a real-time task has several subtasks which need to be executed at different nodes. Some of these subtasks can be executed in parallel on different nodes without violating their precedence relationships, if any, among them. To better exploit the parallelism, it becomes necessary to assign separate deadlines to subtasks and schedule them independently. We use three subtask deadline assignment policies which we have introduced earlier to develop a bidding-based fault-tolerant scheduling algorithm for distributed real-time systems. A local scheduler which resides on each node, tries to determine a schedule for each subtask according to the primary-backup approach. In this paper we discuss the algorithm and present the results of simulation studies conducted to establish the efficacy of our algorithm",2001,0,
677,678,The Error Reduced ADI-CPML Method for EMC Simulation,"In this paper, convolutional perfectly matched layer (CPML) is developed for the recently proposed error reduced (ER) ADI-FDTD method to solve electromagnetic compatibility problems efficiently. Its numerical results are examined and compared with the conventional ADI-CPML method. It is found that for a CFL number equal to 5, the reflection error of the ER- ADI-CPML is approximately 12 dB better than the conventional ADI-CPML method.",2007,0,
678,679,Identification of Errors in Power Flow Controller Parameters,"Transmission open access allows power transactions to take place between remote parts of an interconnected system. As a result, some parts of the transmission system may experience unusual power flows during certain power transactions. One way to circumvent possible congestion is to use power flow control devices. These devices which are also referred as flexible AC transmission system (FACTS) devices, allow rerouting of power flows in the system. The amount of power flowing through such a device can be controlled via device parameters. Hence, proper monitoring of these parameters is important for reliable operation and system security. In this paper, an identification method for detecting and identifying errors associated with power controller parameters will be presented. The method is based on the available measurements such as the power flows and injections which are used by the state estimators at the control center. Hence, the method can be implemented easily as part of the existing energy management functions",2006,0,
679,680,The use of historical defect imagery for yield learning,"The rapid identification of yield detracting mechanisms through integrated yield management is the primary goal of defect sourcing and yield learning. At future technology nodes, yield learning must proceed at an accelerated rate to maintain current defect sourcing cycle times despite the growth in circuit complexity and the amount of data acquired on a given wafer lot. As integrated circuit fabrication processes increase in complexity, it has been determined that data collection, retention, and retrieval rates will continue to increase at an alarming rate. Oak Ridge National Laboratory (ORNL) has been working with International SEMATECH to develop methods for managing the large volumes of image data that are being generated to monitor the status of the manufacturing process. This data contains an historical record that can be used to assist the yield engineer in the rapid resolution of manufacturing problems. To date there are no efficient methods of sorting and analyzing the vast repositories of imagery collected by off-line review tools for failure analysis, particle monitoring, line width control and overlay metrology. In this paper we will describe a new method for organizing, searching, and retrieving imagery using a query image to extract images from a large image database based on visual similarity",2000,0,
680,681,An iron core probe based inter-laminar core fault detection technique for generator stator cores,"A new technique for detecting incipient interlaminar insulation failure of laminated stator cores of large generators is proposed in this paper. The proposed scheme is a low flux induction method that employs a novel probe for core testing. The new probe configuration, which uses magnetic material and is scanned in the wedge depression area, significantly improves the sensitivity of fault detection as well as user convenience compared to existing methods. Experimental results from various test generators tested in factory, field and lab environments under a number of fault conditions are presented to verify the sensitivity and reliability of the proposed scheme.",2003,0,
681,682,Experiments on Fault-Tolerant Self-Reconfiguration and Emergent Self-Repair,"This paper presents a series of experiments on fault tolerant self-reconfiguration of the ATRON robotic system. For self-reconfiguration we use a previously described distributed control strategy based on meta-modules that emerge, move and stop. We perform experiments on three different types of failures: 1) Action failure: On the physical platform we demonstrate how roll-back of actions are used to achieve tolerance to collision with obstacles and other meta-modules. 2) Module failure: In simulation we show, for a 500 module robot, how different degrees of catastrophic module failure affect the robot's ability to shape-change to support an insecure roof. 3) Robot failure: In simulation we demonstrate how robot faults such as a broken robot bone can be emergent self-repaired by exploiting the redundancy of self-reconfigurable modules. We conclude that the use of emergent, distributed control, action roll-back, module redundancy, and self-reconfiguration can be used to achieve fault tolerant, self-repairing robots",2007,0,
682,683,Runtime Diversity against Quasirandom Faults,"Complex software based systems that have to be highly reliable, are increasingly confronted with fault types whose corresponding failures appear to be random, although they have a systematic cause. This paper introduces and defines these ""quasirandom"" faults. They have certain inconvenient common properties such as their difficulty to be reproduced, their strong state dependence and their likelihood to be found in operational systems after testing. However, these faults are also likely to be detected or tolerated with the help of diversity in software, and even low level diversity which can be achieved during runtime is a promising means against them. The result suggests, that runtime diversity can improve software reliability in complex systems.",2009,0,
683,684,"A secure modular exponential algorithm resists to power, timing, C safe error and M safe error attacks","This paper proposes a method for protecting public key schemes from timing and fault attacks. In general, this is accomplished by implementing critical operations using ""branch-less"" path routines. More particularly, the proposed method provides a modular exponentiation algorithm without any redundant computation does not have a store operation with non-certain destination so that it can protect the secret key from many known attacks.",2005,0,
684,685,Detailed radiation fault modeling of the Remote Exploration and Experimentation (REE) first generation testbed architecture,"The goal of the NASA HPCC Remote Exploration and Experimentation (REE) Project is to transfer commercial supercomputing technology into space. The project will use state of the art, low-power, non-radiation-hardened, COTS hardware chips and COTS software to the maximum extent possible, and will rely on software-implemented fault tolerance to provide the required levels of availability and reliability. We outline the methodology used to develop a detailed radiation fault model for the REE Testbed architecture. The model addresses the effects of energetic protons and heavy ions which cause single event upset and single event multiple upset events in digital logic devices and which are expected to be the primary fault generation mechanism. Unlike previous modeling efforts, this model will address fault rates and types in computer subsystems at a sufficiently fine level of granularity (i.e., the register level) that specific software and operational errors can be derived. We present the current state of the model, model verification activities and results to date, and plans for the future. Finally, we explain the methodology by which this model will be used to derive application-level error effects sets. These error effects sets will be used in conjunction with our Testbed fault injection capabilities and our applications' mission scenarios to replicate the predicted fault environment on our suite of onboard applications",2000,0,
685,686,Topology discovery for network fault management using mobile agents in ad-hoc networks,"Managing today's complex and increasingly heterogeneous networks requires in-depth knowledge and extensive training as well as collection of very large amount of data. Fault management is one of the functional areas of network management that entails detection, identification and correction of anomalies that disrupt services of a network. The task of fault management is even harder in ad-hoc networks where the topology of the network changes frequently. It is very inefficient if not impossible to discover the ad-hoc network topology using traditional practices of network discovery. We propose a mobile multi agent system for topology discovery that will allow fault management functions in ad-hoc network. Comparison to current mobile agent based topology discovery systems is also presented",2005,0,
686,687,Timing-based delay test for screening small delay defects,"The delay fault test pattern set generated by timing unaware commercial ATPG tools mostly affects very short paths, thereby increasing the escape chance of smaller delay defects. These small delay defects might be activated on longer paths during functional operation and cause a timing failure. This paper presents an improved pattern generation technique for transition fault model, which provides a higher coverage of small delay defect that lie along the long paths, using a commercial no-timing ATPG tool. The proposed technique pre-processes the scan flip-flops based on their least slack path and the detectable delay defect size. A new delay defect size metric based on the affected path length and required increase in test frequency is developed. We then perform pattern generation and apply a novel pattern selection technique to screen test patterns affecting longer paths. Using this technique will provide the opportunity of using existing timing unaware ATPG tools as slack based ATPG. The resulting pattern set improves the defect screening capability of small delay defects",2006,0,
687,688,Exploiting Memory Soft Redundancy for Joint Improvement of Error Tolerance and Access Efficiency,"Technology roadmap projects nanoscale multibillion- transistor integration in the coming years. However, on-chip memory becomes increasingly exposed to the dual challenges of device-level reliability degradation and architecture-level performance gap. In this paper, we propose to exploit the inherent memory soft (<i>transient</i>) <i>redundancy</i> for on-chip memory design. Due to the mismatch between fixed cache line size and runtime variations in memory spatial locality, many irrelevant data are fetched into the memory thereby wasting memory spaces. The proposed soft-redundancy allocated memory detects and utilizes these memory spaces for jointly achieving efficient memory access and effective error control. A runtime reconfiguration scheme is also proposed to further enhance the soft-redundancy allocation. Simulation results demonstrate 74.8% average error-control coverage ratio on the SPEC CPU2000 benchmarks with average of 59.5% and 41.3% reduction in memory miss rate and bandwidth usage, respectively, as compared to the existing memory techniques. Furthermore, the proposed technique is fully scalable with respect to various memory configurations.",2009,0,
688,689,Towards Identifying the Best Variables for Failure Prediction Using Injection of Realistic Software Faults,"Predicting failures at runtime is one of the most promising techniques to increase the availability of computer systems. However, failure prediction algorithms are still far from providing satisfactory results. In particular, the identification of the variables that show symptoms of incoming failures is a difficult problem. In this paper we propose an approach for identifying the most adequate variables for failure prediction. Realistic software faults are injected to accelerate the occurrence of system failures and thus generate a large amount of failure related data that is used to select, among hundreds of system variables, a small set that exhibits a clear correlation with failures. The proposed approach was experimentally evaluated using two configurations based on Windows XP. Results show that the proposed approach is quite effective and easy to use and that the injection of software faults is a powerful tool for improving the state of the art on failure prediction.",2010,0,
689,690,Fault tolerance of feed-forward artificial neural network architectures targeting nano-scale implementations,"Several circuit architectures have been proposed to overcome logic faults due to the high defect densities that are expected to be encountered in the first generations of nanoelectronic systems. How feed-forward artificial neural networks can possibly be exploited for the purpose of conceiving highly reliable Boolean gates is the topic of this paper. Computer simulations show that feed-forward artificial neural networks can be trained to absorb faults while implementing Boolean functions of various complexity. Using this approach, it can be shown that very high device failure rates (up to 20%) can be accommodated. The cost is to be paid in terms of hardware overhead, which is comparable to the area cost of conventional hardware redundancy measures.",2007,0,
690,691,Fault tolerance evaluation using two software based fault injection methods,"A silicon independent C-Based model of the TTP/C protocol was implemented within the EU-founded project FIT. The C-based model is integrated in the C-Sim simulation environment. The main objective of this work is to verify whether the simulation model of the TTP/C protocol behaves in the presence of faults in the same way as the existing hardware prototype implementation. Thus, the experimental results of the software implemented fault injection applied in the simulation model and in the hardware implementation of the TTP/C network have been compared. Fault injection experiments in both the hardware and the simulation model are performed using the same configuration setup, and the same fault injection input parameters (fault injection location, fault type and the fault injection time). The end result comparison has shown a complete conformance of 96.30%, while the cause of the different results was due to hardware specific implementation of the built-in-self-test error detection mechanisms.",2002,0,
691,692,Error detection and unit conversion,"The article discusses the accuracy mathematical modeling languages (MML) for biomedicine, for example in cardiac electrophysiology. Unit balance checking is showed that it can be automated. The implemented example is JSim (http://www.physiome.org/ jsim/), which is general and can be applied to other systems in which units can be specified and checked. ODE-based simulator Physiome CellML Environment is also discussed.",2009,0,
692,693,Implementation of Web-Based Fault Diagnosis Using Improved Fuzzy Petri Nets,"According to the current application and maintenance situation of numerical control equipment (NCE), a novel remote fault diagnosis expert system is designed to prevent fault occurrence and quicken the recovering process by online real-time monitoring the working state of NCEs. The article addresses the overall framework and relevant application technology of fault diagnosis system (FDS) and emphasizes on the establishment of fuzzy expert system (FES). Improved fuzzy Petri nets (FPNs) model and concurrent reasoning algorithm are applied to handle the fuzziness and concurrency of fault and inadequate and uncertain information. Utilization of simple matrix operation to realize complicated reasoning process that simplifies the diagnostic reasoning decision-making process. Meanwhile, it can be realized easily by computer programming. Finally, a practical fault instance is presented to demonstrate the feasibility and validity of this method.",2009,0,
693,694,An object-based approach to optical proximity correction,"As the feature sizes of integrated circuits have been continually reducing to below exposure wavelength, some correcting techniques, such as OPC and PSM are indispensable to compensate for the distortions on wafer images. In this paper, we describe an object-based approach to OPC named OPCM, which is a model-based OPC tool. Also, a rule-based OPC has been adopted to enhance the practicability of the software",2001,0,
694,695,Generator dynamics influence on currents distribution in fault condition,"Current flow calculation results along the elements of a complex power system are analyzed in this work, during a three-phase short-circuit taking into account relative rotor swing. Analysis is implemented on the examples when the infinite bus fault point is supplied by one or more generators. It is shown that generator swing neglected during short-circuits, essentially changing current distribution in the system, can lead to impermissible mistakes in calculation results.",2000,0,
695,696,"An automated fault analysis system for SP energy networks: Requirements, design and implementation","The proliferation of monitoring equipment on modern electrical power transmission networks is causing an increasing amount of monitoring data to be captured by transmission network operators. Traditional manual data analysis techniques fail to meet the analysis and reporting requirements of the utilities which have chosen to invest in monitoring. The volume of monitoring data, the complexities in analysing multiple related data sources and the preparation of internal reports based on that analysis, render timely manual analysis impractical, if not intractable. In 2006, the authors reported on the first online trials of the protection engineering diagnostics agents (PEDA) system, an automated fault diagnosis system which integrated legacy intelligent systems for the analysis of SCADA and digital fault recorder (DFR) data in order to provide automatic post fault assessment of protection system performance. In this paper the authors revisit the requirements of the TNO where PEDA was trialled. Based on a new formal specification of requirements carried out in 2008, the authors discuss the requirements met by the current version of PEDA and how PEDA could be augmented to meet these new requirements highlighted in this latest analysis of the utilities' requirements.",2009,0,
696,697,Impact of correlation errors on optimum Kalman filter matrices gains identification in multicoordinate systems,"This paper investigates the impact that errors in the innovation correlation calculations has upon the steady-state Kalman filter gain identification. This issue arises in all real time applications, where the correlations must he calculated from experimental data. The algorithm proposed by [L. Hong (1991)] is considered and equations describing the impact are established. Simulation results are presented and discussed. Finally, experimental results for the algorithm in [L. Hong (1991)], applied to estimate the states of a servo system, are presented.",2005,0,
697,698,Optimal Cluster-Cluster Design for Sensor Network with Guaranteed Capacity and Fault Tolerance,"Sensor networks have recently gained a lot of attention from the research community. To ensure scalability sensor networks are often partitioned into clusters, each managed by a cluster head. Since sensors self organize in the form of clusters within a hierarchal wireless sensor network, it is necessary for a sensor node to perform target tracking cooperating with a set of sensors that belong to another cluster. The increased flexibility allows for efficient and optimized use of sensor nodes. While most of the previous research focused on the optimal communication of sensors in one cluster, very little attention has been paid to the efficiency of cooperation among the clusters. This paper proposes a heuristic algorithm of designing optimal structure across clusters to allow the inter-cluster flow of communication and resource sharing under reliability constraints. Such a guarantee simultaneously provides fault tolerance against node failures and high capacity through multi-path routing.",2007,0,
698,699,Fault treatment with net condition/event systems: a first approach,"The paper presents a preliminary report on modeling parts of a modular production system, their dedicated controllers, and the appropriate methods of fault treatment on the level of net condition/event systems (NCES). To achieve practicability, NCES support a systematic and modular way of modeling more complex systems as well as concurrent and non-deterministic behavior which is highly beneficial for modeling and control of DES in failure situations as studies of existing methods show.",2001,0,
699,700,SLICED: Slide-based concurrent error detection technique for symmetric block ciphers,"Fault attacks, wherein faults are deliberately injected into cryptographic devices, can compromise their security. Moreover, in the emerging nanometer regime of VLSI, accidental faults will occur at very high rates. While straightforward hardware redundancy based concurrent error detection (CED) can detect transient and permanent faults, it entails 100% area overhead. On the other hand, time redundancy based CED can only detect transient faults with minimum area overhead but entails 100% time overhead. In this paper we present a general time redundancy based CED technique called SLICED for pipelined implementations of symmetric block cipher. SLICED SLIdes one encryption over another and compares their results for CED as a basis for protection against accidental faults and deliberate fault attacks.",2010,0,
700,701,A Support System for Teaching Computer Programming Based on the Analysis of Compilation Errors,"A system was developed to support teaching computer programming to a group of students who have common questions and make common mistakes on practice computer programs. The system extrapolates the causes and syntaxes of students' compilation errors by analyzing the trends of past compilation errors and presents the extrapolated result to the teacher in real time. By using the system, a teacher can understand in real time students' programming mistakes when they are writing computer programs, and can appropriately teach computer programming to a group of students who have common problems",2006,0,
701,702,The research on a new fault wave recording device in generator-transformer units,"This paper presents a new kind of distributed fault recorder including the design of the system structure, the hardware and software design of the recorder. The recorder adopts NI CompaceRIO series programmable automation controller (PAC) in the hardware while virtual instrument technology in the software. The network communication based on TCP/IP between the client and the server is adopted in the power plant. Moreover, an improved frequency tracking algorithm is presented in the monitoring of the electric quantity to improve the detection precision and the processing speed. The detection and operation results show that it has improved the performance greatly and realized authenticity, integrity and reliability and so on.",2009,0,
702,703,Rotor fault detection using the instantaneous power signature,"The aim of this paper is to present a method to detect broken rotor bar faults by estimating a global modulation index which corresponds to the contribution of all detected modulating frequencies in the stator current. We show that additional information carried by instantaneous power improves the detection of the sidebands and consequently the monitoring too. In fact, the instantaneous power method can be interpreted as a modulation operation in the time domain that translates the spectral components specific to the broken rotor bars to the band [0-50]Hz.",2004,0,
703,704,A system level approach in designing dual-duplex fault tolerant embedded systems,"This paper presents an approach for designing embedded systems able to tolerate hardware faults, defined as an evolution of our previous work proposing an hardware/software co-design framework for realizing reliable embedded systems. The framework is extended to support the designer in achieving embedded systems with fault tolerant properties minimizing overheads and limiting power consumption. A reference system architecture is proposed; the specific hardware/software implementation and reliability methodologies (to achieve the fault tolerance properties) are the result of an enhanced hw/sw partitioning process driven by the designer' constraints and by the reliability constraints, set at the beginning of the design process. By introducing also the reliability constraints during specification, the final system can benefit from the introduced redundancy also for performance gains, while limiting area, time, performance and power consumption overheads.",2002,0,
704,705,A 32-bit COTS-based fault-tolerant embedded system,"This paper presents a 32-bit fault-tolerant (FT) embedded system based on commercial off-the-shelf (COTS) processors. This embedded system uses two 32-bit Pentium processors with master/checker (M/C) configuration and an external watchdog processor (WDP) for implementing a behavioral-based error detection scheme called committed instructions counting (CIC). The experimental evaluation was performed using both power-supply disturbance (PSD) and software-implemented fault injection (SWIFI) methods. A total of 9000 faults have been injected into the embedded system to measure the coverage of error detection mechanisms, i.e., the checker processor and the CIC scheme. The results show that the M/C configuration is not enough for this system and the CIC scheme could cover the limitation of the M/C configuration.",2005,0,
705,706,A software fault tolerance method for safety-critical systems: effectiveness and drawbacks,"An automatic software technique suitable for on-line detection of transient errors due to the effects of the environment (radiation, EMC,...) is presented. The proposed approach, particularly well suited for low-cost safety-critical microprocessor-based applications, has been validated through fault injection experiments and radiation testing campaigns. The experimental results demonstrate the effectiveness of the approach in terms of fault detection capabilities. Undetected faults have been analyzed to point out the limitations of the method.",2002,0,
706,707,IFRA: Post-silicon bug localization in processors,"IFRA overcomes challenges associated with an expensive step in post-silicon validation of processors - pinpointing the bug location and the instruction sequence that exposes the bug from a system failure. On-chip recorders collect instruction footprints (information about flows of instructions, and what the instructions did as they passed through various design blocks) during the normal operation of the processor in a post-silicon system validation setup. Upon system failure, the recorded information is scanned out and analyzed off-line for bug localization. Special self-consistency-based program analysis techniques, together with the test program binary of the application executed during post-silicon validation, are used. Major benefits of using IFRA over traditional techniques for post-silicon bug localization are: 1. It does not require full system-level reproduction of bugs, and, 2. It does not require full system-level simulation. Simulation results on a complex super-scalar processor demonstrate that IFRA is effective in accurately localizing electrical bugs with very little impact on overall chip area.",2009,0,
707,708,Parallel computation of configuration space on reconfigurable mesh with faults,"A reconfigurable mesh (RMESH) can be used to compute robotic paths in the presence of obstacles, where the robot and obstacle images are represented and processed in mesh processors. For a non-point-like robot, we can compute the so-called configuration space to expand the obstacles, so that the robot can be reduced to a reference point to facilitate the robot's motion planning. In this paper, we present algorithms to compute the configuration space in a reconfigurable mesh that contains sparsely distributed faulty processors. Robots of rectangular and circular shapes are treated. It is seen that, in terms of computing the configuration space, a reconfigurable mesh can tolerate faulty processors without much extra cost-the computation takes the optimal O(1) time in both fault-free and faulty reconfigurable meshes",2000,0,
708,709,Nonstationary Motor Fault Detection Using Recent Quadratic TimeFrequency Representations,"As the use of electric motors increases in the aerospace and transportation industries where operating conditions continuously change with time, fault detection in electric motors has been gaining importance. Motor diagnostics in a nonstationary environment is difficult and often needs sophisticated signal processing techniques. In recent times, a plethora of new time-frequency distributions has appeared, which are inherently suited to the analysis of nonstationary signals while offering superior frequency resolution characteristics. The Zhao-Atlas-Marks distribution is one such distribution. This paper proposes the use of these new time-frequency distributions to enhance nonstationary fault diagnostics in electric motors. One common myth has been that the quadratic time-frequency distributions are not suitable for commercial implementation. This paper also addresses this issue in detail. Optimal discrete-time implementations of some of these quadratic time-frequency distributions are explained. These time-frequency representations have been implemented on a digital signal processing platform to demonstrate that the proposed methods can be implemented commercially.",2008,0,
709,710,Towards understanding the effects of intermittent hardware faults on programs,"Intermittent hardware faults are bursts of errors that last from a few CPU cycles to a few seconds. They are caused by process variations, circuit wear-out, and temperature, clock or voltage fluctuations. Recent studies show that intermittent fault rates are increasing due to technology scaling and are likely to be a significant concern in future systems. We study the propagation of intermittent faults to programs; in particular, we are interested in the crash behaviour of programs. We use a model of a program that represents the data dependencies in a fault-free trace of the program and we analyze this model to glean some information about the length of intermittent faults and their effect on the program under specific fault and crash models. The results of our study can aid fault detection, diagnosis and recovery techniques.",2010,0,
710,711,A Method of Detecting Vulnerability Defects Based on Static Analysis,"This paper proposes a method for detecting vulnerability defects caused by tainted data based on state machine. It first uses state machine to define various defect patterns. If the states of state machine is considered as the value propagated in dataflow analysis and the union operation of the state sets as the aggregation operation of dataflow analysis, the defect detection can be treated as a forward dataflow analysis problem. To reduce the false positives caused by intraprocedural analysis, the dynamic information of program was represented approximately by abstract value of variables, and then infeasible path can be identified when some variable's abstract value is empty in the state condition. A function summary method is proposed to get the information needed for performing interprocedural defect detection. The method proposed has been implemented in a defect testing tools.",2010,0,
711,712,Distortion correction of LDMOS power amplifiers using hybrid RF second harmonic injection/digital predistortion linearization,"An LDMOS RF power amplifier for RF multichannel wireless systems with improved IMD performance characteristics is presented. The application of two combined linearization methods is being tested with the help of circuit simulation software ADS. The injection of the fundamental signal's second harmonic in the RF amplifier, as well as a digital predistortion technique, is combined in order to achieve IMD improvement. By proper selection of phase and amplitude of the injected second harmonic signal, it is possible to reduce IMD products that have already been reduced by the well established method of digital predistortion",2006,0,
712,713,Identifying the root causes of memory bugs using corrupted memory location suppression,"We present a general approach for automatically isolating the root causes of memory-related bugs in software. Our approach is based on the observation that most memory bugs involve uses of corrupted memory locations. By iteratively suppressing (nullifying) the effects of these corrupted memory locations during program execution, our approach gradually isolates the root cause of a memory bug. Our approach can work for common memory bugs such as buffer overflows, uninitialized reads, and double frees. However, our approach is particularly effective in finding root causes for memory bugs in which memory corruption propagates during execution until an observable failure such as a program crash occurs.",2008,0,
713,714,Towards a Model of Fault Tolerance Technique Selection in Static and Dynamic Agent-Based Inter-Organizational Workflow Management Systems,"Research in workflow management systems design references the mobile agent computing paradigm where agents have been shown to increase the total capacity of a workflow system through the decoupling of execution management from a statically designated workflow engine, although coordinating fault tolerance mechanisms has been shown to be a downside due to increased overall execution times. To address this issue, we develop a model for comparing the effects of two fault tolerance techniques: local and remote checkpointing. The model enables an examination of fault tolerance coordination impacts on execution time while concomitantly taking into account the dynamic nature of a workflow environment. A proposed use for the model includes providing for selecting and configuring agent-based fault tolerance approaches based on changes in environmental variables - an approach that allows the owners of a workflow management system to reap the scaling efficiency benefits of the mobile agent paradigm without being forced to make trade-offs in execution performance.",2005,0,
714,715,Impact of configuration errors on DNS robustness,"During the past twenty years the Domain Name System (DNS) has sustained phenomenal growth while maintaining satisfactory user-level performance. However, the original design focused mainly on system robustness against physical failures, and neglected the impact of operational errors such as mis-configurations. Our measurement efforts have revealed a number of mis-configurations in DNS today: delegation inconsistency, lame delegation, diminished server redundancy, and cyclic zone dependency. Zones with configuration errors suffer from reduced availability and increased query delays up to an order of magnitude. The original DNS design assumed that redundant DNS servers fail independently, but our measurements show that operational choices create dependencies between servers. We found that, left unchecked, DNS configuration errors are widespread. Specifically, lame delegation affects 15% of the measured DNS zones, delegation inconsistency appears in 21% of the zones, diminished server redundancy is even more prevalent, and cyclic dependency appears in 2% of the zones. We also noted that the degrees of mis-configuration vary from zone to zone, with the most popular zones having the lowest percentage of errors. Our results indicate that DNS, as well as any other truly robust large-scale system, must include systematic checking mechanisms to cope with operational errors.",2009,0,
715,716,Detecting defects on planar circuits by using non-contacting magnetic probe,"Recently, the research of non-contacting measurement with magnetic coupling theorem mostly choose CPW(coplanar waveguide) loop-type circuits as probes. It has advantage of low cost, easy to fabricate and simple designing. While moving the probe, different relative position between planar circuit and magnetic probe cause different strength of coupling. Variation of resonance frequency due to changing magnetic coupling and electric coupling from metal strip outline can be observed. The relation between planar circuit and magnetic probe is analyzed by full-wave EM simulation and some simple measurement. Furthermore, the LC equivalent circuit has also been built for analyzing. At last, the possibility of doing quickly defect-detecting by sweeping the circuits at special frequency will be discussed.",2010,0,
716,717,An Application of Semantic Annotations to Design Errors,"As current engineered systems (e.g. aviation systems) have been equipped with automated and computer-based artefacts, human-system interaction (e.g. human computer interaction) has been an important issue. Design errors that are attributable to human-system interaction failures are not pure engineering design issues, but a multidisciplinary subject with related other areas such as management, psychology, physiology or ergonomics. To identify such design errors (called design-induced errors) in accident reports is important for designing more reliable systems. However, the lack of precise definitions of the concept of design-induced error and the diversity of expression of such failures make it difficult to retrieve relevant documents from accident reports. This paper describes how an ontology and annotation scheme can help to overcome such limitations. Engineering designers can be assisted by the developed ontology and annotation scheme to reason on the issues of design induced error",2006,0,
717,718,The effects of Gaussian weighting errors in hybrid SC/MRC combiners,"The paper examines the impact of Gaussian distributed weighting errors (in the channel gain estimates used for coherent combination) on the statistics of the output of hybrid selection/maximal-ratio (SC/MRC) receiver as well as the degradation of the average symbol error rate (ASER) performance from the ideal case. New expressions for the probability density function (PDF), cumulative distribution function (CDF) and moment generating function (MGF) of the coherent hybrid SC/MRC combiner output signal-to-noise ratio (SNR) are derived. The MGF is then used to derive exact closed form ASER formulas for binary and M-ary modulations employing a nonideal hybrid SC/MRC receiver in Rayleigh fading. Results for both SC and MRC are obtained as limiting cases. The effect of the weighting errors on the outage rate of error probability and the average combined SNR are also investigated. These analytical results provide some insights into the trade-off between diversity gain and combination losses with the increasing order of diversity branches in an energy-sharing communication system",2000,0,
718,719,An Extension of Differential Fault Analysis on AES,"In CHES 2006, M. Amir et al. introduced a generalized method of differential fault attack (DFA) against AES-128. Their fault models cover all locations before the 9th round in AES-128. However, their method cannot be applied to AES with other key sizes, such as AES-192 and AES-256. On the differential analysis, we propose a new method to extend DFA on AES with all key sizes. Our results in this study will also be beneficial to the analysis of the same type of other iterated block ciphers.",2009,0,
719,720,A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction,"In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: >75% percentage of correctly classified files, a recall of >80%, and a false positive rate <30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.",2008,0,
720,721,Smoothing Algorithm for Tetrahedral Meshes by Error-Based Quality Metric,"Smoothing or geometrical optimization is one of basic procedures to improve the quality of mesh. This paper first introduces an error-based mesh quality metric based on the concept of optimal Delaunay triangulations, and then examines the smoothing scheme which minimizes the interpolation error among all triangulations with the same number of vertices. Facing to its deficiency, a modified smoothing scheme and corresponding optimization model for tetrahedral mesh that avoid illegal elements are proposed. The optimization model is solved by integrating chaos search and BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm efficiently. Quality improvement for tetrahedral mesh is realized through alternately applying the smoothing approach suggested and topological optimization technique. Testing results show that the proposed approach is effective to improve mesh quality and suitable for combining with topological technique.",2010,0,
721,722,Fault diagnosis based on timed automata: Diagnoser verification,"The paper deals with the supervisory control problem based on vector synchronous product (VSP) of automata. A necessary and sufficient condition for the existence of such a controller is given, which is based on the notion of vs-controllability. Furthermore, a more general framework called vector synchronous product with communication is proposed. In addition, isomorph and homomorph of two VSPs are defined. Some simplified traffic examples are used to illustrate the notions and the result",2006,0,
722,723,Analogous view transfer for gaze correction in video sequences,"This paper provides a framework for doing facial gaze correction in video sequences. The proposed framework involves stages of face registration, face parameter mapping, and face synthesis. We introduce the concept of analogous views, and derive a novel formulation which extends view transfers based on epipolar geometry to cope with non-rigid motion. Additionally, a disparity mapping function is derived which is learned from training data and handles both spatial disparities as well as pixel-value changes. The disparity mapping function generalizes to facial expressions, illumination conditions and individuals not in the training set, as shown by the results obtained.",2002,0,
723,724,A learning-based approach for fault tolerance on grid resources scheduling,"While Grid environment has developed increasingly, unfortunately the importance of fault tolerance has not been remarkable in Grid resource management. On the other hand, the cost of computing by grid is important because grid is an economy-based system. Most organizations intend to spend little on their own computations by grid. Therefore, using a better approach to resource scheduling to avoid fault is necessary. This paper presents a new approach on fault tolerance mechanisms for the resource scheduling on grid by using Case-Based Reasoning technique in a local fashion. This approach applies a specific structure in order to prepare fault tolerance between executer nodes to retain system in a safe state with minimum data transferring. Certainly, this algorithm increases fault tolerant confidence therefore, performance of grid will be high.",2009,0,
724,725,More on general error locator polynomials for a class of binary cyclic codes,"Recently, the general error locator polynomials have been widely used in the algebraic decoding of binary cyclic codes. This paper utilizes the proposed general error locator polynomial to develop an algebraic decoding algorithm for a class of the binary cyclic codes. This general error locator polynomial differs greatly from the previous general error locator polynomial. Each coefficient of the proposed general error locator polynomial is expressed as a binary polynomial in the single syndrome and the degrees of nonzero terms in the binary polynomial satisfy at least one congruence relation.",2010,0,
725,726,A property oriented fault detection approach for link state routing protocol,"This paper proposes a new approach to fault detection for a link state routing system-property oriented analysis and detection (POD). A routing system is modeled as a set of distributed processes. A property is defined as state predicate(s) over system variables. For the link state routing protocol, the high-level overall converging property P is defined as the synchronization among routing information bases maintained by all processes. We decompose the routing protocol into different computation phases. For each phase, we use invariant state predicates (safety property) and the liveness property as our guide for observation and analysis. The goal of the detection algorithm is to construct a validation path based on the history to determine if the fault is natural or malicious once the stable property P is rendered invalid by faults. The contribution of this paper is twofold: first, a new detection approach is proposed that differs from traditional signature-based or profile-based intrusion detection paradigms in the sense that it utilizes the stable property as a starting point, and correlates the history and future to validate changes in the system; second, by exploring the primary concerned system properties, we show that detection effort can be conducted in a more focused and systematic fashion",2000,0,
726,727,On the 2-Adic Complexity and the k-Error 2 -Adic Complexity of Periodic Binary Sequences,"A significant difference between the linear complexity and the 2-adic complexity of periodic binary sequences is pointed out in this correspondence. Based on this observation, we present the concept of the symmetric 2-adic complexity of periodic binary sequences. The expected value of the 2-adic complexity is determined, and a lower bound on the expected value of the symmetric 2-adic complexity of periodic binary sequences is derived. We study the variance of the 2-adic complexity of periodic binary sequences, and the exact value for it is given. Because the k-adic complexity of periodic binary sequences is unstable, we present the concepts of the <i>kappa</i>-error 2-adic complexity and the k-error symmetric 2-adic complexity, and lower bounds on them are also derived. In particular, we give tighter upper and lower bounds for the minimum k-adic complexity of l-sequences by substituting two symbols within one period.",2008,0,
727,728,A Framework for Proactive Fault Tolerance,"Fault tolerance is a major concern to guarantee availability of critical services as well as application execution. Traditional approaches for fault tolerance include checkpoint/restart or duplication. However it is also possible to anticipate failures and proactively take action before failures occur in order to minimize failure impact on the system and application execution. This document presents a proactive fault tolerance framework. This framework can use different proactive fault tolerance mechanisms, i.e., migration and pause/un-pause. The framework also allows the implementation of new proactive fault tolerance policies thanks to a modular architecture. A first proactive fault tolerance policy has been implemented and preliminary experimentations have been done based on system-level virtualization and compared with results obtained by simulation.",2008,0,
728,729,Study of the Dispersion Characteristics of One Dimensional EBG with Defects,"In this paper we propose a simplified model for studying the Brillouin diagrams based on dielectric multilayers inside a parallel plate waveguide. The main objective of this work is the use of a simplified approach in modelling EBG structures with periodical defects. The effect of layer permittivity, defect length and periodicity were studied using simulation software with appropriated periodical boundary conditions. Physical insights and intuitive justifications for the simulation findings and concepts are also presented. It is shown that the two forbidden band-gaps can either be controlled independently by varying the permittivity or the size of the defects",2005,0,
729,730,Poisonedwater: an adaptive approach to reducing the reputation ranking error in P2P networks,"This paper preliminarily proposes a reputation ranking algorithm called ldquoPoisonedwaterrdquo to resist front peer attack - peers that gain high reputation values by always cooperating with other peers and then promote their malicious friends through passing most of their reputation values to those malicious peers. Specifically, we introduce a notion of Poisoned Water (PW) that iteratively floods from identified malicious peers in the reverse direction of the incoming trust links towards other peers. Furthermore, we propose the concept of spreading factor (SF) that is logistically correlated to each peer's PW level. Then, we design the new reputation ranking algorithm seamlessly integrated with peers' recommendation ability (represented as SF), to infer the more accurate reputation ranking for each peer. Simulation results show that, in comparison with Eigentrust, Poisonedwater can significantly reduce the ranking error ratio up to 20%, when P2P systems exist many malicious peers and front peers.",2009,0,
730,731,Distributed Fault Management for Computational Grids,"Grid resources having heterogeneous architectures, being geographically distributed and interconnected via unreliable network media, are at the risk of failure. Grid environment consists of unreliable resources; therefore, fault tolerant mechanisms can not be ignored. Some scientific jobs require long commitments of grid resources whose failures may not be overlooked. We need a flexible management of these failures by considering the failure of fault manager itself. In this paper we propose the concept of distributed management of failures without engaging the resources for this particular task exclusively. Resources performing the fault management may also participate in serving the long running user jobs. Each sub-job of the main user job is inspected by an individual resource. In case of failure inspector resource takes over in place of inspected resource. Contributions of this paper are: elimination of single point of failure and proposed concept's ability to be integrated with variety of grid middleware",2006,0,
731,732,Spherical Near-Field Antenna Measurements: A Review of Correction Techniques,"Following an introductory review of spherical near-field scanning measurements, with emphasis on the general applicability of the technique, we present a survey of the various methods to improve measurement accuracy by correcting the acquired data before performing the transform and by special processing of the resulting data following the transform. A post-processing technique recently receiving additional attention is the IsoFilter technique that assists in suppressing extraneous stray signals due to scattering from antenna range apparatus.",2007,0,
732,733,Filter Hardware Cost Reduction by Means of Error Feedback,"The article presents an uncommon application of the error feedback-improved IIR filter. A simple method to reduce the hardware cost (silicon area) of the biquadratic section implementation by means of error feedback (EF) is described. The optimization method utilizes the fact that the filter with an EF is more resistant to roundoff noise than a filter without it. An iterative method is used to reduce the occupied silicon area. First the standard IIR filter is designed with the requested quantization properties. Then the EF-improved biquadratic section is designed to attain the same roundoff noise properties. The occupied silicon areas of both solutions are compared then. Although implementation of EF results in more arithmetic components and more complex filter control, the resulting structure attaining the same quantization noise is smaller under defined circumstances (filter with poles close to the unit circle). Results show it is possible to spare up to 22% of the occupied silicon area. Our findings are valid for FPGA as well as ASIC implementation of the IIR filters. Our method has an advantage in using a standard and already verified filtering IP core which results in design time reduction.",2007,0,
733,734,The use of PSA for fault detection and characterization in electrical apparatus,"The monitoring of the actual condition of high voltage apparatus has become more and more important in the last years. One well established tool to characterize the actual condition of electric equipment is the measurement and evaluation of partial discharge data. Immense effort has been put into sophisticated statistic software-tools, to extract meaningful analyses out of data sets, without taking care of relevant correlations between consecutive discharge pulses. In contrast to these classical methods of partial discharge analysis the application of the Pulse Sequence Analysis allows a far better insight into the local defects. The detailed analysis of sequences of discharges in a voltage- and a current-transformer shows that the sequence of the partial discharge signals may change with time, because either different defects are active at different measuring times or a local defect may change with time as a consequence of the discharge activity. Hence for the evaluation of the state of degradation or the classification of the type of defect the analysis of short `homogeneous' sequences or sequence correlated data is much more meaningful than just the evaluation of a set of independently accumulated discharge data. This is demonstrated by the evaluation of measurements performed on different commercial hv-apparatus",2000,0,
734,735,Automated fault tree generation and risk-based testing of networked automation systems,"In manufacturing automation domain safety and availability are the most important factors to ensure productivity. In modern software intensive networked automation systems it became quite hard to ensure which non-functional requirements are related to these factors as well as whether these are satisfied or not. This is due to the prevalence of manual efforts in several analyses phases where complexity of the system often makes it hard to obtain comprehensive overview and thus makes it difficult to ascertain the presence of certain undesired consequences. Since design, development and following verification and validation activities are largely dependent upon the result of the analyses the product is largely affected. To address these problems automated fault tree generation is presented in this paper. It uses distinct modeling artifacts and information to automatically compose formal models of the system. Embedding hardware and network failures it is then ascertained through model checking whether the system satisfies certain safety and availability properties or not. This information is used to compose the fault tree. Proposed approach will improve completeness and correctness in fault trees and will consequently help in improving the quality of the system. Furthermore, it is also shown how the artifacts of this analysis can be used to produce test goals and test cases to validate the software constituents of the system and assure traceability between testing activity and safety requirements.",2010,0,
735,736,Quasi-cyclic generalized ldpc codes with low error floors,"In this paper, a novel methodology for designing structured generalized LDPC (G-LDPC) codes is presented. The proposed design results in quasi-cyclic G-LDPC codes for which efficient encoding is feasible through shift-register-based circuits. The structure imposed on the bipartite graphs, together with the choice of simple component codes, leads to a class of codes suitable for fast iterative decoding. A pragmatic approach to the construction of G-LDPC codes is proposed. The approach is based on the substitution of check nodes in the protograph of a low-density parity-check code with stronger nodes based, for instance, on Hamming codes. Such a design approach, which we call LDPC code doping, leads to low-rate quasi-cyclic G-LDPC codes with excellent performance in both the error floor and waterfall regions on the additive white Gaussian noise channel.",2008,0,
736,737,A new H.264/AVC error resilience model based on Regions of Interest,"Video transmission over the Internet can sometimes be subject to packet loss which reduces the end-user's quality of experience (QoE). Solutions aiming at improving the robustness of a video bitstream can be used to subdue this problem. In this paper, we propose a new region of interest-based error resilience model to protect the most important part of the picture from distortions. We conduct eye tracking tests in order to collect the region of interest (RoI) data. Then, we apply in the encoder an intra-prediction restriction algorithm to the macroblocks belonging to the RoI. Results show that while no significant overhead is noted, the perceived quality of the video's RoI, measured by means of a perceptual video quality metric, increases in the presence of packet loss compared to the traditional encoding approach.",2009,0,
737,738,The z990 first error data capture concept,"Superior availability is one of the outstanding features of modern zSeries machines, among the most highly rated of any existing computer platforms in this reqard. Many features contribute to this characteristic, some in hardware, some in software. This paper describes the first error data capture (FEDC) concept in the zSeries 990. The concept is used for both zSeries integration efficiency and its ability to gain field data for problem determination in the user environment. FEDC is not a single function, but part of all internal software (firmware) in the z990. This paper explains the overall concept and implementation details of the various internal functional layers (subsystems).",2004,0,
738,739,Enhanced server fault-tolerance for improved user experience,"Interactive applications such as email, calendar, and maps are migrating from local desktop machines to data centers due to the many advantages offered by such a computing environment. Furthermore, this trend is creating a marked increase in the deployment of servers at data centers. To ride the price/performance curves for CPU, memory and other hardware, inexpensive commodity machines are the most cost effective choices for a data center. However, due to low availability numbers of these machines, the probability of server failures is relatively high. Server failures can in turn cause service outages, degrade user experience and eventually result in lost revenue for businesses. We propose a TCP splice-based Web server architecture that seamlessly tolerates both Web proxy and backend server failures. The client TCP connection and sessions are preserved, and failover to alternate servers in case of server failures is fast and client transparent. The architecture provides support for both deterministic and non-deterministic server applications. A prototype of this architecture has been implemented in Linux, and the paper presents detailed performance results for a PHP-based Webmail application deployed over this architecture.",2008,0,
739,740,An adaptive distance relaying algorithm with a morphological fault detector embedded,"This paper presents an adaptive distance relaying algorithm (ADRA) for transmission line protection. In ADRA, a fault detector designed based on mathematical morphology (MM) is used to determine the occurrence of a fault. The Euclidean norm of the detector output is then calculated for fault phase selection and fault type classification. With respect to a specific type of fault scenario, an instantaneous circuit model applicable to a transient fault process is constructed to determine the position of the fault. The distance between the fault position and the relay is calculated by a differential equation of the instantaneous circuit model which is resolved in a recursive manner within each sampling interval. Due to the feature of recursive calculation, the protection zone of the relay varies from a small length to large, which increases as an augment in the sample window length. ADRA is evaluated on a transmission model based on PSCAD/EMDTC, under a variety of different fault distances, fault types, fault resistances and loading angles, respectively. The simulation results show that in comparison with conventional DFT-based protection methods, by which the fault distance is calculated using phasor measurements of voltage and current signals in a fixed-length window, ADRA requires much fewer samples to achieve a same degree of the accuracy of fault distance calculation, which enables much faster tripping, and its protection zone can be extended as more samples are used.",2009,0,
740,741,Fault-tolerant defect prediction in high-precision foundry,"High-precision foundry production is subjected to rigorous quality controls in order to ensure a proper result. Such exams, however, are extremely expensive and only achieve good results in a posteriori fashion. In previous works, we presented a defect prediction system that achieved a 99% success rate. Still, this approach did not take into account sufficiently the geometry of the casting part models, resulting in higher raw material requirements to guarantee an appropriate outcome. In this paper, we present here a fault-tolerant software solution for casting defect prediction that is able to detect possible defects directly in the design phase by analysing the volume of three-dimensional models. To this end, we propose advanced algorithms to recreate the topology of each foundry part, analyze its volume and simulate the casting procedure, all of them specifically designed for an robust implementation over the latest graphic hardware that ensures an interactive design process.",2010,0,
741,742,Bounds on the Decoding Error Probability of Binary Block Codes over Noncoherent Block AWGN and Fading Channels,"We derive upper bounds on the decoding error probability of binary block codes over noncoherent block additive white Gaussian noise (AWGN) and fading channels, with applications to turbo codes. By a block AWGN (or fading) channel, we mean that the carrier phase (or fading) is assumed to be constant over each block but independently varying from one block to another. The union bounds are derived for both noncoherent block AWGN and fading channels. For the block fading channel with a small number of fading blocks, we further derive an improved bound by employing Gallager's first bounding technique. The analytical bounds are compared to the simulation results for a coded block-based differential phase shift keying (B-DPSK) system under a practical noncoherent iterative decoding scheme proposed by Chen et al. We show that the proposed Gallager bound is very tight for the block fading channel with a small number of fading blocks, and the practical noncoherent receiver performs well for a wide range of block fading channels",2006,0,
742,743,ConfErr: A tool for assessing resilience to human configuration errors,"We present ConfErr, a tool for testing and quantifying the resilience of software systems to human-induced configuration errors. ConfErr uses human error models rooted in psychology and linguistics to generate realistic configuration mistakes; it then injects these mistakes and measures their effects, producing a resilience profile of the system under test. The resilience profile, capturing succinctly how sensitive the target software is to different classes of configuration errors, can be used for improving the software or to compare systems to each other. ConfErr is highly portable, because all mutations are performed on abstract representations of the configuration files. Using ConfErr, we found several serious flaws in the MySQL and Postgres databases, Apache web server, and BIND and djbdns name servers; we were also able to directly compare the resilience of functionally-equivalent systems, such as MySQL and Postgres.",2008,0,
743,744,Diagnostic and protection of inverter faults in IPM motor drives using wavelet transform,"This paper presents a novel faults diagnostic and protection technique for interior permanent magnet (IPM) motor drives using wavelet transform. The proposed wavelet based diagnostic and protection technique for inverter faults is developed and implemented in real-time for a voltage source inverter fed IPM motor. In the proposed technique, the motor currents of different faulted and unfaulted conditions of an IPM motor drive system are preprocessed by wavelet packet transform. The wavelet packet transformed coefficients of motor currents are used as inputs of a three-layer wavelet neural network. The performances of the proposed diagnostic and protection technique are investigated in simulation and experiments. The proposed technique is experimentally tested on a laboratory 1-hp IPM motor drive using the ds1102 digital signal processor board. The test results showed satisfactory performances of the proposed diagnostic and protection technique in terms of speed, accuracy and reliability.",2008,0,
744,745,FTDIS: A Fault Tolerant Dynamic Instruction Scheduling,"In this work, we target the robustness for controller scheduler of type Tomasulo for SEU faults model. The proposed fault-tolerant dynamic scheduling unit is named FTDIS, in which critical control data of scheduler is protected from driving to an unwanted stage using Triple Modular Redundancy and majority voting approaches. Moreover, the feedbacks in voters produce recovery capability for detected faults in the FTDIS, enabling both fault mask and recovery for system. As the results of analytical evaluations demonstrate, the implemented FTDIS unit has over 99% fault detection coverage in the condition of existing less than 4 faults in critical bits. Furthermore, based on experiments, the FTDIS has a 200% hardware overhead comparing to the primitive dynamic scheduling control unit and about 50% overhead in comparision to a full CPU core. The proposed unit also has no performance penalty during simulation. In addition, the experiments show that FTDIS consumes 98% more power than the primitive unit.",2010,0,
745,746,Single-switch power factor correction AC/DC converter with storage capacitor size reduction,"In universal line applications with hold-up time requirement, the single-stage PFC AC/DC converters may not be more attractive than the conventional two-stages approach if the size and cost of the storage capacitor are too high. Furthermore, computer related applications, in which the holdup time is a very important requirement, will have to comply with Class D limits of the low frequency harmonic regulation IEC 61000-3-2. Therefore, for these applications, a not very distorted line current will be required. In this paper, a new single-stage AC/DC converter suitable for universal line applications is proposed. The main difference with other solutions is the low voltage swing on the storage capacitor while the line varies within its universal range. This feature allows reducing the size and cost of the storage capacitor. Additional advantages of the proposed converter are topology simplicity (single-switch converter) and IEC 61000-3-2 Class D compliance. The experimental results confirms the above mentioned advantages.",2003,0,
746,747,A Java API for advanced faults management,"The paper proposes an alternative for modeling managed resources using Java and telecommunication network management standards. It emphasizes functions related to fault management, namely: diagnostic testing and performance monitoring. Based on Java management extension (JMX<sup>TM</sup>), specific extensions are proposed to facilitate diagnostic testing and performance measurements implementation. The new API also called Java fault management extension (JFMX) consists of managed objects that model real resources being tested or monitored and support objects defined for the need of diagnostic testing and performance measurements. The paper discusses four Java implementations of a 3-tier client/server scenario focusing on the SystemUnderTest package of the new API to instrument a minimalist managed system scenario. These implementations are respectively built on top of the following Java based communication infrastructures: JMX/JFMX, RMI, CORBA/Java, and Voyager<sup>TM</sup>. The paper extends the Voyager implementation with JMX/JFMX and uses their dynamic and advanced features to provide a highly efficient solution. The later implementation also uses the mobile agent paradigm to overcome well-known limitations of the RPC based implementations",2001,0,
747,748,Operating system function reuse to achieve low-cost fault tolerance,The aim of this article is to propose a new approach to fault tolerance in single processor embedded systems which is centred on the operating system. Particular attention is put on low-cost techniques that exploit functions already present in the system in a different than-usual way to achieve protection with little or no intervention at the application level. An example is given: the realization of a checkpoint and rollback scheme through the context switch function.,2004,0,
748,749,PSC-PWM in fault tolerant drive system for EMA operation,The introduction of EMA Systems requires the use of redundant inverters to drive the EMA and ensure reliability and safety. Redundant converters allow the implementation of fault tolerant control and high quality operation. Fault control has been implemented by means of redundant converter and fault detection system.,2010,0,
749,750,A fault-tolerant real-time supervisory scheme for an interconnected four-tank system,"In this paper, the implementation of a Command Governor (CG) strategy on a real-time computing system is described for the supervision of a laboratory four-tank test-bed. In particular, the real-time architecture has been developed on the RTAI/Linux operating system kernel and the CG module has been implemented in C++ on a general purpose off-the-shelf computing unit. An accurate model of the the four-tank process has been derived from both physical and experimental data and the applicability of the proposed method has been proved by means of real-time tests, which testified on the CG strategy ability to enforce the prescribed operative constraints even under unexpected adverse conditions, e.g. water pumps failures.",2010,0,
750,751,Evaluation of fault tolerance latency from real-time application's perspectives,"Information on Fault Tolerance Latency (FTL), which is defined as the total time required by all sequential steps taken to recover from an error, is important to the design and evaluation of fault-tolerant computers used in safety-critical real-time control systems with deadline information. In this paper, we evaluate FTL in terms of several random and deterministic variables accounting for fault behaviors and/or the capability and performance of error-handling mechanisms, while considering various fault tolerance mechanisms based on the trade-off between temporal and spatial redundancy, and use the evaluated FTL to check if an error-handling policy can meet the Control System Deadline (CSD) for a given real-time application",2000,0,
751,752,Research of Remote Fault Diagnosis System Based on Internet,"The technology of intelligent multi-agents is applied to design remote fault diagnosis system based on Internet. The system owns the kernel of the remote fault diagnosis platform, (RFDP) and the members of manufacturers and enterprise client. It is a distributed, remote monitoring and on-line diagnosis system. The system has overcome the function limitations of 2 kinds of traditional client/server architecture, equipment client-end remote diagnostic mode and manufacturer-end remote diagnostic mode. The platform has a rapid diagnosis response and makes it easy to realize information transmitting timely between platform and diagnosis members. For this reason, RFDP which holds a great ability for enabling on-line monitoring, general fault diagnosis, repairing service and updating knowledge base rapidly, can give a good service for remote distributed multi- equipments and multi-manufactures. As a common diagnosis platform, the system can be applied to various remote mechanical and electronic equipment diagnosis areas such as CNC and press machine diagnosis.",2007,0,
752,753,Application of compensation method in calculating symmetrical short circuit fault,"Based on compensation method, Symmetrical short-circuit fault current formula and nodal voltage formula are deduced in this paper. In the deduction process the time of calculating matrix inversion is eliminated for the node-admittance matrix being not modified when the fault occurs, however triangular matrix method is applied to calculate nodal impedance matrix at the program entrance in the process based on the original network nodal admittance matrix, thus the solution of the electrical network state variables is speed up with preparing data for fault calculation in advance. After further assumptions and simplification, short-circuit current formula is more efficient to estimate the size of short-circuit current, and it is very suitable for real-time online applications. At the same time, current coefficient power contributing which is different from power current distribution coefficient is put forward. A optimal node is found for new power supply to limit short-circuit current by judging the size of current coefficient power contributing.",2010,0,
753,754,On the design of error detection and correction cryptography schemes,"The paper introduces the method of modifying cryptography encryption and decryption units, which includes circuitry of checks that operations have been performed without errors. This technique is based on addition to storage devices, error correction codes, and module check of arithmetic and logic units operations",2000,0,
754,755,A platooning controller robust to vehicular faults,This paper presents a platooning controller for a four-wheel-driving four-wheel-steering vehicle to follow another. The controller is based on the full-state tracking theory and utilizes a vehicular model that makes it able to continue to operate when faults are detected at its steering systems or driving motors which are disabled accordingly. The unified controller is also able to track and follow the target either moving forward in front or moving backward in the back of the vehicle making the real-time implementation of different tracking modes simple. Tracking stability is secured by the proper selection of design parameters. Simulations show the proposed control scheme works properly even in the presence of faults at several different parts.,2004,0,
755,756,A new automated instrumentation for emulation-based fault injection,"Soft errors are an increasing threat in up-to-date technologies, so robustness evaluation has become an important part of digital circuit design. Emulation-based fault injection techniques have proved to be an efficient approach to perform such evaluations. In this paper, we propose new optimizations further improving the experimental duration and the instrumentation cost while maintaining the maximum flexibility for the dependability evaluation process.",2010,0,
756,757,Automatic Generation of Detection Algorithms for Design Defects,"Maintenance is recognised as the most difficult and expansive activity of the software development process. Numerous techniques and processes have been proposed to ease the maintenance of software. In particular, several authors published design defects formalising ""bad"" solutions to recurring design problems (e.g., anti-patterns, code smells). We propose a language and a framework to express design defects synthetically and to generate detection algorithms automatically. We show that this language is sufficient to describe some design defects and to generate detection algorithms, which have a good precision. We validate the generated algorithms on several programs",2006,0,
757,758,Analysis of fault-tolerant five-phase IPM synchronous motor,"The choice of a multi-phase motor is a potentially fault-tolerant solution and gives rise to many advantages, respect to the traditional three-phase motor drives. In this paper an high torque density five-phase IPM synchronous motor has been studied, and the motor performance have been evaluated in the case of healthy-mode and faulty-mode operation.",2008,0,
758,759,Space shuttle fault tolerance: Analog and digital teamwork,"The Space Shuttle control system (including the avionics suite) was developed during the 1970s to meet stringent survivability requirements that were then extraordinary but today may serve as a standard against which modern avionics can be measured. In 30 years of service, only two major malfunctions have occurred, both due to failures far beyond the reach of fault tolerance technology: the explosion of an external fuel tank, and the destruction of a launch-damaged wing by re-entry friction. The Space Shuttle is among the earliest systems (if not the earliest) designed to a ldquoFO-FO-FSrdquo criterion, meaning that it had to Fail (fully) Operational after any one failure, then Fail Operational after any second failure (even of the same kind of unit), then Fail Safe after most kinds of third failure. The computer system had to meet this criterion using a Redundant Set of 4 computers plus a backup of the same type, which was (ostensibly!) a COTS type. Quadruple redundancy was also employed in the hydraulic actuators for elevons and rudder. Sensors were installed with quadruple, triple, or dual redundancy. For still greater fault tolerance, these three redundancies (sensors, computers, actuators) were made independent of each other so that the reliability criterion applies to each category separately. The mission rule for Shuttle flights, as distinct from the design criterion, became ldquoFO-FS,rdquo so that a mission continues intact after any one failure, but is terminated with a safe return after any second failure of the same type. To avoid an unrecoverable flat spin during the most dynamic flight phases, the overall system had to continue safe operation within 400 msec of any failure, but the decision to shut down a computer had to be made by the crew. Among the interesting problems to be solved were ldquocontrol sliveringrdquo and ldquosync holes.rdquo The first flight test (Approach and Landing only) was the proof of the pudding: when a key wire harness solder - joint was jarred loose by the Shuttle's being popped off the back of its 747 mother ship, one of the computers ldquowent bananasrdquo (actual quote from an IBM expert).",2009,0,
759,760,Assessing the impact of active guidance for defect detection: a replicated experiment,"Scenario-based reading (SBR) techniques have been proposed as an alternative to checklists to support the inspectors throughout the reading process in the form of operational scenarios. Many studies have been performed to compare these techniques regarding their impact on the inspector performance. However, most of the existing studies have compared generic checklists to a set of specific reading scenarios, thus confounding the effects of two SBR key factors: separation of concerns and active guidance. In a previous work we have preliminarily conducted a repeated case study at the University of Kaiserslautern to evaluate the impact of active guidance on inspection performance. Specifically, we compared reading scenarios and focused checklists, which were both characterized as being perspective-based. The only difference between the reading techniques was the active guidance provided by the reading scenarios. We now have replicated the initial study with a controlled experiment using as subjects 43 graduate students in computer science at University of Bari. We did not find evidence that active guidance in reading techniques affects the effectiveness or the efficiency of defect detection. However, inspectors showed a better acceptance of focused checklists than reading scenarios.",2004,0,
760,761,A Reducing Transmission-Line Fault Current Method,"In this paper, a reducing transmission-line fault current method with capacitor compensators is proposed to limit the transmission-line fault current in power systems. In the normal mode of operation, the shunt capacitors banks as reactive power compensators that delivers reactive power to increase the power factor and used on medium-length and long transmission lines to increase line loadability and to maintain voltages near rated values. Their important effect is to reduce line-voltage drops and to increase the power factor and the steady-state stability limit. When faults states occurs, the capacitor another effect is to reduce transmission-line fault current peak value. Simulations performed in MATLAB/Simulink environment indicate that the proposed performance for capacitor compensators performs well to limit the fault currents of transmission lines and line-voltage drops.",2010,0,
761,762,Recovery of fault-tolerant real-time scheduling algorithm for tolerating multiple transient faults,"The consequences of missing deadline of hard real time system tasks may be catastrophic. Moreover, in case of faults, a deadline can be missed if the time taken for recovery is not taken into account during the phase when tasks are submitted or accepted to the system. However, when faults occur tasks may miss deadline even if fault tolerance is employed. Because when an erroneous task with larger execution time executes up to end of its total execution time even if the error is detected early, this unnecessary execution of the erroneous task provides no additional slack time in the schedule to mitigate the effect of error by running additional copy of the same task without missing deadline. In this paper, a recovery mechanism is proposed to augment the fault-tolerant real-time scheduling algorithm RM-FT that achieves node level fault tolerance (NLFT) using temporal error masking (TEM) technique based on rate monotonic (RM) scheduling algorithm. Several hardware and software error detection mechanisms (EDM), i.e. watchdog processor or executable assertions, can detect an error before an erroneous task finishes its full execution, and can immediately stops execution. In this paper, using the advantage of such early detection by EDM, a recovery algorithm RM-FT-RECOVERY is proposed to find an upper bound, denoted by Edm Bound, on the execution time of the tasks, and mechanism is developed to provide additional slack time to a fault-tolerant real-time schedule so that additional task copies can be scheduled when error occurs.",2007,0,
762,763,A Method to Evaluate Voltages to Earth During an Earth Fault in an HV Network in a System of Interconnected Earth Electrodes of MV/LV Substations,"An easy and swift method to evaluate, in a system of interconnected earth electrodes, earth potentials on earthing systems of medium-voltage/low-voltage (MV/LV) substations, in an event of single-line-to-earth fault inside a high-voltage/medium- voltage (HV/MV) station, is presented. The advantage of the method is the simplicity of the mathematical model for solving complex systems of any size with a sufficient accuracy for practical purposes. This paper shows the results of simulations, performed on networks with different extensions and characteristics, organized in easy-to-read graphs and tables. A comparison of these results with the values obtained according to the procedure explained in the IEC-Standard 60909-3, and a study on the accuracy of the method has been made. Moreover, some considerations on the inclusion of earth electrodes of HV/MV stations within global earthing systems are done.",2008,0,
763,764,Characterization of Upset-induced Degradation of Error-mitigated Highspeed I/O's Using Fault Injection,"Fault-injection experiments on Virtex-II FPGAs quantify failure and degradation modes in I/O channels incorporating triple modular redundancy (TMR). With increasing frequency (to 100 MHz), full TMR under both I/O standards investigated shows more configuration bits have a measurable performance effect.",2005,0,
764,765,Fault detection and protection system for the power converters with high-voltage IGBTs,"This paper addresses problems related to the design and implementation of a fault detection and protection system for high-voltage (HV) NPT IGBT-based converters. An isolated half-bridge power converter topology is investigated, which seems to be very attractive for the high-power electronic converters due to its overall simplicity, small component count and low realization costs. This converter is to be applied in rolling stock with its demanding reliability and safety requirements. Clearly, the robust control and protection system is essential.",2008,0,
765,766,All Bits Are Not Equal - A Study of IEEE 802.11 Communication Bit Errors,"In IEEE 802.11 Wireless LAN (WLAN) systems, techniques such as acknowledgement, retransmission, and transmission rate adaptation, are frame-level mechanisms designed for combating transmission errors. Recently sub-frame level mechanisms such as frame combining have been proposed by the research community. In this paper, we present results obtained from our bit error study for identifying sub-frame error patterns because we believe that identifiable bit error patterns can potentially introduce new opportunities in channel coding, network coding, forward error correction (FEC), and frame combining mechanisms. We have constructed a number of IEEE 802.11 wireless LAN testbeds and conducted extensive experiments to study the characteristics of bit errors and their location distribution. Conventional wisdom dictates that bit error probability is the result of channel condition and ought to follow corresponding distribution. However our measurement results identify three repeatable bit error patterns that are not induced by channel conditions. We have verified that such error patterns are present in WLAN transmissions in different physical environments and across different wireless LAN hardware platforms. We also discuss our current hypotheses for the reasons behind these bit error probability patterns and how identifying these patterns may help improving WLAN transmission robustness.",2009,0,
766,767,Evaluation of replication and fault detection in P2P-MPI,"We present in this paper an evaluation of fault management in the grid middleware P2P-MPI. One of P2P-MPI's objective is to support environments using commodity hardware. Hence, running programs is failure prone and a particular attention must be paid to fault management. The fault management covers two issues: fault-tolerance and fault detection. P2P-MPI provides a transparent fault tolerance facility based on replication of computations. Fault detection concerns the monitoring of the program execution by the system. The monitoring is done through a distributed set of modules called failure detectors. In this paper, we report results from several experiments which show the overhead of replication, and the cost of fault detection.",2009,0,
767,768,Testing of LUT delay aliasing faults in SRAM based FPGAs using half-frequencies,"In this paper, we present a technique for testing the delay aliasing faults associated with LUTs in SRAM based FPGAs. We compare the outputs of two identical LUTs when one is operated at half the frequency of the other. A Built in Self Test (BIST) circuitry consisting of a Test Pattern Generator, a Comparator, and the Circuit Under Test (CUT) is mapped on the FPGA. Application of input sequence vectors at half frequencies to the LUTs enable the detection of delay and aliasing faults which may go undetected by other techniques. The technique is verified using VHDL based simulations. The results are also experimentally verified using a Virtex II FPGA board.",2007,0,
768,769,Experimental studies on faults detection using residual generator,"In this paper one will develop the faults detection and localization method using residual vectors, in order to emphasize the noises, disturbances and faults on the outputs L<inf>1</inf> and L<inf>2</inf> of the control level plant with two coupled tanks Quanser Water Level Control Two Tank Module. The proposed method was theoretically developed and experimentally verified in this plant and allowed detection and localization of two faults created in a real plant. The experiments presented were realized using Matlab Simulink program.",2010,0,
769,770,Efficient Memory Error Coding for Space Computer Applications,"For the secure transaction of data between the central processing unit (CPU) of a satellite on board-computer and its local random access memory (RAM), the program memory has been usually designed with triple modular redundancy (TMR), which is a hardware implementation that includes replicated memory circuits and voting logic to detect and correct a faulty value. TMR error correction technique allows single correction of one error bit per stored word. For computers on board a satellite, there is however a definite risk of two error bits occurring within one byte of stored data. In this paper, the application of the quasi-cyclic codes to the routine error protection of SRAM program memory for satellites in low Earth orbit is described and implemented in field programmable gate array (FPGA) technology. The proposed device is transparent to the routine transfer of data between CPU and its local RAM",2006,0,
770,771,Immune Systems Inspired Approach to Anomaly Detection and Fault Diagnosis for Engines,"As more electronic devices are integrated into automobiles to improve the reliability, drivability and maintainability, automotive diagnosis becomes increasingly difficult to deal with. Unavoidable design defects, quality variations in the production process as well as different usage patterns make it is infeasible to foresee all possible faults that may occur to the vehicle. As a result, many systems rely on limited diagnostic coverage provided by a diagnostic strategy which tests only for a priori known or anticipated failures, and presumes the system is operating normally if the full set of tests is passed. To circumvent these difficulties and provide a more complete coverage for detection of any fault, a new paradigm for design of automotive diagnostic systems is needed. An approach inspired by the functionalities and characteristics of natural immune system is presented and discussed in the paper. The feasibility of the newly proposed paradigm is also partially demonstrated through application examples.",2007,0,
771,772,Research on Optimal Placement of Travelling Wave Fault Locators in Power Grid,"Taking the full network observability of power system operation state, maximum state measurement redundancy and minimum number of travelling wave fault location device (TFD) as objectives, an TFD optimal placement scheme for power grid fault location with travelling wave is presented in the paper. The scheme contains two steps: static processing and dynamic configuration. Terminal substations should install TFD. Then taking the terminal substations as starting point, the whole network is separated into several unattached branches. The branch which includes the most number of substations, via the longest line, and has not any loop, can install TFD at its both terminals. And then combining with the practical length of each line and coverage range of substations, the optimal disposition of TFD can successfully accomplish. A novel network-based fault location algorithm is also designed with travelling wave velocity on-line measuring and every TFD recorded travelling arrival times fusing. EMTP simulation results show that the TFD optimal placement scheme can use less TFDs to locate all faults in the whole power grid with economy and high reliability. The location error is no more than 100 m.",2008,0,
772,773,A Unity Power Factor Correction Preregulator with Fast Dynamic Response Based on a Low-Cost Microcontroller,"Low cost passive Power Factor Correction (PFC) and Single-Stage PFC converters cannot draw a sinusoidal input current and are only suitable solutions to supply low power levels. PFC preregulators based on the use of a multiplier solve such drawbacks, but a second stage DC/DC converter is needed to obtain fast output voltage dynamics. The output voltage response of PFC preregulators can be improved by increasing the corner frequency of the output voltage feedback loop. The main drawback to obtaining a faster converter output response is the distortion of the input current. This paper describes a simple control strategy to obtain a sinusoidal input current. Based on the static analysis of output voltage ripple, a modified sinusoidal reference is created using a low cost microcontroller in order to obtain a input sinusoidal current. This reference replaces the traditional rectified sinusoidal input voltage reference in PFC preregulators with multiplier control. Using this circuitry, PFC preregulator topologies with galvanic isolation are suitable solutions to design a power supply with fast output voltage response (10 ms or 8.33 ms) and low line current distortion. Finally, theoretical and simulated results are validated using a 500 W prototype.",2007,0,
773,774,Adaptive and Fault Tolerant Simulation of Relativistic Particle Transport with Data-Level Checkpointing,"Many scientific applications exhibit high demands on memory storage and computing capability. Improvements in commodity processors and networks have provided an opportunity to support such scientific applications within an everyday computing infrastructure. Good applications need the ability to work in constantly changing environments. Adaptability and fault tolerance are essential. Based on simulation of relativistic particle transport, this paper proposes a data-level checkpointing scheme for common scientific applications. This scheme takes advantage of the regular program layout, dominant computing loops, and fine-grained iterations. Without handling stack and heap segments directly, only application data is saved and restored as the computation state. Checkpointing interval can be dynamically adjusted to satisfy sensitivity and efficiency requirements for feasible fault tolerance. With this periodic but fixed-location checkpointing scheme, the MPI- based simulation system can be reconfigured by being shut down first and then restarted on same or different computer clusters. Application data can be redistributed for the new configuration. Experimental results have demonstrated this scheme's efficiency and effectiveness.",2008,0,
774,775,An intelligent FFT-analyzer with harmonic interference effect correction and uncertainty evaluation,"In the paper, the problem of the correction of harmonic interference effects on FFT results is discussed. A procedure for the effect evaluation and correction is proposed and implemented in an intelligent FFT-analyzer able also to provide the results with their uncertainty.",2003,0,
775,776,Forward error correction strategies for media streaming over wireless networks,"The success of next-generation mobile communication systems depends on the ability of service providers to engineer new added-value multimedia-rich services, which impose stringent constraints on the underlying delivery/transport architecture. The reliability of real-time services is essential for the viability of any such service offering. The sporadic packet loss typical of wireless channels can be addressed using appropriate techniques such as the widely used packet-level forward error correction. In designing channel-aware media streaming applications, two interrelated and challenging issues should be tackled: accuracy of characterizing channel fluctuations and effectiveness of application-level adaptation. The first challenge requires thorough insight into channel fluctuations and their manifestations at the application level, while the second concerns the way those fluctuations are interpreted and dealt with by adaptive mechanisms such as FEC. In this article we review the major issues that arise when designing a reliable media streaming system for wireless networks.",2008,0,
776,777,Tight exponential upper bounds on the ML decoding error probability of block codes over fully interleaved fading channels,"We derive tight exponential upper bounds on the decoding error probability of block codes which are operating over fully interleaved Rician fading channels, coherently detected and maximum-likelihood decoded. It is assumed that the fading samples are statistically independent and that perfect estimates of these samples are provided to the decoder. These upper bounds on the bit and block error probabilities are based on certain variations of the Gallager bounds. These bounds do not require integration in their final version and they are reasonably tight in a certain portion of the rate region exceeding the cutoff rate of the channel. By inserting interconnections between these bounds, we show that they are generalized versions of some reported bounds for the binary-input additive white Gaussian noise channel.",2003,0,
777,778,Fault location using traveling wave for power networks,"Fault location using traveling wave has been applied in extra-high voltage power grids successfully. Due to its complication and high cost, it is not easy for this technique to be accepted for use in distribution system. A new traveling wave fault location system is developed simply in a cost-effective way for power networks (especially for distribution system) in this paper. Two traveling wave sensors are developed to capture the current traveling wave flowing from the capacitive equipment to earth and the voltage traveling waves in all three phases. The outputs of the sensors are then applied to the trigger and time tagging by using Global Position System (GPS) receiver. The fault position is calculated by the traveling wave arrival times in every power station where only one fault locator is installed. The fault location system is tested in the power system. Testing results show that the fault locator has high precision and robustness.",2004,0,
778,779,Novel method for selective detection of earth faults in high impedance grounded distribution networks,"An elementary and reliable detection of earth faults in impedance grounded networks results in considerable benefits for the utility both in terms of outage duration and personal safety. This report describes an entirely new, only current measuring method; a method, which fulfils the standards of cost efficiency and reliability. Despite the seeming simplicity of the approach it is also demonstrated that it is an excellent method to detect arcing cable earth faults.",2005,0,
779,780,Fault Tolerance of Tornado Codes for Archival Storage,"This paper examines a class of low density parity check (LDPC) erasure codes called Tornado codes for applications in archival storage systems. The fault tolerance of Tornado code graphs is analyzed and it is shown that it is possible to identify and mitigate worst-case failure scenarios in small (96 node) graphs through use of simulations to find and eliminate critical node sets that can cause Tornado codes to fail even when almost all blocks are present. The graph construction procedure resulting from the preceding analysis is then used to construct a 96-device Tornado code storage system with capacity overhead equivalent to RAID 10 that tolerates any 4 device failures. This system is demonstrated to be superior to other parity-based RAID systems. Finally, it is described how a geographically distributed data stewarding system can be enhanced by using cooperatively selected Tornado code graphs to obtain fault tolerance exceeding that of its constituent storage sites or site replication strategies",2006,0,
780,781,Floating-point error analysis based on affine arithmetic,"During the development of floating-point signal processing systems, an efficient error analysis method is needed to guarantee the output quality. We present a novel approach to floating-point error bound analysis based on affine arithmetic. The proposed method not only provides a tighter bound than the conventional approach, but also is applicable to any arithmetic operation. The error estimation accuracy is evaluated across several different applications which cover linear operations, nonlinear operations, and feedback systems. The accuracy decreases with the depth of computation path and also is affected by the linearity of the floating-point operations.",2003,0,
781,782,A novel approach to faulted-phase selection using current traveling waves and wavelet analysis,"The early traveling wave faulted-phase selectors, due to lack of effective tool to process transient signals, had to directly used instantaneous values of signals so that they cannot overcome such bad influence as noise disturbance. Fortunately, wavelet analysis, with its time-frequency localization ability and the wavelet transform modulus maxima (WTMM) concept, is well suited to treat with singularity of fault-generated traveling waves in EHV/UHV transmission lines. This paper presents a novel approach to fast and accurate phase-selection, which used the WTMM of initial model current traveling waves according to the fault characteristic relations deduced from the boundary conditions of various types of faults. The criterion is explicit in characteristics and physical concepts, and is apt to be realized. A large number of EMTP simulations demonstrated the new faulted-phase selection algorithm.",2002,0,
782,783,A Case Study of Bias in Bug-Fix Datasets,"Software quality researchers build software quality models by recovering traceability links between bug reports in issue tracking repositories and source code files. However, all too often the data stored in issue tracking repositories is not explicitly tagged or linked to source code. Researchers have to resort to heuristics to tag the data (e.g., to determine if an issue is a bug report or a work item), or to link a piece of code to a particular issue or bug. Recent studies by Bird et al. and by Antoniol et al. suggest that software models based on imperfect datasets with missing links to the code and incorrect tagging of issues, exhibit biases that compromise the validity and generality of the quality models built on top of the datasets. In this study, we verify the effects of such biases for a commercial project that enforces strict development guidelines and rules on the quality of the data in its issue tracking repository. Our results show that even in such a perfect setting, with a near-ideal dataset, biases do exist - leading us to conjecture that biases are more likely a symptom of the underlying software development process instead of being due to the used heuristics.",2010,0,
783,784,Concurrent and simple digital controller of an AC/DC converter with power factor correction based on an FPGA,"Nowadays, most digital controls for power converters are based on DSPs. This paper presents a field programmable gate array (FPGA) based digital control for a power factor correction (PFC) flyback AC/DC converter. The main difference from DSP-based solutions is that FPGAs allow concurrent operation (simultaneous execution of all control procedures), enabling high performance and novel control methods. The control algorithm has been developed using a hardware description language (VHDL), which provides great flexibility and technology independence. The controller has been designed as simple as possible while maintaining good accuracy and dynamic response. Simulations and experimental results show the feasibility of the method, opening interesting possibilities in power converters control.",2003,0,
784,785,Feature set evaluation and fusion for motor fault diagnosis,"This paper proposes a novel approach to the feature fusion in motor fault diagnosis with the main aim of improving the performance and reliability of clustering and identification of the fault patterns. In addition, the significance of individual feature sets in specific fault scenarios, which is normally gained by engineers through experience, is investigated by using flexible Non-Gaussian modeling of the historical data. Furthermore the comparison is made by applying individual and fusion of feature sets to the probabilistic distributions of trained models using a Maximum a Posteriori (MAP) approach. To carry out the task, current waveforms are collected non-invasively from three-phase DC motors. Waveforms are then compressed into time, frequency and wavelet feature sets to form the input to the clustering algorithm. The result demonstrates the suitability of specific feature sets in different motor modes and the efficiency of fusion which is carried out with a Winner Takes All (WTA) approach.",2010,0,
785,786,High-speed serial communication with error correction using 0.25 m CMOS technology,"In this paper we propose a novel design for an autonomous high-speed serial off and on-chip communication system which incorporates impedance tuning, error correction with a packet transfer and a parallel asynchronous interface. The constructed transmitter-receiver pair has throughput of 5 Gbit/s. With error correction and packet transfer overhead accounted for this construct has bandwidth of 500 <bytes/s. The circuit has been simulated using HSpice with 0.25 m TMSC CMOS technology",2001,0,
786,787,"Attacking ""bad actor"" and ""no fault found"" electronic boxes","A the percentage of what are termed ""bad actor"" and no fault found (NFF) electronic box in military weapon systems is steadily growing. These are boxes that fail during operation, but test NFF during back shop testing, or, that fail during back shop testing and then test NFF at the depot repair facility. During operation, an electronic box is stressed by various environmental conditions which are normally absent on a test bench. If there are cold or cracked solder joints, corroded or dirty connector contacts, loose crimp joints, hairline cracks in a ribbon cable trace, or other intermittent conditions, the intermittency can occur while the box is under stress conditions, yet seldom occur while the box is on a test bench at room temperature. Very little concerted effort is currently focused on detecting, isolating and repairing these intermittent problems. Virtually all testing activity simply tests the unit for normal operation, one function, one circuit, or one set of circuits at a time. If an intermittent circuit is not displaying its intermittent nature at the instant it is being tested, the intermittency remains undetected. A three-pronged effort is currently underway to attack and repair bad actor and NFF electronic boxes. The first is to collect detailed repair data to identify which boxes are bad actor and NFF units. The second is to collect test data to determine which units yield inconsistent test results between back shop testing and depot testing, and why. The third is to employ a system that detects and isolates electronic box intermittent circuits. This paper describes the success realized to date by employing each of the three techniques described above, and how they are now effectively being employed together to reduce maintenance costs and improve avionics reliability for the F-16 weapon system.",2005,0,
787,788,Software-Implemented Fault Injection at Firmware Level,"Software-implemented fault injection is an established method to emulate hardware faults in computer systems. Existing approaches typically extend the operating system by special drivers or change the application under test. We propose a novel approach where fault injection capabilities are added to the computer firmware. This approach can work without any modification to operating system and / or applications, and can support a larger variety of fault locations. We discuss four different strategies in X86/X64 and Itanium systems. Our analysis shows that such an approach can increase portability, the non-intrusiveness of the injector implementation, and the number of supported fault locations. Firmware-level fault injection paves the way for new research directions, such as virtual machine monitor fault injection or the investigation of certified operating systems.",2010,0,
788,789,Application of neural networks and filtered back projection to wafer defect cluster identification,"During an electrical testing stage, each die on a wafer must be tested to determine whether it functions as it was originally designed. In the case of a clustered defect on the wafer, such as scratches, stains, or localized failed patterns, the tester may not detect all of the defective dies in the flawed area. To avoid the defective dies proceeding to final assembly, an existing tool is currently used by a testing factory to detect the defect cluster and mark all the defective dies in the flawed region or close to the flawed region; otherwise, the testing factory must assign five to ten workers to check the wafers and hand mark the defective dies. This paper proposes two new wafer-scale defect cluster identifiers to detect the defect clusters, and compares them with the existing tool used in the industry. The experimental results verify that one of the proposed algorithms is very effective in defect identification and achieves better performance than the existing tool.",2002,0,
789,790,Proactive fault management based on risk-augmented routing,"Carrier networks need to provide their customers with high availability of communication services. Unfortunately, failures are managed by recovery mechanisms getting involved only after the failure occurrence to limit the impact on traffic flows. However, there are often forewarning signs that a network device will stop working properly. We propose to take into account this risk exposure in order to improve the performance of the existing restoration mechanisms, in particular for IP networks. Based on an embedded and real-time risk-level assessment, we can perform a proactive fault-management and isolate the failing routers out of the routed topology, and thus totally avoid service unavailability. Our novel approach enables routers to preventively steer traffic away from risky paths by temporally tuning OSPF link cost.",2010,0,
790,791,Optimized resource allocation in grid networks using genetic algorithm with error rate factor,"Grid computing is an emerging computing paradigm that will have significant impact on the next generation information infrastructure. Due to the largeness and complexity of grid system, its quality of service, performance and reliability are difficult to model, analyze and evaluate. In real time evaluation, various noises will influence the model and which in turn accounts for increase in packet loss and Bit Error Rate (BER). Therefore, a novel optimization model for maximizing the expected grid service profit is mandatory. In our work, to achieve the improvement in the end to end grid network performance, an optimizer, which is based on Genetic Algorithm (GA) with Fitness Evaluation parameters considers BER and Service Execution Time, is designed in the RMS. This paper presents the novel tree structured model, is better than other existing models for grid computing performance and reliability analysis by not only considering data dependence and failure correlations, but also takes link failure, packet loss & BER real time parameters in account. The algorithm based on the Graph theory and Probability theory.",2009,0,
791,792,Improving SNR for DSM Linear Systems Using Probabilistic Error Correction and State Restoration: A Comparative Study,"Smaller feature sizes and lower supply voltages make DSM devices more susceptible to soft errors generated by alpha particles and neutrons as well as other sources of environmental noise. In this scenario, soft-error/noise tolerant techniques are necessary for maintaining the SNR of critical DSP applications. This paper studies linear DSP circuits and discusses two low cost techniques for improving the SNR of DSP filters. Both techniques use a single checksum variable for error detection. This gives a distance two code that is traditionally good for error detection but not correction. In this paper, such a code is used to improve SNR rather than perfectly remove the error. The first technique, 'checksum-based probabilistic error correction', uses the value indicated by the checksum variable to probabilistically correct the error and achieves up to 5 dB improvement in the SNR value. The second technique, 'state restoration', works well when the length of burst errors is small and the error magnitude is large. A general error statistics has been defined as a random process and the distribution of SNR is compared for the two proposed techniques",2006,0,
792,793,Simulation of Elman Neural network Extension Strategy Generator to Pattern Deformation Error in Flexibility Material Treating Field,"After analyzing flexibility material processing (such as quilting processing) influencing factor of pattern deformation, the edges of the original image and the deformation image are extracted. Then they are changed into coordinate. On top of it, the data are put into the Elman neural networks to train which has been built and the original image is used to as the teacher signal to tutoring. At last, the matter extenics model is built qua the database's data of the extension decision strategy generator by analysis the simulation result.",2008,0,
793,794,The dual parameterization approach to optimal least square FIR filter design subject to maximum error constraints,"This paper is concerned with the design of linear-phase finite impulse response (FIR) digital filters for which the weighted least square error is minimized, subject to maximum error constraints. The design problem is formulated as a semi-infinite quadratic optimization problem. Using a newly developed dual parameterization method in conjunction with the Caratheodory's dimensional theorem, an equivalent dual finite dimensional optimization problem is obtained. The connection between the primal and the dual problems is established. A computational procedure is devised for solving the dual finite dimensional optimization problem. The optimal solution to the primal problem can then be readily obtained from the dual optimal solution. For illustration, examples are solved using the proposed computational procedure",2000,0,
794,795,Model of Reliability of the Software with Coxian Distribution of Length of Intervals between the Moments of Detection of Errors,"The generalized software reliability model on the basis of nonstationary Markovian system of service is proposed. Approximation by distribution of Cox allows investigating growth of software reliability for any kinds of distribution of time between the moments of detection of errors and exponential distributions of time of their correction. The model allows receiving the forecast of important characteristics: the number of the corrected and not corrected errors, required time of debugging, etc. The diagram of transitions between states of the generalized model and system of the differential equations are presented. The example of calculation with use of the offered model is considered, research of influence of variation coefficient of Cox distribution of duration of intervals between the error detection moments on values of look-ahead characteristics is executed.",2010,0,
795,796,Managing Faults in the Service Delivery Process of Service Provider Coalitions,"In recent years, IT Service Management (ITSM) has become one of the most researched areas of IT. Incident Management and Problem Management form the basis of the tooling provided by an Incident Ticket System (ITS). As more compound or interdependent services are collaboratively offered by providers, the delivery of a service therefore becomes a responsibility of more than one provider's organization. In the ITS systems of various providers seemingly unrelated tickets are created and the connection between them is not realized automatically. The introduction of automation will reduce human involvement and time required for incident resolution.In this paper we consider a collaborative service delivery model that supports both per-request services and continuous high-availability services. In the case of high availability service the information stored in the ITS of the provider often includes information on the outage of a particular service rather than on the failure of a particular request. In this paper we offer an information model that consolidates and supports inter-organizational incident management and probabilistic model for fault discovery.",2009,0,
796,797,Adaptive partition size temporal error concealment for H.264,"Existing temporal error concealment methods for H.264 often decide the partition size of the lost macroblock (MB) before recovering the motion information, without actual quality comparison between different partition modes. In this paper, we propose to select the best partition mode by minimizing the Weighted Double-Sided External Boundary Matching Error (WDS-EBME), which jointly measures the inter-MB boundary discontinuity, inter-partition boundary discontinuity and intrapartition block artifacts in the recovered MB. The proposed method estimates the best motion vectors for each of the candidate partition modes, calculates the overall WDS-EBME values for them, and selects the partition mode with the smallest overall WDS-EBME to recover the lost MB. We also propose a progressive concealment order for the 4times4 partition mode. Test results show that the adaptive partition size method always outperforms the fixed partition size methods. Both the adaptive and fixed partition size methods are much superior to the temporal error concealment (TEC) method in the H.264 reference software.",2008,0,
797,798,A fault-tolerance mechanism in grid,"Grid appears as an effective technology coupling geographically distributed resources for solving large-scale problems in the wide area network. Fault tolerance in grid system is a significant and complex issue to secure a stable and reliable performance. Until now, various techniques exist for detecting and correcting faults in distributed computing systems. Unfortunately, few energy focus on fault-tolerance in grid environment, especially with the emergence of OGSA. A new fault-tolerant mechanism is needed to detect and recover service faults and nodes crash. Based on our previous work on Java threads state capturing and existing mobile agent techniques, we put forward a fault-tolerant mechanism providing effective fault-handling and recovering methods.",2003,0,
798,799,Building a Transformer Defects Database for UHF Partial Discharge Diagnostics,"In the case of a defective transformer, when a partial discharge is detected and recorded, critical information can be deduced from its pattern, such as the type of defect, its criticality or even information on the level of degradation of the insulation. This information can help to determine the remaining life of the transformer and thus provide criteria for its maintenance and operation. In this paper different artificial PD patterns will be recorded in the laboratory, representative of specific transformer defects, in order to build a database for comparison purposes when measuring on-line. This can greatly improve the recognition and identification of the defect and thus help take some important life assessment conclusions on the transformer.",2007,0,
799,800,RMS bounds and sample size considerations for error estimation in linear discriminant analysis,"The validity of a classifier depends on the precision of the error estimator used to estimate its true error. This paper considers the necessary sample size to achieve a given validity measure, namely RMS, for resubstitution and leave-one-out error estimators in the context of LDA. It provides bounds for the RMS between the true error and both the resubstitution and leave-one-out error estimators in terms of sample size and dimensionality. These bounds can be used to determine the minimum sample size in order to obtain a desired estimation accuracy, relative to RMS. To show how these results can be used in practice, a microarray classification problem is presented.",2010,0,
800,801,Icon based error concealment for JPEG and JPEG 2000 images,"This paper describes methods to recover the useful data in JPEG and JPEG 2000 compressed images and to estimate data for those portions of the image where correct data cannot be recovered. These techniques are designed to handle the loss of hundreds of bytes in the file. No use is made of restart markers or other optional error detection features of JPEG and JPEG 2000, but an uncorrupted low resolution version of the image, such as an icon, is assumed to be available. These icons are typically present in Exif or JFIF format JPEG files.",2003,0,
801,802,An efficient spatial domain error concealment method for H.264 video,"This paper presents an efficient spatial domain error concealment method for the forthcoming video coding standard H.264. In H.264, a frame is divided into 44 blocks during the encoding procedure. For natural image signal, the blocks are smoothly connected with each other. Based on this property, a linear smoothness constraint equation that describes the connection of the lost block and its neighboring blocks can be constructed. By solving this equation, the coefficients of lost block can be recovered. Because the reconstructed high frequency coefficients may be affected by noise, the recovered center pixel may have obvious error. To eliminate the error, we use the recovered pixels that are on the boundaries of the lost block and average pixel difference to interpolate the center pixels. The implementation is simple and is suitable for real-time video application. Experimental results show our method has better recovery result than conventional approach.",2003,0,
