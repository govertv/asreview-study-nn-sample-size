,record_id,title,abstract,year,label_included,duplicate_record_id
0,1,Using Developer Information as a Factor for Fault Prediction,"We have been investigating different prediction models to identify which files of a large multi-release industrial software system are most likely to contain the largest numbers of faults in the next release. To make predictions we considered a number of different file characteristics and change information about the files, and have built fully- automatable models that do not require that the user have any statistical expertise. We now consider the effect of adding developer information as a prediction factor and assess the extent to which this affects the quality of the predictions.",2007,1,
1,2,Reliability analysis of protective relays in fault information processing system in China,"The reliability indices of protective relays are first put forward in this paper. A Markov probability model is then established to evaluate the reliability of relay protection. With the state space analytical method, all the steady state probabilities and state transition probabilities can be calculated utilizing the data stored in the fault information processing system. We can get an equation that represents the influence of routine test intervals on relay unavailability. Based on this, the optimum routine test interval for protective relays can be determined. This paper also proposes an efficient method of processing large amount of information by the fault information processing system and evaluating the reliability of protective relays with it, and the corresponding software package is also developed. The application of it to an actual power system in China proves the method to be correct and effective",2006,0,
2,3,Test effort optimization by prediction and ranking of fault-prone software modules,"Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using ID3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the NASA projects data set of PROMOSE repository.",2010,1,
3,4,A Rough Set Model for Software Defect Prediction,High assurance software requires extensive and expensive assessment. Many software organizations frequently do not allocate enough resources for software quality. We research the defect detectors focusing on the data sets of software defect prediction. A rough set model is presented to deal with the attributes of data sets of software defect prediction in this paper. Appling this model to the most famous public domain data set created by the NASA's metrics data program shows its splendid performance.,2008,1,
4,5,Using Faults-Slip-Through Metric as a Predictor of Fault-Proneness,"Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (Naive Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based techniques (genetic programming and artificial immune recognition systems) on FST data collected from two large industrial projects from the telecommunication domain. Results: Using area under the receiver operating characteristic (ROC) curve and the location of (PF, PD) pairs in the ROC space, the faults slip-through metric showed impressive results with the majority of the techniques for predicting fault-prone modules at both integration and system test levels. There were, however, no statistically significant differences between the performance of different techniques based on AUC, even though certain techniques were more consistent in the classification performance at the two test levels. Conclusions: We can conclude that the faults-slip-through metric is a potentially strong predictor of fault-proneness at integration and system test levels. The faults-slip-through measurements interact in ways that is conveniently accounted for by majority of the data mining techniques.",2010,1,
5,6,Reducing Features to Improve Bug Prediction,"Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.",2009,1,
6,7,Variance Analysis in Software Fault Prediction Models,"Software fault prediction models play an important role in software quality assurance. They identify software subsystems (modules,components, classes, or files) which are likely to contain faults. These subsystems, in turn, receive additional resources for verification and validation activities. Fault prediction models are binary classifiers typically developed using one of the supervised learning techniques from either a subset of the fault data from the current project or from a similar past project. In practice, it is critical that such models provide a reliable prediction performance on the data not used in training. Variance is an important reliability indicator of software fault prediction models. However, variance is often ignored or barely mentioned in many published studies. In this paper, through the analysis of twelve data sets from a public software engineering repository from the perspective of variance, we explore the following five questions regarding fault prediction models: (1) Do different types ofclassification performance measures exhibit different variance? (2) Does the size of the data set imply a more (or less) accurate prediction performance? (3) Does the size of training subset impact model's stability? (4) Do different classifiers consistently exhibit different performance in terms of model's variance? (5) Are there differences between variance from 1000 runs and 10 runs of 10-fold cross validation experiments? Our results indicate that variance is a very important factor in understanding fault prediction models and we recommend the best practice for reporting variance in empirical software engineering studies.",2009,1,
7,8,Evaluating Defect Prediction Models for a Large Evolving Software System,"A plethora of defect prediction models has been proposed and empirically evaluated, often using standard classification performance measures. In this paper, we explore defect prediction models for a large, multi-release software system from the telecommunications domain. A history of roughly 3 years is analyzed to extract process and static code metrics that are used to build several defect prediction models with random forests. The performance of the resulting models is comparable to previously published work. Furthermore, we develop a new evaluation measure based on the comparison to an optimal model.",2009,1,
8,9,Application of neural network for predicting software development faults using object-oriented design metrics,"In this paper, we present the application of neural network for predicting software development faults including object-oriented faults. Object-oriented metrics can be used in quality estimation. In practice, quality estimation means either estimating reliability or maintainability. In the context of object-oriented metrics work, reliability is typically measured as the number of defects. Object-oriented design metrics are used as the independent variables and the number of faults is used as dependent variable in our study. Software metrics used include those concerning inheritance measures, complexity measures, coupling measures and object memory allocation measures. We also test the goodness of fit of neural network model by comparing the prediction result for software faults with multiple regression model. Our study is conducted on three industrial real-time systems that contain a number of natural faults that has been reported for three years (Mei-Huei Tang et al., 1999).",2002,1,
9,10,Using the Conceptual Cohesion of Classes for Fault Prediction in Object-Oriented Systems,"High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.",2008,1,
10,11,Software Fault Prediction using Language Processing,"Accurate prediction of faulty modules reduces the cost of software development and evolution. Two case studies with a language-processing based fault prediction measure are presented. The measure, refereed to as a QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgements of software quality. The two case studies consider the measure's application to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and QALP score. Results, while complex, show that little correlation exists in the first case study, while statistically significant correlations exists in the second. In this second study the QALP score is helpful in predicting faults in modules (files) with its usefulness growing as module size increases.",2007,1,
11,12,Visualization of test information to assist fault localization,"One of the most expensive and time-consuming components of the debugging process is locating the errors or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. The paper presents a technique that uses visualization to assist with these tasks. The technique uses color to visually map the participation of each program statement in the outcome of the execution of the program with a test suite, consisting of both passed and failed test cases. Based on this visual mapping, a user can inspect the statements in the program, identify statements involved in failures, and locate potentially faulty statements. The paper also describes a prototype tool that implements our technique along with a set of empirical studies that use the tool for evaluation of the technique. The empirical studies show that, for the subject we studied, the technique can be effective in helping a user locate faults in a program.",2002,1,
12,13,Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization,"Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization (ADMPSO) based on the PSO classification technique. ADMPSO can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.",2010,1,
13,14,Use of relative code churn measures to predict system defect density,"Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models, we show that while absolute measures of code chum are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.",2005,1,
14,15,Predicting defects in SAP Java code: An experience report,"Which components of a large software system are the most defect-prone? In a study on a large SAP Java system, we evaluated and compared a number of defect predictors, based on code features such as complexity metrics, static error detectors, change frequency, or component imports, thus replicating a number of earlier case studies in an industrial context. We found the overall predictive power to be lower than expected; still, the resulting regression models successfully predicted 50-60% of the 20% most defect-prone components.",2009,1,
15,16,Empirical assessment of machine learning based software defect prediction techniques,"The wide-variety of real-time software systems, including telecontrol/telepresence systems, robotic systems, and mission planning systems, can entail dynamic code synthesis based on runtime mission-specific requirements and operating conditions. This necessitates the need for dynamic dependability assessment to ensure that these systems perform as specified and not fail in catastrophic ways. One approach in achieving this is to dynamically assess the modules in the synthesized code using software defect prediction techniques. Statistical models; such as stepwise multi-linear regression models and multivariate models, and machine learning approaches, such as artificial neural networks, instance-based reasoning, Bayesian-belief networks, decision trees, and rule inductions, have been investigated for predicting software quality. However, there is still no consensus about the best predictor model for software defects. In this paper; we evaluate different predictor models on four different real-time software defect data sets. The results show that a combination of IR and instance-based learning along with the consistency-based subset evaluation technique provides a relatively better consistency in accuracy prediction compared to other models. The results also show that ""size"" and ""complexity"" metrics are not sufficient for accurately predicting real-time software defects.",2005,1,
16,17,Cost Curve Evaluation of Fault Prediction Models,"Prediction of fault prone software components is one of the most researched problems in software engineering. Many statistical techniques have been proposed but there is no consensus on the methodology to select the ""best model"" for the specific project. In this paper, we introduce and discuss the merits of cost curve analysis of fault prediction models. Cost curves allow software quality engineers to introduce project-specific cost of module misclassification into model evaluation. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Through the analysis of sixteen projects from public repositories, we observe that software quality does not necessarily benefit from the prediction of fault prone components. The inclusion of misclassification cost in model evaluation may indicate that even the ""best"" models achieve performance no better than trivial classification. Our results support a recommendation to adopt cost curves as one of the standard methods for software quality model performance evaluation.",2008,1,
17,18,A practical method for the software fault-prediction,"In the paper, a novel machine learning method, SimBoost, is proposed to handle the software fault-prediction problem when highly skewed datasets are used. Although the method, proved by empirical results, can make the datasets much more balanced, the accuracy of the prediction is still not satisfactory. Therefore, a fuzzy-based representation of the software module fault state has been presented instead of the original faulty/non-faulty one. Several experiments were conducted using datasets from NASA Metrics Data Program. The discussion of the results of experiments is provided.",2007,1,
18,19,An investigation of the relationships between lines of code and defects,"It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.",2009,1,
19,20,Predicting the location and number of faults in large software systems,"Advance knowledge of which files in the next release of a large software system are most likely to contain the largest numbers of faults can be a very valuable asset. To accomplish this, a negative binomial regression model has been developed and used to predict the expected number of faults in each file of the next release of a system. The predictions are based on the code of the file in the current release, and fault and modification history of the file from previous releases. The model has been applied to two large industrial systems, one with a history of 17 consecutive quarterly releases over 4 years, and the other with nine releases over 2 years. The predictions were quite accurate: for each release of the two systems, the 20 percent of the files with the highest predicted number of faults contained between 71 percent and 92 percent of the faults that were actually detected, with the overall average being 83 percent. The same model was also used to predict which files of the first system were likely to have the highest fault densities (faults per KLOC). In this case, the 20 percent of the files with the highest predicted fault densities contained an average of 62 percent of the system's detected faults. However, the identified files contained a much smaller percentage of the code mass than the files selected to maximize the numbers of faults. The model was also used to make predictions from a much smaller input set that only contained fault data from integration testing and later. The prediction was again very accurate, identifying files that contained from 71 percent to 93 percent of the faults, with the average being 84 percent. Finally, a highly simplified version of the predictor selected files containing, on average, 73 percent and 74 percent of the faults for the two systems.",2005,1,
20,21,Predictive data mining model for software bug estimation using average weighted similarity,"Software bug estimation is a very essential activity for effective and proper software project planning. All the software bug related data are kept in software bug repositories. Software bug (defect) repositories contains lot of useful information related to the development of a project. Data mining techniques can be applied on these repositories to discover useful interesting patterns. In this paper a prediction data mining technique is proposed to predict the software bug estimation from a software bug repository. A two step prediction model is proposed In the first step bug for which estimation is required, its summary and description is matched against the summary and description of bugs available in bug repositories. A weighted similarity model is suggested to match the summary and description for a pair of software bugs. In the second step the fix duration of all the similar bugs are calculated and stored and its average is calculated, which indicates the predicted estimation of a bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.",2010,1,
21,22,A Comparative Study of Ensemble Feature Selection Techniques for Software Defect Prediction,"Feature selection has become the essential step in many data mining applications. Using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. We present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly-used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 13,600 classification models. Experimental results indicate that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.",2010,1,
22,23,Developing fault predictors for evolving software systems,"Over the past several years, we have been developing methods of predicting the fault content of software systems based on measured characteristics of their structural evolution. In previous work, we have shown there is a significant linear relationship between code churn, a synthesized metric, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code churn. We have begun a new investigation of this relationship with a flight software technology development effort at the jet propulsion laboratory (JPL) and have progressed in resolving the limitations of the earlier work in two distinct steps. First, we have developed a standard for the enumeration of faults. Second, we have developed a practical framework for automating the measurement of these faults. we analyze the measurements of structural evolution and fault counts obtained from the JPL flight software technology development effort. Our results indicate that the measures of structural attributes of the evolving software system are suitable for forming predictors of the number of faults inserted into software modules during their development. The new fault standard also ensures that the model so developed has greater predictive validity.",2003,1,
23,24,Locating where faults will be [software testing],"The goal of this research is to allow software developers and testers to become aware of which files in the next release of a large software system are likely to contain the largest numbers of faults or the highest fault densities in the next release, thereby allowing testers to focus their efforts on the most fault-prone files. This is done by developing a negative binomial regression model to help predict characteristics of new releases of a software system, based on information collected about prior releases and the new release under development. The same prediction model was also used to allow a tester to select the files of a new release that collectively contain any desired percentage of the faults. The benefit of being able to make these sorts of predictions accurately should be clear: if we know where to look for bugs, we should be able to target our testing efforts there and, as a result, find problems more quickly and therefore more economically. Two case studies using large industrial software systems are summarized. The first study used seventeen consecutive releases of a large inventory system, representing more than four years of field exposure. The second study used nine releases of a service provisioning system with two years of field experience.",2005,1,
24,25,Estimating software fault-proneness for tuning testing activities,"The article investigates whether a correlation exists between the fault-proneness of software and the measurable attributes of the code (i.e. the static metrics) and of the testing (i.e. the dynamic metrics). The article also studies how to use such data for tuning the testing process. The goal is not to find a general solution to the problem (a solution may not even exist), but to investigate the scope of specific solutions, i.e., to what extent homogeneity of the development process, organization, environment and application domain allows data computed on past projects to be projected onto new projects. A suitable variety of case studies is selected to investigate a methodology applicable to classes of homogeneous products, rather than investigating if a specific solution exists for few cases.",2000,1,
25,26,The Application of Gray-Prediction Theory in the Software Defects Management,"Software defects are the parts of software products that software development company have to face. How to deal with them suitably is very important for software company's survival. This article will use the collecting data of software defects. Then, according to GM (1, 1), which is the core theory of the gray-prediction, establish the prediction model. Finally, we gain the prediction values. The results show that software company can improve software quality, control development process and allocate resources effectively.",2009,1,
26,27,Tree-based software quality estimation models for fault prediction,"Complex high-assurance software systems depend highly on reliability of their underlying software applications. Early identification of high-risk modules can assist in directing quality enhancement efforts to modules that are likely to have a high number of faults. Regression tree models are simple and effective as software quality prediction models, and timely predictions from such models can be used to achieve high software reliability. This paper presents a case study from our comprehensive evaluation (with several large case studies) of currently available regression tree algorithms for software fault prediction. These are, CART-LS (least squares), S-PLUS, and CART-LAD (least absolute deviation). The case study presented comprises of software design metrics collected from a large network telecommunications system consisting of almost 13 million lines of code. Tree models using design metrics are built to predict the number of faults in modules. The algorithms are also compared based on the structure and complexity of their tree models. Performance metrics, average absolute and average relative errors are used to evaluate fault prediction accuracy.",2002,1,
27,28,Predicting fault prone modules by the Dempster-Shafer belief networks,"This paper describes a novel methodology for predicting fault prone modules. The methodology is based on Dempster-Shafer (D-S) belief networks. Our approach consists of three steps: first, building the D-S network by the induction algorithm; second, selecting the predictors (attributes) by the logistic procedure; third, feeding the predictors describing the modules of the current project into the inducted D-S network and identifying fault prone modules. We applied this methodology to a NASA dataset. The prediction accuracy of our methodology is higher than that achieved by logistic regression or discriminant analysis on the same dataset.",2003,1,
28,29,Building a genetically engineerable evolvable program (GEEP) using breadth-based explicit knowledge for predicting software defects,"There has been extensive research in the area of data mining over the last decade, but relatively little research in algorithmic mining. Some researchers shun the idea of incorporating explicit knowledge with a Genetic Program environment. At best, very domain specific knowledge is hard wired into the GP modeling process. This work proposes a new approach called the Genetically Engineerable Evolvable Program (GEEP). In this approach, explicit knowledge is made available to the GP. It is considered breadth-based, in that all pieces of knowledge are independent of each other. Several experiments are performed on a NASA-based data set using established equations from other researchers in order to predict software defects. All results are statistically validated.",2004,1,
29,30,Applying Novel Resampling Strategies To Software Defect Prediction,"Due to the tremendous complexity and sophistication of software, improving software reliability is an enormously difficult task. We study the software defect prediction problem, which focuses on predicting which modules will experience a failure during operation. Numerous studies have applied machine learning to software defect prediction; however, skewness in defect-prediction datasets usually undermines the learning algorithms. The resulting classifiers will often never predict the faulty minority class. This problem is well known in machine learning and is often referred to as learning from unbalanced datasets. We examine stratification, a widely used technique for learning unbalanced data that has received little attention in software defect prediction. Our experiments are focused on the SMOTE technique, which is a method of over-sampling minority-class examples. Our goal is to determine if SMOTE can improve recognition of defect-prone modules, and at what cost. Our experiments demonstrate that after SMOTE resampling, we have a more balanced classification. We found an improvement of at least 23% in the average geometric mean classification accuracy on four benchmark datasets.",2007,1,
30,31,Transparent combination of expert and measurement data for defect prediction: an industrial case study,"Defining strategies on how to perform quality assurance (QA) and how to control such activities is a challenging task for organizations developing or maintaining software and software-intensive systems. Planning and adjusting QA activities could benefit from accurate estimations of the expected defect content of relevant artifacts and the effectiveness of important quality assurance activities. Combining expert opinion with commonly available measurement data in a hybrid way promises to overcome the weaknesses of purely data-driven or purely expert-based estimation methods. This article presents a case study of the hybrid estimation method HyDEEP for estimating defect content and QA effectiveness in the telecommunication domain. The specific focus of this case study is the use of the method for gaining quantitative predictions. This aspect has not been empirically analyzed in previous work. Among other things, the results show that for defect content estimation, the method performs significantly better statistically than purely data-based methods, with a relative error of 0.3 on average (MMRE).",2010,1,
31,32,Software defect association mining and defect correction effort prediction,"Much current software defect prediction work focuses on the number of defects remaining in a software system. In this paper, we present association rule mining based methods to predict defect associations and defect correction effort. This is to help developers detect software defects and assist project managers in allocating testing resources more effectively. We applied the proposed methods to the SEL defect data consisting of more than 200 projects over more than 15 years. The results show that, for defect association prediction, the accuracy is very high and the false-negative rate is very low. Likewise, for the defect correction effort prediction, the accuracy for both defect isolation effort prediction and defect correction effort prediction are also high. We compared the defect correction effort prediction method with other types of methods - PART, C4.5, and Naive Bayes - and show that accuracy has been improved by at least 23 percent. We also evaluated the impact of support and confidence levels on prediction accuracy, false-negative rate, false-positive rate, and the number of rules. We found that higher support and confidence levels may not result in higher prediction accuracy, and a sufficient number of rules is a precondition for high prediction accuracy.",2006,1,
32,33,Assessing the applicability of fault-proneness models across object-oriented software projects,"A number of papers have investigated the relationships between design metrics and the detection of faults in object-oriented software. Several of these studies have shown that such models can be accurate in predicting faulty classes within one particular software product. In practice, however, prediction models are built on certain products to be used on subsequent software development projects. How accurate can these models be, considering the inevitable differences that may exist across projects and systems? Organizations typically learn and change. From a more general standpoint, can we obtain any evidence that such models are economically viable tools to focus validation and verification effort? This paper attempts to answer these questions by devising a general but tailorable cost-benefit model and by using fault and design data collected on two mid-size Java systems developed in the same environment. Another contribution of the paper is the use of a novel exploratory analysis technique - MARS (multivariate adaptive regression splines) to build such fault-proneness models, whose functional form is a-priori unknown. The results indicate that a model built on one system can be accurately used to rank classes within another system according to their fault proneness. The downside, however, is that, because of system differences, the predicted fault probabilities are not representative of the system predicted. However, our cost-benefit model demonstrates that the MARS fault-proneness model is potentially viable, from an economical standpoint. The linear model is not nearly as good, thus suggesting a more complex model is required.",2002,1,
33,34,A Novel Evaluation Method for Defect Prediction in Software Systems,"In this paper, we propose a novel evaluation method for defect prediction in object-oriented software systems. For each metric to evaluate, we start by applying it to the dependency graph extracted from the target software system, and obtain a list of classes ordered by their predicted degree of defect under that metric. By utilizing the actual defect data mined from the subversion database, we evaluate the quality of each metric through means of a weighted reciprocal ranking mechanism. Our method can tell not only the overall quality of each evaluated metric, but also the quality of the prediction result for each class, especially those costly ones. Evaluation results and analysis show the efficiency and rationality of our method.",2010,1,
34,35,Merits of using repository metrics in defect prediction for open source projects,Many corporate code developers are the beta testers of open source software. They continue testing until they are sure that they have a stable version to build their code on. In this respect defect predictors play a critical role to identify defective parts of the software. Performance of a defect predictor is determined by correctly finding defective parts of the software without giving any false alarms. Having high false alarms means testers/developers would inspect bug free code unnecessarily. Therefore in this research we focused on decreasing the false alarm rates by using repository metrics. We conducted experiments on the data sets of Eclipse project. Our results showed that repository metrics decreased the false alarm rates on the average to 23% from 32% corresponding up to 907 less files to inspect.,2009,1,
35,36,Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods,"We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.",2007,1,
36,37,A study on fault-proneness detection of object-oriented systems,"Fault-proneness detection in object-oriented systems is an interesting area for software companies and researchers. Several hundred metrics have been defined with the aim of measuring the different aspects of object-oriented systems. Only a few of them have been validated for fault detection, and several interesting works with this view have been considered. This paper reports a research study starting from the analysis of more than 200 different object-oriented metrics extracted from the literature with the aim of identifying suitable models for the detection of the fault-proneness of classes. Such a large number of metrics allows the extraction of a subset of them in order to obtain models that can be adopted for fault-proneness detection. To this end, the whole set of metrics has been classified on the basis of the measured aspect in order to reduce them to a manageable number; then, statistical techniques were employed to produce a hybrid model comprised of 12 metrics. The work has focused on identifying models that can detect as many faulty classes as possible and, at the same time, that are based on a manageably small set of metrics. A compromise between these aspects and the classification correctness of faulty and non-faulty classes was the main challenge of the research. As a result, two models for fault-proneness class detection have been obtained and validated",2001,1,
37,38,Fault Prediction using Early Lifecycle Data,"The prediction of fault-prone modules in a software project has been the topic of many studies. In this paper, we investigate whether metrics available early in the development lifecycle can be used to identify fault-prone software modules. More precisely, we build predictive models using the metrics that characterize textual requirements. We compare the performance of requirements-based models against the performance of code-based models and models that combine requirement and code metrics. Using a range of modeling techniques and the data from three NASA projects, our study indicates that the early lifecycle metrics can play an important role in project management, either by pointing to the need for increased quality monitoring during the development or by using the models to assign verification and validation activities.",2007,1,
38,39,Naive Bayes Software Defect Prediction Model,"Although the value of using static code attributes to learn defect predictor has been widely debated, there is no doubt that software defect predictions can effectively improve software quality and testing efficiency. Many data mining methods have already been introduced into defect predictions. We noted there have several versions of defect predictor based on Naive Bayes theory, and analyzed their difference estimation method and algorithm complexity. We found the best one which is Multi- variants Gauss Naive Bayes (MvGNB) by performing prediction performance evaluation, and we compared this model with decision tree learner J48. Experiment results on the benchmarking data sets of MDP made us believe that MvGNB would be useful for defect predictions.",2010,1,
39,40,Mutual Fault-tolerant and Standby SCADA System Based on MAS for Multi-area Centralized Control Centers,The general policies to construct the mutual fault- tolerant and standby SCADA system based on multi-agent technology for multi- area centralized control centers were presented in the paper in order to raise the safety and operational reliability of the power grid without additional equipment investment. The economic efficiency and feasibility of the system construction based on the policies are analyzed. The architecture of MAS and the function design of the Agents are introduced in detail and the specific implementation scheme and the corresponding key technologies are elucidated. The data and application fault-tolerance of SCADA system is realized to guarantee the reliability and continuity of the power grid operation.,2006,0,
40,41,Fault Management Driven Design with Safety and Security Requirements,"This paper exemplifies principles of embedded system design that props safety and security using operational errors management in frame of a dedicated Computer-Based System architecture. After reviewing basic principles of Cyber-Physical Systems as a novel slant (or marker?) to modeling and design in this domain, attention is focused on a real-world solution of a safety and security critical embedded system application offering genuine demonstration of that approach. The contribution stresses those features that distinguish the real project from a demonstration case study.",2010,0,
41,42,Reduction of faults in software testing by fault domination,"Although mutation testing is one of the practical ways of enhancing test effectiveness in software testing, it could be sometimes infeasible in practical work for a large scale software so that the mutation testing becomes time-consuming and even in prohibited time. Therefore, the number of faults assumed to exist in the software under test should be reduced so as to be able to confine the time complexity of test within a reasonable period of time. This paper utilizes the concept of fault dominance and equivalence, which has long been employed in hardware testing, for revealing a novel way of reducing the number of faults assumed to hide in software systems. Once the number of faults assumed in software is decreased sharply, the effectiveness of mutation testing will be greatly enhanced and become a feasible way of software testing. Examples and experimental results are presented to illustrate the effectiveness and the helpfulness of the technology proposed in the paper.",2007,0,
42,43,Electrical Test Structures for the Characterisation of Optical Proximity Correction,"Simple electrical test structures have been designed that will allow the characterisation of corner serif forms of optical proximity correction. The structures measure the resistance of a short length of conducting track with a right angled corner. Varying amounts of OPC can be applied to the outer and inner corners of the feature and the effect on the resistance of the track measured. These structures have been simulated and the results are presented in this paper. In addition a preliminary test mask has been fabricated which has test structures suitable for on-mask electrical measurement. Measurement results from these structures are also presented. Furthermore structures have been characterised using an optical microscope, a dedicated optical mask metrology system, an AFM scanner and finally a FIB system. In the future the test mask will be used to print the structures using a step and scan lithography tool so that they can be measured on-wafer. Correlation of the mask and wafer results will provide a great deal of information about the effects of OPC at the CAD level and the impact on the final printed features.",2007,0,
43,44,A real-time fault diagnosis system for UPS based on FFT frequency analysis,"UPS provides emergency power when utility power is not available, so the reliability of UPS is more important than inverter drive systems. In this paper, a fault diagnosis system for UPS is proposed using FFT frequency analysis of output current of inverter side of UPS under linear and nonlinear load conditions. Software PLL for precise synchronization of one period sampling and double buffer memory for real time processing are proposed. Experimental results show the increase of even harmonics including dc offset in case of fault conditions such as increase of resistance and delay or misfiring of IGBT turn-on, and prove the possibility of UPS fault diagnosis system if the criteria for fault decision are well defined.",2010,0,
44,45,5B: emerging technologies - reliable and fault-tolerant wireless sensor networks,"Wireless sensor networks create invisible interconnections with the physical world for the measurement, monitoring, and management of data from multiple sensors and probes with little constraint on location. These networks provide distributed processing, data storage, wireless communication, and dedicated application software with high reliability, inherent redundancy, failure-tolerant security and easily encrypted privacy. They have enormous potential to transform our society and are subjects of intense current research and application development. Three enabling hardware technologies which constitute a network node are microprocessors, MEMS sensors, and low-power radios. Sensor networks represent the paradigm shift in computing where they anticipate our needs and sometimes act on our behalf. The objective of this presentation is to discuss the reliable and fault-tolerant wireless sensor networks, focusing on environmental, behavioral, and biomedical areas. Special focus will be on wearable monitors and body wireless sensor network. An example of physiological monitoring by body area network will be discussed.",2005,0,
45,46,Application method of wavelets in the fault diagnosis of motion system,"In motion system, many types of faults are related with the abnormity of torque signal. A method was presented which was based on the wavelets function. The compactly supported orthonormal wavelets were introduced. Under it, the fault could be detected, and the type of fault could be diagnosed as well. For using convenience, the flow chart of using this method was also offered. On X-Y motion control system, collision experiments were implemented for test the given method. Using the sampled torque signals, the practicability was verified from the clear diagnosis of different types of collisions.",2008,0,
46,47,Fault detection in Flexible Assembly Systems using Petri net,"A significant part of the activities in a manufacturing system involve assembly tasks. Nowadays, these tasks are object of automation due to the market increasing demand for quality, productivity and variety of the products. Consequently, the automation of assembly systems should consider flexibility to face product diversification, functionalities, delivery times, and volumes involved. However, these systems are vulnerable to faults due to the characteristic of their mechanism and the complex interaction among their control devices. In this context, the present work is focused on the modeling design of flexible assembly systems control, including the occurrence of faults. The proposed method structures a sequence of steps for the models construction of assembly processes and their fault detection, based on the theory of discrete events systems and Petri net. This work use in special, production flow schema/mark flow graph (PFS/MFG) technique to describe and model the flexible assembly systems control through a rational and systematic procedure, as well as, the processes data record based on quantitative techniques for fault detection. This approach is applied to a flexible assembly systems installed and in operation to compare the effectiveness of the developed procedure.",2008,0,
47,48,An Evaluation of Similarity Coefficients for Software Fault Localization,"Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important technique for the development of dependable software. In this paper we study different similarity coefficients that are applied in the context of a program spectral approach to software fault localization (single programming mistakes). The coefficients studied are taken from the systems diagnosis/automated debugging tools Pinpoint, Tarantula, and AMPLE, and from the molecular biology domain (the Ochiai coefficient). We evaluate these coefficients on the Siemens Suite of benchmark faults, and assess their effectiveness in terms of the position of the actual fault in the probability ranking of fault candidates produced by the diagnosis technique. Our experiments indicate that the Ochiai coefficient consistently outperforms the coefficients currently used by the tools mentioned. In terms of the amount of code that needs to be inspected, this coefficient improves 5% on average over the next best technique, and up to 30% in specific cases",2006,0,
48,49,Fault-Tolerance in Universal Middleware Bridge,Universal middleware bridge (UMB) provides seamless interoperation among heterogeneous home network middleware. There have been high demands for the UMB components (UMB core and adaptors) to have fault- tolerance capabilities. This paper presents a TMO structuring approach together with new implementation techniques for the fault-tolerant TMO-replica structuring scheme called PSTR. PSTR implementations of UMB components provide fault tolerance capabilities essential in realizing high reliability for the UMB facility.,2008,0,
49,50,Performance evaluation of a fault-tolerant irregular network,"In an attempt to improve the fault-tolerance of the Omega network, this paper examines the performance of the proposed Theta network (THN), and compares it with other networks having similar characteristics. The irregular nature of the network has the inherent advantage of improving the latency of the network. Analytical results exhibit the favorable performance of THN at low cost, making the reliability degrade gracefully with time, while maintaining full-access capability over a reasonably long time. We study methods for routing requests in the presence and absence of faulty components in THN, where 50% of the requests pass at the minimum path length of 2.",2002,0,
50,51,The effect of registration error on tracking distant augmented objects,"We conducted a user study of the effect of registration error on performance of tracking distant objects in augmented reality. Categorizing error by types that are often used as specifications, we hoped to derive some insight into the ability of users to tolerate noise, latency, and orientation error. We used measurements from actual systems to derive the parameter settings. We expected all three errors to influence userspsila ability to perform the task correctly and the precision with which they performed the task. We found that high latency had a negative impact on both performance and response time. While noise consistently interacted with the other variables, and orientation error increased user error, the differences between ldquohighrdquo and ldquolowrdquo amounts were smaller than we expected. Results of userspsila subjective rankings of these three categories of error were surprisingly mixed. Users believed noise was the most detrimental, though statistical analysis of performance refuted this belief. We interpret the results and draw insights for system design.",2008,0,
51,52,Improving Bug Assignment with Bug Tossing Graphs and Bug Similarities,"In open-source software development the bug report is usually assigned to a developer for bug fixing. A large number of bug reports are tossed (reassigned) to other developers, for example because the bugs have been assigned by mistake. The tossing events increase bug-fix time. In order to quickly identify the fixer to bug reports we present an approach based on the bug tossing history and textual similarities between bug reports. This proposed approach is evaluated on Eclipse and Mozilla. The results show that our approach can significantly improve the efficiency of bug assignment: the bug resolver is often identified with fewer tossing events.",2010,0,
52,53,Doppler estimation and correction for shallow underwater acoustic communications,"Reliable mobile underwater acoustic communication systems must compensate for strong, time-varying Doppler effects. Many Doppler correction techniques rely on a single bulk correction to compensate first-order effects. In many cases, residual higher-order effects must be tracked and corrected using other methods. The contributions of this paper are evaluations of (1) signal-to-noise ratio (SNR) performance from three Doppler estimation and correction methods and (2) communication performance of Doppler correction with static vs. adaptive equalizers. The evaluations use our publicly available shallow water experimental dataset, which consists of 360 packet transmission samples (each 0.5s long) from a five-channel receiver array.",2010,0,
53,54,Forward error protection for robust video streaming based on distributed video coding principles,"This paper proposes an error resilient coding scheme that employs distributed video coding tools. A bitstream, produced by any standard motion-compensated predictive codec (MPEG-x, H.26x), is sent over an error-prone channel. In addition, a Wyner-Ziv encoded auxiliary bitstream is sent as redundant information to serve as a forward error correction code. At the decoder side, error concealed reconstructed frames are used as side information by the Wyner-Ziv decoder, and the corrected frame is used as a reference by future frames, thus reducing drift. We explicitly target the problem of rate allocation at the encoder side, by estimating the channel induced distortion in the transform domain. Experimental results conducted over a simulated error-prone channel reveal that the proposed scheme has comparable or better performance than a scheme where forward error correction codes are used. Moreover the proposed solution shows good performance when compared to a scheme that uses the intra-macroblock refresh procedure.",2008,0,
54,55,Analysis on Interruption and Plane Layout of Shear Wall for Frame-Shear Wall Structure with Top Fault Shear Wall,"According to the force-deformation characteristics of frame-shear wall structure, the calculation and analysis model of the ""Style Box"" type layout of frame-shear wall structure is advanced with the basic and simple arrangement of frame-shear wall structure. The different plane and vertical layout with interrupting shear walls of the frame-shear wall structure with top fault shear walls is discussed primarily by using the method of plane layout and vertical interruptable position being considered at the same time. Based on the main parameters of the frame-shear wall structure with top fault shear walls in different ways, it puts forward the importance of the shear wall location of the plane layout to the overall performance of the frame-shear wall structure with top fault shear walls except the lateral stiffness of the shear wall corresponding to the shear wall interrupted ratio.",2009,0,
55,56,Behavioral modular description of fault tolerant distributed systems with AADL Behavioral Annex,"AADL is an architecture description language intended for model-based engineering of high-integrity distributed systems. The AADL Behavior Annex (AADL-BA) is an extension allowing the refinement of behavioral aspects described through an AADL architectural description. When implementing Distributed Real-time Embedded system (DRE), fault tolerance concerns are integrated by applying replication patterns. We considered a simplified design of the primary backup replication pattern as a running example to analyze the modeling capabilities of AADL and its annex. Our contribution lies in the identification of the drawbacks and benefits of this modeling language for accurate description of the synchronization mechanisms integrated in this example.",2010,0,
56,57,Reducing human error in simulation in General Motors,"We focus on the steps taken to minimize human error in simulation modeling in General Motors. While errors are costly and undesirable in any field, they are especially harmful in simulation which has been struggling to gain acceptance in the business world for a long time. The solution discussed can be summarized as ""enter the data once and use the best tool for the job"".",2003,0,
57,58,Analysis of pressure and Blanchard altitude errors computed using atmospheric data obtained from an F-18 aircraft flight,"Pressure altitude is commonly utilized as an altitude reference for an inertial navigation system (INS) to damp the error growth in the inherently unstable vertical channel. A precise altitude reference for use in the INS vertical channel can be obtained using the Blanchard algorithm, which computes altitude from atmospheric pressure, temperature, aircraft ground velocity, and wind velocity data. This paper computes both the pressure and Blanchard altitudes for an entire test flight of an F-18 aircraft from the atmospheric data measured during the flight. The flight repeats 4 cycles of a climb, level-off, dive, level-off trajectory. The altitude computed from GPS during flight is considered to be the truth altitude. The errors in the pressure and Blanchard altitudes are computed and compared. In addition both altitude errors are analyzed in order to determine the scale factor, bias offset, and time delay utilizing the least square error fit method. The Blanchard altitude is a much more precise altitude reference than pressure altitude during actual flight of an F-18 aircraft.",2002,0,
58,59,Residual error models for the SOLT and SOLR VNA calibration algorithms,"Uncertainty calculation of vector network analyzers (VNAs) using the SOLT or SOLR calibration algorithms is often performed using residual directivity, match and tracking. In the literature the uncertainty equations are often stated without a derivation from a proper model equation. In this paper we derive the model equations for both the SOLT and SOLR calibration, the two cases do not result in the same model equation. The results are also compared to the commonly used expressions for uncertainty in the EA guidelines for VNA evaluation. For one-port measurements our results confirm the expressions in the EA guide but for two-ports there are significant differences. The symbolically derived model equations are verified using numerical simulations.",2007,0,
59,60,A method for dead reckoning parameter correction in pedestrian navigation system,"This paper presents a method for correcting dead reckoning parameters, which are heading and step size, for a pedestrian navigation system. In this method, the compass bias error and the step size error can be estimated during the period that the Global Positioning System (GPS) signal is available. The errors are used for correcting those parameters to improve the accuracy of position determination using only the dead reckoning system when the GPS signal is not available. The results show that the parameters can be estimated with reasonable accuracy. Moreover, the method also helps to increase the positioning accuracy when the GPS signal is available.",2003,0,
60,61,Single-stage power factor correction converter with parallel power processing for wide line and load changes,"A new single-phase single-stage power factor correction converter with a simple auxiliary circuit is proposed. Using parallel power processing, this converter can be operated in wide line and load changes while limiting the link voltage below 400 V. Experimental results show that the measured power factor and efficiency are about 0.98 and 81%, respectively, at rated condition and the auxiliary circuit to reduce the link voltage is effective",2002,0,
61,62,Using variable-length error-correcting codes in MPEG-4 video,Reversible variable length (RVL) codes are used in MPEG-4 video coding to improve its error resilience. Algorithms used to design variable-length error-correcting (VLEC) codes are modified so as to construct efficient RVL codes with a smaller average length than those found in the literature. It is also shown that RVL codes are a special (weak) class of VLEC codes. Consequently more powerful VLEC codes can be used in the MPEG-4 codec and it is shown that performance gains of up to 20 dB in peak signal to noise ratio (PSNR) can be obtained using a soft-decision sequential decoder with relatively simple VLEC codes. This increase in performance is obtained at the expense of an order of magnitude increase in decoding complexity,2005,0,
62,63,Restoration of Directional Overcurrent Relay Coordination in Distributed Generation Systems Utilizing Fault Current Limiter,"A new approach is proposed to solve the directional overcurrent relay coordination problem, which arises from installing distributed generation (DG) in looped power delivery systems (PDS). This approach involves the implementation of a fault current limiter (FCL) to locally limit the DG fault current, and thus restore the original relay coordination. The proposed restoration approach is carried out without altering the original relay settings or disconnecting DGs from PDSs during fault. Therefore, it is applicable to both the current practice of disconnecting DGs from PDSs, and the emergent trend of keeping DGs in PDSs during fault. The process of selecting FCL impedance type (inductive or resistive) and its minimum value is illustrated. Three scenarios are discussed: no DG, the implementation of DG with FCL and without FCL. Various simulations are carried out for both single- and multi-DG existence, and different DG and fault locations. The obtained results are reported and discussed.",2008,0,
63,64,The Design Of Embedded Bus monitoring And Fault Diagnosis System Based On Protocol SAE J1939,"Embedded bus monitoring and fault diagnosis system, which was based on protocol SAE J1939 was designed in this paper. And this system took the 32-bit embedded one as a hardware platform, customized a WinCE6.0 operation system and used EVC as the tool to design the embedded application. The functions of CAN communication, protocol defamations etc were realized. Good human-computer interaction is developed and the system has already been applied on the bus.",2010,0,
64,65,A Fault Analysis and Classifier Framework for Reliability-Aware SRAM-Based FPGA Systems,"This paper presents a new framework for the analysis of SRAM-based FPGA systems with respect to their dependability properties against single, multiple and cumulative upsets errors. The aim is to offer an environment for performing fault classification and error propagation analyses for designed featuring fault detection or tolerance techniques against soft errors, where the focus is not only the overall achieved fault coverage, but an understanding of the fault/error relation inside the internal elements of the system. We propose a fault analyzer/classifier laying on top of a classical fault injection engine, used to monitor the evolution of the system after a fault as occurred, with respect to the applied reliability-oriented design technique. The paper introduces the framework and reports some experimental results of its application to a case study, to highlight the benefits of the proposed solution.",2009,0,
65,66,Stress wave analysis of turbine engine faults,"Stress Wave Analysis (SWAN) provides real-time measurement of friction and mechanical shock in operating machinery. This high frequency acoustic sensing technology filters out background levels of vibration and audible noise, and provides a graphic representation of machine health. By measuring shock and friction events, the SWAN technique is able to detect wear and damage at the earliest stages and is able to track the progression of a defect throughout the failure process. This is possible because as the damage progresses, the energy content of friction and shock events increases. This `stress wave energy' is then measured and tracked against normal machine operating conditions. This paper describes testing that was conducted on several types of aircraft and industrial gas turbine engines to demonstrate SWAN's ability to accurately detect a broad range of discrepant conditions and characterize the severity of damage",2000,0,
66,67,High-level vulnerability over space and time to insidious soft errors,"The integrity of computational results is being increasingly threatened by soft errors, especially for computations that are large-scale or performed under harsh conditions. Existing methods for soft error estimation do not clearly characterize the vulnerability associated with a particular result. 1) We propose a metric which captures the intrinsic vulnerability over space and time (VST) to soft errors that corrupt computational results. The method of VST estimation bridges the gap between the inherently low-level faults and the high-level computational failures that they eventually cause. 2) We define a model of an insidious soft error and try to clear up confusion around the concept of silent data corruption. 3) We present experimental results from three vulnerability studies involving floating-point addition, CORDIC, and FFT computations. The results show that traditional vulnerability metrics can be confounded by seemingly reliable but inefficient implementations which actually incur high vulnerability per computation. The VST method characterizes vulnerability accurately, provides a figure-of-merit for comparing alternative implementations of an algorithm, and in some cases uncovers pronounced and unexpected fluctuations in vulnerability.",2008,0,
67,68,Image Defect Recognition Based on Rough Set,"This paper applies rough set theory to recognition system for image defect, and designs a decision algorithm on rough set suitable for image defect recognition. Firstly, the image is made regionalization and sequential discrete set is proposed, the continuous attributes of image is discretized. Then the decision table model on discrete condition attributes and decision attributes is constructed. Further the condition attributes significance function and reduction algorithm is given. A novel approach for decision rule analysis and rough set recognition is proposed. Finally, this paper takes the example for fabric defect recognition to validate these algorithms. The result shows the rough set algorithm is effective for image defect recognition with less calculation and fast speed.",2009,0,
68,69,Using design patterns and constraints to automate the detection and correction of inter-class design defects,"Developing code free of defects is a major concern for the object oriented software community. The authors classify design defects as those within classes (intra-class), those among classes (inter-classes), and those of semantic nature (behavioral). Then, we introduce guidelines to automate the detection and correction of inter-class design defects. We assume that design patterns embody good architectural solutions and that a group of entities with organization similar, but not equal, to a design pattern represents an inter-class design defect. Thus, the transformation of such a group of entities, such that its organization complies exactly with a design pattern, corresponds to the correction of an inter-class design defect. We use a meta-model to describe design patterns and we exploit the descriptions to infer sets of detection and transformation rules. A constraint solver with explanations uses the descriptions and rules to recognize groups of entities with organizations similar to the described design patterns. A transformation engine modifies the source code to comply with the recognized distorted design patterns. We apply these guidelines on the Composite pattern using PTIDEJ, our prototype tool that integrates the complete guidelines",2001,0,
69,70,Error localization for robust video transmission,"The convergence of Internet, multimedia and mobile applications has led to an increased demand for efficient and reliable video data transmission over heterogeneous networks. Due to their coding efficiency, variable-length codes (VLC) are usually employed in the entropy coding stage of video compression standards. However, error propagation is a major problem associated with VLC. We propose the use of a class of self-synchronizing VLC (SSVLC) to achieve the dual goal of optimal coding efficiency and optimal error localization. Performance evaluation has confirmed that the use of SSVLC provides better performance than standard VLC techniques.",2002,0,
70,71,On the error-control coding techniques used in GSM/EDGE radio access networks,In this paper the error-control coding techniques used in GSM/EDGE Radio Access Network (GERAN) are considered. Application of coding schemes is restricted by the corresponding traffic channels (TCH). Knowing the general scheme is necessary for modeling the work of the complete error-control system. This knowledge can be useful for the implementation of educational software used for the investigation of the properties of different codecs and their characteristics in radio channels using various radio channel models.,2004,0,
71,72,A Multi-Agent Fault Detection System for Wind Turbine Defect Recognition and Diagnosis,This paper describes the use of a combination of anomaly detection and data-trending techniques encapsulated in a multi-agent framework for the development of a fault detection system for wind turbines. Its purpose is to provide early error or degradation detection and diagnosis for the internal mechanical components of the turbine with the aim of minimising overall maintenance costs for wind farm owners. The software is to be distributed and run partly on an embedded microprocessor mounted physically on the turbine and on a PC offsite. The software will corroborate events detected from the data sources on both platforms and provide information regarding incipient faults to the user through a convenient and easy to use interface.,2007,0,
72,73,Motion correction of PET images using realignment for intraframe movement,"A method is presented for the motion correction of PET images using realignment for intraframe movement. A newly introduced aspect of the method is that it corrects not only interframe but also intraframe movement, using currently used PET images (which are not corrected for intraframe movement) and motion tracking data. Although our method requires motion tracking data, it does not require very short time image acquisition nor list mode data acquisition. So, it is applicable to currently used PET images. In our method the following hypothesis is assumed. That is if no movement happens, there exists a linear model such that counts of each voxel per unit time follows the model. Model parameters may depend on the voxels. In a simple example, our method successfully corrected motion artifact and estimates the parameters accurately. It may give a simple and practical solution to the motion correction problems.",2003,0,
73,74,Study on Data Mining for Grounding Fault Line Selection in 6kV Ineffectively Grounded System of Coal Mine,"A great amount of fault wave has been recorded by the devices for detecting phase-to-ground faults in ineffectively grounded systems. However, a better method hasn't found for effectively taking advantage of these data to improve the result of fault line selection. Data mining techniques can be used for fault line selection in ineffectively grounded system to gain knowledge from the existing data and to improve the technique of fault line selection. This paper briefly describes the principles, methods and implementation of data mining techniques, classifies the fault samples of ineffectively grounded systems by using clustering analysis method, employs different fault line selection methods according to the types of faults, and consequently provides a set of criteria for modeling of typical ineffectively grounded systems and verifying the validity of real-time fault line selections. The validity of the methods has been convinced by the calculation using the data obtained from the real performance of a substation in coal mine. It has been shown to be promising to employ the data mining techniques in ineffectively grounded systems fault detection. This paper provides very good methods for resolving the difficulties with onsite tests, enhancing the techniques of fault line selection and establishing the fault detection management systems.",2010,0,
74,75,Fault-Tolerant Coverage Planning in Wireless Networks,"Typically wireless networks coverage is planned with static redundancy to compensate temporal variations in the environment. As a result, the service still is delivered but the network coverage could have entered a critical state, meaning that further changes in the environment may lead to service failure. Service failures have to be explicitly notified by the applications. Therefore, in this paper we propose a methodology for fault-tolerant coverage planning. The idea is detecting the critical state and removing it by on-line system reconfiguration, and restoration of the original static redundancy. Even in case of a failure the system automatically generates a new configuration to restore the service, leading to shorter repair times. We describe how this approach can be applied to wireless mesh networks, often used in industrial applications like manufacturing, automation and logistics. The evaluation results show that the underlying model used for error detection and system recovery is accurate enough to correctly identify the system state.",2008,0,
75,76,Detection of high impedance fault in distribution feeder using wavelet transform and artificial neural networks,"This work presents a novel analysis method that can simulate the potential effect of high impedance fault (HIF). The proposed method offers a new scheme for protecting the overhead distribution feeder. The wavelet transform (WT) method was successfully applied in many fields. The characteristics of scaling and translation of WT can be used to identify stable and transient signals. Discrete wavelet transforms (DWT) are initially used to extract distinctive features of the voltage and current signals, and are transformed into a series of detailed and approximated wavelet components. The coefficients of variation of the wavelet components are then calculated. This information is introduced into the training artificial neural networks (ANN) to determine an HIF from the operations of the switches. The simulated results clearly reveal that the proposed method can accurately identify the HIF in the distribution feeder.",2004,0,
76,77,A hierarchical framework for fault propagation analysis in complex systems,"In complex systems, there are few critical failure modes. Prognostic models are focused at predicting the evolution of those critical faults, assuming that other subsystems in the same system are performing according to their design specifications. In practice, however, all the subsystems are undergoing deterioration that might accelerate the time evolution of the critical fault mode. This paper aims at analyzing this aspect, i.e. interaction between different fault modes in various subsystems, of the failure prognostic problem. The application domain focuses on an aero propulsion system of the turbofan type. Creep in the high-pressure turbine blade is one of the most critical failure modes of aircraft engines. The effects of health deterioration of low-pressure compressor and high-pressure compressor on creep damage of high-pressure turbine blades are investigated and modeled.",2009,0,
77,78,Data Mining Using Rough Sets and Orthogonal Signal Correction-Orthogonal Partial Least Squares Analysis,"The paper put forward Data mining using rough sets and orthogonal signal correction-orthogonal partial least squares analysis (RS-OSC-OPLS/O2PLS). first, dimensionality reduction and de-noising with rough sets and orthogonal signal correction;second, Data mining using orthogonal partial least squares analysis. The method was proved to be feasible and effective after tested with 13 kinds of nationalities crowds data.",2010,0,
78,79,Estimation of Systematic Errors of MODIS Thermal Infrared Bands,"This letter reports a statistical method to estimate detector-dependent systematic error in Moderate Resolution Imaging Spectroradiometer (MODIS) thermal infrared (TIR) Bands 20-25 and 27-36. There exist scan-to-scan overlapped pixels in MODIS data. By analyzing a sufficiently large amount of those most overlapped pixels, the systematic error of each detector in the TIR bands can be estimated. The results show that the Aqua MODIS data are generally better than the Terra MODIS data in 160 MODIS TIR detectors. There are no detector-dependent systematic errors in Bands 31 and 32 for both Terra and Aqua MODIS data. The maximum detector errors are 3.00 K in Band 21 of Terra and -8.15 K in that of Aqua for brightness temperatures of more than 250 K",2006,0,
79,80,Comparing Web Services Performance and Recovery in the Presence of Faults,"Web-services are supported by a complex software infrastructure that must ensure high performance and availability to the client applications. Web services industry holds a well established platform for performance benchmarking (e.g., TPC-App and SPEC jAppServer2004 benchmarks). In addition, several studies have been published recently by main vendors focusing web services performance. However, as peak performance evaluation has been the main focus, the characterization of the impact of faults in such systems has been largely disregarded. This paper proposes an approach for the evaluation and comparison of performance and recovery time in web services infrastructures. This approach is based on fault injection and is illustrated through a concrete example of benchmarking three alternative software solutions for web services deployment.",2007,0,
80,81,Evidence-Based Analysis and Inferring Preconditions for Bug Detection,"An important part of software maintenance is fixing software errors and bugs. Static analysis based tools can tremendously help and ease software maintenance. In order to gain user acceptance, a static analysis tool for detecting bugs has to minimize the incidence of false alarms. A common cause of false alarms is the uncertainty over which inputs into a program are considered legal. In this paper we introduce evidence-based analysis to address this problem. Evidence-based analysis allows one to infer legal preconditions over inputs, without having users to explicitly specify those preconditions. We have found that the approach drastically improves the usability of such static analysis tools. In this paper we report our experience with the analysis in an industrial deployment.",2007,0,
81,82,"PSoC design in GM(1,1) error analysis and its application in temperature prediction","In the study of prediction filed, no matter what methods we used, the main purpose is to minimize the prediction error; however, the goals cannot be fulfilled completely. Even we choose GM(1,1) model, which in the newest soft computing method, we also need to minimize the prediction error. Hence, in this paper, we focus on the influence parameter alpha in GM(1,1) model in the first, then, analyze the characteristics of alpha step by step, and use numerical method to find the prediction error corresponding with alpha value. Second, after the mathematics model is presented, we use PSoC to design a GM(1,1) error analysis model, which based on the characteristic of GM(1,1) model. Also an example, which is temperature prediction case is given to assist us to implement our approach in the final section.",2008,0,
82,83,Calculation of transverse voltages of communication lines induced by the fault current of power system,"A double-line model is presented to calculate the transverse voltage of communication lines induced by the fault current of power lines. By dividing the communication line into several fictitious segments, a chain composed by the coupling P1-type circuit with distributed source is formed. The enhanced node voltage analysis (ENVA) is also developed in order to evaluate such a kind of model. The ENVA cuts the number of nodes down greatly because of treating the active and coupling impedance branches as a whole. In addition, the transverse voltages in time domain can be obtained easily from those calculated in frequency domain by means of fast Fourier transform. The numerical examples prove the validity and efficiency of the method by comparison with analytical results. The model is of significance to the design and the rights-of-way selection of power lines and communication lines.",2002,0,
83,84,General review of fault diagnostic in wind turbines,"Global wind electricity-generating capacity increased by 28.7 percent in 2008 to 120,798 Gigawatts. This represents a twelve-fold increase from a decade ago, when world wind-generating capacity stood at less than 5 GW [1]. With wind becoming a key part of the electrical mix in Denmark (20% with 3.1 GW), Spain (8% with 10 GW), and Germany (6% with 18.4 GW), wind turbine reliability is having a bigger effect on overall electrical grid system performance and reliability [1]. This shows the impact of faults and downtime on the reliability of wind turbine especially for offshore wind farms which although are some of the most environmentally friendly and efficient methods to generate electricity in the world. However, the maintenance costs are high because of their remote location. This can amount to as much as 25 to 30% of the total energy production [2]. The aim of this paper is to present an overview of fault detection in wind turbines, study and analyze the faults and their root-causes. The paper also explores different techniques used in early fault detection to form base information for future work to build a general fault diagnostic scheme for wind turbines.",2010,0,
84,85,Reconfigurable context-free grammar based data processing hardware with error recovery,"This paper presents an architecture for context-free grammar (CFG) based data processing hardware for re-configurable devices. Our system leverages on CFGs to tokenize and parse data streams into a sequence of words with corresponding semantics. Such a tokenizing and parsing engine is sufficient for processing grammatically correct input data. However, most pattern recognition applications must consider data sets that do not always conform to the predefined grammar. Therefore, we augment our system to detect and recover from grammatical errors while extracting useful information. Unlike the table look up method used in traditional CFG parsers, we map the structure of the grammar rules directly onto the field programmable gate array (FPGA). Since every part of the grammar is mapped onto independent logic, the resulting design is an efficient parallel data processing engine. To evaluate our design, we implement several XML parsers in an FPGA. Our XML parsers are able to process the full content of the packets up to 3.59 Gbps on Xilinx Virtex 4 devices",2006,0,
85,86,Autonomous Fault Recovery Technology for Achieving Fault-Tolerance in Video on Demand System,"With the advances of compression technology, storage devices and networks, video on demand (VoD) service is becoming popular. The system needs to provide continuous service and heterogeneous service levels for users. However, these requirements cannot be satisfied in conventional VoD system which is constructed on redundant content servers and centralized management. In this paper, autonomous VoD system is proposed to meet the requirements. The system is constructed on faded information field architecture. Under the proposed architecture, autonomous fault detection and fault recovery technologies are proposed to achieve fault-tolerance for continuous service. The effectiveness of the proposed technologies are proved through simulation. The results show that an average of 30% improvement in recovery time and users' video service can be recovered without stopping compared with conventional VoD system",2006,0,
86,87,Event-based fault detection of manufacturing cell: Data inconsistencies between academic assumptions and industry practice,"Some problems with event-based faults in manufacturing systems cannot be handled by existing fault detection solutions, including finding faults in event-based data for systems for which limited information is known. A new fault detection solution that finds faults in event-based data using model generation is presented here. This solution assumes that some information is known about the system from its design information and data structure. An example application of this solution is presented for a Ford machining cell that has been experiencing a gantry waiting problem. In the course of this example application, five inconsistencies were found between relatively common academic assumptions made by this fault detection solution (as well as others) and the actual cell's set-up and data. These inconsistencies and possible means of addressing them are discussed. Some of these means to resolve the inconsistencies have been implemented, and preliminary results in generating models using the fault detection solution are presented.",2010,0,
87,88,Error monitoring for optical metropolitan network services,"Service providers rely on performance monitoring capabilities not only to ensure integrity of their network but also to support service-level agreements with their customers. The depth of monitoring is directly tied to the technology and protocol used in the transport layer of the network. Next-generation services based on enterprise-centric, non-SONET/SDH protocols, such as Gigabit Ethernet and Fibre Channel, as well as managed protocol-independent wavelength transport, have created a number of challenges for service providers because of the differences in how error monitoring is performed. In this article we describe and compare protocol-dependent and protocol-independent error monitoring techniques that apply to these service offerings",2002,0,
88,89,A method of inverter circuit fault diagnosis based on BP neural network and D-S evidence theory,"With the study and analysis on intelligent fault diagnosis for inverting circuit, an improved diagnosis method combined BP neuron network and D-S evidence theory was proposed. Each measuring point was extracted by BP neural network to obtain the local diagnosis, which is adopted to design the belief function of D-S evidence theory. Multiple monitoring points' information is fused to receive the comprehensive global diagnosis result. The experimental results show that this method has the better feasibility and effectiveness on fault diagnosis in inverter's key components-inverting circuit.",2010,0,
89,90,On fault diagnosis tree and its control flow,"The coarse-grained organization of the existing fault diagnosis scheme can not realize the automatic and intelligent diagnosis. This paper provides a tree structure of fault diagnosis scheme named T04FDS, which integrates the fault classification relations and nesting relation of part and whole. By building the state transform system to represent the diagnosis process, and describing the control flow with operations of stack. Furthermore the thesis presents the physical realization of T04FDS fault diagnosis, and finally proves the feasibility of this method by giving an example of fault diagnosis scheme for computer wireless network card.",2009,0,
90,91,Master-Slave TMR Inspired Technique for Fault Tolerance of SRAM-Based FPGA,"In order to increase reliability and availability of Static-RAM based field programmable gate arrays (SRAM-based FPGAs), several methods of tolerating defects and permanent faults have been developed and applied. These methods are not well adapted for handling high fault rates for SRAM based FPGAs. In this paper, both single and double faults affecting configurable logic blocks (CLBs) are addressed. We have developed a new fault-tolerance technique that capitalizes on the partial reconfiguration capabilities of SRAM-based FPGA. The proposed fault-tolerance method is based on triple modular redundancy (TMR) combined with master-slave technique, and exploiting partial reconfiguration to tolerate permanent faults. Simulation results on reliability improvement corroborate the efficiency of the proposed method and prove that it compares favorably to previous methods.",2010,0,
91,92,Robust Speech Recognition Using a Cepstral Minimum-Mean-Square-Error-Motivated Noise Suppressor,"We present an efficient and effective nonlinear feature-domain noise suppression algorithm, motivated by the minimum-mean-square-error (MMSE) optimization criterion, for noise-robust speech recognition. Distinguishing from the log-MMSE spectral amplitude noise suppressor proposed by Ephraim and Malah (E&M), our new algorithm is aimed to minimize the error expressed explicitly for the Mel-frequency cepstra instead of discrete Fourier transform (DFT) spectra, and it operates on the Mel-frequency filter bank's output. As a consequence, the statistics used to estimate the suppression factor become vastly different from those used in the E&M log-MMSE suppressor. Our algorithm is significantly more efficient than the E&M's log-MMSE suppressor since the number of the channels in the Mel-frequency filter bank is much smaller (23 in our case) than the number of bins (256) in DFT. We have conducted extensive speech recognition experiments on the standard Aurora-3 task. The experimental results demonstrate a reduction of the recognition word error rate by 48% over the standard ICSLP02 baseline, 26% over the cepstral mean normalization baseline, and 13% over the popular E&M's log-MMSE noise suppressor. The experiments also show that our new algorithm performs slightly better than the ETSI advanced front end (AFE) on the well-matched and mid-mismatched settings, and has 8% and 10% fewer errors than our earlier SPLICE (stereo-based piecewise linear compensation for environments) system on these settings, respectively.",2008,0,
92,93,On-board fault-tolerant SAR processor for spaceborne imaging radar systems,"A real-time high-performance and fault-tolerant FPGA-based hardware architecture for the processing of synthetic aperture radar (SAR) images has been developed for advanced spaceborne radar imaging systems. In this paper, we present the integrated design approach, from top-level algorithm specifications, system architectures, design methodology, functional verification, performance validation, down to hardware design and implementation.",2005,0,
93,94,Fault-Tolerant BPEL Workflow Execution via Cloud-Aware Recovery Policies,"BPEL is the de facto standard for business process modeling in today's enterprises and is a promising candidate for the integration of business and scientific applications that run in Grid or Cloud environments. In these distributed infrastructures, the occurrence of faults is quite likely. Without sophisticated fault handling, workflows are frequently abandoned due to software or hardware failures, leading to a waste of CPU hours. The fault handling mechanisms provided by BPEL are well suited for handling faults of the business logic, but infrastructure-induced errors should be handled automatically to avoid over-complication of workflow design and keep concerns separated. This paper identifies classes of faults that can be resolved automatically by the infrastructure, and provides a policy-based approach to configure this automatic behavior without the need for adding explicit fault handling mechanisms to the BPEL process. The proposed approach provides automatic redundancy of services using a Cloud infrastructure to allow substitution of defective services. An implementation based on the ActiveBPEL engine and Amazon's Elastic Compute Cloud is presented.",2009,0,
94,95,Neural fault isolator for Wireless Sensor Networks,"Wireless sensor networks are emerging as an innovative technology that can help to improve business processes. In such environments malfunctions and break-down states must be efficiently diagnosed to reduce to a minimum the economic losses. In this paper we present a fault isolation approach based on neural networks, which utilizes only a minimum set of information such as the sensor value, node ID and timestamp as inputs. We believe that this information set could be provided by any WSN regardless of its specific implementation. This abstraction makes the fault isolator generically applicable in enterprise business systems. The neural fault isolator was evaluated in a trial with 36 nodes and has proved to be highly efficient in the isolation of failed components.",2008,0,
95,96,Test Compaction for Transition Faults under Transparent-Scan,"Transparent-scan was proposed as an approach to test generation and test compaction for scan circuits. Its effectiveness was demonstrated earlier in reducing the test application time for stuck-at faults. We show that similar advantages exist when considering transition faults. We first show that a test sequence under the transparent-scan approach can imitate the application of broadside tests for transition faults. Test compaction can proceed similar to stuck-at faults by omitting test vectors from the test sequence. A new approach for enhancing test compaction is also described, whereby additional broadside tests are embedded in the transparent-scan sequence without increasing its length or reducing its fault coverage",2006,0,
96,97,Constrained free form deformation based algorithm for geometric distortion correction of echo planar diffusion tensor images,"In order to differentiate between normal and abnormal variations in brain diffusion tensor images, it is necessary to develop medical atlases. Atlas creation requires removal of spatial distortions in individual subject diffusion weighted images. In this paper we suggest a new approach using non-linear warping based on optic flow to map both baseline and diffusion weighted echo planar images to the anatomically correct T2 weighted spin echo image. The method is readily implemented and does not require a pre-processing step of rigid alignment. A global histogram matching precedes the base line EP image correction. A Markov random field based classification algorithm was implemented to cluster T2 weighted images into four different tissue type classes. This information was then used to synthesize diffusion based image models used in the warping algorithm to correct the geometric distortions in the diffusion weighted EP images.",2004,0,
97,98,Detecting Single and Multiple Faults Using Intelligent DSP and Agents,"In this paper intelligent agents and DSP techniques are integrated to detect single and multiple faults in electrical circuits. Agents are used to model the AC electrical circuit. A DSP engine is embedded into the agents to analyse the signals, i.e. the energy transfer between the physical components. An AC to DC rectifier circuit is chosen as test-bed for the proposed solutions",2006,0,
98,99,Real-time model based sensor fault tolerant control system on a chip,"In this paper, we proposed a model based sensor fault tolerant control system embedded in a generic PIC microcontroller for use in a temperature control system. The model based fault tolerant control algorithm is embedded in the microcontroller for stand-alone real-time implementation. The algorithm consists of a PID controller element (for nominal control) and a fault compensating element. Results from simulations and real time implementation are shown to demonstrate the ease of real time implementation.",2009,0,
99,100,Efficiency enhancement of microstrip patch antenna with defected ground structure,"Defected ground structures (DGS) have been developed to improve characteristics of many microwave devices. Although the DGS has advantages in the area of the microwave filter design, microstrip antenna design for different applications such as cross polarization reduction and mutual coupling reduction etc., it can also be used for the antenna size reduction. The etching of a defect in the ground plane is a unique technique for the antenna size reduction. The DGS is easy to be an equivalent LC resonator circuit. The value of the inductance and capacitance depends on the area and size of the defect. By varying the various dimensions of the defect, the desired resonance frequency can be achieved. In this paper the effect of dumbbell shaped DGS, to the size reduction of a microstrip patch antenna is investigated. Then a cavity backed structure is used to increase the efficiency of the microstrip patch antenna, in which the electric walls are placed surrounding the patch. The simulation is carried out with IE3D full wave EM simulator.",2008,0,
100,101,Effects of Defects on the In-plane Dynamic Energy Absorption of Metal Honeycombs,"The in-plane dynamic energy absorption of metal honeycombs with defects consisting of missing cells are analyzed using explicit dynamic finite element method. Two types of structural defects (a single defect located in the center of the model and a double defect) are firstly introduced. Then the influence of the defects and the impact velocities on the energy absorption abilities of metal honeycombs is investigated. Researches show that single and isolated defects reduce the absorbed energy of cellular materials. The separation distance between two defects has little effect on the dynamic energy absorption, while the size of the single defect has great influence on it. These results will provide some useful guides for the safety evaluation and the dynamic energy absorption design of metal honeycombs.",2010,0,
101,102,Fault Diagnosis on Board for Analog to Digital Converters,"This paper describes a general purpose high reliable data acquisition system which allows A/D converter testing by histogram and two tone tests for the fault diagnosis on the same board. A reliability analysis has been carried out in order to optimize the project, the components choice and redundancy configuration. The software has been written in Matlab and LabVIEW, with an easy graphical user interface.",2007,0,
102,103,Safing and fault protection for the MESSENGER mission to Mercury,"The MErcury Surface, Space ENvironment, GEochemistry, and Ranging (MESSENGER) mission is a NASA Discovery-class, deep-space mission to orbit the planet Mercury. Its purpose is to map the planet surface using various scientific instruments and explore the interior of the planet using measurements from instruments such as a magnetometer and observation of planetary libration. This paper discusses the architecture and implementation of the methods by which faults in the MESSENGER spacecraft are detected and the effects of those faults mitigated. The responsibility of the redundant Fault Protection Processors (FPPs) is to detect faults and take autonomous corrective actions that will keep the spacecraft healthy and safe.",2002,0,
103,104,A new error concealment algorithm for H.264 video transmission,"In this paper, a new error concealment algorithm for the new coding standard H.264 is presented. The algorithm consists of a block size determination step to determine the size type of the lost block and a motion vector recovery step to find the lost motion vector from multiple reference frames. The main feature of this algorithm are as follows. In the block size determination step, we propose a criterion to determine the size type of the lost block from the current frame. In the motion vector recovery step, the optimal motion vector for the lost block chosen from multiple previous reference frames with the minimum value of the side match distortion. The proposed algorithm not only can determine the most correct mode for the lost block, but also can save much more computation time for motion vector recovery. Experimental results show that the proposed algorithm achieves 0.47 dB improvement over the conventional VM method.",2004,0,
104,105,An EMF activity tree based BPEL defect pattern testing method,"For testing BPEL defects efficiently, a novel BPEL defect pattern testing architecture based on the EMF activity tree technology is proposed. The EMF activity tree that is similar to abstract syntax tree is used to describe the BPEL service process structure. The mapping method from the DOM object tree of a BPEL file to the EMF activity tree and the recursive algorithm to generate an EMF activity tree are represented in detail. A typical EMF activity tree is shown and the visitor design pattern based traversal method is stated. The directions to enhance this technology are illustrated finally.",2010,0,
105,106,Fault tolerant generator systems for wind turbines,"The objective of this paper is to review the possibilities of applying fault tolerance in generator systems for wind turbines based on what has been presented in the literature. In order to make generator systems fault tolerant in a suitable way, it is necessary to gain insight into the probability of different failures, so that suitable measures can be taken. Therefore, a literature survey of reliability of wind turbines, electrical machines and power electronic converters is given. Five different ways of achieving fault tolerance identified in the literature are discussed together with their applicability for wind turbines: (1) converters with redundant semiconductors, (2) fault tolerant converter topologies, (3) fault tolerance by increasing the number of phases, (4) fault tolerance of switched reluctance machines, and (5) design for fault tolerance of PM machines and converters. Because converters fail more often than machines, it makes sense to use of fault tolerant converter topologies. Increasing the number of phases is a useful form of fault tolerance because it can be achieved without increasing the cost significantly.",2009,0,
106,107,Fault recovery in linear systems via intrinsic evolution,"We investigate fault recovery using reconfiguration for analog linear feedback control systems. We assume any faults occur only within the linear system and accessibility to its internal circuitry is impossible. Consequently, the only way to restore service - even degraded service - is by inserting a compensation network into the control loop. System failures are manifested by a change in the original bandwidth. The compensators are evolved intrinsically.",2004,0,
107,108,Distance estimation technique for single line-to-ground faults in a radial distribution system,A simple yet powerful algorithm to estimate the distance to a single line-to-ground fault on a distribution feeder is proposed. The algorithm is implemented in a power monitor instrument and the estimation of distance is made within the instrument itself. The algorithm is designed to work where the only available data to the instrument are a single point measurement taken at the substation and the positive and zero-sequence impedance of the primary feeder. The single point measurement consists of three-phase voltage and current waveforms. Network topology data are not available to the algorithm. The new technique accommodates computational power and data constraints while maintaining adequate accuracy of the measurements,2000,0,
108,109,Optimal Wavelet Design for Multicarrier Modulation with Time Synchronization Error,"Wavelet packet based multi-carrier modulation (WPMCM) is an efficient transmission technique which has the advantage of being a generic scheme whose characteristics can be customized to fulfill a design specification. However, WPMCM is sensitive and vulnerable to time synchronization errors because its symbols overlap. In this paper, we design new wavelets to alleviate WPMCM's vulnerability to timing errors. First, a filter design framework that facilitates the development of new wavelet bases is built. Then the expressions for errors due to time offset in WPMCM transmission are derived and stated as a convex optimization problem. Finally, an optimal filter that best handles these deleterious effects is designed by means of semi definite programming (SDP). Through computer simulations the performance advantages of the newly designed filter over standard wavelet filters are proven.",2009,0,
109,110,Optimum design of a class of fault tolerant isotropic Gough-Stewart platforms,"Optimal geometric design is of key importance to the performance of a manipulator. First, this paper extends the work in Y. Yi, et al., (2004) to generate a class of isotropic Gough-Stewart platforms (GSPs) with an odd number of struts. Then, it develops methods for finding a highly fault tolerant GSP from that class. Two optimization criteria are considered, isotropy and fault tolerance. To meet the mission critical needs imposed by laser weapons applications, nine-strut isotropic GSPs that retain kinematic stability despite the loss of any three struts are found. First, we develop methods for generating a five parameter class of isotropic nine-strut GSPs. Next, new measures of fault tolerance are introduced and used to optimize the free parameter space. The optimized design is much more fault tolerant than the GSP currently baselined for the airborne laser.",2004,0,
110,111,Designing Fault Tolerant Web Services Using BPEL,"The Web services technology provides an approach for developing distributed applications by using simple and well defined interfaces. Due to the flexibility of this architecture, it is possible to compose business processes integrating services from different domains. This paper presents an approach, which uses the specification of services orchestration, in order to create a fault tolerant model combining active and passive replication technique. This model support fault of crash. The characteristics and the results obtained by implementing this model are described along this paper.",2008,0,
111,112,Sub-cycle detection of incipient cable splice faults to prevent cable damage,"This paper presents an innovative method for subcycle detection of incipient cable failures caused by self-clearing faults occurring in cable splices due to insulation breakdown. Because of their short duration, conventional overcurrent protection will not detect these types of faults. The protection scheme described in this paper has been integrated into a universal relay platform. It is fast enough to operate for sub-cycle faults and has the logic to differentiate them from other types of faults. Imminent cable failure can be detected",2000,0,
112,113,"Software-Based Online Detection of Hardware Defects Mechanisms, Architectural Support, and Evaluation","As silicon process technology scales deeper into the nanometer regime, hardware defects are becoming more common. Such defects are bound to hinder the correct operation of future processor systems, unless new online techniques become available to detect and to tolerate them while preserving the integrity of software applications running on the system. This paper proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extension (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible: testing techniques can be modified/upgraded in the field to trade off performance with reliability without requiring any change to the hardware. We evaluated our technique on a commercial chip-multiprocessor based on Sun's Niagara and found that it can provide very high coverage, with 99.22% of all silicon defects detected. Moreover, our results show that the average performance overhead of software-based testing is only 5.5%. Based on a detailed RTL-level implementation of our technique, we find its area overhead to be quite modest, with only a 5.8% increase in total chip area.",2007,0,
113,114,Optimizing Joint Erasure- and Error-Correction Coding for Wireless Packet Transmissions,"To achieve reliable packet transmission over a wireless link without feedback, we propose a layered coding approach that uses error-correction coding within each packet and erasure-correction coding across the packets. This layered approach is also applicable to an end-to-end data transport over a network where a wireless link is the performance bottleneck. We investigate how to optimally combine the strengths of error- and erasure-correction coding to optimize the system performance with a given resource constraint, or to maximize the resource utilization efficiency subject to a prescribed performance. Our results determine the optimum tradeoff in splitting redundancy between error-correction coding and erasure-correction codes, which depends on the fading statistics and the average signal to noise ratio (SNR) of the wireless channel. For severe fading channels, such as Rayleigh fading channels, the tradeoff leans towards more redundancy on erasure-correction coding across packets, and less so on error-correction coding within each packet. For channels with better fading conditions, more redundancy can be spent on error-correction coding. The analysis has been extended to a limiting case with a large number of packets, and a scenario where only discrete rates are available via a finite number of transmission modes.",2008,0,
114,115,Modeling transformers with internal incipient faults,"Incipient fault detection in transformers can provide early warning of electrical failure and could prevent catastrophic losses. To develop transformer incipient fault detection technique, a transformer model to simulate internal incipient faults is required. This paper presents a methodology to model internal incipient winding faults in distribution transformers. These models were implemented by combining deteriorating insulation models with an internal short circuit fault model. The internal short circuit fault model was developed using finite element analysis. The deteriorating insulation model, including an aging model and an arcing model connected in parallel, was developed based on the physical behavior of aging insulation and the arcing phenomena occurring when the insulation was severely damaged. The characteristic of the incipient faults from the simulation were compared with those from some potential experimental incipient fault cases. The comparison showed the experimentally obtained characteristic's of terminal behavior of the faulted transformer were similar to the simulation results from the incipient fault models",2002,0,
115,116,An Efficient Technique for Error-Free Implementation of H.264 Using Algebraic Integer Encoding,"Video coding technology plays a key role in various multimedia applications. H.264 is the newest video coding standard and has achieved a significant improvement in coding efficiency. The 4*4 integer transform, as one of the key techniques in H.264 video compression standard, is very important for the whole performance of H.264 codec. In this paper we propose a novel algorithm for fast and error-free (infinite-precision) implementation of H.264 based on algebraic integer-encoding scheme. The proposed algorithm has regular structure. Simulation results show that this algorithm will result in reduction of computation complexity while enhancing the quality of obtained image simultaneously. Determining the quality of an image is an open problem that is highly dependent on the specific application that this image will be used for. We propose new measuring quantities for image quality.",2010,0,
116,117,Detection of Rotor Faults in Squirrel-Cage Induction Motors using Adjustable Speed Drives,"The need for detection of rotor faults at an earlier stage, so that maintenance can be planned ahead, has pushed the development of monitoring methods with increasing sensitivity and noise immunity. Addressing diagnostic techniques based on current signatures analysis (MCSA), the characteristic components introduced by specific faults in the current spectrum are investigated and a diagnosis procedure correlate the amplitudes of such components to the fault extent. In this paper, the impact of feedback control on asymmetric rotor cage induction machine behavior is analyzed. It is shown that the variables usually employed in diagnosis procedures assuming open-loop operation are no longer effective under closed-loop operation. Simulation results show that signals already present at the drive are suitable to effective diagnostic procedure. The utilization of the current regulator error signals and the influence of the regulators gains on their utilization in rotor failure detection are the aim of the present work. The use of a band-pass filter bank to detect the presence of sidebands is also proposed in the paper",2006,0,
117,118,FPGA Implementation of Wideband IQ Imbalance Correction in OFDM Receivers,"This paper describes the implementation of a digital compensation scheme, called CSAD, for correcting the effects of wideband gain and phase imbalances in dual-branch OFDM receivers. The proposed scheme is implemented on a Xilinx Virtex-4 field programmable gate array (FPGA). The flexible architecture of the implementation makes it readily adaptable for different broadband applications, such as DVB-T/H, WLAN, and WiMAX. The proposed correction scheme is resilient against multipath fading and frequency offset. When applied to DVB-T, it is shown that an 11-bit arithmetic precision is sufficient to achieve the required BER of 2x10<sup>-4</sup> at an SNR of 16.5 dB. Using this bit-precision, the implementation consumes 1686 Virtex-4 slices equivalent to about 42600 gates.",2008,0,
118,119,Error prediction for multi-classification,"This paper describes an error prediction mechanism for multiclassification systems. First, a multiclassification system is constructed by combining a suite of two-class classifiers. While training, each sub-classifier does not utilize all the training data and the remaining data are used for testing purpose. Thus, the classification system can predict its own performance after training. We have tested this mechanism on several well-known benchmark datasets. Experimental results are demonstrated for its effectiveness.",2005,0,
119,120,Automatic detection and correction of purple fringing using the gradient information and desaturation,"This paper proposes a method to automatically detect and correct purple fringing that is one of the color artifacts due to characteristics of charge coupled device sensors in a digital camera. The proposed method consists of two steps. In the first step, we detect purple fringed regions that satisfy specific properties: hue characteristics around highlight regions with large gradient magnitudes. In the second step, color correction of the purple fringed regions is made by desaturating the pixels in the detected regions. The proposed method is able to detect purple fringe artifacts more precisely than Kang's method. It can be used as a post processing in a digital camera.",2008,0,
120,121,SOA-Based Alarm Integration for Multi-Technology Network Fault Management,"In order for the service provider to offer better quality of telecom services to customers, one of the possible ways is to monitor and control all kinds of deployed network resources, which are used to support the operation of these services, and to proactively analyze and recover any trouble reported from the network resources. In this study, two system integration scenarios along with the associated interface specifications for multi-technology resource alarm notification and retrieval services have been described. An NGOSS-based development methodology was followed, and a number of useful commercials tools were introduced to facilitate the evolution and transformation of legacy BSS/OSS, so that these BSS/OSS are able to support loosely-coupled interoperability by using standards-based interfaces based on the service-oriented architecture. The functionalities of multi-technology network fault management were realized by means of JMS and Web Service techniques. The implementation described in this paper shows the feasibility of the proposed development methodology.",2008,0,
121,122,Fault Diagnosis of Bearings in Rotating Machinery Based on Vibration Power Signal Autocorrelation,"Since fault in a great number of bearings commences from a single point defect, research on this category of faults has shared a great deal in predictive diagnosis literature. Single point defects will cause certain characteristic fault frequencies to appear in machine vibration spectrum. In traditional methods, data extracted from frequency spectrum has been used to identify damaged bearing part. Because of impulsive nature of fault strikes, and complex modulations present in vibration signal, a simple spectrum analysis may result in erroneous conclusions. When a shaft rotates at constant speed, strikes due to a single point defect repeat at constant intervals. Each strike shows a high energy distribution around it. This paper considers the time intervals between successive impulses in auto-correlated vibration power signals. The most frequent interval between successive impulses determines the period of defective part. This period is related to fault frequency and therefore shows the defective part. A comparison of results extracted from the traditional and the proposed methods shows the efficiency improvement of the second method in respect of the first one",2006,0,
122,123,A Distributed Fault-Tolerant Algorithm for Event Detection Using Heterogeneous Wireless Sensor Networks,"Distributed event detection using wireless sensor networks has received growing interest in recent years. In such applications, a large number of inexpensive and unreliable sensor nodes are distributed in a geographical region to make firm and accurate local decisions about the presence or absence of specific events based on their sensor readings. However, sensor readings can be unreliable, due to either noise in the sensor readings or hardware failures in the devices, and may cause nodes to make erroneous local decisions. We present a general fault-tolerant event detection scheme that allows nodes to detect erroneous local decisions based on the local decisions reported by their neighbors. This detection scheme does not assume homogeneity of sensor nodes and can handle cases where nodes have different accuracy levels. We prove analytically that the derived fault-tolerant estimator is optimal under the maximum a posteriori (MAP) criterion. An equivalent weighted voting scheme is also derived. Further, we describe two new error models that take into account the neighbor distance and the geographical distributions of the two decision quorums. These models are particularly suitable for detection applications where the event under consideration is highly localized. Our fault-tolerant estimator is simulated using a network of 1024 nodes deployed randomly in a square region and assigned random probability of failures",2006,0,
123,124,Diagnosis of rotor faults in brushless DC (BLDC) motors operating under non-stationary conditions using windowed Fourier ridges,"There are several applications where the motor is operating in continuous non-stationary operating conditions. Actuators in the aerospace and transportation industries are examples of this kind of operation. Diagnostics of faults in such applications is, however, challenging. A novel method using windowed Fourier ridges is proposed in this paper for the detection of rotor faults in BLDC motors operating under continuous non-stationarity. Experimental results are presented to validate the concept and depict the ability of the proposed algorithm to track and identify rotor faults. The proposed algorithm is simple and can be implemented in real-time without much computational burden.",2005,0,
124,125,Supervision and fault management of process-tasks and terminology,"The supervision of technical processes is aimed at showing the present state, indicating undesired or unpermitted states, and taking appropriate actions to avoid damage or accidents. The deviations from normal process behavior result from faults and errors, which can be attributed to many causes. They may result in some shorter or longer time periods with malfunctions or failures if no counteractions are taken. One reason for supervision is to avoid these malfunctions or failures. In the following article the basic tasks of supervision are shortly described.",2007,0,
125,126,An industrial case study of implementing and validating defect classification for process improvement and quality management,"Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements",2005,0,
126,127,Evaluation of soft-bit error sequence generators at the output of the decoding process,"Soft decision decoding algorithms are widely used in modern wireless systems; convolutional and turbo codes are usually adopted as the inner scheme thanks to their capability to correct symbol errors at low SNR values. Such algorithms can achieve high coding gains using soft decoding, and modern digital hardware technology enables efficient and low cost practical implementations. We apply the experience gained in previous work, concerning the simulation of bit error processes (Costamagna, E. et al., Proc. IEEE, vol.90 p.842-59, 2002), to implement soft-bit generative models based on hidden Markov chains and chaotic attractors. Both the input and the output of the demodulation process of a GSM-GPRS and a 3GPP UMTS transceiver are observed, developing our earlier analysis (Costamagna et al., IEEE 58th VTC, 2003), and the quality of the soft-bit sequences generated for the input is evaluated comparing the sequences obtained at the output of the demodulator when simulated or target sequences are supplied at the input. Moreover, the deep significance of some statistical features exhibited by the sequences in order to describe their error burst behavior is briefly discussed.",2004,0,
127,128,Procedure call duplication: minimization of energy consumption with constrained error detection latency,"This paper presents a new software technique for detecting transient hardware errors. The objective is to guarantee data integrity in the presence of transient errors and to minimize energy consumption at the same time. Basically, we duplicate computations and compare their results to detect errors. There are three choices for duplicate computations: (1) duplicating every statement in the program and comparing their results, (2) re-executing procedures with duplicated procedure calls and comparing the results, (3) re-executing the whole program and comparing the final results. Our technique is the combination of (1) and (2): Given a program, our technique analyzes procedure call behavior of the program and determines which procedures should have duplicated statements (choice (1)) and which procedure calls should be duplicated (choice (2)) to minimize energy consumption while controlling error detection latency constraints. Then, our technique transforms the original program into the program that is able to detect errors with reduced energy consumption by re-executing the statements or procedures. In benchmark program simulation, we found that our technique saves over 25% of the required energy on average compared to previous techniques that do not take energy consumption into consideration",2001,0,
128,129,The complexity of adding failsafe fault-tolerance,"In this paper, we focus our attention on the problem of automating the addition of failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. And, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of adding failsafe fault-tolerance in distributed programs is NP-hard. Towards this end, we reduce the 3-SAT problem to the problem of adding failsafe fault-tolerance. We also identify a class of specifications, monotonic specifications and a class of programs, monotonic programs. Given a (positive) monotonic specification and a (negative) monotonic program, we show that failsafe fault-tolerance can be added in polynomial time. We note that the monotonicity restrictions are met for commonly encountered problems such as Byzantine agreement, distributed consensus, and atomic commitment. Finally, we argue that the restrictions on the specifications and programs are necessary to add failsafe fault-tolerance in polynomial time; we prove that if only one of these conditions is satisfied, the addition of failsafe fault-tolerance is still NP-hard.",2002,0,
129,130,Comparisons of error control techniques for wireless video multicasting,"This paper explores three different methods, employed separately and in combination, to improve the quality of video delivery on wireless local area networks. The approaches are: leader-driven multicast (LDM), monitoring MAC layer unicast (re)transmissions by other receivers; application-level forward error correction (FEC) using block erasure codes; negative feedback from selected receivers in the form of extra parity requests (EPR). The performance of these three methods is evaluated using both experiments on a mobile computing testbed and simulation. The results indicate that, while LDM is helpful in improving the raw packet reception rate, the combination of FEC and EPR is most effective in improving the frame delivery rate",2002,0,
130,131,Research of remote fault diagnostic system based on Grid,"So far, methods of fault diagnosis have been numerous. But as the complexity of modern equipment, the variability of fault, as well as trends in global manufacturing, it is difficult for any separate organization of independent to complete all the work of fault diagnosis. This article reviewed the equipment fault diagnostic technology in the course of development, in particular, remote fault diagnostic system based on Internet technology, and then on this basis, this paper proposed a new fault diagnostic method, that is grid-based remote fault diagnosis, which integrates the resources of related organization to work together. The architecture of this diagnostic system is proposed, and also the work flow of this system is described.",2009,0,
131,132,Finding Faults: Manual Testing vs. Random+ Testing vs. User Reports,"The usual way to compare testing strategies, whether theoretically or empirically, is to compare the number of faults they detect. To ascertain definitely that a testing strategy is better than another, this is a rather coarse criterion: shouldn't the nature of faults matter as well as their number? The empirical study reported here confirms this conjecture. An analysis of faults detected in Eiffel libraries through three different techniques-random tests, manual tests, and user incident reports-shows that each is good at uncovering significantly different kinds of faults. None of the techniques subsumes any of the others, but each brings distinct contributions.",2008,0,
132,133,Adaptive Fault Management of Parallel Applications for High-Performance Computing,"As the scale of high-performance computing (HPC) continues to grow, failure resilience of parallel applications becomes crucial. In this paper, we present FT-Pro, an adaptive fault management approach that combines proactive migration with reactive checkpointing. It aims to enable parallel applications to avoid anticipated failures via preventive migration and, in the case of unforeseeable failures, to minimize their impact through selective checkpointing. An adaptation manager is designed to make runtime decisions in response to failure prediction. Extensive experiments, by means of stochastic modeling and case studies with real applications, indicate that FT-Pro outperforms periodic checkpointing, in terms of reducing application completion times and improving resource utilization, by up to 43 percent.",2008,0,
133,134,Fault-accommodating thruster force allocation of an AUV considering thruster redundancy and saturation,"A new approach to the fault-accommodating allocation of thruster forces of an autonomous underwater vehicle (AUV) is investigated in this paper. This paper presents a framework that exploits the excess number of thrusters to accommodate thruster faults during operation. First, a redundancy resolution scheme is presented that considers the presence of an excess number of thrusters along with any thruster faults and determines the reference thruster forces to produce the desired motion. This framework is then extended to incorporate a dynamic state feedback technique to generate reference thruster forces that are within the saturation limit of each thruster. Results from both computer simulations and experiments are provided to demonstrate the viability of the proposed scheme",2002,0,
134,135,Designing Real-Time and Fault-Tolerant Middleware for Automotive Software,"Automotive software development poses a great deal of challenges to automotive manufacturers since an automobile is inherently distributed and subject to fault-tolerance and real-time requirements. Middleware is a software layer that can handle the intrinsic complexities of distributed systems and arises as an indispensable run-time platform for automotive systems. This paper explains the concept of middleware by enumerating its functions and categorizes middleware according to adopted communication models. It also extracts five essential requirements of automotive middleware and proposes a middleware design for automotive systems based on the message-oriented middleware (MOM) structure. The proposed middleware effectively addresses the derived requirements and includes many essential features such as real-time guarantee, fault-tolerance, and a global time base",2006,0,
135,136,Design of Energy-Efficient High-Speed Links via Forward Error Correction,"In this brief, we show that forward error correction (FEC) can reduce power in high-speed serial links. This is achieved by trading off the FEC coding gain with specifications on transmit swing, analog-to-digital converter (ADC) precision, jitter tolerance, receive amplification, and by enabling higher signal constellations. For a 20-in FR4 link carrying 10-Gb/s data, we demonstrate: 1) an 18-mW/Gb/s savings in the ADC; 2) a 1-mW/Gb/s reduction in transmit driver power; 3) up to 6?? improvement in transmit jitter tolerance; and 4) a 25- to 40-mV improvement in comparator offset tolerance with 3?? smaller swing.",2010,0,
136,137,Fault classification for distance protection,This paper presents an overview of power system fault classification methods and challenges. It also contains some ideas about structured testing.,2002,0,
137,138,On-line Fault Diagnosis Model of the Hydropower Units Based on MAS,"The paper introduced a novel on-line fault diagnosis system model of the hydropower units based on multi-agent system. In allusion to the classical MAS-based fault diagnosis model, it proposes a new function of information interactive between the mission-controlled subsystem and the task decomposition subsystem to increase the transmission rate of control signals and designs the status-monitoring subsystem to detect the abnormal signals directly from local to increase the fault diagnostic sensitivity. In the fault-diagnosis subsystem, a multi-agent interactive parallel structure is designed to meet the requirements of the high reliability and good real-time. A Java-based language so called as JAFMAS is used to build a multi-agent cooperation platform. Experimental results show the effectiveness and feasibility of the proposed method.",2009,0,
138,139,Non-differential protection of a generator's stator utilizing fault transients,"This paper presents a novel protection scheme for detecting faults on the stator of a generator unit which is directly connected to a distribution system. In the scheme, a multi-channel fault transient detection unit, using the outputs of the current transformers (CTs) at the output of the generator terminal, is employed to extract the fault-generated transient current signals. The detector unit is tuned to extract two bands of fault generated transient signals with different center frequencies. A spectral comparison technique is applied to firstly compute the spectral energies of the two band signals, and then the fault diagnosis determines whether it is an internal and external fault by comparing the ratio of the two signals with a predefined threshold. The scheme offers advantages of immunity to CT saturation, and is capable of detecting both low level and interturn faults. In addition, the protection scheme is also simple in application, and is cost-effective in that it only requires one set of CTs. Simulation studies show that the proposed technique can give correct responses for various fault conditions",2001,0,
139,140,Towards fault tolerance pervasive computing,"Pervasive computing exists in the user's environment, the technology is sustainable if it is invisible to the user and does not intrude the user's consciousness. This requires that functioning of the multitude of devices in the environment be oblivious to the user. Therefore, the system has to be resilient to various kinds of faults and should be able to function despite faults. In addition, pervasive computing provides a platform for context-aware computing that enables automatic configuration of a pervasive system based on the environment context. The aim of this article is to highlight the various challenges and issues that confront fault tolerance pervasive computing, discuss their implications, prevent some solutions to these problems, and describe how some of these solutions are implemented in our system.",2005,0,
140,141,Globally optimal uneven error-protected packetization of scalable code streams,"In this paper, we present a family of new algorithms for rate-fidelity optimal packetization of scalable source bit streams with uneven error protection. In the most general setting where no assumption is made on the probability function of packet loss or on the rate-fidelity function of the scalable code stream, one of our algorithms can find the globally optimal solution to the problem in O(N<sup>2</sup>L<sup>2</sup>) time, compared to a previously obtained O(N<sup>3</sup>L<sup>2</sup>) complexity, where N is the number of packets and L is the packet payload size. If the rate-fidelity function of the input is convex, the time complexity can be reduced to O(NL<sup>2</sup>) for a class of erasure channels, including channels for which the probability function of losing n packets is monotonically decreasing in n and independent erasure channels with packet erasure rate no larger than N/2(N + 1). Furthermore, our O(NL<sup>2</sup>) algorithm for the convex case can be modified to rind an approximation solution for the general case. All of our algorithms do away with the expediency of fractional bit allocation, a limitation of some existing algorithms.",2004,0,
141,142,A Fault Recovery Approach in Fault-Tolerant Processor,"A fault recovery scheme of a fault-tolerant processor for embedded systems is introduced in this paper. The microarchitecture of the fault-tolerant processor called RSED is modified from superscalar processor architecture. The fault-tolerant mechanism of RSED is implemented mainly using temporal redundancy technique. Fault recovery scheme is an important part of the fault-tolerant mechanism. In order to resolve the problem of possible single point of failures, a novel TMR approach is adopted to generate re-execution instruction address. Compared with similar works, the fault recovery scheme proposed can recover processor execution more reliably.",2009,0,
142,143,Reaching efficient fault-tolerance for cooperative applications,"Cooperative applications are widely used, e.g. as parallel calculations or distributed information processing systems. Whereby such applications meet the users demand and offer a performance improvement, the susceptibility to faults of any used computer node is raised. Often a single fault may cause a complete application failure. On the other hand, the redundancy in distributed systems can be utilized for fast fault detection and recovery. So, we followed an approach that is based an duplication of each application process to detect crashes and faulty functions of single computer nodes. We concentrate on two aspects of efficient fault-tolerance-fast fault detection and recovery without delaying the application progress significantly. The contribution of this work is first a new fault detecting protocol for duplicated processes. Secondly, we enhance a roll forward recovery scheme so that it is applicable to a set of cooperative processes in conformity to the protocol",2000,0,
143,144,Straight-Edge Extraction in Distorted Images Using Gradient Correction,"Many camera lenses, particularly low-cost or wide-angle lenses, can cause significant image distortion. This means that features extracted naively from such images will be incorrect. A traditional approach to dealing with this problem is to digitally rectify the image to correct the distortion, and then to apply computer vision processing to the corrected image. However, this is relatively expensive computationally, and can introduce additional interpolation errors. We propose instead to apply processing directly to the distorted image from the camera, modifying whatever algorithm is used to correct for the distortion during processing, without a separate rectification pass. In this paper we demonstrate the effectiveness of this approach using the particular classic problem of gradient-based extraction of straight edges. We propose a modification of the Burns line extractor that works on a distorted image by correcting the gradients on the fly using the chain rule, and correcting the pixel positions during the line-fitting stage. Experimental results on both real and synthetic images under varying distortion and noise show that our gradient-correction technique can obtain approximately a 50% reduction in computation time for straight-edge extraction, with a modest improvement in accuracy under most conditions.",2009,0,
144,145,Automatic Spelling Correction Rule Extraction and Application for Spoken-Style Korean Text,"Nowadays, spoken-style text is prevailing because lots of information are being written in spoken-style such as Short-Message-Service(SMS) messages. However, the spokenstyle text contains more spelling errors than the traditional written-style text. In this paper, we propose a rule-based spelling correction system which can automatically extract spelling correction rules from the correction corpus and apply extracted rules to spelling errors of input sentences. In order to preserve both high precision and high recall, we devise a candidate-elimination algorithm which determines appropriate context size of spelling correction rules based on rule accuracy. Experimental results showed that the proposed system can extract 42,537 spelling correction rules and apply the rules to correct spelling errors on the test corpus and thus, the rate of precision is increased from 31.08% to 79.04% on the basis of message unit.",2007,0,
145,146,Value-based scheduling of distributed fault-tolerant real-time systems with soft and hard timing constraints,We present an approach for scheduling of fault-tolerant embedded applications composed of soft and hard real-time processes running on distributed embedded systems. The hard processes are critical and must always complete on time. A soft process can complete after its deadline and its completion time is associated with a value function that characterizes its contribution to the quality-of-service of the application. We propose a quasi-static scheduling algorithm to generate a tree of fault-tolerant distributed schedules that maximize the application's quality value and guarantee hard deadlines.,2010,0,
146,147,Trend analysis techniques for incipient fault prediction,"This paper extends the application of the Laplace Test Statistic for trend analysis and prediction of incipient faults for power systems. The extensions proposed in this paper consider the situation where two parameters believed to contribute explicitly to the eventual failure are monitored. The developed extensions applied to actual incipient failure events provide promising results for prediction of the impending failure. It is demonstrated that by incorporating two parameters in the trend analysis, the robustness to outliers is increased and the flexibility is augmented by increasing the degrees of freedom in the generation of the alarm signal.",2009,0,
147,148,Notice of Retraction<BR>Comprehensive Evaluation of Certain Power Vehicle Fault Based on Rough Sets,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>With the advent of intelligence, people put forward higher request to the fault diagnosis of weapon. As the base of certain weapon system, the power vehicle can work well or not is directly relate to the tasks of the battle and training can complete smoothly or not. Firstly, the paper introduces the fundamental conception of Rough Sets and gives the method of ascertaining the evaluation value and the weights. Secondly, based on the operation characteristics of certain power vehicle, the paper analyses and creates an index system of the power vehicle, then RS is used to impersonally distribute weights. Finally, the comprehensive evaluation model based RS is given out, and the fault sequence and evaluating grade are educed. The computing results show this practical model is very significant to the weapon maintenance and offers a reference for the evaluation of civil equipment fault.",2009,0,
148,149,Fisher Discriminance of Fault Predict for Decision-Making Systems,"A new technology of fault prediction was presented based on the neural network and Fisher discriminance in statistics. First, many enough character of running situation of decision-making were extracted from the real-time observation data. Secondly, the FP software systems were designed and the algorithm of FP of decision-making systems was presented. Finally, a simply example indicated that the algorithm is effectively.",2009,0,
149,150,FAIL-MPI: How Fault-Tolerant Is Fault-Tolerant MPI?,"One of the topics of paramount importance in the development of cluster and grid middleware is the impact of faults since their occurrence in grid infrastructures and in large-scale distributed systems is common. MPI (message passing interface) is a popular abstraction for programming distributed and parallel applications. FAIL (FAult Injection Language) is an abstract language for fault occurrence description capable of expressing complex and realistic fault scenarios. In this paper, we investigate the possibility of using FAIL to inject faults in a fault-tolerant MPI implementation. Our middleware, FAIL-MPI, is used to carry quantitative and qualitative faults and stress testing",2006,0,
150,151,An Integrated Silicon Carbide (SiC) Based Single Phase Rectifier with Power Factor Correction,"Silicon carbide (SiC) based power devices exhibit superior properties such as very low switching losses, fast switching behavior, improved reliability and high temperature operation capabilities. These properties contribute toward the ability to increase switching frequency, decrease the size of passive components and switches, and reduce the need for cooling, thus making the devices an excellent candidate for AC/DC power supplies. In this paper a SiC based integrated single phase rectifier with power factor correction (PFC) is presented. The proposed topology has many advantages including fewer semiconductor components; the presence of AC side inductor resulting in reduced EMI interference, and higher performance. This approach takes advantage of the superior properties of SiC devices and the reduced number of devices in the proposed converter to achieve higher efficiency, smaller size and better performance at high temperature. A performance and efficiency evaluation of the rectifier is presented and the results are compared with benchmark Si solutions",2005,0,
151,152,Neutron Soft Errors in Xilinx FPGAs at Lawrence Berkeley National Laboratory,The Lawrence Berkeley National Laboratory cyclotron offers broad-spectrum neutrons for single event effects testing. We discuss results from this beamline for neutron soft upsets in Xilinx Virtex-4 and -5 field-programmable-gate-array (FPGA) devices.,2008,0,
152,153,TIP-OPC: a new topological invariant paradigm for pixel based optical proximity correction,"As the 193 nm lithography is likely to be used for 45 nm and even 32 nm processes, much more stringent requirement will be posed on optical proximity correction (OPC) technologies. Currently, there are two OPC approaches - the model-based OPC (MB-OPC) and the inverse lithography technology (ILT). MB-OPC generates masks which is less complex compared with ILT. But ILT produces much better results than MB-OPC in terms of contour fidelity because ILT is a pixel based method. Observing that MB-OPC preserves the mask shape topologies which leads to a lower mask complexity, we combine the strengths of both methods - the topology invariant property and the pixel based mask representation. To the best of our knowledge, it is the first time that this topological invariant pixel based OPC (TIP-OPC) paradigm is proposed, which fills the critical hole of the OPC landscape and potentially has many new applications. Our technical novelty includes the lithography friendly mask topological invariant operations, the efficient fast Fourier transform based cost function sensitivity computation and the TIP-OPC algorithm. The experimental results show that TIP-OPC can achieve much better post OPC contours compared with MB-OPC while maintaining the mask shape topologies.",2007,0,
153,154,A New Iterative Approach to the Corrective Security-Constrained Optimal Power Flow Problem,"This paper deals with techniques to solve the corrective security-constrained optimal power flow (CSCOPF) problem. To this end, we propose a new iterative approach that comprises four modules: a CSCOPF which considers only a subset of potentially binding contingencies among the postulated contingencies, a (steady-state) security analysis (SSSA), a contingency filtering (CF) technique, and an OPF variant to check post-contingency state feasibility when taking into account post-contingency corrective actions. We compare performances of our approach and its possible variants with classical CSCOPF approaches such as the direct approach and Benders decomposition (BD), on three systems of 60, 118, and 1203 buses.",2008,0,
154,155,Systems Reliability Analysis and Fault Diagnosis Based on Bayesian Networks,"This paper presents the application of Bayesian networks(BN) to the reliability analysis and fault diagnosis of systems. For systems, it is essential to do reliability analysis. Also it is necessary to do fault diagnosis when a system failed, but the better way is to do fault diagnosis before the system has failed. Bayesian networks have many special characteristics, one of them is that they are parallel structures, so the time of solving is much shorter than that of many common methods. Bayesian networks permit not only computing the reliability indices of a system but also presenting the effect of each component or some components on the system reliability to distinguish the unsubstantial part of the system, so we can know which part is the weakest one of the system. Two examples proved the validity and superiority of the method in the application of the reliability analysis and fault diagnosis of system.",2009,0,
155,156,Prediction Error Prioritizing Strategy for Fast Normalized Partial Distortion Motion Estimation Algorithm,"A prediction error prioritizing-based normalized partial distortion search algorithm for fast motion estimation is proposed in this letter. The distortion behavior of each pixel in a macroblock is first analyzed to point out the priority/order of sum of absolute difference calculation. Afterward, the normalized partial distortion search algorithm is applied for half-stop of the distortion calculation. In addition, a dynamic search range decision algorithm is adopted for automatically changing the size of the search range to further increase the motion estimation speed. The computational complexity can be reduced significantly through the proposed algorithm, though leaving a PSNR degradation that could be dismissed.",2010,0,
156,157,Fault-Tolerant Policy for Optical Network Based Distributed Computing System,"The optical network based distributed computing system has been thought as a promising technology to support large-scale data-intensive distributed applications. For such a system with so many heterogeneous resources and middlewares involved, faults seem to be inevitable. However, for those applications that need to be finished before the given deadline, a fault in the system will lead to the failure of the application. Therefore, fault-tolerant policy is necessary to improve the performance of the system when faults could happen. In this paper, we address to the fault-tolerant problem for the optical network based distributed computing system. We first propose an overlay approach which applies the existing fault-tolerant policies for distributed computing and optical network. Then we present a joint fault-tolerant policy which takes into account the fault tolerance for computing resource and network resource in the same time. We compare the performances of different polices by simulation. The simulation results show that the joint fault-tolerant policy achieves much better performances compared to overlay approaches.",2008,0,
157,158,Silicon Wafer Defect Extraction Based on Morphological Filter and Watershed Algorithm,"Defect extraction techniques are studied regarding the silicon wafer surface defect. We design a new filter based on multiple structuring elements and suggest an improved marker-based and region merging watershed. To begin with, the filter which generalized close-opening and open-closing filter based on the morphological filter with multiple structuring elements is introduced to eliminate the noise and simplify the image and morphological gradient image while preserving the details. And then in order to reduce the over-segmentation of the watershed algorithm, this paper suggests an improved marker-based and region merging method, region average gray value and edge strength criterion is used in merging operation and has a good effect on segmentation. Finally, the improved watershed algorithm is applied to the filtered gradient image to get the defect contours. The experiments show that this method can eliminate the noises and extract accurately location and closed region contours, which lays a good foundation for defect feature extraction and selection.",2008,0,
158,159,Diagnostic fault detection & intelligent reconfiguration of fuel delivery systems,"The reliable operation of an engines fuel delivery system is fundamental. A failure in the fuel system that impacts the ability to deliver fuel to the engine will have an immediate effect on system performance and safety. There are very few diagnostic systems that monitor the health of the fuel system and even fewer that can accommodate for detected faults. Current diagnostic techniques call for careful maintenance of fuel system components. These techniques tend to be backward thinking in that they are based on previous experience which is not always a good indicator for future systems. This paper describes a technique developed at the Penn State Applied Research Laboratories Condition Based Maintenance Department for fault detection and reconfiguration for fuel delivery system components. This technique has been applied to a diesel engine test rig. The test rig is fully instrumented with sensors including those for fuel pressure. Even though this technique is being applied on a diesel engine, the approach is fully compatible to any fuel delivery system",2005,0,
159,160,Evaluation of security and fault tolerance in mobile agents,"The reliable execution of a mobile agent is a very important design issue in building a mobile agent system and many fault-tolerant schemes have been proposed so far. Security is a major problem of mobile agent systems, especially when money transactions are concerned . Security for the partners involved is handled by encryption methods based on a public key authentication mechanism and by secret key encryption of the communication. In this paper, we examine qualitatively the security considerations and challenges in application development with the mobile code paradigm. We identify a simple but crucial security requirement for the general acceptance of the mobile code paradigm, and evaluate the current status of mobile code development in meeting this requirement. We find that the mobile agent approach is the most interesting and challenging branch of mobile code in the security context. Therefore, we built a simple agent- based information retrieval application, the Traveling Information Agent system, and discuss the security issues of the system in particulars.",2008,0,
160,161,"Comprehensive Analysis of Performance, Fault-Tolerance and Scalability in Grid Resource Management System","The management of the large scale heterogeneous resources is a critical issue in grid computing. The resource management system (RMS) is an essential component of grids. To ensure the QoS of the upper layer service, it raises high requirement for the performance, fault-tolerance and scalability of RMS. In this paper, we study three typical structures of RMS, including centralized, hierarchical and peer-to-peer structures, and make a comprehensive analysis of performance, fault tolerance and scalability. We put forward the performance, fault tolerance and scalability evaluation metrics of the RMS, and give the mathematical expressions and detailed calculation processes. Besides, we make further discussions on the interactions of the performance, fault-tolerance and scalability, and make a comparison of the RMSs with the three typical structures. We believe that the results of this work will help system architects make informed choices for building the RMS.",2009,0,
161,162,Recent improvements on the specification of transient-fault tolerant VHDL descriptions: a case-study for area overhead analysis,"We present a new approach to design reliable complex circuits with respect to transient faults in memory elements. These circuits are intended to be used in harmful environments like radiation. During the design flow, this methodology is also used to perform an early-estimation of the obtained reliability level. Usually, this reliability estimation step is performed in the laboratory, by means of radiation facilities (particle accelerators). By doing so, the early-estimated reliability level is used to balance the design process into a trade-off between maximum area overhead due to the insertion of redundancy and the minimum reliability required for a given application. This approach is being automated through the development of a CAD tool (FT-PRO). Finally, we present also a case-study of a simple microprocessor used to analyze the FT-PRO performance in terms of the area overhead required to implement the fault-tolerant circuit.",2000,0,
162,163,Error calculation techniques and their application to the Antenna Measurement Facility Comparison within the European Antenna Centre of Excellence,"This paper gives an overview of the ongoing activities under the Antenna Measurement activity of the Antenna Centre of Excellence (ACE) network within the EU 6th framework research program. In particular, in this work an attempt is made to establish a common uncertainty estimation criteria in spherical near field and far field antenna measurement systems. The results from this activity are important instruments to verify the measurements accuracies for antenna measurement ranges as well as to investigate and evaluate possible improvements in measurement set-ups and procedures. These results will be used in the facility comparison campaigns in order to calculate a reference pattern for each of the high accuracy reference antennas (VAST 12, SATIMO SH800 and SATIMO SH2000) measured during the last 4 years by different institutions in Europe and US.",2007,0,
163,164,GNSS pseudorange error density tracking using Dirichlet Process Mixture,"In satellite navigation system, classical localization algorithms assume that the observation noise is white-Gaussian. This assumption is not correct when the signal is reflected on the surrounding obstacles. That leads to a decrease of accuracy and of continuity of service. To enhance the localization performances, a better observation noise density can be use in an adapted filtering process. This article aims to show how the Dirich-let Process Mixture can be employed to track the observation density on-line. This sequential estimation solution is adapted when the noise is non-stationary. The approach will be tested under a simulation scenario with multiple propagation conditions. Then, this density modeling will be used in Rao-Blackwellised Particle Filter.",2010,0,
164,165,On handling dependent evidence and multiple faults in knowledge fusion for engine health management,"Diagnostic architectures that fuse outputs from multiple algorithms are described as knowledge fusion or evidence aggregation. Knowledge fusion using a statistical framework such as Dempster-Shafer (D-S) has been used in the context of engine health management. Fundamental assumptions made by this approach include the notion of independent evidence and single fault. In most real world systems, these assumptions are rarely satisfied. Relaxing the single fault assumption in D-S based knowledge fusion involves working with a hyper-power set of the frame of discernment. Computational complexity limits the practical use of such extension. In this paper, we introduce the notion of mutually exclusive diagnostic subsets. In our approach, elements of the frame of discernment are subsets of faults that cannot be mistaken for each other, rather than failure modes. These subsets are derived using a systematic analysis of connectivity and causal relationship between various components within the system. Specifically, we employ a special form of reachability analysis to derive such subsets. The theory of D-S can be extended to handle dependent evidence for simple and separable belief functions. However, in the real world the conclusions of diagnostic algorithms might not take the form of simple or separable belief functions. In this paper, we present a formal definition of algorithm dependency based on three metrics: the underlying technique an algorithm is using, the sensors it is using, and the feature of the sensor that the algorithm is using. With this formal definition, we partition evidence into highly dependent, weakly dependent and independent evidence. We present examples from a Honeywell auxiliary power unit to illustrate our modified D-S method of evidence aggregation",2006,0,
165,166,Transmission Line Fault Location Using Two-Terminal Data Without Time Synchronization,"This letter presents a new transmission line fault location method that uses current and voltage sinusoidal phasors at both ends, without necessity of data synchronization. The main difference among the classical Johns method resides in the fact that the proposed method is based on magnitude of fault point voltage and does not demand exact phase angles of the acquired signals. Simulated and real case results are presented, showing that the proposed algorithm is robust, accurate, and provides adequate performance. Practical applications confirm that the synchronization is not really necessary, making the method faster and easier to apply than classical methods in many real situations",2007,0,
166,167,Measurement-based frame error model for simulating outdoor Wi-Fi networks,"We present a measurement-based model of the frame error process on a Wi-Fi channel in rural environments. Measures are obtained in controlled conditions, and careful statistical analysis is performed on the data, providing information which the network simulation literature is lacking. Results indicate that most network simulators use a frame loss model that can miss important transmission impairments even at a short distance, particularly when considering antenna radiation pattern anisotropy and multi-rate switching.",2009,0,
167,168,A new design technique for optimum logic filter using matrix type-B error correcting coding,"A brief examination in digital communications gives that the receiver has to decide and distinguish between a number of discrete signals in background noise. For this case an optimum filter is designed and some techniques are developed as f.i. the MAP (Maximum a posteriori probability), the maximum likelihood - ML, the matched filter, the Kalman filter, etc. In this paper we introduce a new design technique, which we called the optimum logic filter (OLF), using sophisticated matrix type-B error correcting coding.",2005,0,
168,169,Incremental fault-tolerant design in an object-oriented setting,"With the increasing emphasis on dependability in complex, distributed systems, it is essential that system development can be done gradually and at different levels of detail. We propose an incremental treatment of faults as a refinement process on object-oriented system specifications. An intolerant system specification is a natural abstraction from which a fault-tolerant system can evolve. With each refinement step a fault and its treatment are introduced, so the fault-tolerance of the system increases during the design process. Different kinds of faults are identified and captured by separate refinement relations according to how the tolerant system relates to abstract properties of the intolerant one in terms of safety, and liveness. The specification language utilized is object-oriented and based upon first-order predicates on communication traces. Fault-tolerance refinement relations are formalized within this framework",2001,0,
169,170,Fault-tolerant control of PMSM drive unit,"Since the Fuel-Cell Vehicle's demonstration in the public transport, its fault diagnosis and fault tolerant control strategy become more and more important. This paper studies on the PMSM drive of FCV and presents a sensorless control algorithm in the fault mode based analytical redundancy. Simulation analysis and experiment verification are presented to compare the control algorithm using Expanded Kalman Filter (EKF) and Phase Locked Loop.",2009,0,
170,171,The Learning with Errors Problem (Invited Survey),"In this survey we describe the Learning with Errors (LWE) problem, discuss its properties, its hardness, and its cryptographic applications.",2010,0,
171,172,Group communication protocols under errors,"Group communication protocols constitute a basic building block for highly dependable distributed applications. Designing and correctly implementing a group communication system (GCS) is a difficult task. While many theoretical algorithms have been formalized and proved for correctness, only few research projects have experimentally assessed the dependability of GCS implementations under complex error scenarios. This paper describes a thorough error-injection experimental campaign conducted on Ensemble, a popular GCS. By employing synthetic benchmark applications, we stress selected components of the GCS $the group membership service, the FIFO-ordered reliable multicast - under various error models, including errors in the memory (text and heap segments) and in the network messages. The data show that about 5-6% of the failures are due to an error escaping Ensemble's error-containment mechanism and manifesting as a fail silence violation. This constitutes an impediment to achieving high dependability, the natural objective of GCSs. Our results are derived for a particular system (Ensemble), and more investigation involving other GCSs is required to generalize the conclusions. Nevertheless, through an accurate analysis of the failure causes and the error propagation patterns, this paper offers insights into the design and the implementation of robust GCSs.",2003,0,
172,173,Extended fault modeling used in the space shuttle PRA,"A probabilistic risk assessment (PRA) has been completed for the space shuttle with NASA sponsorship and involvement. This current space shuttle PRA is an advancement over past PRAs conducted for the space shuttle in the technical approaches utilized and in the direct involvement of the NASA centers and prime contractors. One of the technical advancements is the extended fault modeling techniques used. A significant portion of the data collected by NASA for the space shuttle consists of faults, which are not yet failures but have the potential of becoming failures if not corrected. This fault data consists of leaks, cracks, material anomalies, and debonding faults. Detailed, quantitative fault models were developed for the space shuttle PRA which involved assessing the severity of the fault, detection effectiveness, recurrence control effectiveness, and mission-initiation potential. Each of these attributes was transformed into a quantitative weight to provide a systematic estimate of the probability of the fault becoming a failure in a mission. Using the methodology developed, mission failure probabilities were estimated from collected fault data. The methodology is an application of counter-factual theory and defect modeling which produces consistent estimates of failure rates from fault rates. Software was developed to analyze all the relevant fault data collected for given types of faults in given systems. The software allowed the PRA to be linked to NASA's fault databases. This also allows the PRA to be updated as new fault data is collected. This fault modeling and its implementation with FRAS was an important part of the space shuttle PRA.",2004,0,
173,174,DMTracker: finding bugs in large-scale parallel programs by detecting anomaly in data movements,"While software reliability in large-scale systems becomes increasingly important, debugging in large-scale parallel systems remains a daunting task. This paper proposes an innovative technique to find hard-to-detect software bugs that can cause severe problems such as data corruptions and deadlocks in parallel programs automatically via detecting their abnormal behaviors in data movements. Based on the observation that data movements in parallel programs typically follow certain patterns, our idea is to extract data movement (DM)-based invariants at program runtime and check the violations of these invariants. These violations indicate potential bugs such as data races and memory corruption bugs that manifest themselves in data movements. We have built a tool, called DMTracker, based on the above idea: automatically extract DM-based invariants and detect the violations of them. Our experiments with two real-world bug cases in MVAPICH/MVAPICH2, a popular MPI library, have shown that DMTracker can effectively detect them and report abnormal data movements to help programmers quickly diagnose the root causes of bugs. In addition, DMTracker incurs very low runtime overhead, from 0.9% to 6.0%, in our experiments with High Performance Linpack (HPL) and NAS Parallel Benchmarks (NPB), which indicates that DMTracker can be deployed in production runs.",2007,0,
174,175,Lightweight Fault-Tolerance for Peer-to-Peer Middleware,"We address the problem of providing transparent, lightweight, fault-tolerance mechanisms for generic peer-to-peer middleware systems. The main idea is to use the peer-to-peer overlay to provide for fault-tolerance rather than support it higher up in the middleware architecture, e.g. in the form of services. To evaluate our approach we have implemented a fault-tolerant middleware prototype that uses a hierarchical peer-to-peer overlay in which the leaf peers connect to sensors that provide data streams. Clients connect to the root of the overlay and request streams that are routed upwards through intermediate peers in the overlay up to the client. We report encouraging preliminary results for latency, jitter and resource consumption for both the non-faulty and faulty cases.",2010,0,
175,176,Automated Support for Propagating Bug Fixes,"We present empirical results indicating that when programmers fix bugs, they often fail to propagate the fixes to all of the locations in a code base where they are applicable, thereby leaving instances of the bugs in the code. We propose a practical approach to help programmers to propagate many bug fixes completely. This entails first extracting a programming rule from a bug fix, in the form of a graph minor of an enhanced procedure dependence graph. Our approach assists the programmer in specifying rules by automatically matching simple rule templates; the programmer may also edit rules or compose them from scratch. A graph matching algorithm for detecting rule violations is then used to locate the places in the code base where the bug fix is applicable. Our approach does not require that rules occur repeatedly in the code base. We present empirical results indicating that the approach nevertheless exhibits good precision.",2008,0,
176,177,Bandwidth effect on distance error modeling for indoor geolocation,"In this paper we introduce a model for the distance error measured from the estimated time of arrival (TOA) of the direct path (DP) between the transmitter and the receiver in a typical multipath indoor environment. We use the results of a calibrated Ray tracing software in a sample office environment. First we divide the whole floor plan into LOS and Obstructed LOS (OLOS), and then we model the distance error in each environment considering the variation of bandwidth of the system. We show that the behavior of the distance error in LOS environment can be modeled as Gaussian, while behavior of the OLOS is a mixture of Gaussian and exponential distribution. We also related the statistics of the distributions to the bandwidth of the system.",2003,0,
177,178,Impact of solid-state fault current limiters on protection equipment in transmission and distribution systems,"Solid-state fault current limiters (SSFCLs) offer a number of benefits when incorporated within transmission and distribution systems. SSFCLs can limit the magnitude of a fault current seen by a system using different methods, such as inserting a large impedance in the current path or controlling the voltage applied to the fault. However, these two methods can introduce a few problems when SSFCLs are used in a system along with other protection equipment such as protective relays and sensors. An experiment was designed and implemented to evaluate the behavior of the protective relays in a mimic distribution system with a SSFCL. This paper introduces the details of the experiment and the result shows that the distorted current and voltage waveforms resulting from the action of the SSFCL disturb the protective equipment.",2010,0,
178,179,Considering Fault Correction Lag in Software Reliability Modeling,"The fault correction process is very important in software testing, and it has been considered into some software reliability growth models (SRGMs). In these models, the time-delay functions are often used to describe the dependency of the fault detection and correction processes. In this paper, a more direct variable ""correction lag"", which is defined as the difference between the detected and corrected fault numbers, is addressed to characterize the dependency of the two processes. We investigate the correction lag and find that it appears Bell-shaped. Therefore, we adopt the Gamma function to describe the correction lag. Based on this function, a new SRGM which includes the fault correction process is proposed. And the experimental results show that the new model gives better fit and prediction than other models.",2008,0,
179,180,Effects of Defects on the Thermal and Optical Performance of High-Brightness Light-Emitting Diodes,"Defects in terms of voids, cracks, and delaminations are often generated in light-emitting diodes (LEDs) devices and modules. During various manufacturing processes, accelerated testing, inappropriate handling, and field applications, defects are most frequently induced in the early stage of process development. One loading is due to the nonuniform loads caused by temperature, moisture, and their gradients. In this research, defects in various cases are modeled by a nonlinear finite-element method (FEM) to investigate the existence of interfaces, interfacial open and contacts in terms of thermal contact resistance, stress force nonlinearity, and optical discontinuity, in order to analyze their effects on the LED's thermal and optical performance. The simulation results show that voids and delaminations in the die attachment would enhance the thermal resistance greatly and decrease the LED's light extraction efficiency, depending on the defects' sizes and locations generated in packaging.",2009,0,
180,181,The minimum worst case error of fuzzy approximators,"The approximation capability of fuzzy systems is an important topic of research when the systems are regarded as input-output maps. By using the notion of information-based complexity (IBC), we derive the minimum worst case error of a fuzzy approximator, which is independent of the detailed construction of the fuzzy rule bases",2001,0,
181,182,Evaluation of risk in canal irrigation systems due to non-maintenance using fuzzy fault tree approach,"The safety and performance of many existing irrigation systems could be improved by doing the preventive maintenance activities. Modeling canal irrigation systems in terms of condition and performance that can be directly correlated with particular canal system maintenance activities. There are two categories of scheduled maintenance activity in irrigation maintenance systems: maintenance may be targeted towards restoring deliveries (""restorative maintenance""), or towards reducing the risk of failures (""preventative maintenance""). This paper covers the latter kind of maintenance scheduling by 'risk analysis'. The purpose of this risk analysis is to forecast the impact of preventative maintenance on deliveries from the main channel systems. After gaining the experience from the preliminary risk analysis through questionnaire based survey and failure history of past record, a 'fuzzy fault tree (FFT)' method is developed for the rapid risk assessment and it is necessitated for the irrigation system manager/engineer. 'Risk analysis' of irrigation systems from the point of view of maintenance is studied and applied to the Tirunelveli Channel Systems located in India. The effectiveness is calculated for each preventative maintenance tasks and it is ranked according to the effectiveness/cost ratio.",2003,0,
182,183,Waveform matching approach for fault diagnosis of a high-voltage transmission line employing harmony search algorithm,"An accurate and effective technology for fault diagnosis of a high-voltage transmission line plays an important role in supporting rapid system restoration. The fault diagnosis of a high-voltage transmission line involves three major tasks, namely fault-type identification, fault location and fault time estimation. The diagnosis problem is formulated as an optimisation problem in this work: the variables involved in the fault diagnosis problem, such as the fault location, and the unknown variables such as ground resistance, are taken into account as optimisation variables; the sum of the discrepancy of the approximation components of the actual and expected waveforms is taken as the optimisation objective. Then, according to the characteristics of the formulated optimisation problem, the harmony search, an effective heuristic optimisation algorithm developed in recent years, is employed to solve this problem. Test results for a sample power system have shown that the developed fault diagnosis model and method are correct and efficient.",2010,0,
183,184,Fault tolerance in autonomic computing environment,"Since the characteristic of current information systems is the dynamic change of their configurations and scales with non-stop provision of their services, the system management should inevitably rely on autonomic computing. Since fault tolerance is one of the important system management issues, it should also be incorporated in an autonomic computing environment. This paper argues what should be taken into consideration and what approach could be available to realize the fault tolerance in such environments.",2002,0,
184,185,Fault Location Using Sparse IED Recordings,"Basic goal of power system is to continuously provide electrical energy to users. Like with any other system, failures in power system can occur. In those situations it is critical that remedial actions are applied as soon as possible. To apply correct remedial actions it is very important that accurate fault condition and location are detected. In this paper, different fault location algorithms followed with description of intelligent techniques used for implementation of corresponding algorithms are presented. New approach for fault location using sparse measurements is examined. According to available data, it decides between different algorithms and selects an optimal one. New approach is developed by utilizing different data structures in order to efficiently implement algorithm decision engine, which is presented in paper.",2007,0,
185,186,A Bio-network Based Fault-Tolerant Architecture for Supervisory System,"Supervisory systems show increasing importance in many plants to ensure the secure and stable production. And the fault tolerance ability is a key problem. Concerned with the problem of low automation level and high operation cost, a layered Bio-Network is studied inspired by biology system, and Bio-Entity as its functional unit is analyzed. In the bottom layer, the communication infrastructure is built on Web Service to hide the difference among underlying heterogeneous systems. In the Bio-System layer, each Bio-Entity is delegated by an agent to gain the functions. Then, in the application layer, various services are encapsulated upon Bio-Entity. The application interface makes the integration with other systems more convenient. Based on Bio-Network, a novel framework of supervisory system for typical waterworks is proposed. The fault tolerant functions are analyzed in the context of Bio-Network. The practical application proves that Bio-Network based system configuration is improved and the cost is reduced.",2008,0,
186,187,Error Analysis of the Complex Kronecker Canonical Form,"In some interesting applications in control and system theory, i.e. in engineering, in ecology (Leslie population model), in financial/actuarial (Leontief multi input - multi output) science, linear descriptor (singular) differential/difference equations with time-invariant coefficients and (non-) consistent initial conditions have been extensively used. The solution properties of those systems are based on the Kronecker canonical form, which is an important component of the Matrix Pencil Theory. In this paper, we present some preliminary results for the error analysis of the complex Kronecker canonical form based on the Euclidean norm. Finally, under some weak assumptions an interesting new necessary condition is also derived.",2010,0,
187,188,Joint Fault-Tolerant Design of the Chinese Space Robotic Arm,"In this paper, joint reliability design for the Chinese space robotic arm has been discussed. Redundant controller unit, redundant can bus communication unit and latch-up power protection unit have been outlined. The fault tree of the joint has been built. Moreover, the new algorithm of auto-adjust thresholding value has been presented for fault detection, and the fault-tolerant strategies of joint have been proposed. Experimental results demonstrate the effectiveness of the joint fault-tolerant design",2006,0,
188,189,Fault tolerance in scalable agent support systems: integrating DARX in the AgentScape framework,"Open multi-agent systems need to cope with the characteristics of the Internet, e.g., dynamic availability of computational resources, latency, and diversity of services. Large-scale multi-agent systems employed on wide-area distributed systems are susceptible to both hardware and software failures. This paper describes AgentScape, a multi-agent system support environment, DARX, a framework for providing fault tolerance in large scale agent systems, and a design for the integration of the two.",2003,0,
189,190,An empirical validation of the relationship between the magnitude of relative error and project size,"Cost estimates are important deliverables of a software project. Consequently, a number of cost prediction models have been proposed and evaluated. The common evaluation criteria have been MMRE, MdMRE and PRED(k). MRE is the basic metric in these evaluation criteria. The implicit rationale of using a relative error measure like MRE, rather than an absolute one, is presumably to have a measure that is independent of project size. We investigate if this implicit claim holds true for several data sets: Albrecht, Kemerer, Finnish, DMR and Accenture-ERP. The results suggest that MRE is not independent of project size. Rather, MRE is larger for small projects than for large projects. A practical consequence is that a project manager predicting a small project may falsely believe in a too low MRE. Vice versa when predicting a large project. For researchers, it is important to know that MMRE is not an appropriate measure of the expected MRE of small and large projects. We recommend therefore that the data set be partitioned into two or more subsamples and that MMRE is reported per subsample. In the long term, we should consider using other evaluation criteria.",2002,0,
190,191,Exact symbol-error probability analysis for orthogonal space-time block codes: two- and higher dimensional constellations cases,"Exact expressions are obtained for the symbol-error probability of orthogonal space-time block codes at the output of the coherent maximum-likelihood decoder in the general case of arbitrary input signal constellation and code. Such expressions are derived for the cases of both deterministic (fixed) and random Rayleigh/Ricean fading channels, and both the two- and higher dimensional constellations.",2004,0,
191,192,Which concurrent error detection scheme to choose ?,"Concurrent error detection (CED) techniques (based on hardware duplication, parity codes, etc.) are widely used to enhance system dependability. All CED techniques introduce some form of redundancy. Redundant systems we subject to common-mode failures (CMFs). While most of the studies of CED techniques focus on area overhead, few analyze the CMF vulnerability of these techniques. In this paper, we present simulation results to quantitatively compare various CED schemes based on their area overhead and the protection (data integrity) they provide against multiple failures and CMFs. Our results indicate that, for the simulated combinational logic circuits, although diverse duplex systems (with two different implementations of the same logic function) sometimes have marginally higher area overhead, they provide significant protection against multiple failures and CMFs compared to other CED techniques like parity prediction",2000,0,
192,193,Optimal scheduling of imprecise computation tasks in the presence of multiple faults,"With the advance of applications such as multimedia, image/speech processing and real-time AI, real-time computing models allowing to express the timeliness versus precision trade-off are becoming increasingly popular. In the imprecise computation model, a task is divided into a mandatory part and an optional part. The mandatory part should be completed by the deadline even under worst-case scenario; however, the optional part refines the output of a mandatory part within the limits of the available computing capacity. A non-decreasing reward function is associated with the execution of each optional part. Since the mandatory parts have hard deadlines, provisions should be taken against faults which may occur during execution. An FT-Optimal framework allows the computation of a schedule that simultaneously maximizes the total reward and tolerates transient faults of mandatory parts. We extend the framework to a set of tasks with multiple deadlines, multiple recovery blocks and precedence constraints among them. To this aim, we first obtain the exact characterization of imprecise computation schedules which can tolerate up to k faults, without missing any deadlines of mandatory parts. Then, we show how to generate FT-Optimal schedules in an efficient way. Our solution works for both linear and general concave reward functions",2000,0,
193,194,Initial Evaluation of the Fracture Behavior of Piezoelectric Single Crystals Due to Artificial Surface Defects,"This study is part of a new research program to develop fundamental understanding of the fracture and fatigue behavior of piezoelectric single crystals through the combination of computational and experimental approaches. In this work we present 1) experimental results on the creation of artificial surface defects in piezoelectric single crystals using a focused ion beam (FIB) system and 2) initial observations on the crystal's fracture behavior under an electrical field. The major advantage of using a FIB is that one can control the size, shape, and orientation of artificial defects precisely, allowing realistic surface defects, e.g., half-penny-shaped, 100 mum long, <1 mum wide, and 50 mum deep. We have demonstrated that multiple artificial defects with varying inclination angles relative to the specimen's crystallographic orientation can be machined in a few hours. In this paper, we report the experimental details of the FIB milling, typical defect shape, and initial results on the effects of high electric field on the fracture behavior of single crystals.",2006,0,
194,195,A parallel and fault tolerant file system based on NFS servers,"One important piece of system software for clusters is the parallel file system. All current parallel file systems and parallel I/O libraries for clusters do not use standard servers, thus it is very difficult to use these systems in heterogeneous environments. However why use proprietary or special-purpose servers on the server end of a parallel file system when you have most of the necessary functionality in NFS servers already? This paper describes the fault tolerance implemented in Expand (Expandable Parallel File System), a parallel file system based on NFS servers. Expand allows the transparent use of multiple NFS servers as a single file system, providing a single name space. The different NFS servers are combined to create a distributed partition where files are stripped. Expand requires no changes to the NFS server and uses RPC operations to provide parallel access to the same file. Expand is also independent of the clients, because all operations are implemented using RPC and NFS protocol. Using this system, we can join heterogeneous servers (Linux, Solaris, Windows 2000, etc.) to provide a parallel and distributed partition. Fault tolerance is achieved using RAID techniques applied to parallel files. The paper describes the design of Expand and the evaluation of a prototype of Expand, using the MPI-IO interface. This evaluation has been made in Linux clusters and compares Expand with PVFS.",2003,0,
195,196,Detection and correction of limit cycle oscillations in second -order recursive digital filter,"In this paper the effects of limit cycle oscillations in recursive second order digital filter is studied and the remedies for curing the problems of limit cycle oscillations are described. Limit cycle deletion using state space representation for second order system implemented with finite word-length register, is depicted. Necessary and sufficient condition to prevent limit cycle oscillation has also been described.",2005,0,
196,197,A Flexible Macroblock Scheme for Unequal Error Protection,"This paper proposes an enhanced error protection scheme using flexible macroblock ordering in H.264/AVC. The algorithm uses a two-phase system. In the first phase, the importance of every macroblock is calculated based on its influence on the current frame and future frames. In the second phase, the macroblocks with the highest impact factor are grouped together in a separate slice group using the flexible macroblock ordering feature of H.264/AVC. By using an unequal error protection scheme, the slice group containing the most important macroblocks can be better protected than the other slice group. The proposed algorithm offers better concealment opportunities than the algorithms which are predefined for flexible macroblock ordering in H.264/AVC.",2006,0,
197,198,Automated defect to fault translation for ASIC standard cell libraries,"Popular generic fault models, which exhibit limited realism for different IC technologies, have been widely misused due to their simplicity and cost-effective implementation. This paper introduces a system for deriving accurate, technology specific fault models that are based on analog defect simulation. The technique is formally defined and a systematic approach is developed. It is supported by a new software tool that provides a push-button solution for the previously tedious task of obtaining accurate ASIC cell defect to fault mappings. Furthermore, upon completion of the cell defect analysis, the tool automatically generates VITAL compliant, defect-injectable, VHDL cell models",2001,0,
198,199,A comparison of neural networks and model-based methods applied for fault diagnosis of electro-hydraulic control systems,The paper aims to investigate two advanced methods used in fault diagnosis of electro-hydraulic (EH) control systems. The theoretical background of the neural network method and model-based approach are presented and the implementation of these methods is summarised with procedures in easy steps to follow for application. The pros and cons of these methods are also analysed based on fault detection capability. It is concluded that a combination of the neural network method and the model-based approach will be beneficial.,2002,0,
199,200,A Predictive Fault Tolerance Agent based on Ubiquitous Computing for A Home Study System,"DOORAE (Distance Object Oriented Collaboration Environment) is a framework for supporting development on multimedia collaborative environment. It provides functions well capable of developing multimedia distance education system for students as well as teachers. It includes session management, access control, concurrency control and handling late comers. There are two approaches to software architecture on which applications for multimedia distance education environment in situation-aware middleware are based. This paper proposes a new model of fault tolerance agent based on situation-aware ubiquitous computing for a multimedia home study system which is based on CARV.",2007,0,
200,201,Model of stator inter-turn short circuit fault in doubly-fed induction generators for wind turbine,"The doubly fed induction generator (DFIG) is an important component of wind turbine systems. It is necessary to identify incipient faults quickly. This paper proposes a complete simulation model of DFIG in wind turbine about inter-turn short circuit fault at stator windings, which is based on multi-circuit theory. A detail analysis about simulation results is presented, especially about short circuit current. By analysis, the apparent 150 Hz, 450 Hz and current phase angle difference are taken as fault features and the fault phase also can be detected by phase angle difference. Both simulated results and experimental results of emulated inter-turn short circuit fault by paralleling a resistance with phase A are carried out. They verify the preceding analysis results. Moreover, their coincidence certificates this model is good and simulation results of inter-turn short circuit fault are correct.",2004,0,
201,202,Self-healing strategies for component integration faults,"Software systems increasingly integrate Off-The-Shelf (OTS) components. However, due to the lack of knowledge about the reused OTS components, this integration is fragile and can cause in the field a lot of failures that result in dramatic consequences for users and service providers, e.g. loss of data, functionalities, money and reputation. As a consequence, dynamic and automatic fixing of integration problems in systems that include OTS components can be extremely beneficial to increase their reliability and mitigate these risks. In this paper, we present a technique for enhancing component-based systems with capabilities to self-heal common integration faults by using a predetermined set of healing strategies. The set of faults that can be healed has been determined from the analysis of the most frequent integration bugs experienced by users according to data in bug repositories available on Internet. An implementation based on AOP techniques shows the viability of this technique to heal faults in real case studies.",2008,0,
202,203,Recursive Evaluation of Fault Tolerance Mechanisms for SLA Management,"Service level agreements (SLAs) have been introduced into the grid in order to build a basis for its commercial uptake. The challenge for Grid providers in agreeing and operating SLA-bound jobs is to ensure their fulfillment even in the case of failures. Hence, fault-tolerance mechanisms are an essential means of the provider's SLA management. The high utilization of commercial operated clusters leads to scenarios in which typically a job migration effects other jobs scheduled. The effects result from the unavailability of enough free resources which would be needed to catch all resource outages. Consequently before initiating a migration, its effects for other jobs have to be compared and the initiation of fault- tolerance (FT-) mechanisms have to be evaluated recursively. This paper presents a measurement for the benefit of initiating a FT-mechanism, the recursive evaluation, and termination condition. Performing such an impact evaluation of an initiated chain of FT-mechanisms is often more profitable than performing a single FT-mechanism and accordingly this is important for the Grid commercialization.",2008,0,
203,204,Corrective control strategies in case of infeasible operating situations,"A new method that deals with power systems infeasible operating situations is proposed in this paper. In case these situations occur, appropriate corrective actions must be efficiently obtained and quickly implemented. In order to accomplish this, it is necessary (a) to quantify the systems unsolvability degree (UD), and (b) to determine a corrective control strategy to pull the system back into the feasible operation region. UD is determined through the smallest distance between the infeasible (unstable) operating point and the feasibility boundary in parameter (load) space. In this paper the control strategies can be obtained by two methods, namely the proportionality method (PM) and the nonlinear programming based method (NLPM). Capacitor banks, tap changing transformers and load shedding are the usual controls available. Simulations have been carried out, for small to large systems, under contingency and heavy load situations, in order to show the efficiency of the proposed method. It can be a very useful tool in operation planning studies, particularly in voltage stability analysis.",2001,0,
204,205,Fault detection using phenomenological models,"There exist many different established approaches to detect system faults. This paper discusses the various system models and the associated fault detection techniques. Specifically, phenomenological models are presented in detail. Fault detection using principal components analysis and the cluster and classify method is illustrated with real operational data from an electrically powered vehicle.",2003,0,
205,206,Using Search Methods for Selecting and Combining Software Sensors to Improve Fault Detection in Autonomic Systems,"Fault-detection approaches in autonomic systems typically rely on runtime software sensors to compute metrics for CPU utilization, memory usage, network throughput, and so on. One detection approach uses data collected by the runtime sensors to construct a convex-hull geometric object whose interior represents the normal execution of the monitored application. The approach detects faults by classifying the current application state as being either inside or outside of the convex hull. However, due to the computational complexity of creating a convex hull in multi-dimensional space, the convex-hull approach is limited to a few metrics. Therefore, not all sensors can be used to detect faults and so some must be dropped or combined with others. This paper compares the effectiveness of genetic-programming, genetic-algorithm, and random-search approaches in solving the problem of selecting sensors and combining them into metrics. These techniques are used to find 8 metrics that are derived from a set of 21 available sensors. The metrics are used to detect faults during the execution of a Java-based HTTP web server. The results of the search techniques are compared to two hand-crafted solutions specified by experts.",2010,0,
206,207,Experimental study on the impact of endoscope distortion correction on computer-assisted celiac disease diagnosis,"The impact of applying barrel distortion correction to endoscopic imagery in the context of automated celiac disease diagnosis is experimentally investigated. For a large set of feature extraction techniques, it is found that contrasting to intuition, no improvement but even significant result degradation of classification accuracy can be observed. For techniques relying on geometrical properties of the image material (shape), moderate improvements of classification accuracy can be achieved. Reasons for this somewhat unexpected results are discussed and ways how to exploit potential distortion correction benefits are sketched.",2010,0,
207,208,A comparison of phase space reconstruction and spectral coherence approaches for diagnostics of bar and end-ring connector breakage faults in polyphase induction motors using current waveforms,"Two signal (waveform) analysis approaches are investigated in this paper for motor drive fault identification-one linear and the other nonlinear. Twenty-one different motor-drive operating conditions including healthy, 1 through 10 broken bars, and 1 through 10 broken end-ring connectors are investigated. Highly accurate numerical simulations of current waveforms for the various operating conditions are generated using the time stepping coupled finite element-state space method for a 208-V, 60-Hz, 2-pole, 1.2-hp, squirrel cage 3-phase induction motor. The linear signal analysis method is based on spectral coherence, whereas the nonlinear signal analysis method is based on stochastic models of reconstructed phase spaces. Conclusions resulting from the comparisons of these two methods are drawn.",2002,0,
208,209,Ontology-based fault diagnosis for industrial control applications,"Traditional fault detection systems in industrial control applications are just able to report occurring faults. Fault diagnosis systems are more desirable for plant operators, as such systems are capable to reduce the number of occurring alarms by elimination of consecutive alarms and prioritization of critical alarms. The disadvantage of those systems is that they have to be implemented anew for every control application, as the system dependencies vary from application to application. Ontology-based fault diagnosis systems do not have this disadvantage. Only the ontology has to be created for the new system, which greatly reduces time and effort for new systems, as old ontologies can be reused for the new system.",2010,0,
209,210,Ground control for the geometric correction of PAN imagery from Indian remote sensing (IRS) satellites,"Efficacy of a hand-held global positioning system (GPS) receiver in stand-alone mode of GPS measurements was investigated by taking measurements on different dates. These measurements were compared to those observed by DGPS and total station for the validation of hand-held GPS receiver accuracy. Ground control point (GCP) coordinates were derived from 1:25,000 and 1:50,000 scale topographic maps, and by using two different types of GPS receivers- a stand-alone hand-held and dual frequency DGPS receivers. GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using affine mapping function. GCPs derived from maps yielded root mean squares (RMS) error from 15 to 35 m respectively. However, GCPs derived by DGPS or stand-alone mode hand-held GPS receiver gave RMS error in the range of 3 to 6 meter, which is very close to spatial resolution of PAN sensor imagery (5.8 m). The mean values of GCP coordinates observed with the help of hand-held GPS receiver in stand-alone mode might prove a cost effective solution for the determination of GCP coordinates in the geometric correction of current high-resolution imagery from IRS satellites.",2003,0,
210,211,Synchronization Probabilities using Conventional and MVDR Beam Forming with DOA Errors,"In this paper, code synchronization probabilities of the direct sequence spread spectrum (DS/SS) system are investigated when the receiver utilizes an adaptive antenna array. Performance studies of three beam forming algorithms in the presence of direction-of-arrival (DOA) errors are presented. The investigated algorithms are the conventional, the minimum variance distortionless response (MVDR), and the MVDR+ADL where the MVDR algorithm is enhanced against DOA error via the adaptive diagonal loading (ADL). The paper includes a large number of analytical and simulation results where the effects of DOA errors are investigated. It can be concluded that the MVDR beam former is more sensitive to DOA errors than the conventional beam former, especially, at large DOA errors and high signal-to-noise-ratio (SNR) values. However, this sensitivity can be reduced notably by using the ADL.",2007,0,
211,212,Fault Tree Reuse Across Multiple Reasoning Paradigms,"The development of a diagnostic model can be a very time-consuming and manually intensive process. One must first analyze the Test Program Set (TPS) to determine the fault tree and then integrate with that any additional knowledge that can be obtained from external data sources (such as test results, maintenance actions from the various maintenance levels, run-time failure information, etc.). The diagnostic models defined in the IEEE Std. 1232-2002 (AI-ESTATE) each define a different method that can be used for a diagnostic reasoner. It has been determined that each of these models utilize the information found in the basic TPS fault tree. As the fault tree represents hard won engineering knowledge that is expensive to reproduce, it is desirable to share the fault tree representations across multiple reasoner models. This paper will layout how each model type in the AI-ESTATE standard utilizes the fault tree to perform diagnostics and how, through the use of the XML representation of the AI-ESTATE fault tree model, that basic fault tree can be shared between reasoner models. It would also be desirable to find a way to gain that fault tree knowledge without having to manually reproduce it. As such, this paper will also describe how that information can at least be semi-automatically extracted from TPS design artifacts.",2006,0,
212,213,Position location error analysis by AOA and TDOA using a common channel model for CDMA cellular environments,"AOA and TDOA are known to be promising methods and are developed separately. Combination or cooperation of the two methods has not been possible due to lack of the applicable channel models. The COST-207 model is utilized to provide the angular information for the AOA method. The angular characteristic of the channel can be obtained from the given temporal channel model, hence fusing of the approaches is now possible. Different properties of the methods are revealed to verify the proposition in literature. As a future research area, the fusing of the two different methods for better reliability and accuracy is mentioned",2000,0,
213,214,"Mutual coupling in microstrip antenna array: evaluation, reduction, correction or compensation","Mutual coupling between the antenna elements in a microstrip antenna array is a potential source of performance degradation, particularly in a highly congested environment. The degradation includes impedance mismatching, increased side-lobe level, deviation of the radiation pattern from the desired one, and decrease of gain due to the excitation of a surface wave. To deal with these problems, the first thing is to evaluate the mutual coupling and to select the element with low mutual coupling. Then, it is still desired to reduce the mutual coupling further by taking some measures. Finally, in certain critical cases, it is necessary to involve the mutual coupling effects accurately through numerical analysis, such as ultra low side lobe arrays and adaptive ing arrays. All these issues are discussed and some numerical examples are given. Due to limited space, the paper focuses mainly on the work done in our laboratory.",2005,0,
214,215,Geometric and shading correction for images of printed materials using boundary,"A novel technique that uses boundary interpolation to correct geometric distortion and shading artifacts present in images of printed materials is presented. Unlike existing techniques, our algorithm can simultaneously correct a variety of geometric distortions, including skew, fold distortion, binder curl, and combinations of these. In addition, the same interpolation framework can be used to estimate the intrinsic illumination component of the distorted image to correct shading artifacts. We detail our algorithm for geometric and shading correction and demonstrate its usefulness on real-world and synthetic data.",2006,0,
215,216,A novel co-evolutionary approach to automatic software bug fixing,"Many tasks in software engineering are very expensive, and that has led the investigation to how to automate them. In particular, software testing can take up to half of the resources of the development of new software. Although there has been a lot of work on automating the testing phase, fixing a bug after its presence has been discovered is still a duty of the programmers. In this paper we propose an evolutionary approach to automate the task of fixing bugs. This novel evolutionary approach is based on co-evolution, in which programs and test cases co-evolve, influencing each other with the aim of fixing the bugs of the programs. This competitive co-evolution is similar to what happens in nature for predators and prey. The user needs only to provide a buggy program and a formal specification of it. No other information is required. Hence, the approach may work for any implementable software. We show some preliminary experiments in which bugs in an implementation of a sorting algorithm are automatically fixed.",2008,0,
216,217,An NN-based atmospheric correction algorithm for Landsat/TM thermal infrared data,"Land surface temperature (LST) is a key variable for studies of global or regional land surface processes, energy and water cycle, and thus, has important applications in various areas. Atmospheric correction is a major issue in LST retrieval using remote sensing data because the presence of the atmosphere always influences the radiation from the ground to the space sensor. Atmospheric correction of thermal infrared (TIR) data for land surface temperature retrieval is to estimate the three atmospheric parameters: transmittance, path radiance and the downward radiance. Typically the atmospheric parameters are obtained using atmospheric profiles combined with a radiative transfer model (RTM). But this approach is time-consuming and expensive, which is impractical for high-speed (near-realtime) operational atmospheric correction. An artificial neural network (NN) based atmospheric correction model for Landsat/TM thermal infrared data is proposed. The multi-layer feed-forward neural network (MFNN) is selected, in which the atmospheric profiles (temperature, humidity and pressure), elevation and scan angle are the input variables, and the atmospheric parameters are the output variables. The MFNN is combined with the radiative transfer simulation, using MODTRAN 4.0 and the latest global assimilated data. Finally, the transmittance and path radiance derived by the MFNN-based algorithm is compared with MODTRAN4.0 results. The RMSE for both parameters are 0.0031 and 0.035 Wm<sup>-2</sup>sr<sup>-1</sup>m<sup>-1</sup>, respectively. The results indicate that the proposed approach can be a practical method for Landsat/TM thermal data in both accuracy and efficiency.",2010,0,
217,218,Fault tree analysis of a fire hazard of a power distribution cabinet with Petri Nets,Motivation of this study is to verify system safety analysis of HAVELSAN Peace Eagle Program developed hardware items for Ground Support Systems. A preliminary hazard analysis for each of the hardware developed items are performed and safety hazard analysis models are constructed with risk assessment of hazards based on their probability of occurrences for future operational and maintenance activities. An example for this kind of analysis the system safety fault tree analysis model of a Ground Support Segment Mission Simulator subsystem Power Distribution Adapter Cabinet design with hazardous risk assessments criteria according to the military standard specifications. Same analysis approach then modeled with Petri Nets that has extensions from fault tree analysis approach and enables the modeler to represent the probability of occurrences in the system design phase. Same model can be built in the specification phase which creates the potential for early validation of the system design behavior.,2010,0,
218,219,Three-Dimensional Pareto-Optimal Design of Inductive Superconducting Fault Current Limiters,"The inductive-type superconducting fault current limiters (LSFCLs) mainly consist of a primary copper coil, a secondary complete or partial superconductor cylinder, and a closed or open magnetic iron core. Satisfactory performance of such device significantly depends on optimal selection of its employed materials and construction dimensions, as well as its electrical, thermal, and magnetic parameters. Therefore, it is very important to identify a comprehensive model describing the LSFCL behavior in a power system prior to its fabrication. When a fault occurs, the dynamic model should essentially characterize the overall phenomena to compare the simulation results by varying LSFCL parameters to maximize the merits of a fault current limiter while minimizing its drawbacks during the normal state. The principle object of this paper is to achieve a feasible and full penetrative approach in 3-D alignments, i.e., a Pareto-optimal design of LSFCLs by means of multicriteria decision-making techniques after defining the LSFCL model in a power system CAD/electromagnetic transients including dc environment.",2010,0,
219,220,Stochastic fault tree analysis with self-loop basic events,"This paper presents an analytical approach for performing fault tree analysis (FTA) with stochastic self-loop events. The proposed approach uses the flow-graph concept, and moment generating function (MGF) to develop a new stochastic FTA model for computing the probability, mean time to occurrence, and standard deviation time to occurrence of the top event. The application of the method is demonstrated by solving one example.",2005,0,
220,221,Coverage gain estimation for multi-burst forward error correction in DVB-H networks,"An approach for increasing the reception robustness of mobile broadcast streaming services has been developed for mobile broadcast systems based on time-slicing, such as DVB-H, employing multi-burst FEC at link or application layer. Multiple bursts will be encoded jointly in order to overcome burst errors caused by signal level variations. The approach shows high potential which can be characterized by a link margin gain due to reduced CNR requirements to cope with fast fading and shadowing. Nevertheless, the achieved gain depends on several system parameters (encoding period and coding rate), the physical environment (correlation of shadowing and multi-path fading) and on the mobility of the users (velocity and trajectory). The paper deals with the coverage estimation and network gain due to multi-burst FEC for vehicular users in a realistic urban scenario. Since the user behavior has to be considered the gain cannot be directly included into the link budget. Thus, a methodology has been developed in order to estimate the coverage of multi-burst FEC services based on dynamic system-level simulations. Results are shown by means of simulations in realistic scenarios and field measurements in urban environments.",2009,0,
221,222,Dynamic strength scaling for delay fault propagation in nanometer technologies,"This paper proposes an algorithm for the detection of resistive delay faults in deep submicron technology using dynamic strength scaling, which is applicable for 45 nm and below. The approach uses an advanced coding system to build logical functions that are sensitive to strength and able to detect even the slightest voltage changes in the circuit. Such changes are caused by interconnection resistive behavior and result in timing-related defects.",2009,0,
222,223,LEON3 ViP: A Virtual Platform with Fault Injection Capabilities,"In addition to functional simulation for validation of hardware/software designs, there are additional robustness requirements that need advanced simulation techniques and tools to analyze the system behavior in the presence of faults. In this paper, we present the design of a fault injection framework for LEON3, a 32bit SPARC CPU based system used by the European Space Agency, described at Transaction Level using System C. First of all an extension of a previous XML formalization of basic binary faults, like memory and CPU registers corruption, is done in order to support TLM2.0transaction's parameters corruptions. Next a novel Dynamic Binary Instrumentation (DBI) technique for C++ binaries is used to insert fault injection wrappers in SystemC transaction path. For binary faults in model components the use of TLM2.0 transport_dbg is proposed. This way each component with fault injection capabilities exposes a standard interface to allow internal component inspection and modification.",2010,0,
223,224,Research on Transformer Fault Diagnosis Expert System Based on DGA Database,"This paper analyzes and designs the transformer fault diagnosis system based on dissolved gas analysis (DGA) database in which DGA data is managed by the Oracle database. The fault diagnosis module includes the single analyzing item and the Integrated analyzing item, such as, improvement three-ratio method, grey relational entropy, fuzzy clustering, artificial neural networks, and so on. They reduce the insufficiency in diagnosis method which is used now. The system realizes each function of the modules by using the lamination method, it is able to diagnose problems existing in oil chromatogram analysis data of transformer, and the accuracy of the system is also testified by practical example.",2009,0,
224,225,Modular fault recovery in timed discrete-event systems: application to a manufacturing cell,"This paper extends the previous results of the authors on fault recovery to timed discrete-event systems (TDES), and discusses the application of the proposed methodology to a manufacturing cell. It is assumed that the plant can be modelled as a TDES, the faults are permanent, and that a diagnosis system is available that detects and isolates faults with a bounded delay (expressed in clock ticks). Thus, the combination of the plant and the diagnosis system, as the system to be controlled, has three modes: normal, transient and recovery. Initially, the plant is in the normal mode. Once a fault occurs, the system enters the transient mode. After the fault is detected and isolated by the diagnosis system, the system enters the recovery mode. This framework does not depend on the diagnosis technique used, as long as lower and upper bounds for diagnosis delay are available. A modular switching supervisory scheme is proposed to satisfy the system specifications. The design consists of a normal-transient supervisor, and multiple recovery supervisors each for recovery from a particular failure mode. The issue of the nonblocking property of the system under supervision, and also supervisor admissibility (controllability), in particular coerciveness, are studied. The proposed approach is applied to a manufacturing cell consisting of two machines and two conveyors. A modular switching supervisor is designed to ensure the specifications in the normal mode are met. In cases of failure, the supervisor sends appropriate recovery commands so that the cell can complete its production cycle",2005,0,
225,226,Managing fault-induced delayed voltage recovery in Metro Atlanta with the Barrow County SVC,"Georgia Transmission Corporation (GTC) commissioned the Barrow County Static Var Compensator (SVC) with a continuous rating of 0 to +260 Mvar in June of 2008. This paper presents the northern Metro Atlanta Georgia area transmission system, the requirements for voltage and var support, the dynamic performance study used to verify performance of the SVC, and provides an overview of the SVC design and control strategy, including the SVC's response to an actual power system disturbance. The Barrow County SVC is connected to the 230 kV bus at the Winder Primary Substation to effectively manage the exposure to fault-induced delayed voltage recovery (FIDVR), where the system voltage remains low (<80%) for several seconds following a disturbance and potentially leading to voltage collapse. The SVC configuration includes two thyristor-switched capacitors for rapid insertion of reactive power following a disturbance to decrease voltage recovery time and control the system's dynamic performance.",2009,0,
226,227,Investigation of fault tolerant of direct torque control in induction motor drive,"AC drives based on direct torque control (DTC) of induction machines are known for their high dynamic performances, obtained with very simple control schemes. So many studies have been performed with ASIC or FPGA DTC implementation. In this paper, we investigate for the tolerance of such drive to sensors defects, when the control algorithm is to be implemented in an FPGA. So, authors specially focus on the influence of the FPGA implementation design on the DTC fault tolerance. Simulations are carried out with system generator (SG) toolbox working in the MATLAB/SIMULINK environment. Results are presented and discussed to evaluate the DTC operating under considered faults.",2004,0,
227,228,Operational Fault Detection in cellular wireless base-stations,"The goal of this work is to improve availability of operational base-stations in a wireless mobile network through non-intrusive fault detection methods. Since revenue is generated only when actual customer calls are processed, we develop a scheme to minimize revenue loss by monitoring real-time mobile user call processing activity. The mobile user call load profile experienced by a base-station displays a highly non-stationary temporal behavior with time-of-day, day-of-the-week and time-of-year variations. In addition, the geographic location also impacts the traffic profile, making each base-station have its own unique traffic patterns. A hierarchical base-station fault monitoring and detection scheme has been implemented in an IS-95 CDMA Cellular network that can detect faults at - base station level, sector level, carrier level, and channel level. A statistical hypothesis test framework, based on a combination of parametric, semi-parametric and non-parametric test statistics are defined for determining faults. The fault or alarm thresholds are determined by learning expected deviations during a training phase. Additionally, fault thresholds have to adapt to spatial and temporal mobile traffic patterns that slowly changes with seasonal traffic drifts over time and increasing penetration of mobile user density. Feedback mechanisms are provided for threshold adaptation and self-management, which includes automatic recovery actions and software reconfiguration. We call this method, Operational Fault Detection (OFD). We describe the operation of a few select features from a large family of OFD features in Base Stations; summarize the algorithms, their performance and comment on future work.",2006,0,
228,229,Fault Tolerance for Manufacturing Components,"This article proposes a multiagent system for industrial production elements that transfers the concept of fault tolerance to the manufacturing levels of the organisation, acting automatically under open protocols when there is degradation or failure of any of the components, ensuring that normal operation is resumed within a delimited time. The main characteristics of this system are the drastic reduction in recovery times, the support for the significant heterogeneity existing in these scenarios and their high level of automation, while practically dispensing with the intervention of system administrators.",2006,0,
229,230,A study of student strategies for the corrective maintenance of concurrent software,"Graduates of computer science degree programs are increasingly being asked to maintain large, multi-threaded software systems; however, the maintenance of such systems is typically not well-covered by software engineering texts or curricula. We conducted a think-aloud study with 15 students in a graduate-level computer science class to discover the strategies that students apply, and to what effect, in performing corrective maintenance on concurrent software. We collected think-aloud and action protocols, and annotated the protocols for a number of behavioral attributes and maintenance strategies. We divided the protocols into groups based on the success of the participant in both diagnosing and correcting the failure. We evaluated these groups for statistically significant differences in these attributes and strategies. In this paper, we report a number of interesting observations that came from this study. All participants performed diagnostic executions of the program to aid program comprehension; however, the participants that used this as their predominant strategy for diagnosing the fault were all unsuccessful. Among the participants that successfully diagnosed the fault and displayed high confidence in their diagnosis, we found two commonalities. They all recognized that the fault involved the violation of a concurrent-programming idiom. And, they all constructed detailed behavioral models (similar to UML sequence diagrams) of execution scenarios. We present detailed analyses to explain the attributes that correlated with success or lack of success. Based on these analyses, we make recommendations for improving software engineering curriculums by better training students how to apply these strategies effectively.",2008,0,
230,231,The feasibility study on the combined equipment between micro-SMES and inductive/electronic type fault current limiter,"The concept of the combined equipment between micro-SMES and inductive/electronic type FCL is proposed in this paper. Having the multifunction for a superconducting device, the new equipment can serve as the protective component for a dual power system. The specification of a testing model was determined and the transient performance was analyzed by Matlab software. The results show that the combined equipment is realizable for a dual power system application, where it has the major function of limiting fault current (FCL function) and the minor function of maintaining power fluctuation (SMES function).",2003,0,
231,232,Effective Static Analysis to Find Concurrency Bugs in Java,"Multithreading and concurrency are core features of the Java language. However, writing a correct concurrent program is notoriously difficult and error prone. Therefore, developing effective techniques to find concurrency bugs is very important. Existing static analysis techniques for finding concurrency bugs either sacrifice precision for performance, leading to many false positives, or require sophisticated analysis that incur significant overhead. In this paper, we present a precise and efficient static concurrency bugs detector building upon the Eclipse JDT and the open source WALA toolkit (which provides advanced static analysis capabilities). Our detector uses different implementation strategies to consider different types of concurrency bugs. We either utilize JDT to syntactically examine source code, or leverage WALA to perform interprocedural data flow analysis. We describe a variety of novel heuristics and enhancements to existing analysis techniques which make our detector more practical, in terms of accuracy and performance. We also present an effective approach to create inter-procedural data flow analysis using WALA for complex analysis. Finally we justify our claims by presenting the results of applying our detector to a range of real-world applications and comparing our detector with other tools.",2010,0,
232,233,A case study of evaluation technique for soft error tolerance on SRAM-based FPGAs,"SRAM-based field programmable gate arrays (FPGAs) are vulnerable to a single event upset (SEU), which is induced by radiation effect. Therefore, the dependable design techniques become important, and the accurate dependability analysis method is required to demonstrate their robustness. Most of present analysis techniques are performed by using full reconfiguration to emulate the soft error. However, it takes long time to analyze the dependability because it requires many times of reconfiguration to complete the soft error injection. In the present paper, we construct the soft error estimation system to analyze the reliability and to reduce the estimation time. Moreover, we apply Monte Carlo simulation to our approach, and identify trade-off between accuracy of error rate and estimation time. As a result of our experimentation for 8-bit full-adder and multiplier, we can show the dependability of the implemented system. Also, the constructed system can reduce the estimation time. According to the result, when performing about 50% circuit Monte Carlo simulation, the error rate is within 20%.",2010,0,
233,234,Effects of finite weight resolution and calibration errors on the performance of adaptive array antennas,"Adaptive antennas are now used to increase the spectral efficiency in mobile telecommunication systems. A model of the received carrier-to-interference plus noise ratio (CINR) in the adaptive antenna beamformer output is derived, assuming that the weighting units are implemented in hardware, The finite resolution of weights and calibration is shown to reduce the CINR. When hardware weights are used, the phase or amplitude step size in the weights can be so large that it affects the maximum achievable CINR. It is shown how these errors makes the interfering signals leak through the beamformer and we show how the output CINR is dependent on power of the input signals. The derived model is extended to include the limited dynamic range of the receivers, by using a simulation model. The theoretical and simulated results are compared with measurements on an adaptive array antenna testbed receiver, designed for the GSM-1800 system. The theoretical model was used to find the performance limiting part in the testbed as the 1 dB resolution in the weight magnitude. Furthermore, the derived models are used in illustrative examples and can be used for system designers to balance the phase and magnitude resolution and the calibration requirements of future adaptive array antennas",2001,0,
234,235,Fault Tolerant Methods for Intermitted Failures in Virtual Large Scale Disks,"Recently, the demand of low cost large scale storages increases. We developed VLSD (Virtual Large Scale Disks) toolkit for constructing virtual disk based distributed storages, which aggregate free spaces of individual disks. However, current implementation of VLSD can mask only stop failure but cannot mask other kinds of failures such as intermitted failure. In this paper, we introduce two classes to VLSD in order to increase the intermitted fault tolerance. One is Retry Disk which retries to read/write at failures, another is VotedRAID1 which masks failures by majority voting. In this paper, we describe these classes in detail and evaluate their fault tolerance.",2010,0,
235,236,Advances in scatter correction for 3D PET/CT,"We report on several significant improvements to the implementation of image-based scatter correction for 3D PET and PET/CT. Among these advances are: a new algorithm to scale the estimated scatter sinogram to the measured data, thereby largely compensating for external scatter; the ability to handle CT image truncation during this scaling; the option to iterate the scatter calculation for improved accuracy; the use of ordered subset estimation maximization (OSEM) reconstruction for the estimated emission images from which the scatter contributions are simulated; reporting of data quality parameters such as scatter and randoms fractions, and noise equivalent count rate (NECR), for each patient bed position; and extensive quality control output. Scatter correction (2 iterations, OSEM) typically requires 15-45 sec per bed. Very good agreement between the estimated scatter and measured emission data for several typical clinical scans is reported for CPS Pico-3D and HiRez LSO PET/CT systems. The data characteristics extracted during scatter correction are useful for patient specific count rate modeling and scan optimization",2004,0,
236,237,Time and frequency domain analyses based expert system for impulse fault diagnosis in transformers,"The presence of insulation failure in the transformer winding is detected using the voltage and current oscillograms recorded during the impulse test. Fault diagnosis in transformers has several parameters such as the severity of fault, the kind of fault and the location of the fault. Detection of major faults involving a large section of the coils have never been a big issue and several visual and computational methods have already been proposed by several researchers. The present paper describes an expert system based on re-confirmative method for the diagnosis of minor insulation failures involving small number of turns in transformers during impulse tests. The proposed expert system imitates the performance of an experienced testing personnel. To identify and locate a fault, an inference engine is developed to perform deductive reasoning based on the rules in the knowledge base and different statistical techniques. The expert system includes both the time-domain and frequency-domain analyses for fault diagnosis. The basic aim of the expert system is to provide a non-expert with the necessary information and interaction in order to make fault diagnosis in a friendly windowed environment. The rules for fault diagnosis have been so designed that these are valid for the range of power transformers used in practice up to a voltage level of 33 kV. The fault diagnosis algorithm has been tested using experimental results obtained for a 3 MVA transformer and simulation results obtained for 5 and 7 MVA transformers",2002,0,
237,238,Fault Detection Using the k-Nearest Neighbor Rule for Semiconductor Manufacturing Processes,"It has been recognized that effective fault detection techniques can help semiconductor manufacturers reduce scrap, increase equipment uptime, and reduce the usage of test wafers. Traditional univariate statistical process control charts have long been used for fault detection. Recently, multivariate statistical fault detection methods such as principal component analysis (PCA)-based methods have drawn increasing interest in the semiconductor manufacturing industry. However, the unique characteristics of the semiconductor processes, such as nonlinearity in most batch processes, multimodal batch trajectories due to product mix, and process steps with variable durations, have posed some difficulties to the PCA-based methods. To explicitly account for these unique characteristics, a fault detection method using the k-nearest neighbor rule (FD-kNN) is developed in this paper. Because in fault detection faults are usually not identified and characterized beforehand, in this paper the traditional kNN algorithm is adapted such that only normal operation data is needed. Because the developed method makes use of the kNN rule, which is a nonlinear classifier, it naturally handles possible nonlinearity in the data. Also, because the FD-kNN method makes decisions based on small local neighborhoods of similar batches, it is well suited for multimodal cases. Another feature of the proposed FD-kNN method, which is essential for online fault detection, is that the data preprocessing is performed automatically without human intervention. These capabilities of the developed FD-kNN method are demonstrated by simulated illustrative examples as well as an industrial example.",2007,0,
238,239,ANN based detection of electrical faults in generator-transformer units,"In the paper a model of decision system based on ANN, will be shown. As protected object the generator-transformer unit has been taking into consideration. The range of detected faults are initially narrows to faults of electromagnetic character (three-phase, two-phase, two-phase to earth and one-phase faults) within the generator - unit transformer - high voltage transmission line configuration.",2004,0,
239,240,Research on remote intelligent fault-diagnosis of CNC lathe based on bayesian networks,"Considering the development of smart machine tools and Internet-based manufacturing and in order to manage the manufacturing process more efficiently, a unit of remote intelligent fault-diagnosis based on Bayesian Networks (BN) was designed and software based on internet was realized as well as the case study concerning CNC lathe. It is a compensation of machine tool's self-detection whose major job is to find the fault of hardware and programming. The case study proved the reliability and advantages of the intelligent model based on BN.",2010,0,
240,241,Soft error optimization of standard cell circuits based on gate sizing and multi-objective genetic algorithm,"A radiation harden technique based on gate sizing and multi-objective genetic algorithm (MOGA) is developed to optimize the soft error tolerance of standard cell circuits. Soft error rate (SER), chip area and longest path delay are selected as the optimization goals and fast fitness evaluation algorithms for the three goals are developed and embedded into the MOGA. All the three goals are optimized simultaneously by optimally sizing the gates in the circuit, which is a complex NP-Complete problem and resolved by MOGA through exploring the global design space of the circuit. Syntax analysis technique is also employed to make the proposed framework can optimize not only pure combinational logic circuit but also the combinational parts of sequential logic circuit. Optimizing experiments carried out on ISCAS'85 and ISCAS'89 standard benchmark circuits show that the proposed optimization algorithm can decrease the SER 74.25% with very limited delay overhead (0.28%). Furthermore, the algorithm can also reduce the area for most of the circuit under test by average 5.23%. The proposed technique is proved to be better than other works in delay and area overhead and suitable to direct the design of soft error tolerance integrated circuits in high reliability realms.",2009,0,
241,242,Nonlinear Systems Fault Diagnosis with Differential Elimination,"The differential elimination algorithm is used to eliminate the non-observed variables of the nonlinear systems. By incorporating the algebraic observability and diagnosability concepts and using numerical differentiation algorithms, another approach to the certain classes of nonlinear systems fault diagnosis problem is presented.",2009,0,
242,243,Fault Tolerant Actuation for Steer-by-Wire Applications,"This paper introduces a R&D project concerned with the development of a fault-tolerant actuation system for steer-by-wire applications. The essential safety and reliability requirements for automotive vehicles are assessed. General redundancy schemes and current practices are examined. The paper then focuses on the use of actuators based on permanent magnetic brushless dc motors, and analyses internal fault-tolerant potentials of the actuator technology with possible control schemes evaluated. Finally key innovations that may provide practical and affordable solutions are discussed.",2007,0,
243,244,A procedure to correct the error in the structure function based thermal measuring methods,In this paper a methodology is presented to correct the systematic error of structure function based thermal material parameter measuring methods. This error stems from the fact that it is practically impossible to avoid parallel heat-flow paths in case of forced one-dimensional heat conduction. With the presented method we show how to subtract the effect of the parallel heat-flow paths from the measured structure function. With this correction methodology the systematic error of structure function based thermal material parameter measuring methods can be practically eliminated. Application examples demonstrate the accuracy increase obtained with the use of the method.,2004,0,
244,245,Backward-compatible robust error protection of JPEG XR compressed video,"The new JPEG XR image encoding standard offers a great compression rate while maintaining a good visual quality. Nonetheless, it has low error robustness, making it unusable in case of unreliable transmission over error prone channels, e.g., wireless channels. An improvement to the standard was developed, which can correct transmission errors, both bit or packet losses, and which is fully compatible with legacy decoders. Data interleaving and channel coding can offer a good protection against transmission errors; different levels of protection can be adopted, in order to trade-off between error protection capabilities and decompressed image quality.",2010,0,
245,246,Application of fuzzy neuro for generator stator earth fault detection,"In this paper the use of a fuzzy neural net for stator earth fault detection is presented. A generator model is simulated using EMTDC software. Earth faults are simulated between 0.1% to 100% distance points from the generator neutral. The combination of both EMTDC simulation and neural network presented in this paper introduces a new, complementary method that performs better in instances where the interpretation of traditional methods is somewhat dubious.",2004,0,
246,247,Research and Realization of Digital Circuit Fault Probe Location Process,"This paper presents three core files relating to circuit fault diagnosis which is generated by LASAR (logic automated stimulus response), i.e. fault dictionary, node truth table and pin connection table, analyses the content of fault dictionary, pin connection table and node truth table, finds the necessary information for fault location, summarizes the procedure of circuit test and fault location. Finally the digital circuit diagnosis system which can locate the fault on the pin of components is designed. With the help of probe, fault location of component pins can be accurately pinpointed.",2008,0,
247,248,New results for fault detection of untimed continuous Petri nets,"In this paper we study fault diagnosis of systems modeled by untimed continuous Petri nets. In particular, we generalize our previous works in this framework where we solved this problem only for special classes of continuous Petri nets, namely state machines and backward conflict free nets. We show that the price to pay for this generalization is that only three diagnosis states can be defined, rather than four. However, this is not a significant restriction because it is in accordance with all the literature on finite state automata.",2009,0,
248,249,Application of Particle Swarm Optimization and RBF Neural Network in Fault Diagnosis of Analogue Circuits,"BP neural network has the shortcoming of over-fitting, local optimal solution, which affects the practicability of BP neural network. RBF neural network is a feedforward neural network, which has the global optimal closing ability. However, the parameters in RBF neural network need determination. Particle swarm optimization is presented to choose the parameters of RBF neural network. The particle swarm optimization-RBF neural network method has high classification performance, and is applied to fault diagnosis of analogue circuits. Finally, the result of fault diagnosis cases shows that the particle swarm optimization - RBF neural network method has higher classification than BP neural network.",2009,0,
249,250,Development of a Testbench for Validation of DMT and DT2 Fault-Tolerant Architectures on SOI PowerPC7448,The purpose of TAFT fault tolerance studies conducted at CNES is to prepare the space community for the significant evolution linked to the usage of COTS components for developing spacecraft supercomputers. CNES has patented the DMT and DT2 fault-tolerant architectures with 'light' features. The development of a DMT/DT2 testbench based on a PowerPC7448 microprocessor from e2v is presented in this paper.,2008,0,
250,251,Compact Power Divider using Defected Ground Structure for Wireless Applications,"Use of different types of defected ground structures (DCS) has been reported in this paper to design compact power dividers in microstrip medium. Unit cell's (of DGS) equivalent circuit has been used to evaluate the performance of power divider. Based on this approach, compact two-way equal power dividers have been designed in GSM (900 MHz) band. Results show a size reduction of 35% and 32% for the power dividers using T shaped DGS and split ring DGS over the conventional power divider.",2008,0,
251,252,Utilisation of motion similarity in Colour-plus-Depth 3D video for improved error resiliency,"Robust 3D stereoscopic video transmission over error-prone networks has been a challenging task. Sustainability of the perceived 3D video quality is essential in case of channel losses. Colour-plus-Depth format on the other hand, has been popular for representing the stereoscopic video, due to its flexibility, low encoding cost compared to left-right stereoscopic video and backwards compatibility. Traditionally, the similarities existing between the colour and the depth map videos are not exploited during 3D video coding. In other words, both components are encoded separately. The similarities include the similarity in motion, image gradients and segments. In this work, we propose to exploit the similarity in the motion characteristics of the colour and the depth map videos by computing only a set of motion vectors and duplicating it for the sake of error resiliency. As the previous research has shown that the stereoscopic video quality is primarily affected by the colour texture quality, especially the motion vectors are computed for the colour video component and the corresponding vectors are used to encode the depth maps. Since the colour motion vectors are protected by duplication, the results have shown that both the colour video quality and the overall stereoscopic video quality are maintained in error-prone conditions at the expense of slight loss in depth map video coding performance. Furthermore, total encoding time is reduced by not calculating the motion vectors for depth map.",2010,0,
252,253,Embryonics+immunotronics: a bio-inspired approach to fault tolerance,"Fault tolerance has always been a standard feature of electronic systems intended for long-term missions. However, the high complexity of modern systems makes the incorporation of fault tolerance a difficult task. Novel approaches to fault tolerance can be achieved by drawing inspiration from nature. Biological organisms possess characteristics such as healing and learning that can be applied to the design of fault-tolerant systems. This paper extends the work on bio-inspired fault-tolerant systems at the University of York. It is proposed that by combining embryonic arrays with an immune inspired network, it is possible to achieve systems with higher reliability",2000,0,
253,254,"Video image based attenuation correction for PETbox, a preclinical PET tomograph","PETBox is a new simplified bench top PET scanner dedicated for pre-clinical imaging of mice. It has only two facing detector heads in a static gantry. Using iterative methods, limited-angle reconstruction of 3D images is possible. The geometry of the PETBox is such that very oblique emission angles are detected traversing significant lengths of tissue, making attenuation correction necessary. To that effect, we have developed a method by which two orthogonal optical views are combined to create a 3-dimensional estimate of the subject. This estimate is used to produce attenuation correction data that significantly improve the quantitative accuracy of the reconstructed images. In this paper, we present the method and evaluate its accuracy.",2009,0,
254,255,Analysis and design of SEPIC converter in boundary conduction mode for universal-line power factor correction applications,"In this paper, a SEPIC converter operated in boundary conduction mode for power factor correction applications with arbitrary output voltage is proposed, analyzed and designed. By developing an equivalent circuit model for the coupled inductor structure, a SEPIC converter with or without coupled inductors (and ripple current steering) can be analyzed and designed in a unified framework. Power factor correction under boundary conduction operation mode can be achieved conveniently using a simple commercially available control IC. Experimental results are provided to validate the circuit design",2001,0,
255,256,Categorization of minimum error forecasting zones using a geostatistic wind model,"In this paper a geostatistic wind direction model is applied to trace a wind speed map, based on data from official measurement weather stations distributed within the region of Andalucia-Spain. Each station's performance is assessed by comparing real measurements to those resulting from the linear interpolation of the rest. Once an error is associated to the station, the error is drawn in a map, in which minimum error zones can be delimited. Frequency and wind speed in each direction are the magnitudes of interest to get a first categorization of wind resources associated to the region. The interest of the method relies in the possibility of forecasting everywhere within the region with an error inside the tolerable margins.",2009,0,
256,257,A novel feature extraction and optimisation method for neural network-based fault classification in TCSC-compensated lines,"The suitability of fault classifiers introduced hitherto to operate correctly under a real TCSC transmission system remains a challenge since the computations are determined based on a number of postulations. This paper describes an alternative approach to fault classification in TCSC tines using artificial neural networks (ANNs). Special emphasis is placed on illustrating a combined wavelet transform and selforganising map (SOM) methodology to extract, validate and optimise the key characteristics of the fault transient phenomena in a TCSC line such that the input features to the ANNs are near optimal. As a result, it is shown that the fault classification proposed provides the ability to accurately classify the fault type, obviating the need for any predefined assumptions. Extensive simulation studies have been made to verify that the proposed method is both powerful and appropriate for fault classification.",2002,0,
257,258,Thermoreflectance imaging of defects in thin-film solar cells,We have identified and characterized various defects in thin-film a-Si and CIGS solar cells with sub-micron spatial resolution using thermoreflectance imaging. A megapixel silicon-based CCD was used to obtain noncontact thermal images simultaneously with visible electroluminescence (EL) images. EL can be indicative of pre-breakdown sites due to trap assisted tunneling and stress induced leakage currents. Physical defects appear at reverse bias voltages of 8 V in a-Si samples. Linear and nonlinear shunt defects are investigated as well as electroluminescent breakdown regions at reverse biases as low as 4.5 V. Pre-breakdown sites with electroluminescence are investigated.,2010,0,
258,259,Induction Motor-Drive Systems with Fault Tolerant Inverter-Motor Capabilities,"A low-cost fault tolerant drive topology for low- speed applications such as ""self-healing/limp-home"" needs for vehicles and propulsion systems, with capabilities for mitigating transistor open-circuit switch and short-circuit switch faults is presented in this paper. The present fault tolerant topology requires only minimum hardware modifications to the conventional off-the-shelf six-switch three-phase drive, with only the addition of electronic components such as triacs/SCRs and fast-acting fuses. In addition, the present approach offers the potential of mitigating not only transistor switch faults but also drive related faults such as rectifier diode short-circuit fault or dc link capacitor fault. In this new approach, some of the drawbacks associated with the known fault mitigation techniques such as the need for accessibility to a motor neutral, overrating the motor to withstand higher fundamental rms current magnitudes above its rated rms level, the need for larger size dc link capacitors, or higher dc bus voltage, are overcome here using the present approach. Given in this paper is a complete set of simulation results that demonstrate the soundness and effectiveness of the present topology.",2007,0,
259,260,On the relationships of faults for Boolean specification based testing,"Various methods of generating test cases based on Boolean specifications have previously been proposed. These methods are fault-based in the sense that test cases are aimed at detecting particular types of faults. Empirical results suggest that these methods are good at detecting particular types of faults. However, there is no information on the ability of these test cases in detecting other types of faults. The paper summarizes the relationships of faults in a Boolean expression in the form of a hierarchy. A test case that detects the faults at the lower level of the hierarchy will always detect the faults at the upper level of the hierarchy. The hierarchy helps us to better understand the relationships of faults in a Boolean expression, and hence to select fault-detecting test cases in a more systematic and efficient manner",2001,0,
260,261,Error sources in in-plane silicon tuning-fork MEMS gyroscopes,"This paper analyzes the error sources defining tactical-grade performance in silicon, in-plane tuning-fork gyroscopes such as the Honeywell-Draper units being delivered for military applications. These analyses have not yet appeared in the literature. These units incorporate crystalline silicon anodically bonded to a glass substrate. After general descriptions of the tuning-fork gyroscope, ordering modal frequencies, fundamental dynamics, force, and fluid coupling, which dictate the need for vacuum packaging, mechanical quadrature, and electrical coupling are analyzed. Alternative strategies for handling these engineering issues are discussed by introducing the Systron Donner/BEI quartz rate sensor, a successful commercial product, and the Analog Device (ADXRS), which is designed for automotive applications.",2006,0,
261,262,Flexible Error Concealment for H.264 Based on Directional Interpolation,"The losses of packets cannot he avoided if real-time video is transported over error prone environments. To conceal missing parts of video pictures, the spatial and temporal correlation feature of natural video sequences is used. However, in some cases - for instance in case of a scene change - there is no temporal correlation available and thus spatial error concealment has to be used. This article proposes flexible spatial error concealment based on directional interpolation method that performs well also if only two neighboring boundaries are used as common for H.264 spatially predicted frames. The proposed method was implemented and tested in a H.264 codec together with other error concealment methods to evaluate their performance",2005,0,
262,263,Nonlinear observers with approximately linear error dynamics: the multivariable case,"Exact error linearization uses nonlinear input-output injection to design observers with linear error dynamics in certain coordinates. This approach can only be applied nongenerically. We propose an observer for a wider class of multivariable systems which uniformly minimizes the nonlinear part of the system that cannot be canceled by nonlinear input-output injection. Our approach is numerical, constructive, and provides locally exponentially stable error dynamics. An example compares our design with a high-gain method",2001,0,
263,264,Time-Varying Network Fault Model for the Design of Dependable Networked Embedded Systems,"Dependability is becoming a key design aspect of today networked embedded systems (NES's) due to their increasing application to safety-critical tasks. Dependability evaluation must be based on modelling and simulation of faulty application behaviors, which must be related to faulty NES behaviors under actual defects. However, NES's behave differently from traditional embedded systems when testing activities are performed on them. In particular, issues arise on the definition of correct behavior, on the best point to observe it, and on the temporal properties of the faults to be injected. The paper describes these issues, discusses some possible solutions and presents a new time-varying network-based fault model to represent failures in a more abstract and efficient way. Finally, the fault model has been used to support the design of a network-based control application where packet losses, end-to-end delay and signal distortion must be carefully controlled.",2009,0,
264,265,Pilot signal-based real-time measurement and correction of phase errors caused by microwave cable flexing in planar near-field tests,Millimeter and submillimeter wave receivers in scanning planar near-field test systems are commonly based on harmonic mixing and thus require at least one flexible microwave cable to be connected to them. The phase errors originated in these cables get multiplied and added to the phase of the final detected signal. A complete submillimeter setup with on-the-fly measurement of phase errors is presented. The novel phase error correction system is based on the use of a pilot signal to measure the phase errors caused by cable flexing. The measured phase error surface in the quiet-zone region of a 310 GHz compact antenna test range (CATR) based on a hologram is shown as an application example. The maximum measured phase error due to the cable within a 8090 cm<sup>2</sup> scan area was 38.,2003,0,
265,266,Research about Software Fault Injection Technology Based on Distributed System,"Firstly, the paper made a contrast between the current domestic and international research condition, and introduced the basic concept of fault injection and distributed system. secondly, it discussed the classification and requirements of fault injection. There are mainly three types of distributed fault, namely the memory fault, CPU fault and correspondence fault. Besides, it discussed the method of distributed software fault injection about DOCTOR and illustrated the comprehensive structure and its respective parts of DOCTOR in detail. Thirdly, it reached a conclusion about the fault model of distributed system of fault injection and its realization method.",2010,0,
266,267,Fault detection and location of open-circuited switch faults in matrix converter drive systems,"Matrix converter based electric vehicles can be effectively applied to military vehicles due to weight and volume reduction as well as high temperature operation with no dc-bus capacitors fragile in a harsh environment. For successful applications for military vehicle areas, satisfactory reliability issues have to be incorporated into the matrix converter drives. This paper proposes a fault diagnostic technique for detecting and locating open-circuited faults in switching components of matrix converter drive systems. In this paper, the fault-mode behaviors of the matrix converter are, in detail, explored under the open-circuited switch fault conditions. Based on the investigated knowledge of the converter behaviors, the proposed scheme enables the matrix converter drive to detect and exactly identify power switches in which open-circuited faults have occurred. The proposed fault diagnostic algorithm is based on monitoring nine voltage errors assigned to nine bi-directional switches of the matrix converter. The voltage error signals are constructed with simple comparison of measured input and output voltages. In case that any of bi-directional switches are associated with open-circuited switch faults, the dedicated voltage error signals rise over a certain threshold value, which can be possible to detect a fault occurrence and locate the faulty switch. Since the developed diagnostic method requires no construction of reference output voltages from the pulsewidth modulation (PWM) reference signals, it can be implemented with simple and robust features. Verification results are presented to demonstrate the feasibility of the proposed technique.",2009,0,
267,268,Provisioning fault-tolerant scheduled lightpath demands in WDM mesh networks,"In this paper, we consider the problem of routing and wavelength assignment (RWA) of fault-tolerant scheduled lightpath demands (FSLDs) in all optical wavelength division multiplexing (WDM) networks under single component failure. In scheduled traffic demands, besides the source, destination, and the number of lightpath demands between a node-pair, their set-up and tear-down times are known, in this paper, we develop integer linear programming (ILP) formulations for dedicated and shared scheduled end-to-end protection schemes under single link/node failure for scheduled traffic demand with two different objective functions: 1) minimize the total capacity required for a given traffic demand while providing 100% protection for all connections; and 2) given a certain capacity, maximize the number of demands accepted while providing 100% protection for accepted connections. The ILP solutions schedule both the primary and end-to-end protection routes and assign wavelengths for the duration of the traffic demands. As the time disjointness that could exist among fault-tolerant scheduled lightpath demands is captured in our formulations, it reduces the amount of global resources required. The numerical results obtained from CPLEX indicate that dedicated scheduled (with set-up and tear-down times) protection provides significant savings (up to 33 %) in capacity utilization over dedicated conventional (without set-up and tear-down times) end-to-end protection scheme; shared scheduled protection provides considerable savings (up to 21 %) in capacity utilization over shared conventional end-to-end protection schemes. Also the numerical results indicate that shared scheduled protection achieves the best performance followed by dedicated scheduled protection scheme, and shared conventional end-to-end protection in terms of the number of requests accepted, for a given network capacity.",2004,0,
268,269,Induced error-correcting code for 2 bit-per-cell multi-level DRAM,"Traditionally, memories employ SEC-DED (Single Error Correcting and Double Error Detecting) Error Correcting Codes (ECC). While such codes have been considered for MLDRAM (Multi-Level Dynamic Random Access Memory), their use is inefficient, due to likely double-bit errors in a single cell. For this reason we propose an induced ECC architecture that uses ECC in such a way that no common error corrupts two bits. Induced ECC allows significant increase in reliability of the MLDRAM",2001,0,
269,270,Design of Timing Error Detectors for Orthogonal Space-Time Block Codes,"We present a method for the design of low complexity timing error detectors in orthogonal space-time block coding (OSTBC) receivers. A general expression for the S-curve of timing error detectors is derived. Based on this result, we obtain sufficient conditions for a difference of threshold crossings timing estimate that is robust to channel fading. A number of timing error detectors for 3- and 4-transmit antenna codes are presented. The performance is evaluated by examining their tracking capabilities within a timing loop of an OSTBC receiver. Symbol-error-rate results are presented showing negligible loss due to timing synchronization. In addition, we study the performance as a function of the timing drift and show that the receiver is able to track up to the normalized timing drift bandwidth of 0.001",2006,0,
270,271,Multi-Agent Fault Diagnosis in Manufacturing Systems Using Soft Computing,"The expeditious and accurate diagnosis of faults in manufacturing systems is essential in order to avoid expensive downtime. Many artificial intelligence approaches to automated fault diagnosis use techniques that are too computationally complex to achieve a diagnosis in real-time or are too inflexible for dynamic systems. Other approaches use either structural or symptom-based reasoning. Functional approaches are unable to provide real-time response due to their computational complexity, whereas, symptom-based approaches are only able to handle situations specifically coded in rules. Current hybrid approaches that combine the two methods are too structured in their approach to switching between the reasoning methods and, therefore fail to provide the flexible, rapid response of humans experts. This paper presents a robust, extensible approach to fault diagnosis that allows unstructured switching between reasoning methods using multiple fuzzy intelligent agents that examine the problem domain from a variety of perspectives.",2007,0,
271,272,Spatial error concealment algorithm based on improved SUSAN operator,"In the transmission of real-time video compressed streams, error concealment method is to restore the damaged or lost data packets. This paper improves the existing spatial error concealment algorithm based on SUSAN detection operator. On the one hand, as recovering the error, the detection pixels were reduced by considering the relationship of nearby pixels. On the other hand, more associated pixels were fully considered. The experimental results show that the proposed algorithm enhances the Peak Signal to Noise Ratio in the case of reducing 4%-8% computational complexity, and is more beneficial to real-time application.",2010,0,
272,273,"Impact of Channel Errors on Decentralized Detection Performance of Wireless Sensor Networks: A Study of Binary Modulations, Rayleigh-Fading and Nonfading Channels, and Fusion-Combiners","We provide new results on the performance of wireless sensor networks in which a number of identical sensor nodes transmit their binary decisions, regarding a binary hypothesis, to a fusion center (FC) by means of a modulation scheme. Each link between a sensor and the fusion center is modeled independent and identically distibuted (i.i.d.) either as slow Rayleigh-fading or as nonfading. The FC employs a counting rule (CR) or another combining scheme to make a final decision. Main results obtained are the following: 1) in slow fading, a) the correctness of using an average bit error rate of a link, averaged with respect to the fading distribution, for assessing the performance of a CR and b) with proper choice of threshold, on/off keying (OOK), in addition to energy saving, exhibits asymptotic (large number of sensors) performance comparable to that of FSK; and 2) for a large number of sensors, a) for slow fading and a counting rule, given a minimum sensor-to-fusion link SNR, we determine a minimum sensor decision quality, in order to achieve zero asymptotic errors and b) for Rayleigh-fading and nonfading channels and PSK (FSK) modulation, using a large deviation theory, we derive asymptotic error exponents of counting rule, maximal ratio (square law), and equal gain combiners.",2008,0,
273,274,A signature-based approach for diagnosis of dynamic faults in SRAMs,"This paper focuses on diagnosis of dynamic faults in SRAMs. The current techniques for fault diagnosis are mainly based on the signature method. Here, we introduce an extension of the signature scheme by taking in account additional information related to the addressing order during March test execution. A first advantage of the proposed approach is its capability to distinguish between static and dynamic faults. Another main feature is the correct identification of the location of the failure in a given memory component: the core-cell array, write drivers, sense amplifiers, address decoders and pre- charge circuits. Moreover, since this approach does not modify the March test, there is no increase of test complexity, conversely to other existing diagnosis techniques.",2008,0,
274,275,Defect tolerance for gracefully-degradable microfluidics-based biochips,"Defect tolerance is an important design consideration for microfluidics-based biochips that are used for safety-critical applications. We propose a defect tolerance methodology based on graceful degradation and dynamic reconfiguration. We first introduce tile-based biochip architecture, which is scalable for large-scale bioassays. A clustered defect model is used to evaluate the graceful degradation method for tile-based biochips. The proposed schemes ensure that the bioassays mapped to a droplet-based microfluidic array during design can be executed on a defective biochip through operation rescheduling and/or resource rebinding. Real-life biochemical procedures, namely polymerase chain reaction (PCR) and multiplexed in-vitro diagnostics on human physiological fluids, are used to evaluate the proposed defect tolerance schemes.",2005,0,
275,276,"Decoding of the (24, 12, 8) extended golay code up to four errors","A new decoder is proposed to decode the (24, 12, 8) binary extended Golay code up to four errors. It consists of the conventional hard decoder for correcting up to three errors, the detection algorithm for four errors and the soft decoding for four errors. For a weight-4 error in a received 24-bit word, Method 1 or 2 is developed to determine all six possible error patterns. The emblematic probability value of each error pattern is then defined as the product of four individual bit-error probabilities corresponding to the locations of the four errors. The most likely one among these six error patterns is obtained by choosing the maximum of the emblematic probability values of all possible error patterns. Finally, simulation results of this decoder in additive white Gaussian noise show that at least 93% and 99% of weight-4 error patterns that occur are corrected if the two E<sub>b</sub>/N<sub>0</sub> ratios are greater than 2 and 5 dB, respectively. Consequently, the proposed method can achieve a better percentage of successful decoding for four errors at variable signal-to-noise ratios than Lu et al.'s algorithm in software. However, the speed of the method is slower than Lu et al.'s algorithm.",2009,0,
276,277,Joint Generalized Antenna Combination and Symbol Detection Based on Minimum Bit Error Rate: A Particle Swarm Optimization Approach,"In order to reduce hardware cost and achieve superior performance in multi-input multi-output (MIMO) systems, this paper proposes a novel scheme for joint antenna combination and symbol detection. More specifically, the new approach simultaneously determines the transformation weighting for antenna combination to lower the RF chains called for and to design the minimum bit error rate (MBER) detector to effectively mitigate the impairment due to interference. The joint decision statistic, however, is highly nonlinear and the particle swarm optimization (PSO) algorithm is employed to reduce the computational overhead. Conducted simulation results show that the new approach yields satisfactory performance with reduced computational overhead compared with pervious works.",2008,0,
277,278,Finite Element Analysis of Switched Reluctance Motor under Dynamic Eccentricity Fault,"This paper describes the results of a two-dimensional finite element analysis carried out on an 8/6 switched reluctance motor for studying the effects of dynamic eccentricity on the static characteristics of the motor. Flux contours, flux-linkage profiles and mutual fluxes are obtained for both healthy and faulty motor. Besides, Static torque profiles of phases are obtained for different degrees of eccentricity and it is shown that at low current; the effect of eccentricity is considerable compared to that of the rated current case. Finally, Fourier analysis of the torque profiles is performed to make their difference visible.",2006,0,
278,279,Effective congestion and error control for scalable video coding extension of the H.264/AVC,"We present an effective congestion and error control mechanism for scalable video coding (SVC) extension of the H.264/AVC video dissemination over Internet. The congestion control is used to determine the appropriate number of SVC video layers based on bandwidth inference congestion (BIC) control protocol for layered multicast scenarios and the error control is achieved by unequal forward error correction (FEC) layered protection using block erasure coding. Through the real Internet streaming experiments, we demonstrate the effectiveness of the proposed layered SVC delivery, in terms of subscription layer, average packet loss rate and PSNRs, under several layered-definition scalabilities.",2008,0,
279,280,Routability estimation of FPGA-based fault injection,"In the past years various approaches to hardware-based fault injection using FPGA-based hardware have been presented. Some approaches insert additional functions at the fault location (any location in the circuit, e.g. I/Os of components or their interconnection nets), while others utilize the reconfigurability of FPGAs. A common feature of each of these methods is the execution of hardware-based fault simulation using the stuck-at fault model at gate level. The expansion of a circuit by insertion of additional functions at the fault location constitutes an overhead of FPGA resources. An optimized mapping of the circuit into an FPGA and a routable placement in the FPGA is difficult to achieve due to the generation of additional functions at the fault locations. Therefore, an optimized assignment of the fault locations to the FPGA-resources (configurable logic blocks, look-up tables, I/O blocks, etc.) precedes and thereby guarantees the mapping and routability of very large circuits in an acceptable runtime. In this paper an approach to node assignment is introduced, which achieves a reduction in FPGA overhead as well as routability of the expanded circuit in a minimal runtime.",2003,0,
280,281,A Research on I.C. Engine Misfire Fault Diagnosis Based on Rough Sets Theory and Neural Network,"A method for diagnosis of misfire fault in internal combustion engine based on exhaust density of HC, CO2, O2 and the engines work parameters are presented in this paper. Rough sets theory is used to simplify attribute parameter reflecting exhaust emission and conditions of internal combustion engine and in which unnecessary properties are eliminated. The engines work parameters, exhaust emission with misfire fault and without fault are tested by the experimentation of CA6100 engine. A diagnosis model which describing the relationship between the misfire degree and the internal combustion engines exhaust emission and work parameters is established based on rough sets theory and RBF neural network. The model reduces the sample size, optimizes the neural network, increase the diagnosis correctness. The model is also trained by test data and MATLAB software. The model has been used to diagnosis internal combustion engine misfire fault, the result illustrates that this diagnosis model is suitable. This system can reduce input node number and overcome some shortcomings, such as neural network scale is too large and the rate of classification is slow.",2010,0,
281,282,High-Intensity Radiated Field fault-injection experiment for a fault-tolerant distributed communication system,"Safety-critical distributed flight control systems require robustness in the presence of faults. In general, these systems consist of a number of input/output (I/O) and computation nodes interacting through a fault-tolerant data communication system. The communication system transfers sensor data and control commands and can handle most faults under typical operating conditions. However, the performance of the closed-loop system can be adversely affected as a result of operating in harsh environments. In particular, High-Intensity Radiated Field (HIRF) environments have the potential to cause random fault manifestations in individual avionic components and to generate simultaneous system-wide communication faults that overwhelm existing fault management mechanisms. This paper presents the design of an experiment conducted at the NASA Langley Research Center's HIRF Laboratory to statistically characterize the faults that a HIRF environment can trigger on a single node of a distributed flight control system.",2010,0,
282,283,On-line fault diagnosis in a Petri Net framework,"The paper addresses the fault detection problem for discrete event systems modeled by Petri nets (PN). Assuming that the PN structure and initial marking are known, faults are modeled by unobservable transitions. The paper recalls a previously proposed diagnoser that works online and employs an algorithm based on the definition and solution of some integer linear programming problems to decide whether the system behavior is normal or exhibits some possible faults. To reduce the on-line computational effort, we prove some results showing that if the unobservable subnet enjoys suitable properties, the algorithm solution may be obtained with low computational complexity. We characterize the properties that the PN modeling the system fault behavior has to fulfill and suitably modify the proposed diagnoser.",2009,0,
283,284,The prediction of fault currents in a large multiwinding reactor transformer,"The fault currents occurring in power transformers are determined by the leakage reactances within the windings. Where the transformers are used in a phase-shifting mode, there is additional coupling between phases which influences the fault current. This work describes the modelling of the self and mutual inductances in a 90 MVA autotransformer with a tertiary winding, on the assumption that the airgap formed by the transformer window dictates the reluctance of the leakage flux paths. Recordings made during a short-circuit between two phases of the tertiary winding show a remarkably close comparison with the predicted waveforms.",2003,0,
284,285,How Long Will It Take to Fix This Bug?,"Predicting the time and effort for a software problem has long been a difficult task. We present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. Our technique leverages existing issue tracking systems: given a new issue report, we use the Lucene framework to search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach using effort data from the JBoss project. Given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating naive predictions by a factor of four.",2007,0,
285,286,Data reduction and clustering techniques for fault detection and diagnosis in automotives,"In this paper, we propose a data-driven method to detect anomalies in operating Parameter Identifiers (PIDs) and in the absence of any anomaly, classify faults in automotive systems by analyzing PIDs collected from the freeze frame data. We first categorize the operating parameter data using automotive domain knowledge. The dataset thus obtained is then analyzed using Principal Component Analysis (PCA) and Independent Component Analysis (ICA) for finding coherence among the PIDs. Then we use clustering algorithms based on both linear distance and information theoretic measures to assign coherent PIDs to the same class or cluster. A comparative analysis of the behavior of PIDs belonging to the same cluster can now be made for detecting anomaly in PIDs. Since a system fault is characterized by the values by of all PIDs across all the clusters, we use the joint probability distribution of the independent components of all PIDs to characterize the fault and find the divergence between the joint distributions of training and test data to classify faults. The proposed method can analyze available parameter data, categorize PIDs into informative or non-informative category, and detect fault condition from the clusters. We demonstrate the algorithm by way of an application to operating parameter data collected during faults in catalytic converters of vehicles.",2010,0,
286,287,Effect of atmospheric correction for different land use on Landsat 7 ETM+ satellite imagery,"Various changes in the atmosphere of the earth and different illuminations resulting from rough terrain change the spectral reflection values of satellite images. Studies making use of real reflection values belonging to the object will provide more accurate data. The atmospheric correction process to be applied in this study is used to prevent the negative effects resulting from atmosphere and different illuminations in order to represent the reflections from the ground on the image in the best way possible. Using atmospheric correction, differentiations in reflection values sensed by different sensors or platforms resulting from atmosphere and some technical problems will be prevented. In this study, the aim is to determine the changes in the spectral reflection values concerning land use following the atmospheric correction to be applied on Landsat image data. For this reason, atmospheric correction was applied on Landsat image data. The relations of each band with each other before and after the correction were determined. The changes between spectral reflection values of all bands before and after correction regarding three different land uses as forest, agricultural area and residential area were examined visually and statistically.",2009,0,
287,288,An Efficient Fault Tolerance Scheme for Preventing Single Event Disruptions in Reconfigurable Architectures,"Reconfigurable architectures are becoming increasingly popular with space related design engineers as they are inherently flexible to meet multiple requirements and offer significant performance and cost savings for critical applications. As the microelectronics industry has advanced, integrated circuit (IC) design and reconfigurable architectures (FPGAs, reconfigurable SoC and etc) have experienced dramatic increase in density and speed. These advancements have serious implications for the reconfigurable architectures when used in space environment where IC is subject to total ionization dose (TID) and single event effects as well. Due to transient nature of single event upsets (SEUs), these are most difficult to avoid in space-borne reconfigurable architectures. We present a unique SEU fault tolerance technique based upon double redundancy with comparison to overcome the overheads associated with the conventional schemes",2006,0,
288,289,Anshan: Wireless Sensor Networks for Equipment Fault Diagnosis in the Process Industry,"Wireless sensor networks provide an opportunity to enhance the current equipment diagnosis systems in the process industry, which have been based so far on wired networks. In this paper, we use our experience in the Anshan Iron and Steel Factory, China, as an example to present the issues from the real field of process industry, and our solutions. The challenges are three fold: First, very high reliability is required; second, energy consumption is constrained; and third, the environment is very challenging and constrained. To address these issues, it is necessary to put systematic efforts on network topology and node placement, network protocols, embedded software, and hardware. In this paper, we propose two technologies i.e. design for reliability and energy efficiency (DRE), and design for reconfiguration (DRC). Using these techniques we developed Anshan, a wireless sensor network for monitoring the temperature of rollers in a continuously annealing line and detecting equipment failures. Project Anshan includes 406 sensor nodes and has been running for four months continuously.",2008,0,
289,290,Heterogeneous Error Protection of H.264/AVC Video Using Hierarchical 16-QAM,"Heterogeneous error protection (HEP) of H.264/AVC coded video is investigated using hierarchical quadrature amplitude modulation (HQAM), which takes into consideration the non- uniformly distributed importance of intracoded frame (I-frame) and predictive coded frame (P-frame) as well as the sensitivity of the coded bitsream against transmission errors. The HQAM constellation are used to give different degrees of error protection of the most important information of the video content. The performance of the transmission system is evaluated under additive Gaussion Noise (AWGN). The simulation results indicate that the strategy produces a high quality of the reconstructed video data compared with uniform protection.",2009,0,
290,291,A design of the novel coupled line bandpass filter using defected ground structure,"In this paper, a novel coupled line bandpass filter with a DGS (Defected Ground Structure) is proposed to realize a compact size with low insertion loss characteristic. The proposed bandpass filter can provide an attenuation pole due to the resonance characteristic of the DGS. The equivalent circuit parameters for the DGS are extracted by using an EM simulation process and the circuit analysis method. The design method for the proposed 3-pole bandpass filter is derived based on coupled line filter theory and the derived equivalent circuit of the DGS. The experimental results show an excellent agreement with theoretical simulation results.",2000,0,
291,292,The use of characteristic features of wireless cellular networks for transmission of GNSS assistance and correction data,"Precise Global Navigation Satellite System (GNSS) positioning using Real Time Kinematics (RTK) correction data is currently utilized in many fields of surveying, mapping and precision agriculture. In the near future, sub decimeter precision data usage is expected to extend to autonomous vehicles navigation and public safety areas. To satisfy this increasing demand of precision positioning correction bandwidth, new techniques and protocols in assistance and correction data transmission are needed. This paper reviews one such possible technique involving sending correction dataset via public wireless cellular networks. The data will be transmitted through a hybrid system integrating correction data broadcasted in the wireless cellular network control plane with AGNSS assistance data and correction metadata in the user plane. Through this system, the bandwidth intensive, low refresh rate data of GNSS system ephemeris, reference station and satellite identification is omitted from the main data stream. Instead, a constant bit rate (CBR) stream for correction data is used and bandwidth is conserved. The results show that the proposed system can achieve scalability required for widespread usage of sub decimeter level positioning data from GNSS.",2010,0,
292,293,An on-line monitoring and multi-layer fault diagnosis system of electrical equipment based on geographic information system,"Automated mapping/facilities management/geographic information system (AM/FM/GIS), which provides a powerful way to process graphic and non-graphic information, can construct a spatial database system with topological structure and analysis function by combining diversified information in power system with geographic position-related graphic information. Based on the AM/FM/GIS and on-line monitoring system, an integrated system is put forward which can implement state monitoring, multi-layer fault diagnosis and assess the faults. By using this integrated system, latent fault and defect can be eliminated, loss due to power cut is reduced and the reliability of running power system is improved. Application indicates it is economical, pragmatic and has excellent performance.",2005,0,
293,294,Coseismic fault rupture detection and slip measurement by ASAR precise correlation using coherence maximization: application to a north-south blind fault in the vicinity of Bam (Iran),"Using the phase differences between satellite radar images recorded before and after an earthquake, interferometry allows mapping the projection along the line of sight (LOS) of the ground displacement. Acquisitions along multiple LOS theoretically allow deriving the complete deformation vector; however, due to the orbit inclination of current radar satellites, precision is poor in the north-south direction. Moreover, large deformation gradients (e.g., fault ruptures) prevent phase identification and unwrapping and cannot be measured directly by interferometry. Subpixel correlation techniques using the amplitude of the radar images allow measuring such gradients, both in slant-range and in azimuth. In this letter, we use a correlation technique based on the maximization of coherence for a radar pair in interferometric conditions, using the complex nature of the data. In the case of highly coherent areas, this technique allows estimating the relative distortion between images. Applied to ASAR images acquired before and after the December 26, 2003 Bam earthquake (Iran), we show that the near-field information retrieved by this technique is useful to constrain geophysical models. In particular, we confirm that the major gradients of ground displacement do not occur across the known fault scarp but approximately 3 km west of it, and we also estimate directly the amplitude of right lateral slip, while retrieving this value from interferometry requires passing through the use of a model for the earthquake fault and slip.",2006,0,
294,295,Reducing cost and tolerating defects in page-based intelligent memory,"Active Pages is a page-based model of intelligent memory specifically designed to support virtualized hardware resources. Previous work has shown substantial performance benefits from off loading data-intensive tasks to a memory system that implements Active Pages. With a simple VLIW processor embedded near each page on DRAM, Active Page memory systems achieve up to 1000X speedups over conventional memory systems. In this study, we examine Active Page memories that share, or multiplex, embedded VLIW processors across multiple physical Active Pages. We explore the trade-off between individual page-processor performance and page-level multiplexing. We find that hardware costs of computational logic can be reduced from 31% of DRAM chip area to 12%, through multiplexing, without significant loss in performance. Furthermore, manufacturing defects that disable up to 50% of the page processors can be tolerated through efficient resource allocation and associative multiplexing",2000,0,
295,296,A defect-to-yield correlation study for marginally printing reticle defects in the manufacture of a 16Mb flash memory device,"This paper presents a defect-to-yield correlation for marginally printing defects in a gate and a contact 4X DUV reticle by describing their respective impact on the lithography manufacturing process window of a 16Mb flash memory device. The study includes site-dependent sort yield signature analysis within the exposure field, followed by electrical bitmap and wafer strip back for the lower yielding defective sites. These defects are verified using both reticle inspection techniques and review of printed resist test wafers. Focus/exposure process windows for defect-free feature and defective feature are measured using both in-line SEM CD data and defect printability simulation software. These process window models are then compared against wafer sort yield data for correlation. A method for characterizing the lithography manufacturing process window is proposed which is robust to both marginally printing reticle defects and sources of process variability outside the lithography module",2000,0,
296,297,PEDS: A Parallel Error Detection Scheme for TCAM Devices,"Ternary content-addressable memory (TCAM) devices are increasingly used for performing high-speed packet classification. A TCAM consists of an associative memory that compares a search key in parallel against all entries. TCAMs may suffer from error events that cause ternary cells to change their value to any symbol in the ternary alphabet ""0"",""1"",""*"". Due to their parallel access feature, standard error detection schemes are not directly applicable to TCAMs; an additional difficulty is posed by the special semantic of the ""*"" symbol. This paper introduces PEDS, a novel parallel error detection scheme that locates the erroneous entries in a TCAM device. PEDS is based on applying an error-detection code to each TCAM entry, and utilizing the parallel capabilities of the TCAM, by simultaneously checking the correctness of multiple TCAM entries. A key feature of PEDS is that the number of TCAM lookup operations required to locate all errors depends on the number of symbols per entry rather than the (orders-of-magnitude larger) number of TCAM entries. For large TCAM devices, a specific instance of PEDS requires only 200 lookups for 100-symbol entries, while a naive approach may need hundreds of thousands lookups. PEDS allows flexible and dynamic selection of trade-off points between robustness, space complexity, and number of lookups.",2009,0,
297,298,Noise identification and fault diagnosis for the new products of the automobile gearbox,"A noise identification and fault diagnosis system for the new products of the automobile gearbox is introduced. The framework of the developed software is described, which includes function modules as data acquisition, feature extracting, time frequency transform, order analysis, learning and training, and so on. The prototype system has been partially put in practice in a certain automobile gear-box manufacture company.",2009,0,
298,299,Soft Defects: Challenge and Chance for Failure Analysis,"Failure analysis on advanced logic and mixed signal ICs more and more has to deal with so called 'soft defects'. In this paper, an analysis flow especially for parameter dependent scan fails is presented. For the two major localization techniques, namely soft defect localization (SDL) and internal signal measurement enhanced activation and localization procedures using test systems are proposed.",2007,0,
299,300,Layout to Logic Defect Analysis for Hierarchical Test Generation,"As shown by previous studies, shorts between the interconnect wires should be considered as the predominant cause of failures in CMOS circuits. Fault models and tools for targeting these defects, such as the bridging fault test pattern generators have been available for a long time. However, this paper proposes a new hierarchical approach based on critical area extraction for identifying the possible shorted pairs of nets on the basis of the chip layout information, combined with logic-level test pattern generation for bridging faults. Experiments on real design layouts will show that only a fraction of all the possible pairs of nets have non-zero shorting probabilities. Furthermore, it will also be proven at the logic-level that nearly all such bridging faults can be tested by a simple and robust one-pattern logic test. The methods proposed in this paper are supported by a design flow implementing existing commercial and academic CAD software.",2007,0,
300,301,A Fault Propagation Approach for Highly Distributed Service Compositions,"Today, the techniques for realizing service compositions (e.g. WS-BPEL) have become mature. Nevertheless, when it comes to execution faults within service compositions, many problems are still unsolved. Especially the propagation and global handling of errors in service compositions yet remains an open issue. In this paper, we describe some preliminary results of our ongoing work in the field of fault propagation and exception handling in service compositions. We provide some service classification criteria and show how they relate to service composition fault handling. Further, we present a fault propagation approach for service compositions.",2008,0,
301,302,Using design based binning to improve defect excursion control for 45nm production,"For advanced device (45 nm and below), we proposed a novel method to monitor systematic and random excursion. By integrating design information and defect inspection results into automated software (DBB), we can identify design/process marginality sites with defect inspection tool. In this study, we applied supervised binning function (DBC) and defect criticality index (DCI) to identify systematic and random excursion problems on 45 nm SRAM wafers. With established SPC charts, we will be able to detect future excursion problem in manufacturing line early.",2007,0,
302,303,Comparison of Outlier Detection Methods in Fault-proneness Models,"In this paper, we experimentally evaluated the effect of outlier detection methods to improve the prediction performance of fault-proneness models. Detected outliers were removed from a fit dataset before building a model. In the experiment, we compared three outlier detection methods (Mahalanobis outlier analysis (MOA), local outlier factor method (LOFM) and rule based modeling (RBM)) each applied to three well-known fault-proneness models (linear discriminant analysis (LDA), logistic regression analysis (LRA) and classification tree (CT)). As a result, MOA and RBM improved Fl-values of all models (0.04 at minimum, 0.17 at maximum and 0.10 at mean) while improvements by LOFM were relatively small (-0.01 at minimum, 0.04 at maximum and 0.01 at mean).",2007,0,
303,304,Algorithm-based fault tolerance for spaceborne computing: basis and implementations,"We describe and test the mathematical background for using checksum methods to validate results returned by a numerical subroutine operating in a fault-prone environment that causes unpredictable errors in data. We can treat subroutines whose results satisfy a necessary condition of a linear form; the checksum tests compliance with this necessary condition. These checksum schemes are called algorithm-based fault tolerance (ABFT). We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent in finite-precision numerical calculations. Two series of tests are described. The first tests the general effectiveness of the linear ABFT schemes we propose, and the second verifies the correct behavior of our parallel implementation of them. We find that under simulated fault conditions, it is possible to choose a fault detection scheme that for average case matrices can detect 99% of faults with no false alarms, and that for a worst-case matrix population can detect 80% of faults with no false alarms",2000,0,
304,305,Bug busters,"One way to deal with bugs is to avoid them entirely. The approach would be wasteful because we'd be underutilizing the many automated tools and techniques that can catch bugs for us. Most tools for eliminating bugs work by tightening the specifications of what we build. At the program code level, tighter specifications affect the operations allowed on various data types, our program's behavior, and our code's style. Furthermore, we can use many different approaches to verify that our code is on track: the programming language, its compiler, specialized tools, libraries, and embedded tests are our most obvious friends. We can delegate bug busting to code. Many libraries come with hooks or specialized builds that can catch questionable argument values, resource leaks, and wrong ordering of function calls. Bugs many be a fact of life, but they're not inevitable. We have some powerful tools to find them before they mess with our programs, and the good news is that these tools get better every year.",2006,0,
305,306,Design and validation of portable communication infrastructure for fault-tolerant cluster middleware,"We describe the communication infrastructure (CI) for our fault-tolerant cluster middleware, which is optimized for two classes of communication: for the applications and for the cluster management middleware. This CI was designed for portability and for efficient operation on top of modern user-level message passing mechanisms. We present a functional fault model for the CI and show how platform-specific faults map to this fault model. Based on this fault model, we have developed a fault injection scheme that is integrated with the CI and is thus portable across different communication technologies. We have used fault injection to validate and evaluate the implementation of the CI itself as well as the cluster management middleware in the presence of communication faults.",2002,0,
306,307,Effect of rotor position error on commutation in sensorless BLDC motor drives,"In this paper, two kinds of commutation modes of the brushless DC(BLDC) motor drives, the delaying commutation and the leading commutation, are discussed in detail. The current of the unexcited phase is calculated under an ideal operation condition, and the condition of circulating current occurring is analyzed. The result with the compensated commutation is provided. The theoretical analysis is confirmed by the experiment results.",2005,0,
307,308,A Fault-Tolerant Active Pixel Sensor to Correct In-Field Hot-Pixel Defects,"Solid-state image sensors develop in-field defects in all common environments. Experiments have demonstrated the growth of significant quantities of hot-pixel defects that degrade the dynamic range of an image sensor and potentially limit low-light imaging. Existing software- only techniques for suppressing hot-pixels are inadequate because these defective pixels saturate at relatively low illumination levels. The redundant fault-tolerant active pixel sensor design is suggested to isolate point-like hot-pixel defects. Emulated hot-pixels have been induced in hardware implementations of this pixel architecture and measurements of pixel response indicate that it generates an accurate output signal throughout the sensor's entire dynamic range, even when standard pixels would be otherwise saturated by the hot defect. A correction algorithm repairs the final image by building a simple look-up table of illumination- response of a working pixel. In emulated hot-pixels, the true illumination value can be recovered with an error of plusmn5% under typical conditions.",2007,0,
308,309,Automatic error recovery in targetless logic emulation,"Targetless logic emulation refers to a verification system in which there are no external hardware targets interfacing with the emulator. In such systems input stimuli to the DUT come either from a user provided vector file or a HDL testbench running on a software simulator and the DUT runs on hardware based logic emulator. Many users use such targetless environment for automated long running verification tests consisting of huge sets of input stimuli, consequently an automatic recovery method is of significant interest in such systems. The automatic error recovery method shall be able to complete the emulation session gracefully skipping error points and subsequently report various errors and mismatch conditions for user debug. The paper presents a novel methodology and verification infrastructure based on periodic checkpointing, which provides a robust way of error condition detection, subsequent restoration of last saved system state and resume emulation run by skipping offending operations. It does not require any special hardware extension and provides a fully customizable checkpoint frequency selection scheme. It is seen to add only a minimal overhead on overall hardware emulation speed.",2009,0,
309,310,Weather radar equation correction for frequency agile and phased array radars,"This paper presents the derivation of a correction to the Probert-Jones weather radar equation for use with advanced frequency agile, phased array radars. It is shown that two additional terms are required to account for frequency hopping and electronic beam pointing. The corrected weather radar equation provides a basis for accurate and efficient computation of a reflectivity estimate from the weather signal data samples. Lastly, an understanding of calibration requirements for these advanced weather radars is shown to follow naturally from the theoretical framework.",2007,0,
310,311,Waveform analysis of the bridge type SFCL during load changing and fault time,"DC reactor type superconducting fault current limiter (SFCL) has drawn the interest of some researchers in developing such device and more research work is being carried out in order to make it practically feasible. We have pointed out one issue that is not properly examined yet on such a device during load changing time. As we know, it is very difficult to introduce DC bias voltage to the reactor coil of the bridge type SFCL and some researchers are developing such device without using DC bias current. In such a case, the voltage drop occurs at the load terminal during the load increasing time caused by the DC reactor's inductance. By using the Electro-Magnetic Transients in DC systems which is the simulator of electric networks (EMTDC) software we carried out analysis of first few half cycles of the voltage and current waveforms after the load is increased. We also performed the same analysis for fault conditions. The peak value of the waveforms is considered in calculating the voltage drop at load terminal during the load changing time. The analysis can be used in selecting an appropriate inductance value for designing such SFCL.",2003,0,
311,312,DuoTracker: Tool Support for Software Defect Data Collection and Analysis,"In today software industry defect tracking tools either help to improve an organizationAs software development process or an individualAs software development process. No defect tracking tool currently exists that help both processes. In this paper we present DuoTracker, a tool that makes possible to track and analyze software defects for organizational and individual software process decision making. To accomplish this, DuoTracker has capabilities to classify defects in a manner that makes analysis at both organizational and individual software processes meaningful. The benefit of this approach is that software engineers are able to see how their personal software process improvement impacts their organization and vice versa. This paper shows why software engineers need to keep track of their program defects, how this is currently done, and how DuoTracker offers a new way of keeping track of software errors. Furthermore, DuoTracker is compared to other tracking tools that enable software developers to record program defects that occur during their individual software processes.",2006,0,
312,313,Investigation of the effects of transmission faults upon a renewable energy generating plant,"In recent years the number of renewable energy generators connected to Ireland's electricity grid has steadily increased. The Republic of Ireland is now expected to source 13.2% of the electricity it consumes from renewables by 2010, which represents a significant challenge to the electricity system operators and planners. This paper describes the modelling and simulation of a small hybrid wind/hydro generating plant connected to the distribution network. The effects upon the plant of transmission network faults and continuous voltage unbalance are investigated.",2005,0,
313,314,Detection of defects in wood slabs by using a microwave imaging technique,"In this paper, an experimental set up based on interrogating microwaves is used to obtain images of the cross section of dielectric cylinders. In particular, a microwave tomographic configuration is used to inspect wood slabs in order to search for defects and voids. The measured data (samples of the scattered electric field) are inverted by using an efficient reconstruction technique, which is able to handle the ill-posedness of the inverse scattering problem. The developed experimental apparatus is validated in this paper by means of several numerical simulations. Preliminary experimental results are also reported.",2007,0,
314,315,Influence of the Transmission Channel Parameters on Error Rates and Picture Quality in DVB Baseband Transmission,"The paper deals with the component analysis of DVB (digital video broadcasting) transmission model in baseband and its source, channel and link coding. The transmission channel model is based on the digital filter design and it can be designed with the variable transmission parameters (e.g. cut-off frequency) and linear distortions with additive noise and reflected signal. Results of achieved BER (bit error rate) and SER (symbol error rate) and corresponding PQE (picture quality evaluation) analysis are presented, including the evaluation of subjective picture quality influence on normalized cut-off frequency of the channel",2006,0,
315,316,Rotor position sensor fault detection Isolation and Reconfiguration of a Doubly Fed Induction Machine control,"In this paper, a Doubly Fed Induction Machine (DFIM) operating in motor mode and supplied by two Voltage Source Inverters (VSI), in stator and rotor sides, is presented. The aim is to analyze the position sensor fault effects on a Direct Torque Control (DTC) of the DFIM. This justifies the necessity of a reconfiguration control when a position sensor fault appears in order to avoid an interruption in system operations. In the other hand, this study emphasizes the close dependency between system performance and the output accuracy of the rotor position sensor. Moreover, simulation results point out the operation system deterioration in case of position sensor fault, which leads in most cases to its shut down in contrast to industrial expectations. This work presents a control reconfiguration for a DFIM speed drive when a position sensor fault occurs, in order to ensure system service continuity. For this purpose, SABER simulation results illustrate the system behavior before and after a position sensor fault. System performance preservation is carried out after control reconfiguration. The proposed solution is relevant especially due to its simplicity.",2009,0,
316,317,Computation and analysis of output error probability for C17 benchmark circuit using bayesian networks error modeling,"The reliability of digital circuits is in question since the new scaled transistor technologies continue to emerge. The major factor deteriorating the circuit performance is the random and dynamic nature of errors encountered during its operation. Output-error probability is the direct measure of circuit's reliability. Bayesian networks error modeling is the approach used to compute error probability of digital circuits. In our paper, we have used this technique to compute and analyze the output error probability of LGSynth's C17 benchmark circuit. The simulations are based on MATLAB and show important relationships among output-error probability, execution time and number of priors involved in the analysis.",2010,0,
317,318,Raising network fault management intelligence,"Most large network management centers have relatively low skilled personnel as their first level operations staff. Many organizations attempt to cope with this situation by restricting the set of problems these people have to deal with to those which are well understood and documented. Several software packages exist which can correlate and filter incoming events from the network and present a select subset to the operator. Unfortunately, programming these fault management applications requires considerable expertise and effort. Often, once the initial development is done, the implementation remains static, while the network itself is dynamic. This paper proposes a methodology for documenting known faults and responses, programming fault correlation engines, continuously examining real behavior, and feeding the result back into the programming process. This results in a continuous improvement in fault management intelligence, with corresponding improvement in network availability and thus value of the network to the organization",2000,0,
318,319,Analysis of Timing Error Aperture Jitter on the Performance of Sigma Delta ADC for Software Radio Mobile Receivers,"Jitter is the limiting effect for high speed analog-to digital converters with high resolution and wide digitization bandwidth, which are required in receivers in order to support high data rates. The rapid development of digital wireless system has led to a need of high resolution and high speed analog to digital converter. The proper selection of data converters, both analog to digital converters and digital to analog converters (DACs) is one of the most challenging steps in designing software radio. The performance of a data converter is dependent upon the accuracy and stability of the clock supplied to the circuits. When data converter employ a high sampling rate, clocking issues become magnified and significant distortion can be result. This paper describes the effect of aperture jitter on the performance of sigma delta ADC and present analytical evaluation of the performance and mean error power spectrum due to aperture jitter application has favored the use of oversampling delta sigma ADC (analog-to-digital converters) due to their better speed-accuracy tradeoff. Delta-sigma modulator is one of the key building blocks, which can be implemented using DT (discrete-time) and CT (continuous-time) techniques. Compared to their DT counterparts, CT delta-sigma modulators have recently attracted more and more attentions due to their advantages in terms of high speed, low power, low noise and intrinsic anti-aliasing capability. In this paper, we concentrate on the discrete implementation. Section 2 presents an aperture jitter effect in SDM in terms of SNR. In the last few years different authors derived formulas to quantify the SNR limiting effect of jitter in ADCs. While Walden used a worst case approach, Kobayashi presented an exact formula which allows calculating the SNR in the presence of an aperture jitter.",2009,0,
319,320,"The Impact of Tower Shadow, Yaw Error, and Wind Shears on Power Quality in a WindDiesel System","To study the impact of aerodynamic aspects of a wind turbine (WT) (i.e., tower shadow, wind shears, yaw error, and turbulence) on the power quality of a wind-diesel system, all electrical, mechanical, and aerodynamic aspects of the WT must be studied. Moreover, the contribution of the diesel generator system and its controllers should be considered. This paper describes how the aerodynamic and mechanical aspects of a WT can be simulated using TurbSim, AeroDyn, and FAST where the electrical parts of WT, diesel generator, its controllers, and electrical loads are modeled by Simulink blocks. Simulation results obtained from the model are used to observe the power and voltage variations at the WT generator terminals under different operating conditions. Furthermore, the effects of tower shadow, wind shears, yaw error, and turbulence on the power quality in a stand-alone wind-diesel system utilizing a fixed-speed WT are studied.",2009,0,
320,321,Based on Compact Type of Wavelet Neural Network Tolerance Analog Circuit Fault Diagnosis,"Based on the classical wavelet neural network, this paper put forward a sort of improved multiple-input multiple-output compact type of wavelet neural network, adopted adaptive learning rate and additional momentum BP algorithm to carry out training, studied its tolerance analog circuit fault diagnosis applications. Simulation results displayed that the compact type of wavelet neural network learning is fast, it can be effective diagnosed and located to tolerance analog circuit fault.",2009,0,
321,322,Lab VIEW based implementation of remedial action for DC arcing faults in a spacecraft,"In this paper remedial action for DC arcing faults in spacecraft has been designed and implemented using Lab VIEW. The Lab VIEW is an innovative graphical programming system designed to facilitate computer controlled data acquisition and analysis. DC arcing faults in spacecraft has been designed and implemented using the experimental data obtained at NASA Glenn research center. It is important to keep the continuity of the power supply and at the same time increase the reliability of spacecraft energy power system. In the frequency domain, fast Fourier transformation (FFT) is used for the feature extraction of the fault signal and odd harmonics frequency components of the phase currents are analyzed.",2003,0,
322,323,A Fault Tolerant Optimization Algorithm based on Evolutionary Computation,"In this paper we describe how an evolutionary algorithm is capable of running on a distributed environment with volatile resources. When executing algorithms in a desktop computing or resource harvesting context, resources can be reclaimed by their owners without warning, which may produce data loss and process to fail. The interest of the algorithm presented in the paper is that although it doesn't keep processes from failing, or data from being lost, it does improve the quality of results because of its design, not employing any special task control, checkpoint/restart or resource redundancies. By means of a series of experiments, we test the performance of the algorithm by studying the number of process failing and the quality of solutions when compared with the classic flavor of the evolutionary algorithm. The new algorithm, which shows its advantages, therefore improve dependability of distributed system",2006,0,
323,324,Efficient Error Correcting Codes for On-Chip DRAM Applications for Space Missions,"New systematic single error correcting codes-based circuits are introduced for random access memories, with ultimate minimal encoding/decoding complexity, low power and high performance. These new, codes-based circuits can be used in combinational circuits and in on-chip random access memories of reconfigurable architectures with high performance and ultimate minimum decoding/encoding complexity. Due to the overhead of parity check bits associated with the error-correcting-codes, there has always been a demand for an efficient and compact code for small memories in terms of data width. The proposed codes give improved performance even for small memories over the other codes. Area and power comparisons have been performed to benchmark the performance index of our codes. The code-centric circuits offer significant advantages over existing error correcting codes-based circuits in the literature in terms of lower size, power and cost which make them suitable for wider range of applications such as those targeting space. The paper describes the new efficient code and associated circuits for its implementation",2005,0,
324,325,Fault Diagnosis for Engine Based on EMD and Wavelet Packet BP Neural Network,"To solve the problem of fault diagnosis for engine, due to the complexity of the equipments and the particularity of the operating environments, generally speaking, there is no one-to-one correspondence between the characteristic parameters and status, so, the methods of diagnosis are very complicated. A novel fault diagnosis method based on empirical mode decomposition (EMD) and wavelet packet BP neural network is proposed in this paper. Firstly, the given signal is analyzed by wavelet packet to remove the noise; Then the de-noised data is decomposed into a number of IMFs by EMD and extract their frequency eigenvectors, then using these eigenvectors as the training samples of the BP network, training the BP network to identify the faults. Finally, the simulation experiments shows that the proposed method for fault diagnosis of engine is effective and the de-nosing process using wavelet packet transform is essential.",2009,0,
325,326,Errors estimation and minimization for the 5-axis milling machine,"This paper presents the tool path optimization algorithms to compute and estimate the non-linear inverse kinematics errors of the 5-axis milling machine. The approach is based on a global approximation of the required surface by a virtual surface constructed from the tool trajectories. Errors are computed from the difference between the required surface and the virtual surface and displayed numerically and graphically through the virtual machine simulator. The simulator is based on 3D representation and employing the inverse kinematics approach to derive the corresponding rotational and translation movement of the mechanism. The simulator makes it possible to estimate the errors of a 3D tool-path based on a prescribed set of the cutter location (CL) points as well as a set of the cutter contact (CC) points with tool inclination angle. Errors, particularly near the vicinity of the large milling errors, are minimized using a discrete algorithm based on a shortest path strategy. Furthermore, the simulator can be used to simulate the milling process, verify the final cut of the actual tool-path before testing with the real machine. Thus, it reduces the cost of iterative trial and errors.",2002,0,
326,327,Soft error assessments for servers,"In order to assess the soft error rate (SER) of a server, it is important to not only quantify the soft error contribution of the individual semiconductor components, but also to account for derating and for SER mitigation like hardening and shielding. Derating describes the fact that not every soft error has an impact. A large number of soft errors vanish based on electrical, logical or timing considerations. They have no impact. Additionally, a server can, to a large degree, be protected from the impact of soft errors by implementing error detection and correction means. In these cases the impact of the soft error is limited to the extra compute time needed for the correction. Summing up the SER contributions from transistors and circuits results in the so-called raw soft error rate, a rate which describes just the bottom layer of the system stack. Powerful protection mechanisms at higher layers can reduce that rate by several orders of magnitude. Awareness of this vertical interaction across the different layers in the system stack leads to servers optimized for robustness.",2010,0,
327,328,On the Use of Dynamic Binary Instrumentation to Perform Faults Injection in Transaction Level Models,"Transaction Level Modelling (TLM) has been widely accepted as systems modelling framework focused in system components communication. This approach allows efficient accurate estimation and rapid design space exploration. Besides of the functional simulation for validation of a hardware/software designs, there are additional reliability requirements that need advanced simulation techniques to analyze the system behaviour in the presence of faults. Several traditional VHDL fault injection mechanisms like mutants or saboteurs have been adapted to SystemC model descriptions. The main drawback of these approaches is the necessity of source code modification to carry out the fault injection campaigns. In this paper, we propose the use of Dynamic Binary Instrumentation (DBI) to perform fault injection in SystemC TLM models. DBI is a technique to intercept software routine calls allowing argument and return value corruption and data structures modification at runtime. This technique needs neither source code modifications nor recompilation of models in order to generate module mutants or in order to insert saboteurs in the signal communication path.",2009,0,
328,329,Online Fault Diagnosis of Discrete Event Systems. A Petri Net-Based Approach,"This paper is concerned with an online model-based fault diagnosis of discrete event systems. The model of the system is built using the interpreted Petri nets (IPN) formalism. The model includes the normal system states as well as all possible faulty states. Moreover, it assumes the general case when events and states are partially observed. One of the contributions of this work is a bottom-up modeling methodology. It describes the behavior of system elements using the required states variables and assigning a range to each state variable. Then, each state variable is represented by an IPN model, herein named module. Afterwards, using two composition operators over all the modules, a monolithic model for the whole system is derived. It is a very general modeling methodology that avoids tuning phases and the state combinatory found in finite state automata (FSA) approaches. Another contribution is a definition of diagnosability for IPN models built with the above methodology and a structural characterization of this property; polynomial algorithms for checking diagnosability of IPN are proposed, avoiding the reachability analysis of other approaches. The last contribution is a scheme for online diagnosis; it is based on the IPN model of the system and an efficient algorithm to detect and locate the faulty state. Note to Practitioners-The results proposed in this paper allow: 1) building discrete event system models in which faults may arise; 2) testing the diagnosability of the model; and 3) implementing an online diagnoser. The modeling methodology helps to conceive in a natural way the model from the description of the system's components leading to modules that are easily interconnected. The diagnosability test is stated as a linear programming problem which can be straightforward programmed. Finally, the algorithm for online diagnosis leads to an efficient procedure that monitors the system's outputs and handles the normal behavior model. This provides an oppo- rtune detection and location of faults occurring within the system",2007,0,
329,330,Impact of correlation errors on the optimum Kalman filter gain identification in a single sensor environment,"The impact of errors in the innovation correlation functions evaluation, related to the suboptimal filter, on the identification of the optimum steady state Kalman filter gains are investigated. This issue arises in all real time applications, where the correlations must be calculated from experimental data. An identification algorithm proposed in the literature, with formal proof of convergence, is revisited and summarized. Based on this algorithm, equations describing this impact are developed. Simulation results are presented and discussed. As contribution, experimental results of the identification algorithm, applied to estimate the states of a position servo systems, are presented.",2004,0,
330,331,A Fault Tolerant Control strategy for an unmanned aerial vehicle based on a Sequential Quadratic Programming algorithm,"In this paper a fault tolerant control strategy for the nonlinear model of an unmanned aerial vehicle (UAV) equipped with numerous redundant controls is proposed. Asymmetric actuator failures are considered and, in order to accommodate them, a sequential quadratic programming (SQP) algorithm which takes into account nonlinearities, aerodynamic and gyroscopic couplings, state and control limitations is implemented. This algorithm computes new trims such that around the new operating point, the faulty linearized model remains nearby from the fault free model. For the faulty linearized models, linear state feedback controllers based on an eigenstructure assignment method are designed to obtain soft transients during accommodation. Real time implementation of the SQP algorithm is also discussed.",2008,0,
331,332,"A Framework to Evaluate the Trade-Off among AVF, Performance and Area of Soft Error Tolerant Microprocessors","Because of the increasing susceptibility of the integrated circuits to soft errors, many techniques have been proposed in all the design levels to reduce the AVF (architecturally vulnerable factor) of microprocessors with extra performance and area overheads. These overheads have a negative impact on the reliability. Conventional reliability evaluation frameworks do not take both performance and area overheads into account. A new metric, mMWTF (modified mean work to failure), is proposed in this paper to capture the trade-off among AVF, performance and area. A quantitative approach to evaluate mMWTF is also presented, in which fault injection is used to estimate the AVF. To modify the conventional fault injection methods which inject only SEU (single event upset), a new method is proposed to injects both SEU and MBU (multi bits upset), the latter of which happens more frequently with the shrinking feature size. Because of the new metric and the new fault injection method, the framework presented in this paper is more accurate than conventional ones. As a case study, two control flow checking techniques are proposed and evaluated in this paper. The evaluation results demonstrate that the techniques with better balance among AVF, performance and area can better improve the reliability of microprocessors.",2008,0,
332,333,Falcon: fault localization in concurrent programs,"Concurrency fault are difficult to find because they usually occur under specific thread interleavings. Fault-detection tools in this area find data-access patterns among thread interleavings, but they report benign patterns as well as actual faulty patterns. Traditional fault-localization techniques have been successful in identifying faults in sequential, deterministic programs, but they cannot detect faulty data-access patterns among threads. This paper presents a new dynamic fault-localization technique that can pinpoint faulty data-access patterns in multi-threaded concurrent programs. The technique monitors memory-access sequences among threads, detects data-access patterns associated with a program's pass/fail results, and reports dataaccess patterns with suspiciousness scores. The paper also presents the description of a prototype implementation of the technique in Java, and the results of an empirical study we performed with the prototype on several Java benchmarks. The empirical study shows that the technique can effectively and efficiently localize the faults for our subjects.",2010,0,
333,334,Enhanced error concealment with mode selection,"Delay sensitive video transmission over error prone networks can suffer from packet erasures when channel conditions are not favorable. Use of error concealment (EC) at the video decoder is necessary in such cases to prevent error induced artefacts making the affected video frames visibly intolerable. This paper proposes an EC method that incorporates enhanced temporal and spatial concealment elements, the use of which is controlled by a mode selection (MS) algorithm well matched to the characteristics of the temporal concealment approach. The performance of the individual enhancements and of the MS algorithm are compared with the respective features of the method employed in the H.264 joint model (JM) decoder and with other state of the art methods. The overall performance of the proposed method is shown to offer significant gains (up to 9 dB) compared to that of the JM decoder for a wide range of natural and animation image sequences without any considerable increase in complexity",2006,0,
334,335,A Fault-Location Method for Application With Current Differential Relays of Three-Terminal Lines,This paper presents a new method for locating faults on three-terminal power lines. Estimation of a distance to fault and indication of a faulted section is performed using three-phase current from all three terminals and additionally three-phase voltage from the terminal at which a fault locator is installed. Such a set of synchronized measurements has been taken into consideration with the aim of developing a fault-location algorithm for applications with current differential relays of three-terminal lines. The delivered fault-location algorithm consists of three subroutines designated for locating faults within particular line sections and a procedure for indicating the faulted line section. Testing and evaluation of the algorithm has been performed with fault data obtained from versatile Alternate Transients Program-Electromagnetic Transients Program simulations. The sample results of the evaluation are reported and discussed.,2007,0,
335,336,Bad Words: Finding Faults in Spirit's Syslogs,"Accurate fault detection is a key element of resilient computing. Syslogs provide key information regarding faults, and are found on nearly all computing systems. Discovering new fault types requires expert human effort, however, as no previous algorithm has been shown to localize faults in time and space with an operationally acceptable false positive rate. We present experiments on three weeks of syslogs from Sandia's 512-node ""Spirit"" Linux cluster, showing one algorithm that localizes 50% of faults with 75% precision, corresponding to an excellent false positive rate of 0.05%. The salient characteristics of this algorithm are (1) calculation of nodewise information entropy, and (2) encoding of word position. The key observation is that similar computers correctly executing similar work should produce similar logs.",2008,0,
336,337,Efficient Test Pattern Compression Method Using Hard Fault Preferring,"This paper describes new compression method that is used for test pattern compaction and compression in algorithm called COMPAS, which utilizes a test data compression method based on pattern overlapping. This algorithm reorders and compresses deterministic test patterns previously generated in an ATPG by overlapping them. Independency of COMPAS on used ATPG is discussed and verified. New method improves compression ratio by preprocessing input data to determine the degree of random test resistance for each fault. This information allows the algorithm to reorder test patterns more efficiently and results to 10% compression ratio improvement in average. Compressed data sequence is well suited for decompression by the scan chains in the embedded tester cores.",2008,0,
337,338,Detection of small defects by THz-waves for non-destructive testing in dielectric layered structures,"In this study, the small defects detection in dielectric layered structures by THz waves for nondestructive testing. Finite element method were used for modelling of the structures.",2010,0,
338,339,"On line sensor fault detection, isolation and accommodation in tactical aerospace vehicle","This paper presents on line sensor fault detection, isolation (FDI) and the associated fault tolerant control (FTC) algorithm for a tactical aerospace vehicle. A study on the analytical redundancy and a sensor fault detection scheme (FDI ) into a flight control system has been performed for a tactical aerospace vehicle using the longitudinal model. There are various methods available in the academic literature to apply FDI and FTC schemes to control systems and some have already been applied to real applications. Among these, observer-based approaches have arisen as one of the most widespread. The basic ideas behind observer-based FDI schemes are the generation of residuals, and the use of an optimal threshold function to differentiate faults from disturbances. Generally, the residuals, also known as diagnostic signals, are generated from estimates of the system's measurements obtained by a Luenberger observer or a Kalman filter. The threshold function is then used to 'detect' the fault by separating the residuals from false faults and disturbances. The change in residual signal is used to detect and isolate the fault and corresponding fault tolerant control action is taken to arrest the failure of the aerospace vehicle. A closed-loop simulation with nonlinear 6-degree of freedom (6-DoF) model shows that the above FDI and FTC scheme will be able to reduce the probability of mission failure due to the fault in one of the sensors.",2004,0,
339,340,Induction machines performance evaluator 'torque speed estimation and rotor fault diagnostic',"This paper proposes a new DSP based tool for evaluating the performance of induction motors based on the data extracted from the stator current. In the proposed algorithm, a pattern recognition technique according to Bayes minimum error classifier is developed to detect incipient rotor faults such as broken rotor bars and static eccentricity in induction motors. Also, part of the algorithm is based on the acceleration method presented in the IEEE Std. 112. It helps to calculate the motor's torque using two line currents and voltages. The use of linear and quadratic time-frequency representations is investigated as a viable solution to the task at hand. Speed information is vital in this approach, so an algorithm to track the speed related saliency induced harmonics from the machine's line current spectrogram is presented. Capturing the harmonics gives the rotor speed that can also be used to extract the feature vector for diagnostic. The implementation of the algorithm on TMS320C6000 family of DSP chips is currently underway. The complete algorithm is then be used to obtain the induction motor's performance curves. This is a complete stand-alone panel mounted induction motor diagnostic tool currently being developed in their lab. This package will be used in conjunction with a drive system (inverter) for online performance monitoring and preventing unwanted shutdown of the induction motor. The difficulties encountered, including a limited dynamic range and the presence of cross terms, are addressed and the suggested solution is provided. Experimental results corroborating the proposed algorithm are presented, and a discussion of the advantages and disadvantages of such an approach is touched upon",2002,0,
340,341,Correction of MR k-space data corrupted by spike noise,"Magnetic resonance images are reconstructed from digitized raw data, which are collected in the spatial-frequency domain (also called k-space). Occasionally, single or multiple data points in the k-space data are corrupted by spike noise, causing striation artifacts in images. Thresholding methods for detecting corrupted data points can fail because of small alterations, especially for data points in the low spatial frequency area where the k-space variation is large. Restoration of corrupted data points using interpolations of neighboring pixels can give incorrect results. The authors propose a Fourier transform method for detecting and restoring corrupted data points using a window filter derived from the striation-artifact structure in an image or an intermediate domain. The method provides an analytical solution for the alteration at each corrupted data point. It can effectively restore corrupted k-space data, removing striation artifacts in images, provided that the following 3 conditions are satisfied. First, a region of known signal distribution (for example, air background) is visible in either the image or the intermediate domain so that it can be selected using a window filter. Second, multiple spikes are separated by the full-width at half-maximum of the point spread function for the window filter. Third, the magnitude of a spike is larger than the minimum detectable value determined by the window filter and the standard deviation of k-space random noise.",2000,0,
341,342,Initial Experiences with a New FPGA Based Traveling Wave Fault Recorder Installed on a MV Distribution Network,"This paper presents the initial results obtained from a newly developed FPGA based traveling wave fault recorder installed on a medium voltage (MV) distribution line. The recorder is capable of recording six input signals, simultaneously sampling at 40 mega samples per second (MSPS) and at 14 bit resolution. It uses high bandwidth 17 MHz Rogowski coils connected to the secondary of a current transformer inside the substation to acquire the high frequency traveling wave components. Initial results during the testing phase show that recorder is capable of recording high fidelity signals relating to switching events. It has also highlighted that the distribution line is subject to many other transient phenomena in addition to faults and switching events which must be taken into consideration when choosing a suitable triggering mechanism.",2008,0,
342,343,Evaluation of the Low Frame Error Rate Performance of LDPC Codes Using Importance Sampling,"We present an importance sampling method for the evaluation of the low frame error rate (FER) performance of LDPC codes under iterative decoding. It relies on a combinatorial characterization of absorbing sets, which are the dominant cause of decoder failure in the low FER region. The biased density in the importance sampling scheme is a mean-shifted version of the original Gaussian density, which is suitably centered between a codeword and a dominant absorbing set. This choice of biased density yields an unbiased estimator for the FER with a variance lower by several orders of magnitude than the standard Monte Carlo estimator. Using this importance sampling scheme in software, we obtain good agreement with the experimental results obtained from a fast hardware emulator of the decoder.",2007,0,
343,344,A New Method for Earth Fault Line Detection Based on Two-Dimensional Wavelet Transform in Distribution Automation,"A novel method based on two-dimensional wavelet transform to detect single-phase faults in distribution systems is proposed in this paper. After structuring analytic signals of zero sequence current, the two-dimensional wavelet transform is applied. Thus the analysis of combined signal of amplitude and phase is realized. Compared with the use of single amplitude or single phase, combined signal carries more details of transient signal. Theoretical analysis and MATLAB based simulation show that the presented method can exactly and effectively choose the faulty line in single phase-to-ground fault",2005,0,
344,345,An FFT-based method to evaluate and compensate gain and offset errors of interleaved ADC systems,"Interleaved analog-digital converter (ADC) systems can be used to increase the sampling rate for a given ADC implementation technique. In theory, the maximum sampling rate that can be achieved is limited only by the bandwidth and the practical limits related to the power and space of integrated circuits. In this paper, a solution to increase the sampling rate of a digitizing system based on interleaved ADCs is presented. An error analysis, which takes into consideration offset and gain errors of the different ADC channels, is performed in order to quantify the effect of such errors in the system's performance. A software method based on the fast Fourier transform is presented for offset and gain error compensation of interleaved ADC associations. Numerical simulations and experimental results are used to validate the theory and the proposed compensation algorithm.",2004,0,
345,346,Fault Diagnosis of Generator Based on D-S Evidence Theory,"It is difficult to identify the fault type with the signal gathered from the sensors. In this paper, a new fusion algorithm based on the Dempster-Shafer theory of evidence and neural networks is brought forward. This method combines the advantages of D-S evidence theory and the BP neural network. Neural networks are used to pretreated the data gathered from the embedded sensors in the monitoring system of hydropower plant. Compared with the approaches that only adopt D-S evidence theory or neural networks, the accuracy of diagnostic results is obviously improved, and the signals analysis proved this conclusion. This method has been applied in the monitoring system of JiLin FengMan hydropower plant successfully.",2008,0,
346,347,Improved error bounds for the erasure/list scheme: the binary and spherical cases,We derive improved bounds on the error and erasure rate for spherical codes and for binary linear codes under Forney's erasure/list decoding scheme and prove some related results.,2004,0,
347,348,Faulted phase selection on double circuit transmission line using wavelet transform and neural network,"Modern numerical relays often incorporate the logic for combined single and three-phase auto-reclosing scheme; single phase to earth faults initiate single-phase tripping and reclosure, and all the other faults initiate three-phase tripping and reclosure. Accurate faulted phase selection is required for such a scheme. This paper presents a novel scheme for detection and classification of faults on double circuit transmission line. The proposed approach uses combination of wavelet transform and neural network, to solve the problem. While wavelet transform is a powerful mathematical tool which can be employed as a fast and very effective means of analyzing power system transient signals, artificial neural network has a ability to classify non-linear relationship between measured signals by identifying different patterns of the associated signals. The proposed algorithm consists of time-frequency analysis of fault generated transients using wavelet transform, followed by pattern recognition using artificial neural network to identify the faulted phase. MATLAB/Simulink software is used to generate fault signals and verify the correctness of the algorithm. The adaptive discrimination scheme is tested by simulating different types of fault and varying fault resistance, fault location and fault inception time, on a given power system model. The simulation results show that the proposed phase selector scheme is able to identify faulted phase on the double circuit transmission line rapidly and correctly.",2009,0,
348,349,Algorithm-Based Fault Tolerance for Fail-Stop Failures,"Fail-stop failures in distributed environments are often tolerated by checkpointing or message logging. In this paper, we show that fail-stop process failures in ScaLAPACK matrix-matrix multiplication kennel can be tolerated without checkpointing or message logging. It has been proved in previous algorithm-based fault tolerance that, for matrix-matrix multiplication, the checksum relationship in the input checksum matrices is preserved at the end of the computation no mater which algorithm is chosen. From this checksum relationship in the final computation results, processor miscalculations can be detected, located, and corrected at the end of the computation. However, whether this checksum relationship can be maintained in the middle of the computation or not remains open. In this paper, we first demonstrate that, for many matrix matrix multiplication algorithms, the checksum relationship in the input checksum matrices is not maintained in the middle of the computation. We then prove that, however, for the outer product version algorithm, the checksum relationship in the input checksum matrices can be maintained in the middle of the computation. Based on this checksum relationship maintained in the middle of the computation, we demonstrate that fail-stop process failures (which are often tolerated by checkpointing or message logging) in ScaLAPACK matrix-matrix multiplication can be tolerated without checkpointing or message logging.",2008,0,
349,350,Maintaining Consistency between Loosely Coupled Services in the Presence of Timing Constraints and Validation Errors,"Loose coupling is often cited as a defining characteristic of service-oriented architectures. Interactions between services take place via messages in an asynchronous environment where communication and processing delays can be unpredictable; further, interacting parties are not required to be on-line at the same time. Despite loose coupling, many service interactions have timing and validation constraints. For example, business interactions that take place using RosettaNet partner interface processes (PIPs) such as request price and availability, request purchase order, notify of invoice, etc. have to meet several timing and message validation constraints. A failure to deliver a valid message within its time constraint could cause mutually conflicting views of an interaction. For example, one party can regard it as timely whilst the other party regards it as untimely, leading to application level inconsistencies. The paper describes how business interactions, such as PIPs can be wrapped by simple handshake synchronisation protocols to provide bilateral consistency, thereby simplifying the task of coordinating peer-to-peer business processes",2006,0,
350,351,Compact Microstrip Quasi-Elliptic Bandpass Filter Using Open-Loop Dumbbell Shaped Defected Ground Structure,"A novel square open-loop dumbbell-shaped defected ground structure (DGS) unit is proposed. This unit provides a quasi-elliptic bandpass characteristic and the two transmission zeros near the passband edges can be controlled by the dimensions of DGS. Two quasi-elliptic bandpass filters using one and two DGS units; centered at 1.5 GHz were designed and implemented. Both the simulation and experimental results show that the DGS filter response is in good accordance with ideal quasi-elliptic model. The prototype filter with two DGS units yields higher order quasi-elliptic filtering and reports the measured 0.72 dB as insertion loss, 34 dB as matching, 51.8 % fractional bandwidth and about 20 dB stopband attenuation up to 10 GHz",2006,0,
351,352,Analyzing the soft error resilience of linear solvers on multicore multiprocessors,"As chip transistor densities continue to increase, soft errors (bit flips) are becoming a significant concern in networked multiprocessors with multicore nodes. Large cache structures in multicore processors are especially susceptible to soft errors as they occupy a significant portion of the chip area. In this paper, we consider the impacts of soft errors in caches on the resilience and energy efficiency of sparse linear solvers. In particular, we focus on two widely used sparse iterative solvers, namely Conjugate Gradient (CG) and Generalized Minimum Residuals (GMRES). We propose two adaptive schemes, (i) a Write Eviction Hybrid ECC (WEH-ECC) scheme for the L1 cache and (ii) a Prefetcher Based Adaptive ECC (PBA-ECC) scheme for the L2 cache, and evaluate the energy and reliability trade-offs they bring in the context of GMRES and CG solvers. Our evaluations indicate that WEH-ECC reduces the CG and GMRES soft error vulnerability by a factor of 18 to 220 in L1 cache, relative to an unprotected L1 cache, and energy consumption by 16%, relative to a cache with strong protection. The PBA-ECC scheme reduces the CG and GMRES soft error vulnerability by a factor of 9 A 10<sup>3</sup> to 8.6 A 10<sup>9</sup>, relative to an unprotected L2 cache, and reduces the energy consumption by 8.5%, relative to a cache with strong ECC protection. Our energy overheads over unprotected L1 and L2 caches are 5% and 14% respectively.",2010,0,
352,353,Application-driven co-design of fault-tolerant industrial systems,"This paper presents a novel methodology for the HW/SW co-design of fault tolerant embedded systems that pursues the mitigation of radiation-induced upset events (which are a class of Single Event Effects - SEEs) on critical industrial applications. The proposal combines the flexibility and low cost of Software Implemented Hardware Fault Tolerance (SIHFT) techniques with the high reliability of selective hardware replication. The co-design flow is supported by a hardening platform that comprises an automatic software hardening environment and a hardware tool able to emulate Single Event Upsets (SEUs). As a case study, we selected a soft-micro (PicoBlaze) widely used in FPGA-based industrial systems, and a fault tolerant version of the matrix multiplication algorithm was developed. Using the proposed methodology, the design was guided by the requirements of the application, leading us to explore several trade-offs among reliability, performance and cost.",2010,0,
353,354,Defect-Tolerant Design and Optimization of a Digital Microfluidic Biochip for Protein Crystallization,"Protein crystallization is a commonly used technique for protein analysis and subsequent drug design. It predicts the 3-D arrangement of the constituent amino acids, which in turn indicates the specific biological function of a protein. Protein crystallization experiments are typically carried out in well-plates in the laboratory. As a result, these experiments are slow, expensive, and error-prone due to the need for repeated human intervention. Recently, droplet-based AdigitalA microfluidics have been used for executing protein assays on a chip. Protein samples in the form of nanoliter-volume droplets are manipulated using the principle of electrowetting-on-dielectric. We present the design of a multi-well-plate microfluidic biochip for protein crystallization; this biochip can transfer protein samples, prepare candidate solutions, and carry out crystallization automatically. To reduce the manufacturing cost of such devices, we present an efficient algorithm to generate a pin-assignment plan for the proposed design. The resulting biochip enables control of a large number of on-chip electrodes using only a small number of pins. Based on the pin-constrained chip design, we present an efficient shuttle-passenger-like droplet manipulation method and test procedure to achieve high-throughput and defect-tolerant well loading.",2010,0,
354,355,Influence of the AC system faults on HVDC system and recommendations for improvement,"The interaction between AC and DC systems in a long distance bulk power transmission system is very complicated. In this paper, taking some cases in China Southern Power Grid as examples, the main functions of the DC control system operating during the AC faults is discussed. During the AC faults, the protection and monitoring system for DC converter and the protection system for converter transformers and auxiliary transformers may work incorrectly, reasons for these cases are analyzed and recommendations are given to solve the defects. Experience from these cases will help us to improve the ability of operation and maintenance to insure the safety and provide useful references for the design of HVDC and the coordination of AC/DC system in China.",2009,0,
355,356,Dense error correction via l1-minimization,"We study the problem of recovering a non-negative sparse signal x isin Ropf<sup>n</sup> from highly corrupted linear measurements y = Ax+e isin Ropf<sup>m</sup>, where e is an unknown (and unbounded) error. Motivated by an observation from computer vision, we prove that for highly correlated dictionaries A, any non-negative, sufficiently sparse signal x can be recovered by solving an lscr<sup>1</sup>-minimization problem: min ||x||<sub>1</sub> + ||e||<sub>1</sub> subject to y = Ax + e. If the fraction rho of errors is bounded away from one and the support of x grows sublinearly in the dimension m of the observation, for large m, the above lscr<sup>1</sup>-minimization recovers all sparse signals x from almost all sign-and-support patterns of e. This suggests that accurate and efficient recovery of sparse signals is possible even with nearly 100% of the observations corrupted.",2009,0,
356,357,Efficient stimuli generators for detection of path delay faults,"This paper presents a way to construct accumulator based test vector generators intended for efficient detection of path delay faults. Experiments conducted using our path delay fault simulator, GFault, shows that our proposed generator can give as much as 30times reduction in test time for circuits in the ISCAS85 benchmark suite compared to an accumulator based pseudo random generator",2005,0,
357,358,A fault analysis and design consideration of pulsed-power supply for high-power laser,"According to the requirements of driving flashlamps, the design of a pulsed-power supply (PPS), based on capacitors as energy storage elements, is presented. Special consideration is given to some possible faults such as capacitor internal short-circuit, bus bar breakdown to ground, flashlamp sudden short or break (open circuit), and closing switch restrike in the preionization branch. These faults were analyzed in detail, and both fault current and voltage waveforms are shown through circuit simulation. Based on the analysis and computation undertaken, the pulsed-power system design and protection requirements are proposed. The preliminary experiments undertaken after circuit simulation demonstrated that the design of the PPS met the project requirements.",2003,0,
358,359,A fault tolerant control architecture for automated highway systems,A hierarchical controller for dealing with faults and adverse environmental conditions on an automated highway system is proposed. The controller extends a previous control hierarchy designed to work under normal conditions of operation. The faults are classified according to the capabilities remaining on the vehicle or roadside after the fault has occurred. Information about these capabilities is used by supervisors in each of the layers of the hierarchy to select appropriate fault handling strategies. We outline the strategies needed by the supervisors and give examples of their detailed operation,2000,0,
359,360,Analysis of interconnect crosstalk defect coverage of test sets,"This paper addresses the problem of evaluating the effectiveness of test sets to detect crosstalk defects in interconnects of deep sub-micron circuits. The fast and accurate estimation technique will enable: (a) evaluation of different existing tests, like functional, scan, logic BIST, and delay tests, for effective testing of crosstalk defects in interconnects, and (b) development of crosstalk tests if the existing tests are not sufficient, thereby minimizing the cost of interconnect testing. Based on a covering relationship we establish between transition tests in detecting crosstalk defects, we develop an abstract crosstalk fault model for circuit interconnects. Based on this fault model, and the covering relationship, we develop a fast and efficient method to estimate the fault coverage of any general test set. We also develop a simulation-based technique to calculate the probability of occurrence of the defects corresponding to each fault, which enables the fault coverage analysis technique to produce accurate estimates of the actual crosstalk defect coverage of a given test set. The crosstalk test and fault properties, as well as the accuracy of the proposed crosstalk coverage analysis techniques, have been validated through extensive simulation experiments. The experiments also demonstrate that the proposed crosstalk techniques are orders of magnitude faster than the alternative method of SPICE-level simulation. Finally, we demonstrate the practical applicability of the proposed fault coverage analysis technique by using it to evaluate the crosstalk fault coverage of logic BIST tests for the buses in a DSP core",2000,0,
360,361,Atmospheric correction of AMSR-E brightness temperatures for dry snow cover mapping,"Differences between the brightness temperatures (spectral gradient) collected by the Advanced Microwave Scanning Radiometer for EOS (AMSR-E) at 18.7 and 36.5 GHz are used to map the snow-covered area (SCA) over a region including the western U.S. The brightness temperatures are corrected to take into account for atmospheric effects by means of a simplified radiative transfer equation whose parameters are stratified using rawinsonde data collected from a few stations. The surface emissivity is estimated from the model, and the brightness temperatures at the surface are computed as the product of the surface temperature and the computed emissivity. The SCA derived from microwave data is compared with that obtained from the Moderate Resolution Imaging Spectroradiometer for both cases of corrected and noncorrected brightness temperatures. The improvement to the SCA retrievals based on the corrected brightness temperatures shows an average value around 7%",2006,0,
361,362,A low-tech solution to avoid the severe impact of transient errors on the IP interconnect,"There are many sources of failure within a system-on-chip (SoC), so it is important to look beyond the processor core at other components that affect the reliable operation of the SoC, such as the fabric included in every one that connects the IP together. We use ARM's AMBA 3 AXI bus matrix to demonstrate that the impact of errors on the IP interconnect can be severe: possibly causing deadlock or memory corruption. We consider the detection of 1-bit transient faults without changing the IP that connects to the bus matrix or the AMBA 3 standard and without adding extra latency while keeping the performance and area overhead low. We explore what can be done under these constraints and propose a combination of techniques for a low-tech solution to detect these rare events.",2009,0,
362,363,The study of analog circuit fault diagnosis method based on circuit transform function,"This is the paper use Laplace Transfer to compute the analog circuit transform function, and compute the fault transform functions with different kinds of fault to generate the fault diagnosis table. The table is used to complete fault diagnosis. At last the software Multisim is used to simulate an analog circuit to do the fault diagnosis, and this method is verified usefully.",2010,0,
363,364,Efficient Fault-Tolerant Backbone Construction in Tmote Sky Sensor Networks,"In this study, we have investigated the effectiveness of building AFault-Tolerant BackboneA for data dissemination in Tmote Sky sensor networks. Tmote Sky sensors provide programmable and adjustable output power for data transmission. Users can control adequate transmission power for each sensor. Based on our measurements of Tmote Sky, there is a steadily transmitted distance for every power level. For certain power level, successfully-transmitted ratio was approximately 100 percent when the distance between sender and receiver was less than the steadily-transmitted distance. In accordance with the character on Tmote Sky, the ideas of fault-tolerant backbone has been made for constructing a fault-tolerant and stable system for Tmote Sky. The fault-tolerant backbone protocol builds up a connected backbone, in which nodes are endowed with a sleep/awake schedule. Practical experimental results reveal the fast fault recovery and high successfully-transmitted ratio can be fulfilled in the realistic system. The following goals in the implementation have been reached, including self-configurable fault-tolerant groups, automatic backbone construction, automatic failure recovery, and route repair.",2009,0,
364,365,Anomaly detection: A robust approach to detection of unanticipated faults,"This paper introduces a methodology to detect as early as possible with specified degree of confidence and prescribed false alarm rate an anomaly or novelty (incipient failure) associated with critical components/subsystems of an engineered system that is configured to monitor continuously its health status. Innovative features of the enabling technologies include a Bayesian estimation framework, called particle filtering, that employs features or condition indicators derived from sensor data in combination with simple models of the systempsilas degrading state to detect a deviation or discrepancy between a baseline (no-fault) distribution and its current counterpart. The scheme provides the probability of abnormal condition and the probability of false alarm. The presence of an anomaly is confirmed for a given confidence level. The efficacy of the proposed anomaly detection architecture is illustrated with test data acquired from components typically found on aircraft and monitored via a test rig appropriately instrumented.",2008,0,
365,366,Research of Remote Fault Diagnosis System Based on Multi-Agent,"A Multi-agent based Remote fault Diagnosis system is an important system for high speed and automation which can not only monitor the status of the remote device, but serve for the remote device. Remote Fault diagnosis system are vital aspects in automation process, in this sense, remote diagnosis systems should support decision-making tools, the enterprise thinking and flexibility. In this paper a kind of Remote Diagnosis System based on multi-agent is presented. This model is based on a generic framework using multi-agent systems. Specifically, this paper analyses the architecture of Remote Fault Diagnosis System and the collaboration mechanism between Agents. The method brought forward in the paper was generally applicable to a general fault diagnosis.",2010,0,
366,367,Fault diagnosis for a delta-sigma converter by a neural network,The diagnosis of faults in a first order -converter is described. The circuit behaviour of fault-free circuits and circuits containing single faults were simulated and characterized by the output bitstream patterns. The latter were compared with that of the ideal fault-free circuit. A Simplified fuzzy ARTMAP was trained with metrics derived from the bitstreams and their assigned class. A diagnostic accuracy of 93% was achieved using just two of the metrics. The technique might be useful for the diagnosis of other circuits.,2004,0,
367,368,The Impact of Link Error Modeling on the Quality of Streamed Video in Wireless Networks,The influence of channel error characteristics on higher layer protocols or methods which are considering or even exploiting the error statistics is significant especially in wireless networks where fading and interference effects result in error pattern correlation properties (error bursts). In this work we are analysing the impact of the channel properties directly on the quality of streamed video. We are focusing on the quality of transmitted H.264/AVC video streaming over UMTS DCH (Dedicated Channel) and compare the quality of the streamed video simulated over measured link error traces (the measurements performed in a live UMTS network) to simulations with a memoryless channel and to models with enhanced error characteristics. The results show that appropriate modeling of the link layer error characteristics is very important but it can also be concluded that the error correlation properties of the link- or the network-layer model do not have an impact on the quality of the video stream as long as the resulting IP packet error probability remains unchanged.,2006,0,
368,369,A portable gait analysis and correction system using a simple event detection method,"Microcontrollers are widely used in the area of portable control systems, though they are only beginning to be used for portable, unobtrusive Functional Electrical Stimulation (FES) systems. This paper describes the initial prototyping of such a portable system. This has the intended use of detecting time variant gait anomalies in patients with hemiplegia, and correcting for them. The system is described in two parts. Firstly, the portable hardware implementing two independent communicating microcontrollers for low powered parallel processing and secondly the simplified low power software. Both are designed specifically for long term, stable use and also to communicate with PC based visual software for testing and evaluation. The system operates by using bend sensors to defect the angles of the hip, knee and ankle of both legs. It computes an error signal with which to produce a stimulation wave cycle, that is synchronised and timed for the new gait cycle from that in which the error was observed. This system uses a PID controller to correct for the instability inherent with such a large time delay between observation and correction.",2002,0,
369,370,Reaction to errors in robot systems,"The paper analyzes the problem of error (failure) detection and handling in robot programming. First an overview of the subject is provided and later error detection and handling in MRROC++ are described. To facilitate system reaction to the detected failures, the errors are classified and certain suggestions are made as to how to handle those classes of errors.",2002,0,
370,371,Final Prediction Error of Autoregressive Model as a New Feature in the Analysis of Heart Rate Variability,"The aim of this study is to offer a new heart rate variability (HRV) index that increases the accuracy in the discrimination of patients with congestive heart failure (CHF) from the control group. For this purpose, final prediction errors (FPE), which shows the quality of the conformity of autoregressive (AR) model, are calculated for model degrees from 1 to 100. Although the optimal AR model order and FPE values are widely used in the literature, they have not been used as possible HRV indices. In this study, we used FPE as an HRV feature for discriminating the patients with CHF from normal subjects and made a comparison with the other common HRV indices. As a result, we showed that FPE of AR model is a possible significant HRV feature.",2007,0,
371,372,One-Dimensional Variational Retrieval of the Wet Tropospheric Correction for Altimetry in Coastal Regions,"The altimeter range is corrected for tropospheric humidity by means of microwave radiometer measurements (Envisat/MWR, Jason-1/JMR, Jason-2/AMR). Over an open ocean, the altimeter/radiometer combination is satisfactory. However, in coastal areas, radiometer measurements are contaminated by the surrounding land surfaces, and the humidity retrieval method is not appropriate anymore. In this paper, a variational assimilation technique is proposed to retrieve the wet tropospheric correction near coasts. The method is first developed on simulations using the data from a meteorological model. A performance assessment is performed, as well as a comparison with a standard algorithm. The method is then applied on actual measurements, thus evaluating its feasibility.",2010,0,
372,373,Compact Test Generation for Small-Delay Defects Using Testable-Path Information,"Testing for small-delay defects requires fault-effect propagation along the longest testable paths. However, the selection of the longest testable paths requires high CPU time and leads to large pattern counts. Dynamic test compaction for small-delay defects has remained largely unexplored thus far. We propose a path-selection scheme to accelerate ATPG based on stored testable critical-path information. A new dynamic test-compaction technique based on structural analysis is also introduced. Simulation results are presented for a set of ISCAS'89 benchmark circuits.",2009,0,
373,374,Use of fault tree analysis for evaluation of system-reliability improvements in design phase,"Traditional failure mode and effects analysis is applied as a bottom-up analytical technique to identify component failure modes and their causes and effects on the system performance, estimate their likelihood, severity and criticality or priority for mitigation. Failure modes and their causes, other than those associated with hardware, primarily electronic, remained poorly addressed or not addressed at all. Likelihood of occurrence was determined on the basis of component failure rates or by applying engineering judgement in their estimation. Resultant prioritization is consequently difficult so that only the apparent safety-related or highly critical issues were addressed. When thoroughly done, traditional FMEA or FMECA were too involved to be used as a effective tool for reliability improvement of the product design. Fault tree analysis applied to the product as a top down in view of its functionality, failure definition, architecture and stress and operational profiles provides a methodical way of following products functional flow down to the low level assemblies, components, failure modes and respective causes and their combination. Flexibility of modeling of various functional conditions and interaction such as enabling events, events with specific priority of occurrence, etc., using FTA, provides for accurate representation of their functionality interdependence. In addition to being capable of accounting for mixed reliability attributes (failure rates mixed with failure probabilities), fault trees are easy to construct and change for quick tradeoffs as roll up of unreliability values is automatic for instant evaluation of the final quantitative reliability results. Failure mode analysis using fault tree technique that is described in this paper allows for real, in-depth engineering evaluation of each individual cause of a failure mode regarding software and hardware components, their functions, stresses, operability and interactions",2000,0,
374,375,Local magnetic error estimation using action and phase jump analysis of orbit data,"It's been shown in previous conferences that action and phase jump analysis is a promising method to measure normal quadrupole components, skew quadrupole components and even normal sextupole components. In this paper, the action and phase jump analysis is evaluated using new RHIC data.",2007,0,
375,376,Mining Frequent Patterns from Software Defect Repositories for Black-Box Testing,"Software defects are usually detected by inspection, black-box testing or white-box testing. Current software defect mining work focuses on mining frequent patterns without distinguishing these different kinds of defects, and mining with respect to defect type can only give limited guidance on software development due to overly broad classification of defect type. In this paper, we present four kinds of frequent patterns from defects detected by black-box testing (called black-box defect) based on a kind of detailed classification named ODC-BD (Orthogonal Defect Classification for Blackbox Defect). The frequent patterns include the top 10 conditions (data or operation) which most easily result in defects or severe defects, the top 10 defect phenomena which most frequently occur and have a great impact on users, association rules between function modules and defect types. We aim to help project managers, black-box testers and developers improve the efficiency of software defect detection and analysis using these frequent patterns. Our study is based on 5023 defect reports from 56 large industrial projects and 2 open source projects.",2010,0,
376,377,Low error rate LDPC decoders,"Low-density parity-check (LDPC) codes have been demonstrated to perform very close to the Shannon limit when decoded iteratively. However challenges persist in building practical high-throughput decoders due to the existence of error floors at low error rate levels. We apply high-throughput hardware emulation to capture errors and error-inducing noise realizations, which allow for in-depth analysis. This method enables the design of LDPC decoders that operate without error floors down to very low bit error rate (BER) levels. Such emulation-aided studies facilitate complex systems designs.",2009,0,
377,378,Fault-tolerant and energy-efficient permutation routing protocol for wireless networks,"A wireless network (WN) is a distributed system where each node is a small hand-held commodity device called a station. Wireless sensor networks have received increasing interest in recent years due to their usage in monitoring and data collection in a wide variety of environments like remote geographic locations, industrial plants, toxic locations or even office buildings. Two of the most important issues related to a WN are their energy constraints and their potential for developing faults. A station is usually powered by a battery which cannot be recharged while on a mission. Hence, any protocol run by a WN should be energy-efficient. Moreover, it is possible that all stations deployed as part of a WN may not work perfectly. Hence, any protocol designed for a WN should work well even when some of the stations are faulty. We design a protocol which is both energy-efficient and fault-tolerant for permutation routing in a WN.",2003,0,
378,379,A novel approach to architecture of radar fault diagnosis system based on mobile agents,"In order to improve radar fault diagnosis system, a new architecture of fault diagnosis system based on mobile agents is proposed. The architecture is based on an embedded network built-in radar system. It utilizes all kinds of mobile fault diagnostic agents in embedded network to detect shortcomings of distributed subsystems in radar. In the architecture, all MFDAs can migrate in embedded network, and can be centralized a personal computer so as to be updated and retrained conveniently for different batches of radar systems. In this paper, three kinds of start-up modes of fault diagnosis are illustrated, two kinds of multi-agent cooperation diagnostic frameworks are introduced, and a kind of structure of MFDA is addressed.",2010,0,
379,380,A Flexible and efficient bit error rate simulation method for high-speed differential link analysis using time-domain interpolation and superposition,"In this paper, a flexible and efficient time-domain method for calculating the bit error rate of high-speed differential links is presented. The method applies interpolation and superposition to the step response of a channel to construct the jittery data or/and clock waveforms at the receiver. With the statistics of the actual reference-crossing points extracted from the constructed receiver waveforms, the bathtub curves can be derived and extrapolated to get the eye margin at the given bit error rate. A software has been developed and applied for high-speed differential link design using the method. Good correlation has been achieved between the simulated results using this method and the measurement data with a bit error rate tester.",2008,0,
380,381,Do Crosscutting Concerns Cause Defects?,"There is a growing consensus that crosscutting concerns harm code quality. An example of a crosscutting concern is a functional requirement whose implementation is distributed across multiple software modules. We asked the question, ""How much does the amount that a concern is crosscutting affect the number of defects in a program?"" We conducted three extensive case studies to help answer this question. All three studies revealed a moderate to strong statistically significant correlation between the degree of scattering and the number of defects. This paper describes the experimental framework we developed to conduct the studies, the metrics we adopted and developed to measure the degree of scattering, the studies we performed, the efforts we undertook to remove experimental and other biases, and the results we obtained. In the process, we have formulated a theory that explains why increased scattering might lead to increased defects.",2008,0,
381,382,Impact of Transmission Network Reinforcement on Improvement of Power System Voltage Stability and Solving the Dynamic Delayed Voltage Recovery and Motor Stalling Problem After System Faults in the Saudi Electricity in the Western Region,"The Saudi Electricity company power system in the Western Region of the Kingdom (SEC-WR) is unique in its load pattern, growth trends and type, generation recourses and network configuration. The power system of the SEC-WR faced and is facing a high load growth. The high load increase gives rise to a very high loading of the transmission system elements, mainly power transformers and cables. The Western Region load is mainly composed of air conditioner (AC) during high load season. In case of faults this nature of load induces delayed voltage recovery following fault clearing on the transmission system. The sustained low voltage following transmission line faults could cause customer interruptions and may be equipment damage. The integrity of the transmission system may also be affected. The transient stability of the system may be affected. This may also influence the stability of the generating units in the system. The existing dynamic model of SEC-WR System has been described. The response of the model to the actual faults is compared with actual records obtained from the dynamic system monitor (DSM) installed in several locations in the SEC-WR System. The solution of the delayed voltage recovery problem after system faults may be achieved by reinforcement of the system, adding static VAr compensators (SVC) to provide the dynamic reactive power support to the system, reducing the fault clearing time and by under voltage load shedding. This paper analyzes and discusses the first alternative, the system reinforcement",2006,0,
382,383,Higher-order corrections to the pi criterion for the periodic operation of chemical reactors,"The present work develops a method to determine higher-order corrections to the pi criterion, derived from basic results of Center Manifold theory. The proposed method is based on solving the Center Manifold PDE via power series. The advantage of the proposed approach is the improvement of the accuracy of the pi criterion in predicting performance under larger amplitudes. The proposed method is applied to a continuous stirred tank reactor, where the yield of the desired product must be maximized.",2009,0,
383,384,Practical Criteria for the Separability of Eddy-Current Testing Signals on Multiple Defects,"Practical quantities have been introduced by the authors to characterize the interaction among multiple defects located in a specimen under eddy-current testing (ECT). If these quantities indicate that the interaction between the pairs of defects is negligible, the signal of an ECT probe for multiple defects can be calculated as the superposition of those signals, which are obtained for each defect as if it would be a single one. Conversely, if the criteria holds, the measured ECT signal can be decomposed into signals associated with the individual defects, which essentially simplifies the solution of the inverse problem of defect reconstruction. As an application of the criteria, the design of a novel barcoding system is presented, which is developed for marking metallic parts by laser treating, and where the applicability of linear signal processing methods for the reading out of the barcode is a requirement.",2008,0,
384,385,On-Line Reconfigurable XGFT Network-on-Chip Designed for Improving the Fault-Tolerance and Manufacturability of the MPSoC Chips,"Large System-on-Chip (SoC) circuits will contain an increasing number of processors which will communicate with each other across Networks-on-Chip (NOC). The faulty processors could be replaced with faultless ones, whereas only a single defect in the NOC can make the whole chip unusable. Therefore, the fault-tolerance of the NOC is a crucial component of the fault-tolerance and manufacturability of the SoCs. This paper presents a fault-tolerant extended generalized fat tree (XGFT) NOC developed for future multi-processor SoCs (MPSoC). Its fault-tolerance is improved with a new version of fault-diagnosis-and-repair (FDAR) system, which makes it possible to diagnose and repair the NOC on-line. It detects such static, dynamic and transient faults which block packets or produce bit errors, and reconfigures the faulty switches to operate correctly. Processors can also use it for reconfiguring the faulty switch nodes after the faults are located with other test methods. Simulation and synthesis results show that slightly defected XGFTs are able to achieve good performance after they are repaired with the FDAR while the costs of the FDAR remain tolerable",2006,0,
385,386,Attitude correction algorithm using GPS measurements for flight vehicles,For flight systems with an on-board seeker the attitude error is the major factor to determine the seeker pointing error at the time of object acquisition. To achieve a desired mission it must be minimized. The proposed algorithm corrects the attitude error in the guidance computer during flight by taking its position and velocity measurements from GPS or radar. This is possible since navigator's position and velocity states are correlated with attitude state. Computer simulation is shown to prove the proposed algorithm.,2002,0,
386,387,Hardware/software optimization of error detection implementation for real-time embedded systems,"This paper presents an approach to system-level optimization of error detection implementation in the context of fault-tolerant real-time distributed embedded systems used for safety-critical applications. An application is modeled as a set of processes communicating by messages. Processes are mapped on computation nodes connected to the communication infrastructure. To provide resiliency against transient faults, efficient error detection and recovery techniques have to be employed. Our main focus in this paper is on the efficient implementation of the error detection mechanisms. We have developed techniques to optimize the hardware/software implementation of error detection, in order to minimize the global worst-case schedule length, while meeting the imposed hardware cost constraints and tolerating multiple transient faults. We present two design optimization algorithms which are able to find feasible solutions given a limited amount of resources: the first one assumes that, when implemented in hardware, error detection is deployed on static reconfigurable FPGAs, while the second one considers partial dynamic reconfiguration capabilities of the FPGAs.",2010,0,
387,388,Development of Defect Classification Algorithm for POSCO Rolling Strip Surface Inspection System,"Surface inspection system (SIS) is an integrated hardware-software system which automatically inspects the surface of the steel strip. It is equipped with several cameras and illumination over and under the steel strip roll and automatically detects and classifies defects on the surface. The performance of the inspection algorithm plays an important role in not only quality assurance of the rolled steel product, but also improvement of the strip production process control. Current implementation of POSCO SIS has good ability to detect defects, however, classification performance is not satisfactory. In this paper, we introduce POSCO SIS and suggest a new defect classification algorithm which is based on support vector machine technique. The suggested classification algorithm shows good classification ability and generalization performance",2006,0,
388,389,Design of a fault-tolerant coarse-grained,"This paper considers the possibility of implementing low-cost hardware techniques which would allow to tolerate temporary faults in the datapaths of coarse-grained reconfigurable architectures (CGRAs). Our goal was to use less hardware overhead than commonly used duplication or triplication methods. The proposed technique relies on concurrent error detection by using residue code modulo 3 and re-execution of the last operation, once an error is detected. We have chosen the DART architecture as a vehicle to study the efficiency of this approach to protect its datapaths. Simulation results have confirmed hardware savings of the proposed approach over duplication.",2010,0,
389,390,"HGRID: Fault Tolerant, Log2N Resource Management for Grids","Grid resource discovery service is currently a very important focus of research. We propose a scheme that presents essential characteristics for efficient, self-configuring and fault-tolerant resource discovery and is able to handle dynamic attributes, such as memory capacity. Our approach consists of an overlay network with a hypercube topology connecting the grid nodes and a scalable, fault-tolerant, self-configuring search algorithm. By design, the algorithm improves the probability of reaching all working nodes in the system even in the presence of failures (inaccessible, crashed or heavy loaded nodes). We analyze the static resilience of the presented approach, that is to say, how well the algorithm is able to find resources without having to update the routing tables. The results show that the presented approach has a high static resilience.",2009,0,
390,391,Adaptive Error-Resilience Transcoding and Fairness Grouping for Video Multicast Over Wireless Networks,"In this paper, we present a two-pass intra-refresh transcoder for on-the-fly enhancing error resilience of a compressed video in a three-tier streaming system. Furthermore, we consider the problem of multicasting a video to multiple clients with diverse channel conditions. We propose a MINMAX loss rate estimation scheme to determine a single intra- refresh rate for all the clients in a multicast group. For the scenario that a quality variation constraint is imposed on the users, we also propose a grouping method to partition a multicast group of heterogeneous users into a minimal number of sub-groups to minimize the channel bandwidth consumption while meeting the quality variation constraint and achieving fairness among all sub-groups. Experimental results show that the proposed method can effectively mitigate the error propagation due to packet loss as well as achieve fairness not only among all sub-groups and also clients in a multicast group.",2007,0,
391,392,Error Control for IPTV over xDSL Networks,"We discuss the necessity of error control for supporting IPTV over imperfect access networks. In particular, we consider typical DSL environments, and examine the physical-layer impairments and error-mitigation techniques. For these networks, we evaluate the performance of two different application-layer Forward Error Correction (FEC) methods. An overview of hybrid error-control methods and recent developments in standardization is also presented.",2008,0,
392,393,A new approach of halftoning based on error diffusion with rough set filtering,"The rough set filtering makes use of the concepts of indiscernibility relations and approximation spaces to define an equivalence class of neighboring pixels in a processing mask, then utilize the statistical mean of the equivalence classes to replace the gray levels of the central pixel in a processing mask. The error diffusion makes use of the correction factor composed of the weighted errors for these pixels prior to addition of the pixel to be processed to diffuse error over the neighboring pixels in a continuous tone image. Both of a system and an algorithm of implementation of halftoning on error diffusion with rough sets are introduced in the paper",2000,0,
393,394,Outage performance of mrt with unequal-power co-channel interference and channel estimation error,In this letter we investigate the outage performance of maximal ratio transmission (MRT) with unequal-power co-channel interference (CCI) and channel estimation error. The exact expression for the outage probability is presented. Our results are applicable to the MRT systems with arbitrary numbers of transmit and receive antennas.,2007,0,
394,395,Optimal placement of sensor in gearbox fault diagnosis based on VPSO,"The optimization layout of the acceleration sensor and application of particle swarm optimization (PSO) algorithm to solve the fitness problems of such optimization are discussed in this paper. Based on the gearbox finite element modeling and the result of modal analysis, use the particle swarm optimization with adaptive velocity (VPSO) algorithm, and take the two kinds of fitness function as evaluation goal, has realized the optimization and positioning of gearbox sensor layout, analyzed optimization result.",2010,0,
395,396,Study of SINS/GPS/DVL integrated navigation system's fault tolerance,This paper put forward several fault tolerant arithmetic combining engineering applications on the AUV (autonomous underwater vehicle). The arithmetic is based on traditional centralized Kalman Filter and can improve SINS/GPS/DVL integrated navigation system's precision and capability of fault tolerant. The simulation and the vehicle tests validate the arithmetic.,2005,0,
396,397,New method for current and voltage measuring offset correction in an induction motor sensorless drive,This paper presents a new algorithm for electromagnetic torque and flux estimation in a sensorless drive when uncompensated dc offset of current and/or voltage sensors are present. The novel feature of the offset error correction algorithm is an attempt not to eliminate the consequence of problem but to identify its source. The algorithm uses the first harmonic of estimated torque and dc value of estimated stator flux to identify the source and value of the current and/or voltage offset error. Identified values can be used for offset cancelation which improves estimation process.,2010,0,
397,398,The Digital Circuit Fault Diagnosis Interface Design and Realization Based on VXI,"This paper discusses in detail the development process of general interface adapter in the digital circuit fault diagnosis system based on VXI. After introducing VXI bus, the paper gives overall description of fault diagnosis system, presents a method of solving the problem about load matching and interface matching, and realizes the function of identification to read and write on the memory of interface circuit and to control chip selection. The method of identity installation in interface circuit to be selected is to give an ID number and add a memory to interface circuit to ensure the accuracy and effectiveness. The paper also describes the method of self diagnosis in the interface circuit that is the key to whole fault diagnosis system.",2008,0,
398,399,The DVB television signal transmission simulation using the forward error correction codes,The contribution deals with the simulation of the digital video signal transmission through the baseband transmission channel model. The simulation model that covers selected phenomena of DVB (digital video broadcasting) system signal processing is presented. The digital video signal is represented with the digital data of one noncompressed video frame that is channel encoded and protected against errors with the forward error correction (FEC) codes. The transmission channel model has influence on transmitted digital data and its distortion and the pertubative signals affect on the data decoding. The developed interactive simulation software (Matlab application) features are outlined too and the conclusion presents efficiency of the used FEC codes.,2003,0,
399,400,Residual Generators for Fault Diagnosis Using Computation Sequences With Mixed Causality Applied to Automotive Systems,"An essential step in the design of a model-based diagnosis system is to find a set of residual generators fulfilling stated fault detection and isolation requirements. To be able to find a good set, it is desirable that the method used for residual generation gives as many candidate residual generators as possible, given a model. This paper presents a novel residual generation method that enables simultaneous use of integral and derivative causality, i.e., mixed causality, and also handles equation sets corresponding to algebraic and differential loops in a systematic manner. The method relies on a formal framework for computing unknown variables according to a computation sequence. In this framework, mixed causality is utilized, and the analytical properties of the equations in the model, as well as the available tools for algebraic equation solving, are taken into account. The proposed method is applied to two models of automotive systems, a Scania diesel engine, and a hydraulic braking system. Significantly more residual generators are found with the proposed method in comparison with methods using solely integral or derivative causality.",2010,0,
400,401,Online drift correction in wireless sensor networks using spatio-temporal modeling,"Wireless sensor networks are deployed for the purpose of sensing and monitoring an area of interest. Sensors in the sensor network can suffer from both random and systematic bias problems. Even when the sensors are properly calibrated at the time of their deployment, they develop drift in their readings leading to erroneous inferences being made by the network. The drift in this context is defined as a slow, unidirectional, long-term change in the sensor measurements. In this paper we present a novel algorithm for detecting and correcting sensors drifts by utilising the spatio-temporal correlation between neigbouring sensors. Based on the assumption that neighbouring sensors have correlated measurements and that the instantiation of drift in a sensor is uncorrelated with other sensors, each sensor runs a support vector regression algorithm on its neigbourspsila corrected readings to obtain a predicted value for its measurements. It then uses this predicted data to self-assess its measurement and detect and correct its drift using a Kalman filter. The algorithm is run recursively and is totally decentralized. We demonstrate using real data obtained from the Intel Berkeley Laboratory that our algorithm successfully suppresses drifts developed in sensors and thereby prolongs the effective lifetime of the network.",2008,0,
401,402,Applying fault-tolerant solutions of circulant graphs to meshes and hypercubes,"Many important architectures such as rings, meshes and hypercubes can be modeled as circulant graphs. As a result, circulant graphs have received a lot of attention, and a new method was developed for designing fault-tolerant solutions for them. We review this method in this paper, and examine its applications to the design of fault-tolerant solutions for meshes and hypercubes. Our results indicate that these solutions are efficient.",2005,0,
402,403,"Vietnamese spelling detection and correction using Bi-gram, Minimum Edit Distance, SoundEx algorithms with some additional heuristics","The spelling checking problem is considered to contain two main phases: the detecting phase and the correcting phase. In this paper, we present a new approach for Vietnamese spelling checking based on Vietnamese characteristics for each phase. Our research approach includes the use of a syllable Bi-gram in combination with parts of speech (POS) to find out suspected syllables. In the correcting phase, we based on minimum edit distance, SoundEx algorithms and some heuristics to build a weight function for assessing suggestion candidates. The training corpus and the test set were collected from e-newspapers.",2008,0,
403,404,A fault-tolerant control architecture for induction motor drives in automotive applications,"This paper describes a fault-tolerant control system for a high-performance induction motor drive that propels an electrical vehicle (EV) or hybrid electric vehicle (HEV). In the proposed control scheme, the developed system takes into account the controller transition smoothness in the event of sensor failure. Moreover, due to the EV or HEV requirements for sensorless operations, a practical sensorless control scheme is developed and used within the proposed fault-tolerant control system. This requires the presence of an adaptive flux observer. The speed estimator is based on the approximation of the magnetic characteristic slope of the induction motor to the mutual inductance value. Simulation results, in terms of speed and torque responses, show the effectiveness of the proposed approach.",2004,0,
404,405,Fault classification and fault distance location of double circuit transmission lines for phase to phase faults using only one terminal data,"An accurate fault classification algorithm for double end fed parallel transmission lines based on application of artificial neural networks is presented in this paper. The proposed method uses the voltage and current available at only the local end of line. This method is virtually independent of the effects of remote end infeed and is insensitive to the variation of fault inception angle and fault location. The Simulation results show that phase-to-phase faults can be correctly detected, classified and located within one cycle after the inception of fault. Large number of faults simulations using MATLAB<sup>A</sup>7.01 have demonstrated the accuracy and effectiveness of the proposed algorithm. The proposed scheme allows the protection engineers to increase the reach setting i.e. greater portion of line length can be protected as compared to conventional techniques. The technique neither requires a communication link to retrieve the remote end data nor zero sequence current compensation for healthy phases.",2009,0,
405,406,Denoising fluorescence endoscopy - A motion compensated temporal recursive video filter with an optimal minimum mean square error parameterization,"Fluorescence endoscopy is an emerging technique for the detection of bladder cancer. A marker substance is brought into the patient's bladder which accumulates at cancer tissue. If a suitable narrow band light source is used for illumination, a red fluorescence of the marker substance is observable. Because of the low fluorescence photon count and because of the narrow band light source, only a small amount of light is detected by the camera's CCD sensor. This, in turn, leads to strong noise in the recorded video sequence. To overcome this problem, we apply a temporal recursive filter to the video sequence. The derivation of a filter function is presented, which leads to an optimal filter in the minimum mean square error sense. The algorithm is implemented as plug-in for the real-time capable clinical demonstrator platform RealTimeFrame and it is capable to process color videos with a resolution of 768times576 pixels at 50 frames per second.",2009,0,
406,407,Compact multilayer coupled stripline LTCC filter with defected ground structure,"A novel multilayer coupled stripline resonator structure is introduced to realize miniature broadband band-pass filter using low temperature co-fired ceramic (LTCC) process with defected ground structure (DGS). Wide bandwidth and good selectivity are obtained by exploiting four resonators and the filter exhibits a high rejection in stopband by adopting the tapered DGS. Moreover, an inductance feed back between the output and input is introduced to produce transmission zeros. Filter with size of <sub>0</sub> /12  <sub>0</sub> /12  h (<sub>0</sub> is the wavelength at the midband frequency; h is the substrate height) is designed, fabricated and measured. The measured responses agree well with simulation results.",2009,0,
407,408,Dynamic Fault Handling Mechanisms for Service-Oriented Applications,"Dynamic fault handling is a new approach for dealing with fault management in service-oriented applications. Fault handlers, termination handlers and compensation handlers are installed at execution time instead of being statically defined. In this paper we present this programming style and our implementation of dynamic fault handling in JOLIE, providing finally a nontrivial example of its usage.",2008,0,
408,409,Towards Software Quality Economics for Defect-Detection Techniques,"There are various ways to evaluate defect-detection techniques. However, for a comprehensive evaluation the only possibility is to reduce all influencing factors to costs. There are already some models and metrics for the cost of quality that can be used in that context. The existing metrics for the effectiveness and efficiency of defect-detection techniques and experiences with them are combined with cost metrics to allow a more fine-grained estimation of costs and a comprehensive evaluation of defect-detection techniques. The current model is most suitable for directly comparing concrete applications of different techniques",2005,0,
409,410,Fault tolerance based on the publish-subscribe paradigm for the BonjourGrid middleware,"How to federate the machines of all Boinc, Condor and XtremWeb projects? If you believe in volunteer computing and want to share more than one project then BonjourGrid may help. In previous works, we proposed a novel approach, called BonjourGrid, to orchestrate multiple instances of Institutional Desktop Grid middleware. It is our way to remove the risk of bottleneck and failure, and to guarantee the continuity of services in a distributed manner. Indeed, BonjourGrid can create a specific environment for each user based on a given computing system of his choice such as XtremWeb, Condor or Boinc. This work investigates, first, the procedure to deploy Boinc and Condor on top of BonjourGrid and, second, proposes a fault tolerant approach based on passive replication and virtualization to tolerate the crash of coordinators. The novelty resides here in an integrated environment based on Bonjour (publication-subscription mecanism) for both the coordination protocol and for the fault tolerance issues. In particular, it is not so frequent to our knowledge to describe and to implement a fault tolerant protocol according to the pub-sub paradigm. Experiments, conducted on the Grid'5000 testbed, illustrate a comparative study between Boinc (respectively Condor) on top of BonjourGrid and a centralized system using Boinc (respectively Condor) and second prove the robustness of the fault tolerant mechanism.",2010,0,
410,411,An approach to detecting domain errors using formal specification-based testing,"Domain testing, a technique for testing software or portions of software dominated by numerical processing, is intended to detect domain errors that usually arise from incorrect implementations of desired domains. This paper describes our recent work aiming to provide support for revealing domain errors using formal specifications. In our approach, formal specifications serve as a means for domain modeling. We describe a strong domain testing strategy that guide testers to select a set of test points so that the potential domain errors can be effectively detected, and apply our approach in two case studies for test cases generation.",2004,0,
411,412,A Robust Error Detection Mechanism for H.264/AVC Coded Video Sequences Based on Support Vector Machines,"Current trends in wireless communications provide fast and location-independent access to multimedia services. Due to its high compression efficiency, H.264/AVC is expected to become the dominant underlying technology in the delivery of future wireless video applications. The error resilient mechanisms adopted by this standard alleviate the problem of spatio-temporal propagation of visual artifacts caused by transmission errors by dropping and concealing all macroblocks (MBs) contained within corrupted segments, including uncorrupted MBs. Concealing these uncorrupted MBs generally causes a reduction in quality of the reconstructed video sequence.",2008,0,
412,413,Track Down HW Function Faults Using Real SW Invariants,System level functional verification by running real software stack on FPGA prototype is essential for achieving a high quality design. But it is hard to find the exact source of hardware function faults while running large closed source system software fails. This paper proposes the idea of tracking down faults through real system software control flow invariants with current trace output hardware support. It captures qualified control flow invariant trace in reference execution and test trace; and tracks down faults through comparing offline invariant trace and test trace. The approach can deal with both deterministic and nondeterministic execution. We implemented the proof of concept in full system simulator Bochs. Our experimentation with the real closed source MS Windows XP suggests that the approach is effective in tracking down hardware function faults.,2009,0,
413,414,Design of a fault-tolerant satellite cluster link establishment protocol,"The design of a protocol for satellite cluster link establishment and management that accounts for link corruption, node failures, and node re-establishment is presented in this paper. This protocol will need to manage the traffic flow between nodes in the satellite cluster, adjust routing tables due to node motion, allow for sub-networks in the cluster, and similar activities. This protocol development is in its initial stages. Preliminary results with eight nodes demonstrate its operations and potential problems that may arise when significant numbers of channel errors are present",2005,0,
414,415,A novel approach to calculate the severity and priority of bugs in software projects,"Discovering and fixing software bugs is a difficult maintenance task, and a considerable amount of effort is devoted by software developers on this issue. In the world of software one cannot get rid of the bugs, fixes, patches etc. each of them have a severity and priority associated to it. There is not yet any formal relation between these components as both of these either depends on the developer and tester or on customer and project manger to be decided on. On one hand, the priority of a component depends on the cost and the efforts associated with it. While on the other, the severity depends on the efforts required to accomplish a particular task. This research paper proposes a formula that can draw a relationship among severity and priority.",2010,0,
415,416,Reducing the soft-error rate of a high-performance microprocessor,"Single-bit upsets from transient faults have emerged as a key challenge in microprocessor design. Soft errors will be an increasing burden for microprocessor designers as the number of on-chip transistors continues to grow exponentially. Unlike traditional approaches, which focus on detecting and recovering from faults, this article introduces techniques to reduce the probability that a fault will cause a declared error. The first approach reduces the time instructions sit in vulnerable storage structures. The second avoids declaring errors on benign faults. Applying these techniques to a microprocessor instruction queue significantly reduces its error rate with only minor performance degradation",2004,0,
416,417,The Orion GN&C data-driven flight software architecture for automated sequencing and fault recovery,"The Orion Crew Exploration Vehicle (CEV) is being designed to include capabilities that allow significantly more automation than either the Space Shuttle or the International Space Station (ISS). In particular, the vehicle flight software has requirements to accommodate increasingly automated missions throughout all phases of flight. This paper presents the Guidance, Navigation & Control (GN&C) flight software architecture designed to provide evolvable automation capability that sequences through software modes and configurations. This software architecture is required to maintain flexibility to address the maturation of operational concepts over time, permit ground and crew operators to gain trust in the system, and provide capabilities for human override of the automation in `off-nominal' situations. To allow for mission flexibility, reconfigurability and reduce the recertification expense over the life of the program, a data-driven approach is used to load the mission event plan as well as the flight software artifacts associated with the GN&C subsystem. The flight software schema for automated mission sequencing is presented with a concept of operations for interactions with ground and crew members. This data is managed through a prototype database of GN&C level sequencing data, which tracks mission specific parameters to aid in the scheduling of GN&C activities. A prototype architecture for fault detection, isolation and recovery interactions with the automation software is presented as part of the upcoming design maturation to respond with appropriate GN&C and vehicle-level actions in `off-nominal' scenarios.",2010,0,
417,418,An experimental study of security vulnerabilities caused by errors,"The paper presents an experimental study which shows that, for the Intel x86 architecture, single-bit control flow errors in the authentication sections of targeted applications can result in significant security vulnerabilities. The experiment targets two well-known Internet server applications: FTP and SSH (secure shell), injecting single-bit control flow errors into user authentication sections of the applications. The injected sections constitute approximately 2-8% of the text segment of the target applications. The results show that out of all activated errors: (a) 1-2% comprised system security (create a permanent window of vulnerability); (b) 43-62% resulted in crash failures (about 8.5% of these errors create a transient window of vulnerability); and (c) 7-12% resulted in fail silence violations. A key reason for the measured security vulnerabilities is that, in the x86 architecture, conditional branch instructions are a minimum of one Hamming distance apart. The design and evaluation of a new encoding scheme that reduces or eliminates this problem is presented.",2001,0,
418,419,Defect Tolerance Based on Coding and Series Replication in Transistor-Logic Demultiplexer Circuits,"We present a family of defect tolerant transistor-logic demultiplexer circuits that can defend against both stuck-ON (short defect) and stuck-OFF (open defect) transistors. Short defects are handled by having two or more transistors in series in the circuit, controlled by the same signal. Open defects are handled by having two or more parallel branches in the circuit, controlled by the same signals, or more efficiently, by using a transistor-replication method based on coding theory. These circuits are evaluated, in comparison with an unprotected demultiplexer circuit, by: 1) modeling each circuit's ability to tolerate defects and 2) calculating the cost of the defect tolerance as each circuit's redundancy factor R, which is the relative number of transistors required by the circuit. The defect-tolerance model takes the form of a function giving the failure probability of the entire demultiplexer circuit as a function of the defect probabilities of its component transistors, for both defect types. With the advent of defect tolerance as a new design goal for the circuit designer, this new form of performance analysis has become necessary.",2007,0,
419,420,Adaptive Correction of Errors from Segmented Digital Ink Texts in Chinese Based on Context,"Digital ink texts in Chinese can neither be converted into users' desired layouts nor be recognized until their characters, lines, and paragraphs are correctly extracted. There are many errors in automatically segmented digital ink texts in Chinese because they are free forms and mixed with other languages, as well as their Chinese characters have small gaps and complex structures. Paragraphs, lines, and characters (recognizable language symbols) in digital ink may be wrongly extracted. An adaptive approach based on context is proposed to correct wrongly extracted these objects. Each extracted object is first adaptively visualized by color and shape labels according to relations between it and its neighbors. Users use simple gestures naturally and easily to merge and split wrongly extracted objects. Contexts are constructed from users' gestures and objects invoked by them, where users' intensions are identified. We have conducted experiments using real-life segmented digital ink texts in Chinese and compared the proposed approach with others. Experimental results demonstrate that the proposed approach is feasible, flexible, effective, and robust.",2010,0,
420,421,Memory Yield Improvement through Multiple Test Sequences and Application-Aware Fault Models,"In this paper, we propose a way to improve the yield of memory products by selecting the appropriate test strategy for memory Built- in Self-Test (BIST). We argue that by testing the memory through a sequence of test algorithms which differ in their fault coverage, it is possible to bin the memory into multiple yield bins and increase the yield and product revenue. Further, the test strategy must take into consideration the usage model of the memory. Thus, a number of video and audio buffers are used in sequential access mode, but are overtested using conventional memory test algorithms which model a large number of defects which do not impact the operation of the buffers. We propose a binning strategy where memory test algorithms are applied in different order of strictness such that bins have a specific defect / fault grade. Depending on the applications some of these bins need not be discarded but sold at a lower price as the functionality would never catch the fault due to its usage of memory. We introduce the notion of a test map for the on-chip memories in a SoC and provide results of yield simulation on two specific test strategies called ""Most Strict First"" and ""Least Strict First"". Our simulations indicate that significant improvements in yield are possible through the adoption of the proposed technique. We show that the BIST controller area and run-time overheads also reduce when information about the usage model of the memory, such as sequential access, is exploited.",2008,0,
421,422,Correcting asr outputs: Specific solutions to specific errors in French,"Automatic speech recognition (ASR) systems are used in a large number of applications, in spite of the inevitable recognition errors. In this study we propose a pragmatic approach to automatically repair ASR outputs by taking into account linguistic and acoustic information, using formal rules or stochastic methods. The proposed strategy consists in developing a specific correction solution for each specific kind of errors. In this paper, we apply this strategy on two case studies specific to French language. We show that it is possible, on automatic transcriptions of French broadcast news, to decrease the error rate of a specific error by 11.4% in one of two the case studies, and 86.4% in the other one. These results are encouraging and show the interest of developing more specific solutions to cover a wider set of errors in a future work.",2008,0,
422,423,The Design of Fault Diagnosis Expert System about Temperature Adjustment System Based on CLIPS,"The fault diagnosis of a certain launching unit's temperature controller is researched with the object-oriented programming method based on expert system theory, the fault-diagnosis expert-system software is designed and developed with VC++ 2008 and CLIPS. The structure of the system is firstly analyzed; The representation of knowledge, the design of database and the production rule are discussed then; lastly the diagnosis flow is studied up and a demonstration of the fault diagnosis is given.",2009,0,
423,424,A highly selective super-wide bandpass filter by cascading HMSIW with asymmetric defected ground structure,"The half mode substrate integrated waveguide (HMSIW) possesses the highpass characteristic of SIW but the size is nearly half reduced. A recently proposed asymmetric defected ground structure (ADGS), composed of two square headed slots connected with a rectangular slot transversely under a microstrip line, exhibits quasi-elliptic-function band-reject characteristics around 3 GHz with high selectivity. Based on the circuit model, the structure of the ADGS is modified to perform well at about 16 GHz. By combining the HMSIW and the modified ADGS, a super-wide bandpass filter operating at about 8-16 GHz with high selectivity at both upper and lower band is proposed. Both simulated and measured results have been presented to demonstrate the validity of the proposed wideband filter.",2010,0,
424,425,A real-time fault tolerant intra-body network,"This paper designs an intra-body network (IBN) of nodes, consisting of small sensors and processing elements (SPEs) placed at different locations within the body and a personal digital assistant placed externally but in close proximity to the body. The sensors measure specific physiological attributes such as electrophysiological and biochemical changes in the myocardium (action potentials of cells), glucose level, blood viscosity etc. and forward them to the processing element. Communication protocols for configuration and data access protocols are proposed. The privacy of the IBN data, fault tolerance and real-time data acquisition are addressed.",2002,0,
425,426,Evaluating speech recognition in the context of a spoken dialogue system: critical error rate,"Evaluating a speech recognition system is a key issue towards understanding its deficiencies and focusing potential improvements on useful aspects. When a system is designed for a given application, it is particularly relevant to have an evaluation procedure that reflects the role of the system in this application. Evaluating continuous speech recognition through word error rate is not completely appropriate when the speech recognizer is used as spoken dialogue system input. Some errors are particularly harmful, when they concern content words for example, while some others do not have any impact on the following comprehension step. The attempt is not to evaluate natural language understanding but to propose a more appropriate evaluation of speech recognition, by making use of semantic information to define the notion of critical errors.",2001,0,
426,427,Detecting VLIW Hard Errors Cost-Effectively through a Software-Based Approach,"Research indicates that as technology scales, hard errors such as wear-out errors are increasingly becoming a critical challenge for microprocessor design. While hard errors in memory structures can be efficiently detected by error correction code, detecting hard errors for functional units cost-effectively is a challenging problem. In this paper, we propose to exploit the idle cycles of the under-utilized VLIW functional units to run test instructions for detecting wear-out errors without increasing the hardware cost or significantly impacting performance. We also explore the design space of this software-based approach to balance the error detection latency and the performance for VLIW architectures. Our experimental results indicate that such a software-based approach can effectively detect hard errors with minimum impact on performance for VLIW processors, which is particularly useful for reliable embedded applications with cost constraints.",2007,0,
427,428,Evaluating the effectiveness of a software fault-tolerance technique on RISC- and CISC-based architectures,"This paper deals with a method able to provide a microprocessor-based system with safety capabilities by modifying the source code of the executed application, only. The method exploits a set of transformations which can automatically be applied, thus greatly reducing the cost of designing a safe system, and increasing the confidence in its correctness. Fault Injection experiments have been performed on a sample application using two different systems based on CISC and RISC processors. Results demonstrate that the method effectiveness is rather independent of the adopted platform",2000,0,
428,429,Experimental validation of fault injection analyses by the FLIPPER tool,The paper discusses the experimental validation of fault injection analyses accomplished with the FLIPPER tool. Validation has been accomplished through accelerated proton testing of a benchmark design provided by the European Space Agency.,2009,0,
429,430,A primary exploration of three-dimensional echocardiographic intra-cardiac virtual reality visualization of atrial septal defect: in vitro validation,"To evaluate the diagnostic value of three-dimensional echocardiography (3-DE) in congenital heart disease such as atrial septal defect (ASD) by virtual reality (VR), ten ASDs with different size and shape were created in ten fresh explained porcine hearts. HP SONOS 5500 imaging system was employed for 3-DE reconstructed and visualized by virtual reality computing techniques. The results showed that all ASDs were successfully reconstructed. The site, geometry were well appraised in its true form. The area, maximum and minimum diameter of ASD were measured on 3D reconstruction and compared with independently measured anatomic date. Good correlation was obtained (r>0.95, P<0.01). In conclusion, VR open an exciting opportunity in the field of diagnosis of 3-DE in congenital heart disease",2005,0,
430,431,A Fault-Tolerant Active Pixel Sensor for Mitigating Hot Pixel Defects,"Hot pixel defects are unavoidable in many solid-state image sensors. Affected pixels accumulate dark signal over the course of an exposure, grossly diminishing dynamic range and often rendering measurements unusable. Experiments suggest the mechanisms causing hot pixels are highly localized and the defect will be confined to a single pixel. A redundant, fault-tolerant active pixel sensor architecture that has previously been applied to other defect types is investigated for the suppression of hot pixels. A recovery scheme using minimal computational power is also described.",2007,0,
431,432,Testing for interconnect crosstalk defects using on-chip embedded processor cores,"Crosstalk effects degrade the integrity of signals traveling on long interconnects and must be addressed during production testing. External testing for crosstalk is expensive due to the need for high-speed testers. Built-in self-test, while eliminating the need for a high-speed tester, may lead to excessive test overhead as well as overly aggressive testing. To address this problem, we propose a new software-based self-test methodology for system-on-chip (SoC) devices based on embedded processors. It enables an on-chip embedded processor core to test for crosstalk in system-level interconnects by executing a self-test program in the normal operational mode of the SoC. We have demonstrated the feasibility of this method by applying it to test the interconnects of a processor-memory system. The defect coverage was evaluated using a system-level crosstalk defect simulation method.",2001,0,
432,433,Adaptive Fuzzy Prediction of Low-Cost Inertial-Based Positioning Errors,"Kalman filter (KF) is the most commonly used estimation technique for integrating signals from short-term high performance systems, like inertial navigation systems (INSs), with reference systems exhibiting long-term stability, like the global positioning system (GPS). However, KF only works well under appropriately predefined linear dynamic error models and input data that fit this model. The latter condition is rather difficult to be fulfilled by a low-cost inertial measurement unit (IMU) utilizing microelectromechanical system (MEMS) sensors due to the significance of their long- and short-term errors that are mixed with the motion dynamics. As a result, if the reference GPS signals are absent or the Kalman filter is working for a long time in prediction mode, the corresponding state estimate will quickly drift with time causing a dramatic degradation in the overall accuracy of the integrated system. An auxiliary fuzzy-based model for predicting the KF positioning error states during GPS signal outages is presented in this paper. The initial parameters of this model is developed through an offline fuzzy orthogonal-least-squares (OLS) training while the adaptive neuro-fuzzy inference system (ANFIS) is implemented for online adaptation of these initial parameters. Performance of the proposed model has been experimentally verified using low-cost inertial data collected in a land vehicle navigation test and by simulating a number of GPS signal outages. The test results indicate that the proposed fuzzy-based model can efficiently provide corrections to the standalone IMU predicted navigation states particularly position.",2007,0,
433,434,Fault tolerant PVFS2 based on data replication,"Aggregating the capacity and bandwidth of the commodity disks in the nodes of a cluster provides cost effective and high performance storage systems. Nevertheless, this strategy could be a feasible approach only if the mean time to failure of disks and nodes is faced. The number of failures increases with the nodes and it is especially important in parallel file systems, like PVFS, because having a file striped over server disks increases the probability of failures. This work proposes a strategy to include data replication in the second version of PVFS in order to provide fault tolerance. We also analyze the performance of the implementation of this approach.",2010,0,
434,435,Development of a motion correction system for respiratory-gated PET study,"A respiratory motion during whole-body imaging has been recognized as a source of image quality degradation and reduces the quantitative accuracy of positron emission tomography (PET) study. The aim of this study is to evaluate respiratory gating system and to develop a respiratory motion correction system using trigger generating device built in-house and gated-PET data acquisition mode. We utilized a commercially available laser optical sensor to detect respiratory motion during PET scanning. Each respiratory cycle is divided into 4 bins defined from average peak interval and irregular peak within the breathing motion. The acquired data within the time bins correspond to different positions within the breathing cycle and stored for the post motion correction. Motion data of diaphragm and chest wall was calculated by CT image acquisition during the normal inspiration and expiration position. In the images of a phantom, the blurring artifact due to breathing motion was reduced by our correction method. This technique improves the quantitative specific activity of the tracer which is distorted because of the respiratory motion.",2004,0,
435,436,Fault Diagnosis Method for Mobile Robots Using Multi-CMAC Neural Networks,"Multi-CMAC (cerebellar model articulation controller) neural networks based fault detection and diagnosis (FDD) method for mobile robots are proposed. Three failure types (system fault, sensor fault, and combined fault) are handled. Mobile robot system consists of several functional modules belonging to different module groups, which execute different tasks. According to the consistency among sensors information between the neighbor modules in the same module group, the method of fault diagnosis is studied. Then, multiple CMAC neural networks are used to implement the diagnosis. One CMAC neural network is set to one module group. In the neural network, the sensor information is used as the inputs and the fault signals are used as the outputs. As an example, the method is implemented on a drive system of a wheeled mobile robot. The simulation results show the effectiveness of the proposed technique.",2007,0,
436,437,Fault Evaluator: A tool for experimental investigation of effectiveness in software testing,"The specifications for many software systems, including safety-critical control systems, are often described using complex logical expressions. It is important to find effective methods to test implementations of such expressions. Analyzing the effectiveness of the testing of logical expressions manually is a tedious and error prone endeavor, thus requiring special software tools for this purpose. This paper presents Fault Evaluator, which is a new tool for experimental investigation of testing logical expressions in software. The goal of this tool is to evaluate logical expressions with various test sets that have been created according to a specific testing method and to estimate the effectiveness of the testing method for detecting specific faulty variations of the original expressions. The main functions of the tool are the generation of complete sets of faults in logical expressions for several specific types of faults; gaining expected (Oracle) values of logical expressions; testing faulty expressions and detecting whether a test set reveals a specific fault; and evaluating the effectiveness of a testing approach.",2010,0,
437,438,Calculation of correction factors to compensate for the reference electric field nonuniformity,"The inaccuracy of the reference electric field nonuniformity assessment is identified, which unnecessarily increases the measurement uncertainty of standardized systems for electric field-meters calibration, making them inadequate for calibration of modern, precision field-meters. By means of numerical field calculation, the correction factors are computed which allow compensation for the reference field nonuniformity. The uncertainty of that calculation is also estimated",2001,0,
438,439,Correction for continuous motion in small animal PET,"In small animal PET imaging experiments, animals are generally required to be anaesthetized to avoid motion artifacts. However, anaesthesia can alter biochemical pathways within the brain, thus affecting the physiological parameters under investigation. The ability to image conscious animals would overcome this problem and open up the possibility of entirely new investigational paradigms.",2008,0,
439,440,Short-circuit fault mitigation methods for interior PM synchronous machine drives using six-leg inverters,"This paper characterizes six-leg inverters to mitigate short-circuit faults for interior permanent magnet (IPM) synchronous machines. Key differences between bus structures in six-leg inverters are identified. For six-leg inverters employing two isolated DC links, it is shown that up to 75% of rated output power could be produced following a single-switch short-circuit fault. A magnet flux ing control method is proposed as a response to stator winding type short-circuit faults. This control method results in a zero-torque fault response by the motor. The important influence of the zero sequence in both the motor and inverter structure is identified and developed for this class of fault. Simulation and experimental results are presented verifying the proposed magnet flux ing control method.",2004,0,
440,441,Automatic red-eye detection and correction,"""Red-eye"" is a phenomenon that causes the eyes of flash photography subjects to appear unnaturally reddish in color. Though commercial solutions exist for red-eye correction, all of them require some measure of user intervention. A method is presented to automatically detect and correct redeye in digital images. First, faces are detected with a cascade of multi-scale classifiers. The red-eye pixels are then located with several refining masks computed over the facial region. The masks are created by thresholding per-pixel metrics, designed to detect red-eye artifacts. Once the redeye pixels have been found, the redness is attenuated with a tapered color desaturation. A detector implemented with this system corrected 95% of the red-eye artifacts in 200 tested images.",2002,0,
441,442,Implementing Probabilistic Risk Assessment with Fault Trees to support space exploration missions,This paper seeks to illustrate the implementation of a Probabilistic Risk Assessment (PRA) methodology as a foundation for space mission support risk assessment and management process. Identifying the risks to delivering expected spacecraft data services to a mission is only the first part of the risk assessment. Arriving at a quantified probability (Likelihood) of the manifestation of these risks is the desired outcome of the process.,2010,0,
442,443,Multi-View Video Coding Using Color Correction,The color variations between multi-view video sequences may degrade the inter-view prediction and result in low coding efficiency. In this paper we propose an efficient multi-view video coding scheme using dominant basic color mapping based color correction. The experimental coding results show that color correction has the potential to make multi-view video coding more efficient.,2008,0,
443,444,"We're Finding Most of the Bugs, but What are We Missing?","We compare two types of model that have been used to predict software fault-proneness in the next release of a software system. Classification models make a binary prediction that a software entity such as a file or module is likely to be either faulty or not faulty in the next release. Ranking models order the entities according to their predicted number of faults. They are generally used to establish a priority for more intensive testing of the entities that occur early in the ranking. We investigate ways of assessing both classification models and ranking models, and the extent to which metrics appropriate for one type of model are also appropriate for the other. Previous work has shown that ranking models are capable of identifying relatively small sets of files that contain 75-95% of the faults detected in the next release of large legacy systems. In our studies of the rankings produced by these models, the faults not contained in the predicted most fault prone files are nearly always distributed across many of the remaining files; i.e., a single file that is in the lower portion of the ranking virtually never contains a large number of faults.",2010,0,
444,445,Fault-Tolerant Algorithm for Distributed Primary Detection in Cognitive Radio Networks,"This paper attempts to identify the reliability of detection of licensed primary transmission based on cooperative sensing in cognitive radio networks. With a parallel fusion network model, the correlation issue of the received signals between the nodes in the worst case is derived. Leveraging the property of false sensing data due to malfunctioning or malicious software, the optimizing strategy, namely fault-tolerant algorithm for distributed detection (FTDD) is proposed, and quantitative analysis of false alarm reliability and detection probability under the scheme is presented. In particular, the tradeoff between licensed transmissions and user cooperation among nodes is discussed. Simulation experiments are also used to evaluate the fusion performance under practical settings. The model and analytic results provide useful tools for reliability analysis for other wireless decentralization-based applications (e.g., those involving robust spectrum sensing).",2009,0,
445,446,China's Research Status Quo and Development Trend of Power Grid Fault Diagnosis,"Fault diagnosis is the basic condition for smart grid to achieve the self-healing function, and it is also one of the important research topics of the intelligent dispatching decision support system. On the basis of analyzing its concept and aiming at the current status of China's studies, this paper reviewed and summarized several intelligent fault diagnosis methods, including expert system, artificial neural networks, rough set theory, data mining techniques, multi-agent technology and the entropy theory, then pointed out their application characteristics and existing problems, and finally the prospects of further development in this field were presented.",2010,0,
446,447,"Transparent, Incremental Checkpointing at Kernel Level: a Foundation for Fault Tolerance for Parallel Computers","We describe the software architecture, technical features, and performance of TICK (Transparent Incremental Checkpointer at Kernel level), a system-level checkpointer implemented as a kernel thread, specifi- cally designed to provide fault tolerance in Linux clusters. This implementation, based on the 2.6.11 Linux kernel, provides the essential functionality for transparent, highly responsive, and efficient fault tolerance based on full or incremental checkpointing at system level. TICK is completely user-transparent and does not require any changes to user code or system libraries; it is highly responsive: an interrupt, such as a timer interrupt, can trigger a checkpoint in as little as 2.5s; and it supports incremental and full checkpoints with minimal overhead-less than 6% with full checkpointing to disk performed as frequently as once per minute.",2005,0,
447,448,Algorithmic Cholesky factorization fault recovery,"Modeling and analysis of large scale scientific systems often use linear least squares regression, frequently employing Cholesky factorization to solve the resulting set of linear equations. With large matrices, this often will be performed in high performance clusters containing many processors. Assuming a constant failure rate per processor, the probability of a failure occurring during the execution increases linearly with additional processors. Fault tolerant methods attempt to reduce the expected execution time by allowing recovery from failure. This paper presents an analysis and implementation of a fault tolerant Cholesky factorization algorithm that does not require checkpointing for recovery from fail-stop failures. Rather, this algorithm uses redundant data added in an additional set of processors. This differs from previous works with algorithmic methods as it addresses fail-stop failures rather than fail-continue cases. The implementation and experimentation using ScaLAPACK demonstrates that this method has decreasing overhead in relation to overall runtime as the matrix size increases, and thus shows promise to reduce the expected runtime for Cholesky factorizations on very large matrices.",2010,0,
448,449,An automated methodology to diagnose geometric defect in the EEPROM cell,"The objective of this paper is to present an automated geometric defect diagnosis methodology for EEPROM cell (AGDE). This method focuses on speeding up the diagnosis process of geometric defects. It is based on a mathematical model generated with a ""design of simulation"" (DOS) technique. The DOS technique takes as input, simulations results of a floating gate transistor with different given geometries and produces, as output, a polynomial equation of the threshold voltage in function of the cell's geometric parameters. The diagnosis process is realized by comparing the measured threshold voltages of an EEPROM cell with the dynamically computed ones. From this comparison, the potentially defective geometric parameters are automatically extracted.",2002,0,
449,450,Multiple transient faults in logic: an issue for next generation ICs?,"In this paper, we first evaluate whether or not a multiple transient fault (multiple TF) generated by the hit of a single cosmic ray neutron can give rise to a bidirectional error at the circuit output (that is an error in which all erroneous bits are 1s rather than 0s, or vice versa, within the same word, but not both). By means of electrical level simulations, we show that this can be the case. Then, we present a software tool that we have developed in order to evaluate the likelihood of occurrence of such bidirectional errors for very deep submicron (VDSM) ICs. The application of this tool to benchmark circuits has proven that such a probability can not be neglected for several benchmark circuits. Finally, we evaluate the behavior of conventional self-checking circuits (generally designed accounting only for single TFs) with respect to such events. We show that the modifications generally introduced to their functional blocks in order to avoid output bidirectional errors due to single TFs (as required when an AUED code is implemented) can significantly reduce (up to the 40%) also the probability to have bidirectional errors because of multiple TFs.",2005,0,
450,451,Comparing fail-silence provided by process duplication versus internal error detection for DHCP server,"This paper uses fault injection to compare the ability of two fault-tolerant software architectures to protect an application from faults. These two architectures are Voltan, which uses process duplication, and Chameleon ARMORs, which use self-checking. The target application is a Dynamic Host Configuration Protocol (DHCP) server, a widely used application for managing IP addresses. NFTAPE, a software-based fault injection environment, is used to inject three classes of faults, namely random memory bit-flip, control-flow and high-level target specific faults, into each software architecture and into baseline Solaris and Linux versions",2001,0,
451,452,Quantification of PET and CT Data Misalignment Errors in Cardiac PET/CT:Clinical and Phantom Studies,"PET/CT units with high temporal resolution (particularly with 64-slice CT capability) are increasingly used as in clinical diagnosis and prognosis of cardiovascular disease. Since the CT sub-system in the combined PET/CT unit is used to perform attenuation correction of acquired PET data, misalignments between patient positioning for both scans can cause artifacts in the myocardial PET images potentially resulting in false positive artifacts. The aim of this study is to evaluate the misalignment effect (induced by spurious or physiological patient motion in-between the two modalities) on regional and global uptake values in the myocardial region. In this study, we used both phantom (RSD thorax phantom) and clinical studies (two FDG and one NH<sub>3</sub> rest/stress). Manual shifts between the CT and PET images ranging from 0 to 20 mm in six different directions were applied. Thereafter, attenuation correction was applied to the emission data using the manually shifted CT images in order to model patient motion between PET and CT. The reconstructed PET images using shifted CT images for attenuation correction were compared with the PET images corrected with the hypothetically misalignment free original CT image. The criteria and figures of merit used included VOI and linear regression analysis. The analysis was performed using 500 VOIs located within the myocardial wall in each PET dataset. The VOIs were uniformly distributed across all myocardial wall regions to assess the overall influence of PET and CT misalignment. The absolute percentage relative difference increased in all simulated movements with increasing misalignments for both phantom and clinical studies (up to 30% in some regions for the 20 mm shift). In conclusion, increasing the misalignment between PET and CT studies resulted in increased changes in the tracer uptake value within the myocardium both on a regional and global basis with respect to the reference as revealed by the various figures of meri- t used. The variation was more significant for right and down movements versus left and up directions.",2009,0,
452,453,Analysis of the Timing System Error of the Constellation Automatic Navigation,"System integrated clock (SIC) plays an important role in implementing the high-accuracy constellation automatic time synchronization and information exchange. In the establishment of SIC, error and noise are unavoidably introduced. In the paper, various error sources in the process are analyzed at first, and then an error-reduction method under the model of two-way plus common view time comparison is put forward and analyzed. Theory research and simulation experiment show that the constellation time synchronization error is below 10 seconds.",2007,0,
453,454,Formal guides for experimentally verifying complex software-implemented fault tolerance mechanisms,"Describes a framework allowing the experimental verification of complex software-implemented fault-tolerance algorithms and mechanisms (FTAMs). This framework takes into account two of the most important aspects which are increasingly required in newly-developed fault-tolerant systems: the considerations of COTS (commercial off-the-shelf) based architectures and the compliance with severe safety certification procedures. The strategy proposed shows how a rigorous FTAM specification, based on a multiple-viewpoint architectural description, may help to mechanically monitor the verification of its implementation under real conditions. The proposed strategy has been instantiated using two mechanized techniques: model checking and fault injection. The preliminary conclusions of the application of this automated approach to a small part of a commercial fault-tolerant system help us clarify its usage and its suitability for validating complex dependable systems",2001,0,
454,455,An improved fault locating system of distribution network based on fuzzy identification,"Fault locating system, which is designed for the fast power recovery, is very important in the economical operating of the distribution network. But, for the uncertainty of the fault information, the incorrect conclusion may be obtained by the traditional fault location calculation, so the most fault locating system can not be employed in the distribution network. In this paper, an improved fault locating system is proposed, which is composed of the fault signal acquisition unit and the fault location analysis center. Fuzzy identification is employed in the fault location analysis center to deal with the uncertainty of fault information. The failure and mistake rates of indicator action are used as the fuzzy parameters to calculate the fuzzy difference of the fault sequence and the standard fault set. The fault indicator is the primary device of fault information acquisition. The radio frequency and GPRS technology construct the communication channel of fault signal acquisition unit, which cuts down the construction cost and also ensures the obtaining accuracy of the fault information. The fault location system is working on the distribution network and operating well. With the accurate fault location, power supply recovers fast. The loss of power failure is reduced effectively.",2010,0,
455,456,"Pattern recognition-a technique for induction machines rotor fault detection ""eccentricity and broken bar fault""","A pattern recognition technique based on Bayes minimum error classifier is developed to detect broken rotor bar faults and static eccentricity in induction motors at the steady state. The proposed algorithm uses stator currents as input without any other sensors. First, rotor speed is estimated from stator currents, then appropriate features are extracted. The produced feature vector is normalized and fed to the trained Bayes minimum error classifier to determine if motor is healthy or has incipient faults (broken bar fault, static eccentricity or both). Only number of poles and rotor slots are needed as pre-knowledge information. Theoretical approach together with experimental results derived from a 3 hp AC induction motor show the strength of this method. In order to cover many different motor load conditions data are derived from 10% to 130% of the rated load for both a healthy induction motor and an induction motor with a rotor having 4 broken bars and/or static eccentricity.",2001,0,
456,457,Sinogram-based motion correction of PET images using optical motion tracking system and list-mode data acquisition,"A head motion during brain imaging has been recognized as a source of image degradation and introduces distortion in positron emission tomography (PET) image. There are several techniques to correct the motion artifact, but these techniques cannot correct the motion during scanning. The aim of this study is to develop a sinogram-based motion correction (SBMC) method to correct directly the head motion during PET scanning using a motion tracking system and list-mode data acquisition. This method is a rebinning procedure by which the lines of response (LOR) are geometrically transformed according to the current values of the six-dimensional motion data. Michelogram was recomposed using rebinned LOR and motion corrected sinogram was generated. In the motion corrected image, the blurring artifact due to motion was reduced by SBMC method.",2002,0,
457,458,Multi-Layer Immune Model for Fault Diagnosis,"Inspired by the multi-layer defense mechanism and incorporates the feedback mechanism in the nature immune system, the paper proposes a multi-layer immune model for fault diagnosis. In the multi-layer model, inherent immune layer direct recognition of known fault that could not cause influence to other nodes; propagation immune layer adopt the structure of the B- lymphocyte network to construct the fault propagation network for the fault localization; Adaptive immune layer learn the unknown fault pattern. Simulation results show that the multilayer immune diagnosis system has the properties of recognition, learning and memory.",2008,0,
458,459,Bit error rate of a digital radio eavesdropper on computer CRT monitors,"An eavesdropper on computer CRT (cathode ray tube) monitors can be used to intercept video information. Its anti-noise performance is analyzed in this paper. Baseband transmission models of digital signals are established according to the operating principle of the eavesdropper. The relationship between the eavesdropper's bit error rate and some parameters, such as intercept distance, superposition times and noise power, is discussed under the circumstances of ISI and ISI-freedom. Good agreement is obtained between experimental results and theoretical analysis.",2004,0,
459,460,High Continuous Availability Digital Information System Based on Stratus Fault-Tolerant Server,"With the construction of harmonious society, health improvement and the rapid development of information technology, People put forward higher requirements for the hospital. Hospital information system as an online services system requires continuous operation. Server system is the key to support hospital operations. System paralyzed accident caused by Server system failure is also not uncommon. Aiming at the problem of insufficient reliability of the traditional Cluster cluster server system, The article made a in-depth technical analysis on the performance of the Stratus fault-tolerant server. Combing with the characteristics of hospital information system, it proposed the digital hospital information system structure based on Stratus fault-tolerant server and explored and analyzed the economic and technical advantages of the program. The application effect demonstrates that the program is of the economic good and can realize continuous availability.",2010,0,
460,461,Estimation of Defects Based on Defect Decay Model: ED^{3}M,"An accurate prediction of the number of defects in a software product during system testing contributes not only to the management of the system testing process but also to the estimation of the product's required maintenance. Here, a new approach called ED<sup>3</sup>M is presented that computes an estimate of the total number of defects in an ongoing testing process. ED<sup>3</sup>M is based on estimation theory. Unlike many existing approaches the technique presented here does not depend on historical data from previous projects or any assumptions about the requirements and/or testers' productivity. It is a completely automated approach that relies only on the data collected during an ongoing testing process. This is a key advantage of the ED<sup>3</sup>M approach, as it makes it widely applicable in different testing environments. Here, the ED<sup>3</sup>M approach has been evaluated using five data sets from large industrial projects and two data sets from the literature. In addition, a performance analysis has been conducted using simulated data sets to explore its behavior using different models for the input data. The results are very promising; they indicate the ED<sup>3</sup>M approach provides accurate estimates with as fast or better convergence time in comparison to well-known alternative techniques, while only using defect data as the input.",2008,0,
461,462,A fault tolerant approach to microprocessor design,"We propose a fault-tolerant approach to reliable microprocessor design. Our approach, based on the use of an online checker component in the processor pipeline, provides significant resistance to core processor design errors and operational faults such as supply voltage noise and energetic particle strikes. We show through cycle-accurate simulation and timing analysis of a physical checker design that our approach preserves system performance while keeping area overheads and power demands low. Furthermore, analyses suggest that the checker is a fairly simple state machine that can be formally verified, scaled in performance, and reused. Further simulation analyses show virtually no performance impacts when our simple checker design is coupled with a high-performance microprocessor model. Timing analyses indicate that a fully synthesized unpipelined 4-wide checker component in 0.25 m technology is capable of checking Alpha instructions at 288 MHz. Physical analyses also confirm that costs are quite modest; our prototype checker requires less than 6% the area and 1.5% the power of an Alpha 21264 processor in the same technology. Additional improvements to the checker component are described which allow for improved detection of design, fabrication and operational faults.",2001,0,
462,463,Tolerating faults while maximizing reward,"The imprecise computation (IC) model is a general scheduling framework that is capable of expressing the precision vs. timeliness tradeoff involved in many current real-time applications. In that model, each task comprises mandatory and optional parts. While allowing greater scheduling flexibility, the mandatory parts in the IC model still have hard deadlines, and hence they must be completed before the task's deadline, even in the presence of faults. In this paper, we address fault-tolerant (FT) scheduling issues for IC tasks. First, we propose two recovery schemes, namely immediate recovery and delayed recovery. These schemes can be readily applied to provide fault tolerance to the mandatory parts by scheduling the optional parts appropriately for recovery operations. After deriving the necessary and sufficient conditions for both schemes, we consider the FT-optimality problem, i.e. generating a schedule which is FT and whose reward is maximum among all possible FT schedules. For immediate recovery, we present and prove the correctness of an efficient FT-optimal scheduling algorithm. For delayed recovery, we show that the FT-optimality problem is NP-hard, and thus is intractable",2000,0,
463,464,Incorporating fault tolerance in analog-to-digital converters (ADCs),The reliability of ADCs used in highly critical systems can be increased by applying a two-step procedure starting with sensitivity analysis followed by redesign. The sensitivity analysis is used to identify the most sensitive blocks which could then be redesigned for better reliability by incorporating fault tolerance. This paper illustrates the steps involved in incorporating fault tolerance in an ADC. Two redesign techniques to improve the reliability of a circuit are presented. Novel selective node resizing algorithms for increased tolerance against -particle induced transients are discussed.,2002,0,
464,465,Multiwave interaction analysis of a coaxial Bragg structure with a localized defect introduced in sinusoidal corrugations,"A multiwave interaction formulation is presented to investigate the effects of a localized defect on the reflective spectrum of a coaxial Bragg structure with sinusoidal corrugations. Good agreement has been achieved between the theoretical results obtained by the present formulation and those simulated by the software HFSS, which confirms the validity and the significance of the multiwave interaction formulation. It is found that, the localized defect creates defected eigenmodes within each reflective band gap of the initial standard Bragg structure, which the parameter can be controlled by the location of the localized defect.",2009,0,
465,466,Coupled field-circuit-mechanical model of an electromagnetic actuator operating in error actuated control system,"An algorithm of coupled field-circuit simulation of the dynamics of an electromagnetic linear actuator operating in error actuated control system is presented. The software consists of three main parts: (a) numerical model of the actuator dynamics which includes equations of a transient electromagnetic field in a non-linear conducting and moving medium, (b) discrete model of electric circuit and (c) optimization solver. Numerical implementation is based on the finite elements. The influence of the PID controller settings on the actuator operation is shown. In order to find optimal parameters of the system the genetic algorithm is applied. The simultaneous optimization of both: actuator structure and regulator settings has been carried out.",2008,0,
466,467,A comprehensive evaluation of capture-recapture models for estimating software defect content,"An important requirement to control the inspection of software artifacts is to be able to decide, based on more objective information, whether the inspection can stop or whether it should continue to achieve a suitable level of artifact quality. A prediction of the number of remaining defects in an inspected artifact can be used for decision making. Several studies in software engineering have considered capture-recapture models to make a prediction. However, few studies compare the actual number of remaining defects to the one predicted by a capture-recapture model on real software engineering artifacts. The authors focus on traditional inspections and estimate, based on actual inspections data, the degree of accuracy of relevant state-of-the-art capture-recapture models for which statistical estimators exist. In order to assess their robustness, we look at the impact of the number of inspectors and the number of actual defects on the estimators' accuracy based on actual inspection data. Our results show that models are strongly affected by the number of inspectors, and therefore one must consider this factor before using capture-recapture models. When the number of inspectors is too small, no model is sufficiently accurate and underestimation may be substantial. In addition, some models perform better than others in a large number of conditions and plausible reasons are discussed. Based on our analyses, we recommend using a model taking into account that defects have different probabilities of being detected and the corresponding Jackknife Estimator. Furthermore, we calibrate the prediction models based on their relative error, as previously computed on other inspections. We identified theoretical limitations to this approach which were then confirmed by the data",2000,0,
467,468,Implementing a reflective fault-tolerant CORBA system,"The use of reflection is becoming popular today for the implementation of non-functional mechanisms such as fault tolerance. The main benefits of reflection are separation of concerns between the application and the mechanisms and transparency from the application programmer point of view. Unfortunately, metaobject protocols (MOPs) available today are not satisfactory with respect to necessary features needed for implementing fault tolerance mechanisms. Previously, we proposed a specialised MOP based on Corba, well adapted for such mechanisms (M.-O. Killijian and J.C. Fabre, 1998). We deliberately focus on the implementation of this metaobject protocol using compile-time reflection and its use for implementing distributed fault tolerance. We present the design and the implementation of a fault-tolerant Corba system using this metaobject together with some preliminary experimental results. From the lessons learnt from this work, we briefly address the benefits of reflection in other layers of a system for dependability issues",2000,0,
468,469,The Optimized Combination of Fault Location Technology Based on Traveling Wave Principle,"The accuracy and the reliability of modern D-type double-ended and A-type single-ended traveling wave fault location principles used for transmission lines is comprehensively evaluated. Based on the evaluation, this paper presents the idea of optimized combination of location based on these two traveling wave principles, and successfully applies the idea in actual fault analysis of transient traveling waves. Compared with the traveling wave location schemes based on D-type or A-type principle alone, this scheme has the greatest advantages of utilizing the A-type traveling wave principle to verify and correct the location results obtained with the D-type traveling wave principle, so that both the location reliability and accuracy are enhanced. Practical applications showed that the optimized combination of traveling wave location schemes is feasible, and the location precision is improved significantly.",2009,0,
469,470,Autonomous cooperation technique to achieve fault tolerance in service oriented community system,"The advancement of mobile telecommunication and wireless technologies is required to provide local but familiar services in daily life, which has not been satisfied through the global services on the Internet. In retail business under evolving market, users request access to unknown but appropriate services based on their preference and situation, and retailers need to be aware of the current requirements of the majority of consumers in specific local trade areas. Because of the transience of the requirements of the users in their trade-areas, the services require being temporary and having the time limit. Therefore the areas of the services need to become narrower with the time. The concept of the service oriented community has been proposed to satisfy both the users and the retailers requirements. It consists of members in the specified area based on services, and they cooperate with each other in order to get mutual benefits. For realization of the service oriented community, the systems require flexibility for the effective provision of the services and fault tolerance for the stable service. In the service oriented community system, the Time Distance has been introduced as the efficient measure of the distance between the users and the retailers. The Time Distance Oriented Service System architecture has been proposed to satisfy these requirements for flexible and stable services, where the nodes are autonomously distribute services and reduce the service area based on the time distance. Here autonomous cooperation technique for achieving fault tolerance is proposed in order to satisfy the requirement of high service availability.",2002,0,
470,471,A fault-tolerant approach to network security,"Summary form only given. The increasing use of the Internet, especially for internal and business-to-business applications has resulted in the need for increased security for all networked systems to avoid unauthorized access and use. A failure of network security can effectively close the business, its availability is vital to operations. Vital functions such as firewalls and VPNs must remain in operation without loss of time for fallover, without loss of data and must be able to be placed even at remote locations where support personnel may not be readily available. Network firewalls are the first, and often are the only, line of defense against an attack. However, the firewall can be a double-edged sword. In operation, the firewall protects the network from everything from Denial of Service attacks to the entry of known viruses and unauthorized intrusion. If the firewall falls, there are generally only two options: Leave the network open to all or shut down access by anyone. The default condition is to close everything off, but this can be as disastrous as leaving the network open. Due to the importance of the firewall, most leading firewall software provides some method of establishing a form of fail-over redundancy for high availability. Yet in most cases this means some form of clustering using a secondary system as a backup with specialty software to detect and respond to a failure of the primary firewall. Such a clustered approach introduces additional complexity when establishing and configuring the firewall and additional complexity when upgrading. It also adds dramatically to the cost, not only in the hardware for the firewall, but in additional software copies and in the expertise for clustering support software required to establish and maintain the cluster. The approach we will discuss examines the creation of network security based on a hardware approach to fault tolerance. This approach will dramatically reduce the system complexity, simultaneously eliminating the need for special clustering software and special expertise for configuring the system for the kind of continuous availability that is the objective of the network security application. In addition, because the hardware approach is something that is designed in from the inception of the system, there are additional advantages. The fault tolerance is not an afterthought, but rather the purpose of the hardware, meaning that the system can be made to function very smoothly with very little administration. Failure of a part of the system is seamlessly recovered by the redundant elements, without loss of data in memory or loss of state for the system. In sum, this paper discusses the ability to create network security that reaches the standard of being continuously available, what is often referred to as the ""Holy Grail of reliability,"" 99.999% uptime",2001,0,
471,472,Research of error correction of LEO satellite orbit prediction for vehicle-borne tracking and position device,"Vehicle-borne tracking and position device is used to track LEO satellite. Because of the absence of the target which might be caused by cloud or zenith blind zone, the forecasting data will be used to acquire the target. While orbit prediction has serious errors, the target is always missed. Meanwhile, in its application to the vehicle-borne tracking and position device, due to the base of instability in the tracking process, it will result in significant difference between predicting data and tracking data, so the target will be not tracked rapidly. We applied tracking data to predict satellite orbits by improving Laplace method, and then corrected the error between Predicted data and actual measured data by interpolation method of Lagrange which improves the accuracy of prediction values. The testing data shows the accuracy of predicted data ranging from 3' to 10' for both azimuth and elevation when extrapolated satellite orbit to 7 seconds time.",2010,0,
472,473,Exploiting Mobile Agents for Structured Distributed Software-Implemented Fault Injection,"Embedded distributed real-time systems are traditionally used in safety-critical application areas such as avionics, healthcare, and the automotive sector. Assuring dependability under faulty conditions by means of fault tolerance mechanisms is a major concern in safety-critical systems. From a validation perspective, Software-Implemented Fault Injection (SWIFI) is an approved means for testing fault tolerance mechanisms. In recent work, we have introduced the concept of using mobile agents for distributed SWIFI in time-driven real-time systems. This paper presents a prototypical implementation of the agent platform for the OSEKtime real-time operating system and the FlexRay communication system. It is further shown, how to implement fault injection experiments by means of mobile agents in a structured manner following a classification of faults in terms of domain, persistence, and perception. Based on experiments conducted on ARM-based platforms, selected results are described in detail to demonstrate the potential of mobile agent based fault injection.",2006,0,
473,474,Analytical approach to internal fault simulation in power transformers based on fault-related incremental currents,"A new method for simulating faulted transformers is presented in this paper. Unlike other methods proposed in the literature, this method uses the data obtained from any sound transformer simulation to obtain the damaged condition by simply adding a set of calculated currents. These currents are obtained from the definition of the fault. The model is fully based on determining the incremental values exhibited by the currents in phases and lines from the prefault to the postfault condition. As a consequence, data obtained from simulation of the sound transformer may be readily used to define the damaged condition. The model is described for light and severe faults, introducing this latter feature as a further add-on feature to the low-level faults simulation. The technique avoids the use of complex routines and procedures devoted to specially simulate the internal fault. Of prompt application to relay testing, the proposed analytical model also gives an insight into the fault nature by means of the investigation of symmetrical components. In contrast with its low complexity, the method has shown to present large accuracy for simulating the fault performance.",2006,0,
474,475,Automated Bug Neighborhood Analysis for Identifying Incomplete Bug Fixes,"Although many static-analysis techniques have been developed for automatically detecting bugs, such as null dereferences, fewer automated approaches have been presented for analyzing whether and how such bugs are fixed. Attempted bug fixes may be incomplete in that a related manifestation of the bug remains unfixed. In this paper, we characterize the completeness of attempted bug fixes that involve the flow of invalid values from one program point to another, such as null dereferences, in Java programs. Our characterization is based on the definition of a bug neighborhood, which is a scope of flows of invalid values. We present an automated analysis that, given two versions P and P' of a program, identifies the bugs in P that have been fixed in P', and classifies each fix as complete or incomplete. We implemented our technique for null-dereference bugs and conducted empirical studies using open-source projects. Our results indicate that, for the projects we studied, many bug fixes are not complete, and thus, may cause failures in subsequent executions of the program.",2010,0,
475,476,Applying FIRMAMENT to test the SCTP communication protocol under network faults,"How to apply a fault injector to evaluate the dependability of a network protocol implementation is the main focus of this paper. In the last years, we have been developing FIRMAMENT, a tool to inject faults directly into messages that pass through the kernel protocol stack. SCTP is a promising new protocol over IP that, due its enhanced reliability, is competing with TCP where dependability has to be guaranteed. Using FIRMAMENT we evaluate the error coverage and the performance degradation of SCTP under faults. Performing a complete fault injection campaign over a third party software give us a deep insight about the additional test strategies that are needed to reach significant dependability measures.",2009,0,
476,477,Performance Evaluation of Probe-Send Fault-tolerant Network-on-chip Router,"With increasing reliability concerns for current and next generation VLSI technologies, fault-tolerance is fast becoming an integral part of system-on-chip and multi-core architectures. Another trend for such architectures is network-on-chip (NoC) becoming a standard for on-chip global communication. In an earlier work, a generic fault-tolerant routing algorithm in the context of NoCs has been presented. The proposed routing algorithm works in two phases, namely path exploration (PE) and normal communication. This paper presents fundamental insights into various novel PE approaches, their feasibility and performance trade-offs for k-ary 2-cube NoCs. The dependence of the normal communication phase on the probability of finding paths and their quality in the first phase emphasizes the PE's significance. One major contribution of this work is the investigation of application of constrained randomness to PE for optimizing the quality of paths. Another contribution is the proposed use of merging of traffic to reduce the reconfiguration time by a large amount (73.8% on an average).",2007,0,
477,478,Impact of Error Characteristics of an Indoor 802.11g WLAN on TCP Retransmissions,"In this paper we present the results of extensive measurements made over an experimental wired-to-wireless testbed, which consisted of a TCP protocol combined with a real-world indoor IEEE 802.11g WLAN. We investigated the effects of signal attenuation due to client distance from the AP on the 802.11 frame error rates (FER) and consequently on the segment loss rates and retransmission behavior of TCP at the sender in the fixed network. Specifically, we experimented with different modulation schemes belonging to the OFDM 802.11g PHY in order to gauge differences in performance between them, comparing real-world FERs calculated from actual frame captures against SNR, for both the forward and reverse WLAN channel directions. We also present real-world distributions of frame retransmissions made over the WLAN by the AP, with useful findings. Our results confirm that the reverse channel of a WLAN possesses a higher FER than the forward channel, and poses a greater threat to TCP's retransmission mechanism.",2008,0,
478,479,Statistical feature representations for automatic wood defects recognition research and applications,"In this paper, we introduce the non-negative matrix factorization (NMF) to decompose the wood images and structure the feature spaces. Local binary pattern (LBP) is used to extract the original spatial local structure features, such as curly edges, etc. and they have better luminance adaptability. Simultaneously, dual-tree complex wavelet transform (DTCWT) is used to extract the energy based statistical features from different directions and frequencies and they can maintain better time-frequency localized characteristics and finite data redundancy. We integrate the features together to choose the proper features to describe the discrepancies between sound woods and defects and then propose an automatic detection system for wood defects recognition. After many cross experiments, we received a better identification rate of more than 90% with good research values and potential applications.",2009,0,
479,480,Error concealment for stereoscopic images using mode selection,"In this paper, a novel error concealment (EC) method for compressed stereoscopic image pairs is presented, which contains a new binocular EC mode and an improved monocular EC mode. The proposed algorithm selects appropriate EC mode to conceal the error block (EB) in the stereoscopic image according to the local characteristic of the EB. The experimental results demonstrate that the proposed scheme has good subjective and objective EC performance in stereoscopic images as compared to monocular mode.",2010,0,
480,481,Fault tolerance of CNC software based on artificial neural network,"This paper proposes an efficient method for realizing the fault tolerance of CNC software with the introduction of artificial neural network (ANN) to the design filed of CNC software. In addition, function aspects (velocity, acceleration, chord error, real time, prediction accuracy) from the experiment on Non-Uniform Rational B-Spline (NURBS) interpolator based on ANN were evaluated in detail. Our experimental results showed that the NURBS interpolation based on ANN not only meet the requirements of function aspects, but also can realize fault tolerance technology, which may provide a new strategy in the improvement of the reliability of CNC software.",2010,0,
481,482,PMSG Wind Turbine Performance Analysis During Short Circuit Faults,"Due to the increasing price of fossil fuels and the security concerns of the nuclear energy, electricity generation using wind turbines has recently attracted significant attention after a period of neglect. Among different types of wind turbine generators, PM synchronous generators (PMSG) offer better performance due to higher efficiency and less maintenance since they do not have rotor current and could be used without a gear box. In addition, the utilization of a double conversion results in higher flexibility compared with other wind turbine systems. This paper develops the model of a PMSG wind turbine and then simulates short circuits to evaluate the performance of the system during short circuit fault. Since PMSG wind turbine systems use a double conversion converter, this paper develops two methods for controlling the converter, a new protection method for capacitor over voltage is also evaluated in this paper.",2007,0,
482,483,A novel fault diagnosis algorithm for K-connected distributed clusters,"In this paper, we propose an on-line two phase (TPD) fault diagnosis algorithm for distributed clusters that follows an arbitrary network topology with connectivity k. Intermediate nodes communicate heartbeat messages between different source destination pairs. The algorithm addresses a realistic fault model considering crash and value faults in the cluster nodes. The algorithm is shown to produce a time complexity of O(l) and message complexity of O(n. e) respectively. The algorithm has been simulated using discrete event simulation techniques and the results show that the algorithm is feasible for large distributed clusters.",2010,0,
483,484,The effect of the time window width of correlation method on single-ended modern traveling wave based fault location principles,"In the technique of single-ended modern traveling wave based fault location principles for transmission lines, traveling wave correlation method is a classical algorithm applied to detect the fault reflected surge. But in actual application, the lack of an effective way to choose the time window results in the limitation of correlation method - the time window width affects the correlation coefficient value which is an important indicator to show the similarity of the waveforms. This paper presents a new conception called optimal time window width of correlation method, and analyzes different factors probably affecting the width by means of EMTP-ATP and Matlab applied to simulations. Further more, the basic idea of new correlation method based on multi-time-window is proposed, which could be applied to improve the reliability of fault location technique.",2008,0,
484,485,Hierarchical application aware error detection and recovery,"Proposed is a four-tired approach to develop and integrate detection and recovery support at different levels of the system hierarchy. The proposed mechanisms exploit support provided by (i) embedded hardware, (ii) operating system, (iii) compiler, and (iv) application.",2004,0,
485,486,A Statistical Approach for Estimating the Correlation between Lightning and Faults in Power Distribution Systems,"The paper deals with the subject of the source-identification of transient voltage disturbances in distribution system buses. In particular, a statistical procedure is proposed for the evaluation of the probability that a lightning flash detected by a lightning location system (LLS) could cause a fault and, therefore, relay interventions, generally associated with voltage dips. The proposed procedure is based on the coordinated use of the information provided by the LLS and the availability of an advanced simulation tool for the accurate simulation of lightning-induced voltages on complex power systems, namely the LIOV-EMTP code. The uncertainty levels of the stroke location and of the peak current estimations provided by the LLS are discussed and their influence on the lightning-fault correlation is analyzed",2006,0,
486,487,Fault-tolerant five-phase permanent magnet motor drives,"In this paper, a control strategy that provides fault tolerance to five-phase permanent magnet motors is introduced. In this scheme, the five-phase permanent magnet (PM) motor continues operating safely under loss of up to two phases without any additional hardware connections. This feature is very important in traction and propulsion applications where high reliability is of major importance. The five-phase PM motors with sinusoidal and quasi-rectangular back-EMFs have been considered. To obtain the new set of phase currents to be applied to the motor during fault in stator phases or inverter legs, the torque producing MMF by the stator is kept constant under healthy and faulty conditions for both cases. Simulation and experimental results are provided to verify that the five-phase motor continues operating continuously and steadily under faulty conditions.",2004,0,
487,488,"Performance of Multicode DS/CDMA With Noncoherent <formula formulatype=""inline""> <img src=""/images/tex/964.gif"" alt=""M""> </formula>-ary Orthogonal Modulation in the Presence of Timing Errors","This paper derives an accurate approximation to the bit error rate (BER) of multicode direct-sequence code-division multiple access (DS/CDMA) with noncoherent <i>M</i>-ary modulation in wideband fading channels when timing errors are made at the receiver employing equal-gain combining (EGC). This reflects the practical scenario where the path delays are estimated imperfectly, leading to synchronization errors between the correlation receivers and the received signals. The analysis can be applied to any type of a fading distribution for both independent and correlated diversity branches. It is shown that the derived analytical expressions are in close agreement with the Monte Carlo system simulations, particularly in the case of small timing errors.",2008,0,
488,489,Fault tolerance as an aspect using JReplica,"Reliability and availability are very important trends in the development process of distributed systems. In order to improve these features, object replication mechanisms have been introduced. Programming replication policies for a given application is not an easy task, and this is the reason why transparency for the programmer has been one of the most important properties offered by all replication models. However, this transparency for the programmer is not always desirable. In this paper we present a replication model, JReplica, based on Aspect Oriented Programming (AOP). JReplica allows the separated specification of the replication code from the functional behaviour of objects, providing not only a high degree of transparency, as done by previous models, but also the possibility for programmers to introduce new behaviour to specify different fault tolerance requirements. Moreover, the replication aspect has been introduced at design time, and in this way, UML has been extended in order to consider replication issues separately when designing fault tolerance systems",2001,0,
489,490,A novel transmission line test and fault location methodology using pseudorandom binary sequences,"A novel pulse echo test methodology, using pseudorandom binary sequence (PRBS) excitation, is presented in this paper as an alternative to time domain reflectometry (TDR) for transmission line fault location and identification. The essential feature of this scheme is the cross correlation (CCR) of the fault response echo with the PRBS test input stimulus input which results in a unique signature for identification of the fault type, if any, or load termination present as well as its distance from the point of test stimulus injection. This fault identification method can used in a number of key industrial applications incorporating printed circuit boards, overhead transmission lines and underground cables in inaccessible locations which rely on a pathway for power transfer or signal propagation. As an improved method PRBS fault identification can be performed over several cycles at low amplitude levels online to reject normal signal traffic and extraneous noise pickup for the purpose of multiple fault coverage, resolution and identification. In this paper a high frequency co-axial transmission line model is presented for transmission line behavioural simulation with PRBS stimulus injection under known load terminations to mimic fault conditions encountered in practice for proof of concept. Simulation results, for known resistive fault terminations, with measured CCR response demonstrate the effectiveness of the PRBS test method in fault type identification and location. Key experimental test results are also presented for a co-axial cable, under laboratory controlled conditions, which substantiates the accuracy of PRBS diagnostic CCR method of fault recognition and location using a range of resistive fault terminations. The accuracy of the method is further validated through theoretical calculation via known co-axial cable parameters, fault resistance terminations and link distances in transmission line experimental testing.",2008,0,
490,491,Lightweight Fault-Tolerance Mechanism for Distributed Mobile Agent-Based Monitoring,"Thanks to asynchronous and dynamic natures of mobile agents, a certain number of mobile agent-based monitoring mechanisms have actively been developed to monitor large scale and dynamic distributed networked systems adaptively and efficiently. Among them, some mechanisms attempt to adapt to dynamic changes in various aspects such as network traffic patterns, resource addition and deletion, network topology and so on. However, failures of some domain managers are very critical to providing correct, real-time and efficient monitoring functionality in a large-scale mobile agent-based distributed monitoring system. In this paper, we present a novel fault- tolerance mechanism to have the following advantageous features appropriate for large-scale and dynamic hierarchical mobile agent-based monitoring organizations. It supports fast failure detection functionality with low failure-free overhead by each domain manager transmitting heart-beat messages to its immediate higher-level manager. Also, it minimizes the number of non-faulty monitoring managers affected by failures of domain managers. Moreover, it allows consistent failure detection actions to be performed continuously in case of agent creation, migration and termination, and is able to execute consistent takeover actions even in concurrent failures of domain managers.",2009,0,
491,492,A Superstring Galaxy Associative Memory Model with Expecting Fault-Tolerant Fields,"The synthesis problems of associative memory models are not better solved until now. Learning from the idea of superstring theory, a design method of superstring galaxy associative memory model with expecting fault-tolerant field is proposed by making the sphere mapping and giving the algorithm of galaxy covering. The method better solves a difficult synthesis problem of associative memory models. The superstring galaxy associative memory model designed by the method can have expecting fault-tolerant fields of the samples.",2009,0,
492,493,A Model-based Simulation Approach to Error Analysis of IT Services,"Utility computing environments provide on-demand IT services to customers. Such environments are dynamic in nature and continuously adapt to changes in requirements and system state. Errors are an important category of environment state changes as such environments consist of a large number of components, and hence, are subject to errors. In this paper, we design and implement a model-based simulation framework that leverages information about existing service components and their interactions, and provides concrete service behavior in presence of a variety of errors. To evaluate the framework, experiments are conducted on a virtualized blade-server based environment. Results show that the framework is effective and practical in analyzing error impacts on IT services.",2007,0,
493,494,Power quality improvement using a new structure of fault current limiter,"In this paper, power quality improvement by using a new structure of non superconducting fault current limiter (NSFCL) is discussed. This structure prevents voltage sags on Point of Common Coupling (PCC) just after fault occurrence, because of its fast operation. On the other hand, previously used structures produce harmonics on load voltage and have ac losses in normal operation. New structure has solved this problem by using dc voltage source. The proposed structure is simulated using PSCAD/EMTDC software and simulation results are presented to validate the effectiveness of this structure.",2010,0,
494,495,3 Faults Tolerant Orthogonal RAID for Large Storage,"Recently, the demand of low cost large scale storage increases. We developed VLSD (Virtual Large Scale Disks) toolkit for constructing virtual disk based distributed storages, which aggregate free spaces of individual disks. However, in order to construct large-scale storage, more than or equal to 3 fault tolerant RAID is important. In this paper, we propose MeshRAID that is 3 fault tolerant orthogonal RAID. And, we implement MeshRAID using VLSD. It is easy to implement MeshRAID using various classes in VLSD. From the viewpoint of its features, MeshRAID is addressed between RAID55 and NaryRAID.",2010,0,
495,496,Responsive Fault-Tolerant Computing in the Era of Terascale Integration State of Art Report,"Scaling in hardware integration process results in IC-process geometry reductions, lower operating voltages and increased clock speeds. This paper first surveys the reliability obstacles these developments give rise to and then points out that computing systems can no longer be safely assumed to fail only by crashing. Yet this assumption is at the core of primary-backup replication which the literature presents as the appropriate, and hence the most widely used, strategy for time-critical fault-tolerant applications. The paper then observes that building computing nodes with announced crash failure mode is a promising way forward to deal with the emerging reliability challenges. Work carried out to assure such a failure mode has also been briefly surveyed.",2008,0,
496,497,Formal Analysis of a Distributed Fault Tolerant Clock Synchronization Algorithm for Automotive Communication Systems,"A synchronized time base is indispensable for a time- triggered system since all activities in such a system are triggered by the passage of time. Distributed fault-tolerant clock synchronization algorithms are normally used to achieve the synchronized time base. As a state-of-the-art representative of the time-triggered systems for automotive applications, FlexRay uses a fault-tolerant mid-point algorithm to achieve the synchronized time base. Correctness of the algorithm plays a crucial role as most of the protocol services rely on the fact that there exists a synchronized time base in the system. Due to the distinguished characteristics of the algorithm, we propose a case-analysis based technique for the formal analysis of the algorithm. We show that the case analysis technique can greatly facilitate our formal analysis of the algorithm. Mechanical support with Isabelle/HOL, a theorem prover, is also discussed.",2008,0,
497,498,Experimental analysis of the errors induced into Linux by three fault injection techniques,"The main goal of the experimental stud), reported in this paper is to investigate to what extent distinct fault injection techniques lead to similar consequences (errors and failures). The target system we are using to carry out our investigation is the Linux kernel as it provides a representative operating system. It is featuring full controllability and observability thanks to its open source status. Three types of software-implemented fault injection techniques are considered, namely: i) provision of invalid values to the parameters of the kernel calls, ii) corruption of the parameters of the kernel calls, and iii) corruption of the input parameters of the internal functions of the kernel. The workload being used for the experiments is tailored to activate selectively each functional component. The observations encompass typical kernel failure modes (e.g., exceptions and kernel hangs) as well as a detailed analysis of the reported error codes.",2002,0,
498,499,A New Neural-Network-Based Fault Diagnosis Approach for Analog Circuits by Using Kurtosis and Entropy as a Preprocessor,"This paper presents a new fault diagnosis method for analog circuits. The proposed method extracts the original signals from the output terminals of the circuits under test (CUTs) by a data acquisition board and finds the kurtoses and entropies of the signals, which are used to measure the high-order statistics of the signals. The entropies and kurtoses are then fed to a neural network as inputs for further fault classification. The proposed method can detect and identify faulty components in an analog circuit by analyzing its output signal with high accuracy and is suitable for nonlinear circuits. Preprocessing based on the kurtosis and entropy of signals for the neural network classifier simplifies the network architecture, reduces the training time, and improves the performance of the network. The results from our examples showed that the trochoid of the entropies and kurtoses is unique when the faulty component's value varies from zero to infinity; thus, we can correctly identify the faulty components when the responses do not overlap. Applying this method for three linear and nonlinear circuits, the average accuracy of the achieved fault recognition is more than 99%, although there are some overlapping data when tolerance is considered. Moreover, all the trochoids converge to one point when the faulty component is open-circuited, and thus, the method can classify not only soft faults but also hard faults.",2010,0,
499,500,A NURBS-based error concealment technique for corrupted images from packet loss,"An error concealment using non-uniform rational B-spline (NURBS) is proposed. NURBS has been employed by many CAD/CAM systems as a fundamental geometry representation. Despite the fact that NURBS has gained tremendous popularity from the CAD/CAM and computer graphics community, its application on exploring the image problem only received little attention. On the other hand, the image contents might be corrupted or lost during transmission. Although there are quite a few existing techniques, yet developing an effective approach to conceal the error remains as one of the hottest research topics. Thus the aim of this study is to develop an image reconstruction technique using NURBS. The key idea is to use NURBS to represent the portion of image data without corruption. By accomplishing this, a single-hidden layer neural network is employed to learn the appropriate control points of NURBS. After learning, NURBS is then used to render the corrupted image data. Experimental results indicate that the proposed approach exhibits promising performance.",2002,0,
500,501,Adaptive Cancellation of a Sinusoidal Disturbance with Rapidly Varying Frequency Using an Augmented Error Algorith,This paper considers a compensator for a sinusoidal disturbance with known but rapidly varying frequency. The compensator is obtained as an adaptive feedforward cancellation algorithm using an augmented error. The system is shown to be Lyapunov stable and equivalent to a linear time-varying controller that includes an internal model of the disturbance. The stability and robustness properties of the augmented error algorithm are validated by simulation results,2005,0,
501,502,In-system partial run-time reconfiguration for fault recovery applications on spacecrafts,"This paper presents a methodology for partially reconfiguring a field programmable gate array (FPGA) device using only limited onboard resources. This paper also seeks to provide a roadmap to developing necessary tools and technologies to help design self-sufficient partial run-time reconfigurable systems for spacecraft avionic systems. To provide a vision for the technology, this paper recommends a few possible applications in spacecraft avionic systems, in fault tolerance and space-saving hardware. In addition, some previous work done on the research for reconfigurable, modular avionics are also presented at the end as an example of applications.",2005,0,
502,503,Application of Aircraft Fuel Fault Diagnostic Expert System Based on Fuzzy Neural Network,Theories of expert system and fuzzy artificial neural network (ANN) are applied to solve the problem of fault diagnosis in the aircraft fuel system. A multilayer neural network model of the aircraft fuel system is put forward and the integrated aircraft fuel fault diagnostic expert system which solves the problems of knowledge representation and knowledge acquisition of traditional expert system is realized. The hardware-in-loop simulation results show that the expert system diagnoses the fault in accessories rapidly and accurately and it is proved that the expert system is significative and helpful for further development in the aircraft fuel fault diagnosis.,2009,0,
503,504,Inverse wave field extrapolation: a different NDI approach to imaging defects,"Nondestructive inspection (NDI) based on ultrasound is widely used. A relatively recent development for industrial applications is the use of ultrasonic array technology. Here, ultrasonic beams generated by array transducers are controlled by a computer. This makes the use of arrays more flexible than conventional single-element transducers. However, the inspection techniques have principally remained unchanged. As a consequence, the properties of these techniques, as far as characterization and sizing are concerned, have not improved. For further improvement, in this paper we apply imaging theory developed for seismic exploration of oil and gas fields on the NDI application. Synthetic data obtained from finite difference simulations is used to illustrate the principle of imaging. Measured data is obtained with a 64-element linear array (4 MHz) on a 20-mm thick steel block with a bore hole to illustrate the imaging approach. Furthermore, three examples of real data are presented, representing a lack of fusion defect, a surface breaking crack, and porosity",2007,0,
504,505,A decomposition approach to the inverse problem-based fault diagnosis of liquid rocket propulsion systems,"The health monitoring of propulsion systems has being been one of the most challenging issues in space launch vehicles, particularly for the manned space missions. The development of an advanced health monitoring system involves many technical aspects, such as failure detection and fault diagnosis as well as the integration of hardware and algorithms, for improving the safety and reliability of propulsion systems. The inverse problem-based strategy provides a new solution to the design of model-based fault diagnosis methods for monitoring the health of propulsion systems. This paper presents a decomposition approach to the inverse problem-based fault diagnosis for a class of liquid rocket propulsion systems. Simulation results are provided for demonstrating the effectiveness of the proposed approach to the inverse problem-based fault diagnosis.",2004,0,
505,506,A Unified Environment for Fault Injection at Any Design Level Based on Emulation,"Sensitivity of electronic circuits to radiation effects is an increasing concern in modern designs. As technology scales down, Single Event Upsets (SEUs) are made more frequent and probable, affecting not only space applications, but also applications at earth's surface, like automotive applications. Fault injection is a method widely used to evaluate the SEU sensitivity of digital circuits. Among the existing fault injection techniques, those based on FPGA emulation have proven to be the fastest ones. In this paper a unified emulation environment which combines two fault injection techniques based on FPGA emulation is proposed. The new emulation environment provides both, a high speed tool for quick fault detection, and a medium speed tool for in-depth analysis of SEUs propagation. The experiments presented here show that the two techniques can be successfully applied in a complementary manner.",2007,0,
506,507,Enhancing Fault Tolerance And Reliability In GAIAOS Through Structured Overlay Network,"GAIAOS event manager is a distributed event service, based on CORBA event service with a centralized entry point, resulting in limited fault resilience and scalability. In this paper, we have proposed a decentralized event service for GAIAOS through the use of DHT based structured overlay network to overcome these problems. The proposed architecture provides a completely distributed event communication mechanism without any centralized entry point. Incorporation of the structured overlay network in GAIAOS results in higher degree of fault resilience and scalability",2006,0,
507,508,Application of non-parametric statistics of the parametric response for defect diagnosis,"This paper presents a method using only the rank of the measurements to separate a part's elevated response to parametric tests from its non-elevated response. The effectiveness of the proposed method is verified on the 130nm ASIC. Good die responses are correlated for same parametric tests at different conditions such as temperature, voltage and or other stress. Nonparametric correlation methods are used to calculate the intra-die correlation. When intra-die correlation is found to be low the elevated vectors that lower correlation are extracted and input to IDDQ-based diagnostic tools. Monte-Carlo simulations are described to obtain confidence bounds of the correlation for good die test response.",2009,0,
508,509,Multi-rate receiver design with IF sampling and digital timing correction,This contribution deals with a fully digital multirate radio receiver suitable for vehicular applications. Timing correction and sample rate conversion are performed by a polynomial interpolator. Three different receiver configurations are considered in terms of computational complexity and BER performance. Careful selection of the intermediate frequency turns out to play a crucial role. System parameters are provided yielding good BER performance for all considered symbol rates. Results are verified by computation of the BER degradation as compared to an analog receiver with synchronized symbol-rate sampling.,2003,0,
509,510,An improved neural network algorithm for classifying the transmission line faults,"This study introduces a new concept of artificial intelligence based algorithm for classifying the faults in power system networks. This classification identifies the exact type and zone of the fault. The algorithm is based on unique type of neural network specially developed to deal with a large set of highly dimensional input data. An improvement of the algorithm is proposed by implementing various steps of input signal preprocessing, through the selection of parameters for analog filtering, and values for the data window and sampling frequency. In addition, an advanced technique for classification of the test patterns is discussed and the main advantages compared to previously used nearest neighbor classifier are shown.",2002,0,
510,511,An error resilience scheme for layered video coding,"Layered video coding combined with prioritized transmission is widely recognized as one of the schemes for providing error resilience in video transport system. We examine the error performance of data partitioning coded MPEG-2 video bitstreams transmitted over channel subject to bit errors. While base layer errors cannot be tolerated, only a limited amount of errors in the enhancement layer is acceptable. Further improvements on bit error resilience can be achieved using the EREC coder in the enhancement layer. Our results show the PSNR gain of up to 3 dB with EREC coded enhancement layer and no errors in the base layer.",2005,0,
511,512,SIED: software implemented error detection,"This paper presents a new error detection technique called software implemented error detection (SIED). The proposed method is based on a new control check flow scheme combined with software redundancy. The distinctive advantage of the SIED approach over other fault tolerance techniques is the fault coverage. SIED is able to cope with faults affecting data and the program control flow. By-applying the proposed approach on several benchmark programs, we evaluate the error detection capabilities by means of several fault injection experiments. Experimental results underline very good error detection capabilities for the obtained hardened version of selected benchmark programs.",2003,0,
512,513,Fault detection techniques for effective line side asset monitoring,"In this paper the results of current research into the state-of-the-art in predictive fault detection and diagnosis methods for railway line-side assets is presented. Research to date has mainly focussed on point machines, track circuits and level crossing systems. It will be argued that, through the use of examples, that the most appropriate method for robust fault detection is based around generic models that are tuned for a particular instance of an asset. Furthermore, once a fault has been detected, it is necessary to have an a priori knowledge of the symptoms that are observable under fault conditions to reliably diagnose faults.",2005,0,
513,514,A Family of Electronic Ballasts Integrating Power Factor Correction and Power Control Stages to Supply HPS Lamps,"This paper presents a family of high power factor electronic ballasts applied to the public lighting system. Flyback, buck-boost, boost or SEPIC converter is employed in the power factor correction stage, integrated to the power control stage through a single active switch. The use of a half-bridge inverter, to supply the lamp, becomes possible through the employment of a flyback converter in the lamp power control stage. The lamp is supplied in a low frequency voltage square waveform in order to guarantee the safe lamp operation, regarding to the acoustic resonance phenomenon. The presented solutions to supply HPS lamps take the advantage of low cost and simplicity. The shared switch characteristics are analyzed and discussed during this work. A comparative analysis among the presented electronic ballasts is performed",2006,0,
514,515,Respiratory motion correction of PET using motion parameters from MR,"Respiratory motion during PET acquisition from the chest/abdomen leads to significant image degradation. Combined PET/MR scanners open up the opportunity to correct motion using MR data acquired simultaneously with PET. As simultaneous human chest/abdomen PET/MR images are currently unobtainable, in this preliminary study we determined motion parameters from respiratory-gated MR and then used these to correct pseudo-PET images generated from the MR. The gated MR images were segmented to typical organ FDG SUV values, smoothed to mimic PET resolution, forward projected into the GE advance geometry and reconstructed separately using OSEM. The MR images were registered using a combined affine and non-rigid B-splines algorithm, with mutual information used as the cost function in a multi-resolution approach. Motion corrected images from both post-reconstruction registration and 4D image reconstruction are shown to be superior to those without motion compensation for most organs.",2009,0,
515,516,Assessing and improving the effectiveness of logs for the analysis of software faults,"Event logs are the primary source of data to characterize the dependability behavior of a computing system during the operational phase. However, they are inadequate to provide evidence of software faults, which are nowadays among the main causes of system outages. This paper proposes an approach based on software fault injection to assess the effectiveness of logs to keep track of software faults triggered in the field. Injection results are used to provide guidelines to improve the ability of logging mechanisms to report the effects of software faults. The benefits of the approach are shown by means of experimental results on three widely used software systems.",2010,0,
516,517,A novel approach to fault diagnostics and prognostics,A novel fault diagnostics and prognostics algorithm based on hidden Markov model (HMM) is proposed. The algorithm combines fault diagnostics and prognostics in a unified framework. The algorithm has been fully tested by using experimental data from a rotating shift testbed in our laboratory.,2003,0,
517,518,Best ANN structures for fault location in single-and double-circuit transmission lines,"The great development in computing power has allowed the implementation of artificial neural networks (ANNs) in the most diverse fields of technology. This paper shows how diverse ANN structures can be applied to the processes of fault classification and fault location in overhead two-terminal transmission lines, with single and double circuit. The existence of a large group of valid ANN structures guarantees the applicability of ANNs in the fault classification and location processes. The selection of the best ANN structures for each process has been carried out by means of a software tool called SARENEUR.",2005,0,
518,519,Defect Data Analysis Based on Extended Association Rule Mining,"This paper describes an empirical study to reveal rules associated with defect correction effort. We defined defect correction effort as a quantitative (ratio scale) variable, and extended conventional (nominal scale based) association rule mining to directly handle such quantitative variables. An extended rule describes the statistical characteristic of a ratio or interval scale variable in the consequent part of the rule by its mean value and standard deviation so that conditions producing distinctive statistics can be discovered As an analysis target, we collected various attributes of about 1,200 defects found in a typical medium-scale, multi-vendor (distance development) information system development project in Japan. Our findings based on extracted rules include: (l)Defects detected in coding/unit testing were easily corrected (less than 7% of mean effort) when they are related to data output or validation of input data. (2)Nevertheless, they sometimes required much more effort (lift of standard deviation was 5.845) in case of low reproducibility, (i)Defects introduced in coding/unit testing often required large correction effort (mean was 12.596 staff-hours and standard deviation was 25.716) when they were related to data handing. From these findings, we confirmed that we need to pay attention to types of defects having large mean effort as well as those having large standard deviation of effort since such defects sometimes cause excess effort.",2007,0,
519,520,A Hybrid Fault-Tolerant Algorithm for MPLS Networks,In this paper we present a novel fault-tolerant algorithm for use in MPLS based networks. The algorithm is employing both protection switching and path rerouting techniques and satisfies four selected performance criteria,2006,0,
520,521,Using software implemented fault inserter in dependability analysis,We investigate program susceptibility to hardware faults in Win32 environment. For this purpose we use the software implemented fault injector FITS. We analyze natural fault resistivity of COTS systems and the effectiveness of various software techniques improving system dependability. The problems of experiment tuning and result interpretation are discussed in context of a wide spectrum of applications.,2002,0,
521,522,Fault-Tolerant Discrete Dynamical Systems Over Finite Ring,"It is worked out some general method of fault- tolerant synthesis for implementations of information-looseness dynamical systems over finite ring, based on application of error control codes. Corresponding self-checking systems are designed complexity and some basic characteristics of designed implementations is characterized.",2007,0,
522,523,A Fault-Tolerant Scheme for Complex Transaction Patterns in J2EE,"End-to-end reliability is an important issue in building large-scale distributed enterprise applications based on multi-tier architecture, but the support of reliability as adopted in conventional replication or transactions mechanisms is not enough due to their distinct objectives - replication guarantees the liveness of computational operations by using forward error recovery, while transactions guarantee the safety of application data by using backward error recovery. Combining the two mechanisms for stronger reliability is a challenging task Current solutions, however, are typically on the assumption of simple transaction pattern where a request from a single client executes in the context of exactly one transaction at the middle-tier application server, and seldom think about some complex patterns, such as client transaction enclosing multiple client requests or nested transactions. In this paper, we first identify four transaction pattern classes, and then propose a fault-tolerant scheme that can uniformly provide exactly-once semantic reliability support for these patterns. In this scheme, application servers are passively replicated to endow business logics with high reliability and high availability. In addition, by replicating transaction coordinator, the blocking problem of 2PC protocol during distributed transactions processing is eliminated. We have implemented this approach and integrated it into our own J2EE application server, OnceAS. Also, its effectiveness is discussed in different transaction patterns and the corresponding performance is evaluated",2006,0,
523,524,Tracking the elusive online help topic. Organizing the review process with defect-tracking software,"Online help systems consist of hundreds of help topics. Keeping track of reviewers, comments about each help topic requires a database to do the job effectively. Rather than develop such a database from scratch, it may be possible to adapt the defect-tracking software already in use in the QA department to this task. This paper describes how technical writers can adapt defect-tracking software to organize the online help review process",2001,0,
524,525,Modeling Defect Enhanced Detection at 1550 nm in Integrated Silicon Waveguide Photodetectors,"Recent attention has been attracted by photo-detectors integrated onto silicon-on-insulator (SOI) waveguides that exploit the enhanced sensitivity to subbandgap wavelengths resulting from absorption via point defects introduced by ion implantation. In this paper, we present the first model to describe the carrier generation process of such detectors, based upon modified Shockley-Read-Hall generation/recombination, and, thus, determine the influence of the device design on detection efficiency. We further describe how the model may be incorporated into commercial software, which then simulates the performance of previously reported devices by assuming a single midgap defect level (with properties commensurate with the single negatively charged divacancy). We describe the ability of the model to highlight the major limitations to responsivity, and thus suggest improvements which diminish the impact of such limitations.",2009,0,
525,526,Fault diagnosis technology based on wavelet analysis and resonance demodulation,"The impulse signal is contained in the fault signals of some pivotal components such as gears and axletrees. Extracting weensy impact information is an important method to diagnose equipment. A mathematical model on the technology of resonant demodulation is put forward in this paper. The model provides the theoretical basis on how to use the technology to extract the weensy impulse signal from the normal low-frequency vibrating signal, at the same time, another method that wavelet analysis is used to extract the weensy impulse information is introduced too. Simulation and practical application manifest that both wavelet analysis and demodulation have good effect on extracting the weensy impulse from the mechanical fault caused by gear and axletree.",2004,0,
526,527,Cluster-Based Error Messages Detecting and Processing for Wireless Sensor Networks,"Wireless sensor networks (WSNs) have emerged as a new technology about acquiring and processing messages for a variety of applications. Faults occurring to sensor nodes are common due to lack of power or environmental interference. In order to guarantee the network reliability of service, it is necessary for the WSN to be able to detect and processes the faults and take appropriate actions. In this paper, we propose a novel approach to distinguish and filter the error messages for cluter-based WSNs. The simulation results show that the proposed method not only can avoid frequent re-clustering but also can save the energy of sensor nodes, thus prolong the lifetime of sensor network.",2008,0,
527,528,Error-Correcting Codes Based on Quasigroups,"Error-correcting codes based on quasigroup transformations are proposed. For the proposed codes, similar to recursive convolutional codes, the correlation exists between any two bits of a codeword, which can have infinite length, theoretically. However, in contrast to convolutional codes, the proposed codes are nonlinear and almost random: for codewords with large enough length, the distribution of the letters, pair of letters, triple of letters, and so on, is uniform. Simulation results of bit-error probability for several codes in binary symmetric channels are presented.",2007,0,
528,529,Multimedia processor-based implementation of an error-diffusion halftoning algorithm exploiting subword parallelism,"Multimedia processor-based implementations of digital image processing algorithms have become important since several multimedia processors are now available and can replace special-purpose hardware-based systems because of their flexibility. Multimedia processors increase throughput by processing multiple pixels simultaneously using a subword-parallel arithmetic and logic unit architecture. The error-diffusion halftoning algorithm employs feedback of quantized output signals to faithfully convert a multi-level image to a binary image or to one with fewer levels of quantization. This makes it difficult to achieve speedup by utilizing the multimedia extension. In this study, the error-diffusion halftoning algorithm is implemented for a multimedia processor using three methods: single-pixel, single-line, and multiple-line processing. The single-pixel approach is the closest to conventional implementations, but the multimedia extension is used only in the filter kernel. The single-line approach computes multiple pixels in one scan-line simultaneously, but requires a complex algorithm transformation to remove dependencies between pixels. The multiple-line method exploits parallelism by employing a skewed data structure and processing multiple pixels in different scan-lines. The Pentium MMX instruction set is used for quantitative performance evaluation including run-time overheads and misaligned memory accesses. A speedup of more than ten times is achieved compared to the software (integer C) implementation on a conventional processor for the structurally sequential error-diffusion halftoning algorithm",2001,0,
529,530,Fault Diagnosis Implementation of Induction Machines based on Advanced Digital Signal Processing Techniques,"In this paper, a comprehensive cross correlation-based fault diagnostic method is proposed for real time DSP implementation. It covers both fault monitoring and decision making stages. In practice, a motor driven by an adjustable speed drive is run at various operating points where the frequency, amplitude and phase of the fault signatures varies with time. These dynamic changes are considered as one of the common factor that yields erroneous fault tracking and unstable fault detection. In this paper, the proposed algorithms deals with the operating point dependent ambiguities and threshold issues. It is theoretically and experimentally verified that the motor fault can continuously be tracked when the operating point changes within a limited range.",2009,0,
530,531,Positive Switching Impulse Discharge Performance and Voltage Correction of Rod-Plane Air Gap Based on Tests at High-Altitude Sites,"The Qinghai-Tibet Railway is the highest railway in the world. Up to now, there were no test and service data for the external insulation of the power-supply project of the railway system above 4000 m above sea level (a.s.l.). The ldquogrdquo parameter method recommended by IEC Publication 60.1 (1989) has a limited applicable range. Therefore, based on the former tests carried on the artificial climate chamber (ACC), in this paper, a series of test investigations is conducted on the positive switching impulse (PSI) discharge performance of rod-plane air gaps with gap spacing of 0.25 to 3.0 m at the six high-altitude sites along the Qinghai-Tibet Railway with altitudes of 2820 to 5050 m. With analyses of the mathematical optimization method on the test results, the new correction method of discharge voltage is proposed. They are also checked and compared with the test results obtained from the simulation tests carried out in the ACC. It is indicated that the 50% PSI discharge voltage <i>U</i> <sub>50</sub> of the air gap at high altitude is a power function of gap spacing <i>d</i>, also a power function of relative pressure of dry air and absolute humidity. The influence law of atmospheric parameters on <i>U</i> <sub>50</sub> obtained at high-altitude sites is the same as that obtained in the ACC. <i>U</i> <sub>50</sub> . obtained in the ACC, is about 8.15% higher than that obtained at high-altitude sites due to the influence of nonsimulated factors, such as ultraviolet ray and cosmic radiation. The ldquogrdquo parameter method is not applicable to the regions with an altitude above 2800 m.",2009,0,
531,532,A randomized error recovery algorithm for reliable multicast,"An efficient error recovery algorithm is essential for a liable multicast in large groups. Tree-based protocols (RMTP, TMTP, LBRRM) group receivers into local regions and select a repair server for performing error recovery in each region. Hence a single server bears the entire responsibility of error recovery for a region. In addition, the deployment of repair servers requires topological information of the underlying multicast tree, which is generally not available at the transport layer. This paper presents RRMP, a randomized reliable multicast protocol which improves the robustness of tree-based protocols by diffusing the responsibility of error recovery among all members in a group. The protocol works well within the existing IP multicast framework and does not require additional support from routers. Both analysis and simulation results show that the performance penalty due to randomization is low and can be tuned according to application requirements",2001,0,
532,533,Combined Use of Fuzzy Set-Covering Theory and Mode Identification Technique for Fault Diagnosis in Power Systems,"After a fault occurs in a power system, generally some operating information of protective relays and circuit breakers could be obtained. Because protective relays and circuit breakers might improperly operate or fail to operate, and some errors and distortion may exist in data acquisition and communication, as a result uncertainties could be involved in the received information. A fault diagnosis model based on fuzzy set- covering theory and mode identification technique is proposed in this paper. With the fuzzy technology, the above mentioned uncertainties could be dealt with very well. Meanwhile, as the protective relays and circuit breakers may fail to operate in some cases, there are several different operating modes in protective relays and circuit breakers even for a same electrical device failure. Based on the received information, the proposed model could identify the most possible operating mode, and then the information corresponding to a fault hypothesis could be obtained. In the proposed model, the proposed fault diagnosis problem is described as a 0-1 integer programming one, and thus could be solved by the widely employed search technology, i.e., the well-known Tabu search method. The feasibility and efficiency of the proposed model is demonstrated by a sample power system.",2007,0,
533,534,H<sub></sub> Dynamic observer design with application in fault diagnosis,"Most observer-based methods applied in fault detection and diagnosis (FDD) schemes use the classical two-degrees of freedom observer structure in which a constant matrix is used to stabilize the error dynamics while a post filter helps to achieve some desired properties for the residual signal. In this paper, we consider the use of a more general framework which is the dynamic observer structure in which an observer gain is seen as a filter designed so that the error dynamics has some desirable frequency domain characteristics. This structure offers extra degrees of freedom and we show how it can be used for the sensor faults diagnosis problem achieving detection and estimation at the same time. The use of weightings to transform this problem into a standard H<inf></inf>problem is also demonstrated.",2005,0,
534,535,Practical considerations in making CORBA services fault-tolerant,"This paper examines the CORBA Naming, Event, Notification, Trading, Time and Security Services, with the objective of identifying the issues that must be addressed in order make these services fault-tolerant. The reliability considerations for each of these services involves strategies for replicating the service objects, and for keeping the states of the replicas consistent. Of particular interest are the sources of non-determinism in each of these services, along with the means for addressing the non-deterministic behavior in the interests of ensuring strong fault tolerance",2002,0,
535,536,Analysis - The terrors and the errors [IT Change Management],"According to last month??s Sophos `Security Threat Report??, concern is increasing that computer applications running critical national infrastructures are vulnerable to malevolent hacks. Such hacks could in theory switch control of power and gas supplies, say, to the keyboards of hostile entities, enabling them to wreak damage and disruption. Similar threats face crucial financial computer platforms that underpin national economies, and even emergency services communication channels.",2010,0,
536,537,Innovative airborne inventory and inspection technology for electric power line condition assessments and defect reporting,"A cost-effective and innovative airborne inventory and inspection patrol system for distributed assets such as transmission lines, pipelines, and roadways has been developed and evaluated. Results show that aerial high-resolution digital visual and spectral images tagged by Global Positioning Satellite (GPS) coordinates can be successfully used to cost-effectively identify the majority of conditions/defects on electric power lines. Experiments show that the condition and defect detection rate of the airborne inventory and inspection system is significantly higher than rates derived from traditional patrols and comparable to values achieved from driving patrols. Geographic information systems (GIS) based mapping tools can be used to quickly and efficiently interpret digital images collected from aerial platforms. Digital images provide an archival record of the condition of the distributed assets to estimate the long-term performance of the assets and to define cost-effective maintenance and replacement schedules",2000,0,
537,538,Advanced Cu CMP defect excursion control for leading edge micro-processor manufacturing,"The introduction of yield sensitive, advanced interconnect technology coupled with the requirement for accelerating yield ramp in today's state-of-the-art semiconductor manufacturing facilities, are driving tool monitoring requirements for fast and accurate defect excursion control. In the Copper CMP module the challenge is accentuated by the relative immaturity of this process, the dominance of single wafer excursions and a high count of nuisance defect types relative to the critical yield-limiting defect types. A manufacturing-worthy Copper CMP tool monitor methodology is described here that improves excursion control through detection and tracking of critical, yield-limiting defect types, independent of non-yield-critical nuisance defect types. High-resolution automatic defect review and classification, a critical component of the methodology, is limited to wafers with high critical-defect counts, reducing monitoring cost and time-to-results. A new trigger sampling feature and intelligent image sampling reduces monitoring cost and time-to-results through minimizing defect review overhead. Integration of such a solution into the manufacturing environment is presented in detail and contrasted next to existing traditional defect excursion control model. Ease-of-use considerations are highlighted with use case examples. The paper will approximate the cost savings to manufacturing such as reducing existing levels of false excursion due to nuisance defects and improving the cycle time in the Cu CMP module. Benefits are achieved by integrating functionality into existing inspection hardware. No additional capital equipment was required.",2002,0,
538,539,Hybrid fault-tolerant control of aerospace vehicles,"We describe our recent results (2001) related to the design of hybrid online failure detection and identification and adaptive reconfigurable control algorithms for aerial and space vehicles. Our approach is based on the multiple models, switching and tuning methodology and its extensions, and has been demonstrated as an efficient tool for hybrid fault tolerant control under subsystem and component failures and structural damage",2001,0,
539,540,Active error recovery for reliable multicast,"An error recovery scheme is essential for large-scale reliable multicast. We design, implement, and evaluate an improved active error recovery scheme for reliable multicast (AERM). The AERM uses soft-state storage to facilitate fast error recovery. It has the following features: a simple NAK suppression and aggregation mechanism, an efficient hierarchical RTT measurement mechanism, an effective local recovery and scoped retransmission mechanism, and a periodical ACK mechanism. We implement the AERM and study its characteristics in NS2. We also compare performance with ARM and AER/NCA, both of which are representative active reliable multicast protocols. The results indicate that AERM can achieve considerable performance improvement with limited support from routers. Our work also confirms that active networks can benefit some applications and become a promising network computing platform in the future",2001,0,
540,541,An Error Concealment Scheme for Entire Frame Losses for H.264/AVC,"In this paper, an error concealment scheme is proposed to conceal an entirely lost frame in a compressed video bitstream due to errors introduced during transmission. The proposed scheme targets low bit rate video transmission applications using H.264/AVC. The motion field of the lost frame is first reconstructed by copying the co-located motion vectors and reference indices from the last decoded reference frame. After the motion field estimation of the missing frame, motion compensation is performed to reconstruct the frame. This technique reuses existing modules of the video decoder and it does not incur extra complexity compared to decoding a normal frame. It has also been adopted as a non-normative decoder option to the JM reference software at the JVT meeting in Poznan, Poland in July 2005 [1] and has been incorporated into the SA4 video ad hoc group's toolkit at the 3GPP meeting at Paris [2] in September 2005. Simulation results will show its improved performance over other simple error concealment schemes such as ""frame copy,"" both subjectively and objectively, without significant complexity overhead.",2006,0,
541,542,A new textual/non-textual classifier for document skew correction,A robust approach is proposed for document skew detection. We use Fourier analysis and SVM to classify textual areas from non-textual areas of documents. We also propose a robust method to determine the skew angle from textual areas. Our approach achieves good performance on documents with large area of non-textual contents.,2002,0,
542,543,Evolutionary design and adaptation of digital filters within an embedded fault tolerant hardware platform,"Finite impulse response filters (FIRs) are crucial device for robust data communication and manipulation. Multiplierless filters have been shown to produce high performance systems with fast signal processing and reduced area. Furthermore, the distributed architecture inherent in multiplierless filters makes it a suitable candidate for fault tolerant design. Alternative approaches to the design of fault tolerant systems have been proposed using evolutionary algorithms (EAs) and the concept of evolvable hardware (EHW). This paper presents an evolvable hardware platform for the automated design and adaptation of multiplierless digital filters. Filters are realised within a dedicated programmable logic array (PLA). The platform employs a genetic algorithm to autonomously configure the PLA for a give set of coefficients. The ability of the platform to adapt to increasing numbers of faults was investigated through the evolution of a 31-tap low-pass FIR filter. Results show that the functionality of filters evolved on the PLA was maintained despite an increasing number of faults covering up to 25% of the PLA area. Additionally, three PLA initialisation methods were investigated to ascertain which produced the fastest fault recovery times. It was shown that seeding a population of random configuration-strings with the best configuration currently obtained resulted in a 6 fold increase in fault recovery speed over other methods investigated",2001,0,
543,544,Defect Identification in Large Area Electronic Backplanes,We describe a rapid testing system for active matrix thin-film transistor (TFT) backplanes which enables the identification of many common processing defects. The technique spatially maps the charge feedthrough from TFTs in the pixel and is suited for pixels with switched-capacitor architecture.,2009,0,
544,545,Design and analysis of a fault-tolerant mechanism for a server-less video-on-demand system,"Video-on-demand (VoD) systems have traditionally been built on the client-server architecture, where a video server stores, retrieves, and transmits video data to video clients for playback This paper investigates a radically different approach to building VoD systems, one where the server, and hence the primary bottleneck, is completely eliminated. This server-less architecture comprises homogeneous hosts, called nodes, which serve both as client and as mini-server. Video data are distributed over all nodes and these nodes cooperatively stream video data to one another for playback. However, unlike traditional video server that runs on high-end server hardware in a carefully controlled and protected data centre, a node in a server less system is likely to be far more unreliable. Therefore it is essential that sufficient data and capacity redundancies are incorporated to maintain an acceptable set-vice reliability. This paper presents and analyzes a fault tolerant mechanism based on inter-node striping and erasure correction codes to tackle this challenge. By formulating the system's reliability as a Markov chain model, we obtain insights into the feasible operating region of the system, such as the amount of redundancy required and the node-level reliability that can be tolerated. Numerical results show that a server-less VoD system of 200 nodes can achieve reliability surpassing that of dedicated video server using a redundancy overhead of only 21.2% even though individual nodes are highly unreliable.",2002,0,
545,546,Performance analysis and improvements for a simulation-based fault injection platform,"In this paper, we study and present two techniques to improve the performance of a simulation-based fault injection platform that inserts bit flips in order to model soft errors on digital circuits. The platform is based on the ESA Data Systems Divisionpsilas SEE simulation tool. In contrast with methods based on emulation, the proposed approach reduces the complexity and costs, supplying a test environment with the same reliability as emulation systems. Only one disadvantage appears when comparing both methodologies: the lower performance of the simulation in cases where the fault injection campaigns are very large. Two proposals have been developed in order to address this drawback: the first one is based on software (through checkpoints) and the second one uses parallel computation.",2008,0,
546,547,Autonomous Decentralized VoD System Architecture and Fault-Tolerant Technology to Assure Continuous Service,"In distributed and ubiquitous computing systems, not only the composing subsystems and their functions, but also the system structure would be changed constantly under the evolving situation. With the advances of compression, storage and network technologies, Video on Demand (VoD) service is becoming more and more popular. However, it is difficult for conventional systems to meet the continuous and heterogeneous requirements from service providers and users simultaneously. This paper introduces an autonomous decentralized VoD system sustained by mobile agents for information service provision and utilization. Under the proposed architecture, autonomous fault detection and recovery technologies are proposed to assure continuous service. The effectiveness of the proposed technology is proved by the simulation. The results show that an average of 30% improvement in recovery time and users' video service can be recovered without stopping compared with the conventional system.",2009,0,
547,548,A novel framework for robust video streaming based on H.264/AVC MGS coding and unequal error protection,"We present a novel framework to provide robust video streaming service over time-varying error-prone network. The scheme is based on the medium granularity scalability (MGS) video coding of the H.264/AVC standard, which adopts a hierarchical prediction structure for the group-of-pictures (GOP). We determine the optimal allocation of protection strength for different network abstraction layer (NAL) units according to their individual importance to the end-to-end video quality. To analyse the importance of the NAL units, we emulate the error concealment if one frame is considered as lost and take into account the propagation distortion within the GOP. An efficient algorithm is proposed to account for the non-convex rate-distortion characteristics associated with the NAL units in the hierarchical GOP. With this framework, we can provide robust video streaming for the range of packet loss rates from 0% to 40% with about 30% additional channel bit-rate for the channel coding. The simulation results demonstrate high flexibility and efficiency of the proposed framework, which can effectively prevent frequent loss of frames.",2009,0,
548,549,Application of an automated PD failure identification system for EMC-assessment strategies of multiple PD defects at HV-insulators,"EMC-assessment of field emission of high voltage insulators can be performed using phase-angle-resolved partial discharge diagnosis. However, with conventional PD-detection systems no satisfying statements about multiple PD defects are possible because of the highly dynamic apparent charge values for different discharge phenomena. This measurement problem can be solved by a new approach. For this purpose phase resolved pulse sequence analysis methods are suitable diagnosis tools without using the apparent charge as a dominating influence. A recently developed feature extraction method based on consecutive u/ values shows good classification results. The problem of multiple PD defects, which occur at the same time, is a new challenge for PD diagnosis systems. For the investigation two reference databases are generated. With the database, which takes these multiple PD defects into account, the diagnosis system WinTED of the University of Wuppertal is able to identify with high reliability actual measurements made by the University of Dortmund",2000,0,
549,550,Design aspects and pattern prediction for phased arrays with subarray position errors,"In modern array design, the antenna elements are often grouped into mechanical units such as printed antenna boards and mechanical subarrays/multipacks. This contributes to a more cost efficient manufacturing process and facilitates integration, handling, reuse and exchange of units, but it also makes the antenna element position errors correlated. Classical papers predict the statistical sidelobe level based on the assumption of uncorrelated errors, but using this for the general case, the statistical sidelobe level is under estimated. In this paper, the statistical sidelobe level for arrays with correlated position errors is predicted. Furthermore, rules of thumb relating antenna element position tolerances and mechanical array design to antenna array performance (sidelobe level) are given. Finally, array design aspects are discussed.",2010,0,
550,551,Double Redundant Fault-Tolerance Service Routing Model in ESB,"With the development of the Service Oriented Architecture (SOA), the Enterprise Service Bus (ESB) is becoming more and more important in the management of mass services. The main function of it is service routing which focuses on delivery of message among different services. At present, some routing patterns have been implemented to finish the messaging, but they are all static configuration service routing. Once one service fails in its operation, the whole service system will not be able to detect such fault, so the whole business function will also fail finally. In order to solve this problem, we present a double redundant fault tolerant service routing model. This model has its own double redundant fault tolerant mechanism and algorithm to guarantee that if the original service fails, another replica service that has the same function will return the response message instead automatically. The service requester will receive the response message transparently without taking care where it comes from. Besides, the state of failed service will be recorded for service management. At the end of this article, we evaluated the performance of double redundant fault tolerant service routing model. Our analysis shows that, by importing double redundant fault tolerance, we can improve the fault-tolerant capability of the services routing apparently. It will solve the limitation of existent static service routing and ensure the reliability of messaging in SOA.",2009,0,
551,552,Stator Current and Motor Efficiency as Indicators for Different Types of Bearing Faults in Induction Motors,"This paper proposes a new approach to use stator current and efficiency of induction motors as indicators of rolling-bearing faults. After a presentation of the state of the art about condition monitoring of vibration and motor current for the diagnostics of bearings, this paper illustrates the experimental results on four different types of bearing defects: crack in the outer race, hole in the outer race, deformation of the seal, and corrosion. The first and third faults have not been previously considered in the literature, with the latter being analyzed in other research works, even if obtained in a different way. Another novelty introduced by this paper is the analysis of the decrease in efficiency of the motor with a double purpose: as alarm of incipient faults and as evaluation of the extent of energy waste resulting from the lasting of the fault condition before the breakdown of the machine.",2010,0,
552,553,Noise-Related Radiometric Correction in the TerraSAR-X Multimode SAR Processor,"Synthetic aperture radar (SAR) image intensity is disturbed by additive system noise. During SAR focusing, pattern corrections that are adapted to the characteristics of the wanted signal, but not to the characteristics of the noise, influence the spatial distribution of the noise power. Particularly in the case of ScanSAR, a distinct residual noise pattern in low backscatter areas results. This necessitates a noise-adapted radiometric correction of the focused image for almost all applications except interferometry. In this paper, we thoroughly investigate this topic. Based on signal theoretical and stochastic considerations, we develop a radiometric correction scheme. Simulations and the application of the algorithm to TerraSAR-X datatakes support the theoretical results.",2010,0,
553,554,Robot fault-tolerance using an embryonic array,"Fault-tolerance, complex structure management and reconfiguration are seen as valuable characteristics. Embryonic arrays represent one novel approach that takes inspiration from nature to improve upon standard techniques. An existing BAE SYSTEMS RASCALTM robot has been augmented so as to improve the motor control system reliability through two biologically-inspired systems: an embryonic array and an artificial immune system. This paper is concerned with the embryonic array; this is novel in that it supports datapath-wide arithmetic and logic functions. The array is configured to provide an autonomous self-repairing hardware motor controller and is realized using a standard Xilinx Virtex FPGA. As with previous embryonic systems, the logic requirement of the array is greater than that of a conventional FPGA or standard modular-redundancy approach. However, the array offers the advantages of both conventional FPGAs and modular-redundancy techniques. It is a reconfigurable computing platform that provides inherent fault-tolerance through its distributed self-repair mechanism.",2003,0,
554,555,Comparing code reading techniques applied to object-oriented software frameworks with regard to effectiveness and defect detection rate,"This paper first reasons on understanding software frameworks for defect detection, and then presents an experimental research for comparing the effectiveness and defect detection rate of code-reading techniques, once applied to C++ coded object-oriented frameworks. We present and discuss the functionality-based approach to framework understanding. Then, we present an experiment that compared three reading techniques for inspection of software frameworks. Two of those reading techniques, namely checklist-based reading, and systematic order-based reading, were adopted from scientific literature, while the third one, namely functionality-based reading, was derived from the functionality-based approach. The results of the experiment are that (1) functionality-based reading is much more effective and efficient than checklist based reading. (2) Functionality-based Reading is significantly more effective and efficient than systematic order-based reading. (3) Systematic order-based reading performs significantly better than checklist based reading for what concerns defect detection rate. However, because we used checklist-based reading and systematic order-based reading quite as they are, with limited adaptation to frameworks, it is too early to draw strong conclusions from the experiment results and improving and replicating this study is strongly recommended.",2004,0,
555,556,Fault detection in IP-based process control networks using data mining,"Industrial process control IP networks support communications between process control applications and devices. Communication faults in any stage of these control networks can cause delays or even shutdown of the entire manufacturing process. The current process of detecting and diagnosing communication faults is mostly manual, cumbersome, and inefficient. Detecting early symptoms of potential problems is very important but automated solutions do not yet exist. Our research goal is to automate the process of detecting and diagnosing the communication faults as well as to prevent problems by detecting early symptoms of potential problems. To achieve our goal, we have first investigated real-world fault cases and summarized control network failures. We have also defined network metrics and their alarm conditions to detect early symptoms for communication failures between process control servers and devices. In particular, we leverage data mining techniques to train the system to learn the rules of network faults in control networks and our testing results show that these rules are very effective. In our earlier work, we presented a design of a process control network monitoring and fault diagnosis system. In this paper, we focus on how the fault detection part of this system can be improved using data mining techniques.",2009,0,
556,557,Investigating effects of neutral wire and grounding in distribution systems with faults,"In some applications, like fault analysis, fault location, power quality studies, safety analysis, loss analysis, etc., knowing the neutral wire and ground currents and voltages could be of particular interest. In order to investigate the effects of neutrals and system grounding on the operation of distribution feeders with faults, a hybrid short circuit algorithm is generalized. In this novel use of the technique, the neutral wire and assumed ground conductor are explicitly represented. Results obtained from several case studies using the IEEE 34-node test network are presented and discussed.",2004,0,
557,558,On Line Fault Detection and an Adaptive Algorithm to Fast Distance Relaying,"This paper presents the design of an hybrid scheme of wavelet transforms and an adaptive Fourier filtering technique for on line fault detection and phasor estimation to fast distance protection of transmission lines. The wavelet transform is used as a signal processing tool. The sampled voltage and current signals at the relay location are decomposed using wavelet transform-Multi Resolution Analysis (MRA). The decomposed signals are used for the fault detection and as input to the phasor estimation algorithm. The phasor estimation algorithm possesses the advantage of recursive computing and a decaying dc offset component is removed from fault signals by using an adaptive compensation method. Fault detection index and a variable data window scheme are embedded in the algorithm. The proposed scheme provides capability for fast tripping decision, taking accuracy into account. Extensive simulation tests and comparative evaluation presented prove the efficacy of the proposed scheme in distance protection.",2008,0,
558,559,Parametric fault trees with dynamic gates and repair boxes,"A new approach is proposed to include s-dependencies in fault tree (FT) models. With respect to previous techniques, the approach presented in this paper is based on two peculiar powerful features. First, adopting a parameterization technique, referred to as parametric FT (PFT), to fold equal subtrees (or basic events) in order to resort to a more compact FT representation. It is shown that parameterization can be conveniently adopted as well for dynamic gates. Second, PFT can be modularized and each module translated into a high level colored Petri net in the form of a stochastic well-formed net (SWN). SWN generate a lumped Markov chain and the saving in the dimension of the state space can be very substantial with respect to standard (non colored) Petri nets. Translation of PFT modules into SWN has proved to be very flexible, and various kinds of new dependencies can be easily accommodated. In order to exploit this flexibility a new primitive, called repair box, is introduced. A repair box, attached to an event, causes the starting of a repair activity of all the components that failed as the event occurs. In contrast to all the previous FT based models, the addition of repair boxes enables the approach to model cyclic behaviors. The proposed approach as dynamic repairable PFT (DRPFT) was referred to. A tool supporting DRPFT is briefly described and the tool is validated by analyzing a benchmark proposed recently in the literature for quantitative comparison [H. Zhu et al., 2001].",2004,0,
559,560,An Enhanced Fault-Tolerant Routing Algorithm for Mesh Network-on-Chip,"Fault-tolerant routing is the ability to survive failure of individual components and usually uses several virtual channels (VCs) to overcome faulty nodes or links. A well-known wormhole-switched routing algorithm for 2-D mesh interconnection network called f-cube3 uses three virtual channels to pass faulty regions, while only one virtual channel is used when a message does not encounter any fault. One of the integral stages of designing network-on-chips (NoCs) is the development of an efficient communication system in order to provide low latency networks. We have proposed a new fault-tolerant routing algorithm based on f-cube3 as a solution to reduce the delay of network packets which uses less number of VCs in comparison with f-cube3. Moreover, in this method we have improved the use of VCs per each physical link by reducing required channels to two. Furthermore, simulations of both f-cube3 and our algorithm based on same conditions have been presented.",2009,0,
560,561,Path diversity with forward error correction (PDF) system for packet switched networks,"Packet loss and end-to-end delay limit delay sensitive applications over the best effort packet switched networks such as the Internet. In our previous work, we have shown that substantial reduction in packet loss can be achieved by sending packets at appropriate sending rates to a receiver from multiple senders, using disjoint paths, and by protecting packets with forward error correction. In this paper, we propose a path diversity with forward error correction (PDF) system for delay sensitive applications over the Internet in which, disjoint paths from a sender to a receiver are created using a collection of relay nodes. We propose a scalable, heuristic scheme for selecting a redundant path between a sender and a receiver, and show that substantial reduction in packet loss can be achieved by dividing packets between the default path and the redundant path. NS simulations are used to verify the effectiveness of PDF system.",2003,0,
561,562,New Approach for Defect Inspection on Large Area Masks,"Besides the mask market for IC manufacturing, which mainly uses 6 inch sized masks, the market for the so called large area masks is growing very rapidly. Typical applications of these masks are mainly wafer bumping for current packaging processes, color filters on TFTs, and Flip Chip manufacturing. To expose e.g. bumps and similar features on 200 mm wafers under proximity exposure conditions 9 inch masks are used, while in 300 mm wafer bumping processes 14 inch masks are handled. Flip Chip manufacturing needs masks up to 28 by 32 inch. This current maximum mask dimension is expected to hold for the next 5 years in industrial production. On the other hand shrinking feature sizes, just as in case of the IC masks, demand enhanced sensitivity of the inspection tools. A defect inspection tool for those masks is valuable for both the mask maker, who has to deliver a defect free mask to his customer, and for the mask user to supervise the mask behavior conditions during its lifetime. This is necessary because large area masks are mainly used for proximity exposures. During this process itself the mask is vulnerable by contacting the resist on top of the wafers. Therefore a regular inspection of the mask after 25, 50, or 100 exposures has to be done during its whole lifetime. Thus critical resist contamination and other defects, which lead to yield losses, can be recognized early. In the future shrinking feature dimensions will require even more sensitive and reliable defect inspection methods than they do presently. Besides the sole inspection capability the tools should also provide highly precise measurement capabilities and extended review options.",2007,0,
562,563,On the bit-error probability of differentially encoded QPSK and offset QPSK in the presence of carrier synchronization,"We investigate the differences between allowable differential encoding strategies and their associated bit-error probability performances for quadrature phase-shift keying (QPSK) and offset QPSK modulations when the carrier demodulation reference signals are supplied by the optimum (motivated by maximum a posteriori estimation of carrier phase) carrier-tracking loop suitable for that modulation. In particular, we show that in the presence of carrier-synchronization phase ambiguity but an otherwise ideal loop, both the symbol and bit-error probabilities in the presence of differential encoding are identical for the two modulations. On the other hand, when in addition the phase error introduced by the loop's finite signal-to-noise ratio is taken into account, it is shown that the two differentially encoded modulations behave differently, and their performances are no longer equivalent. A similar statement has previously been demonstrated for the same modulations when the phase ambiguity was assumed to have been perfectly resolved by means other than differential encoding.",2006,0,
563,564,Robust sensor fault estimation for tolerant control of a civil aircraft using sliding modes,This paper proposes a sensor fault tolerant control scheme for a large civil aircraft. It is based on the application of a robust method for sensor fault reconstruction using sliding mode theory. The novelty lies in the application of the sensor fault reconstruction scheme to correct the corrupted measured signals before they are used by the controller and therefore the controller does not need to be reconfigured to adapt to sensor faults,2006,0,
564,565,High level net models: a tool for permutation mapping and fault detection in multistage interconnection network,"This paper aims at structurising the detection of different types of stuck-at faults for a wide range of multistage interconnection networks (MINs). The results reported so far in this respect have been mainly based on direct combinatorial analysis of the concerned networks with very little consideration towards the modelling aspects. Graphical representation coupled with well-defined semantics allowing formal analysis has already established the Petri net as an effective tool for modelling dynamic systems. However, the existing variants of high level nets had certain limitations in modelling the dynamic behaviour of mapping a permutation through the MIN and further analysis of the same. This has inspired the authors to propose a couple of new high level net models, called MP-net and S-net in their earlier works. The S-net model uses tokens to hold and propagate information apart from controlling the firing of events. It uses two different types of places and transitions each as has been defied subsequently. In this paper, we have concentrated on the detection of faults in MINs using this S-net model",2000,0,
565,566,From Fireflies to Fault-Tolerant Swarms of Robots,"One of the essential benefits of swarm robotic systems is redundancy. In case one robot breaks down, another robot can take steps to repair the failed robot or take over the failed robot's task. Although fault tolerance and robustness to individual failures have often been central arguments in favor of swarm robotic systems, few studies have been dedicated to the subject. In this paper, we take inspiration from the synchronized flashing behavior observed in some species of fireflies. We derive a completely decentralized algorithm to detect non-operational robots in a swarm robotic system. Each robot flashes by lighting up its on-board light-emitting diodes (LEDs), and neighboring robots are driven to flash in synchrony. Since robots that are suffering catastrophic failures do not flash periodically, they can be detected by operational robots. We explore the performance of the proposed algorithm both on a real-world swarm robotic system and in simulation. We show that failed robots are detected correctly and in a timely manner, and we show that a system composed of robots with simulated self-repair capabilities can survive relatively high failure rates.",2009,0,
566,567,Implications of Rent's Rule for NoC Design and Its Fault-Tolerance,"Rent's rule is a powerful tool for exploring VLSI design and technology scaling issues. This paper applies the principles of Rent's rule to the analysis of networks-on-chip (NoC). In particular, a bandwidth-version of Rent's rule is derived, and its implications for future NoC scaling examined. Hop-length distributions for Rent's and other traffic models are then applied to analyse NoC router activity. For fault-tolerant design, a new type of router is proposed based on this analysis, and it is evaluated for mutability and its impact on congestion by further use of the hop-length distributions. It is shown that the choice of traffic model has a significant impact on scaling behaviour, design and fault-tolerant analysis",2007,0,
567,568,Analysis on fault voltage and secondary arc current of single phase refusing-shut of the 500kV extra high voltage transmission line,"It is common knowledge that 500 kV extra high voltage and long distant transmission line join a shunt reactor and a neutral grounding via small reactor; This paper analysis systematically an possible condition of the frequency-regulating resonance over-voltage on single phase cut fault to refusing-shut of the 500 kV extra high voltage transmission line which join a shunt reactor, the system compose an complex series resonance circuits, and present a rational mode of reactive compensation. This paper also build rational mathematic mode on systemic parameter of 500 kV ci-yong transmission line, and resolute detailedly its power frequency component, low frequency component and its DC component of single phase cut fault voltage and secondary arc current by the mean of Laplacian transformation ruling formula. All the this is to offer an farther analysis on switching over-voltage and secondary arc current interrupter of long distant transmission line. In the end, this system also implemented using MATLAB software, compute the transient process on single phase cut fault voltage and secondary arc current.",2009,0,
568,569,BOAs: backoff adaptive scheme for task allocation with fault tolerance and uncertainty management,"We propose the backoff adaptive scheme (BOAs) as a new technique for the automatic allocation of tasks amongst a team of heterogeneous mobile robots. It is an optimal, decentralized decision making scheme that utilizes explicit communication between the agents. A structured and unified framework is also proposed for task specification. This scheme is fault tolerant (to robot malfunctions) and allows for uncertainty in the nature of task specification in terms of the actual number of robots required. Team demography may change without the need for the respecification of tasks. The adaptive feature in BOAs further improves the flexibility of the team. Realistic simulations are carried out to verify the effectiveness of the scheme.",2004,0,
569,570,Fault-tolerant vibration control in a networked and embedded rocket fairing system,"Active vibration control using piezoelectric actuators in a networked and embedded environment has been widely applied to solve the rocket fairing vibration problem. However, actuator failures may lead to performance deterioration or system dysfunction. To guarantee the desired system performance, the remaining actuators should be able to coordinate with each other to compensate for the damaging effects caused by the failed actuator in a timely manner. Further, in the networked control environment, timing issues such as sampling jitter and network-induced delay should be considered in the controller design. In this study, a timing compensation approach is implemented in an adaptive actuator failure compensation controller to maintain the fairing system performance by also considering the detrimental effects from real-time constraints. In addition, time-delay compensation in the networked control system is discussed, which is able to reduce damaging effects of network-induced delays.",2004,0,
570,571,Techniques to enable FPGA based reconfigurable fault tolerant space computing,Reconfigurable computing using field programmable gate arrays (FPGAs) offer significant performance improvements over traditional space based processing solutions. The application of commercial-off-the-shelf (COTS) FPGA processing components requires radiation-effect detection and mitigation strategy to compensate for the FPGAs' susceptibility to single event upsets (SEUs) and single event functional interrupts (SEFIs). A reconfigurable computing architecture that uses external triple modular redundancy (TMR) via a radiation-hardened ASIC provides the most robust approach to SEU and SEFI detection and mitigation. Honeywell has designed a TMR Voter ASIC with an integrated FPGA configuration manager that can automatically reconfigure an upset FPGA upon TMR error detection. The automatic configuration manager also has features to support resynchronizing the upset FPGA with the remaining two FPGAs operating in a self checking pair (SCP) mode. Automating and minimizing reconfiguration times and re synchronization times enables high performance FPGA-based processors to provide high system availability with minimal software/system controller intervention,2006,0,
571,572,A Predictive Method for Providing Fault Tolerance in Multi-agent Systems,"The growing importance of multi-agent applications and the need for a higher quality of service in these systems justify the increasing interest in fault-tolerant multi-agent systems. In this article, we propose an original method for providing dependability in multi- agent systems through replication. Our method is different from other works because our research focuses on building an automatic, adaptive and predictive replication policy where critical agents are replicated to avoid failures. This policy is determined by taking into account the criticality of the plans of the agents, which contain the collective and individual behaviors of the agents in the application. The set of replication strategies applied at a given moment to an agent is then fine-tuned gradually by the replication system so as to reflect the dynamicity of the multi-agent system. We report on experiments assessing the efficiency of our approach.",2006,0,
572,573,Design of a fault-tolerant voter for safety related analog inputs,"This paper introduces a voting scheme for safety-related analog input module to arbitrate between the results of redundant channels in fault-tolerant system. The design approach is a distributed system using a sophisticated form of duplication. For each running process, there is a backup process running on a different CPU. The voter is responsible for checkpointing its state to duplex CPUs. In order to increase the dependability for safety-related controllers, the I/O modules use redundancy to reduce the risk associated with relying upon any single component operating flawlessly. The 1oo2D voting principle is commonly used in fault tolerant I/O modules to provide passive redundancy for masking runtime faults at hardware and software levels, respectively. A dual architecture (1oo2D) which provides high safety integrity to a rating of SIL 3 is presented. The outputs from two identical channels operating in parallel with the same inputs are supplied to a voting unit that arbitrates between them to produce an overall output. Based on the hardware logic model and FPGA technique, the study adopts the hardware voter which has much more advantage in the velocity and reliability. Finally, using modelsim simulations, we verify the effectiveness of the proposed voter design in preserving the hazard-free property of the response of an analog inputs module.",2010,0,
573,574,Research on Multi-agent System Model of Diesel Engine Fault Diagnosis by Case-Based Reasoning,"Oil monitoring technology is a useful method in condition monitoring and fault diagnosis for the machine, especially for low-speed, heavy-load, reciprocated and lubricated diesel engine equipment. But it is difficult to implement intelligent diagnosis because monitored information lacks logical relationship in oil monitoring. To solve this problem, the theory and method of case-based reasoning is adopted for the data processing and fault analysis in oil monitoring with a multi-agent system structure. Detailed definitions of agents in the system were proposed, and the multi-agent system framework was established finally. Multi-agent mechanism brings flexible for case based reasoning. It enhances the capability of solving complicated question in new system, and overcomes the shortcoming of the fault knowledge difficult to update in traditional systems",2006,0,
574,575,Predicting and controlling FPGA Device Heat using System monitor and IBERT (internal bit error ratio tester),The aim of this paper is to present a new methodology and the tools used to predict and control the FPGA Device Heat before starting the design. Knowing that the FPGA silicon heat is crucial as they all have a temperature above and under which their functionalities is not longer guaranteed. The silicon temperature is linked to the different options and strategies used to implement the design. Many tools such ldquouse Xpowerrdquo from Xilinx allows the user to have an estimation of the power consumption. This paper will present a primitive called System monitor which is present in every Virtex 5 to monitor the environment around the FPGA. Monitoring the device environment maximises the probability of getting the FPGA work after implementing required design.,2009,0,
575,576,Traveling Wave Fault Location for Power Cables Based on Wavelet Transform,"In this paper, traveling wave fault location equipment for power cables is designed, and the characteristic waveforms of cable fault point broken down and not broken down are simulated respectively. Then a new traveling wave fault location method based on wavelet transform is presented. Wavelet transform have good performance in denoising and singularity detection, which well solved the difficulty in identifying the initial point of the reflected traveling wave because of the local time-frequency characteristic. The fault distance can be calculated by the round-trip times which the traveling wave spends in the cable. The only required parameter is the length of cable. With the method, the result of fault location is not influenced by the change of propagation velocity of traveling wave. The correctness and effectiveness of this method are analyzed by computer simulation. The obtained results show an acceptable degree of accuracy for fault location.",2007,0,
576,577,"A note on inconsistent axioms in Rushby's ""systematic formal verification for fault-tolerant time-triggered algorithms""",We describe some inconsistencies in John Rushby's axiomatization of time-triggered algorithms that he presented in these transactions and that he formally specifies and verifies in the mechanical theorem-prover PVS. We present corrections for these inconsistencies that have been checked for consistency in PVS,2006,0,
577,578,Fault Tolerant Service Composition in Service Overlay Networks,"In a service overlay network, the services provided by different service providers might span multiple Internet domains. A service provider failure may cause significant performance deterioration. Thus, it is desirable to provide fault tolerant service composition solutions such that the service composition can be switched to the backup service composition solution in case of a service provider failure. To provide 100% protection against a single service provider failure, fault tolerant service composition essentially requires to partition service providers into two disjoint sets, each of them can provide a service composition solution. We study a generalized fault tolerant service composition which aims to find two service composition solutions for each request to minimize the number of shared service providers. Subject to such a primary objective, we also aim to minimize the total service composition cost. We firstly prove that the problem is NP-Complete, and formulate the problem as an integer linear program. We then propose heuristic algorithms to efficiently solve the problem. Simulation results demonstrate the effectiveness of the proposed heuristic algorithms.",2008,0,
578,579,Application of Taguchi technique to reduce positional error in two degree of freedom rotary-rotary planar robotic arm,"In present work, positional accuracy of robotic arm has been discussed. The factors considered in the experiment were the length of links, the mass of both links, the velocity of end point and torque on both links. A considerable reduction in performance variation can be obtained by Taguchi technique. Through simple multifactorial experiments on manipulator, controlled factors can be isolated to provide centering and variance control for a process variable. The primary objective in present work is to investigate the effect of process parameter on performance variation to improve positional accuracy. An attempt has been made to introduce a small variation to current approaches broadly called Taguchi parametric design method. In these methods, there are two broad categories of problems associated with simultaneously minimizing performance variations and bringing the mean on target, viz. Type 1- minimizing variations in performance caused by variations in noise factors (uncontrolled parameters); Type 2-minimizing variations in performance caused by variations in control factors (design variables).",2007,0,
579,580,Fault Tolerant Permanent Magnet Motor Drive Topologies for Automotive X-By-Wire Systems,"Future automobiles will be equipped with by-wire systems to improve reliability, safety and performance. The fault tolerant capability of these systems is crucial due to their safety critical nature. Three fault tolerant inverter topologies for permanent magnet brushless dc motor drives suitable for automotive x-by-wire systems are analyzed. A figure of merit taking into account both cost and post-fault performance is developed for these drives. Simulation results of the two most promising topologies for various inverter faults are presented. The drive topology with the highest post-fault performance and cost effectiveness is built and evaluated experimentally.",2008,0,
580,581,Using memory errors to attack a virtual machine,"We present an experimental study showing that soft memory errors can lead to serious security vulnerabilities in Java and .NET virtual machines, or in any system that relies on type-checking of untrusted programs as a protection mechanism. Our attack works by sending to the JVM a Java program that is designed so that almost any memory error in its address space will allow it to take control of the JVM. All conventional Java and .NET virtual machines are vulnerable to this attack. The technique of the attack is broadly applicable against other language-based security schemes such as proof-carrying code. We measured the attack on two commercial Java virtual machines: Sun's and IBM's. We show that a single-bit error in the Java program's data space can be exploited to execute arbitrary code with a probability of about 70%, and multiple-bit errors with a lower probability. Our attack is particularly relevant against smart cards or tamper-resistant computers, where the user has physical access (to the outside of the computer) and can use various means to induce faults; we have successfully used heat. Fortunately, there are some straightforward defenses against this attack.",2003,0,
581,582,Error propagation of the robotic system for liver cancer coagulation therapy,"The goal of this paper is to establish the error propagation model of the ultrasound-guided robot for liver cancer coagulation therapy, which consists of ultrasound machine, image-guided software subsystem, position tracking unit and needle-driven robot. The target of tumor is transformed to robot coordinate frame to let the robot move to the target. The transformation includes three dimension ultrasound construction, registration between pre-operative model and intra-operative physical body, coordinate transformation from position tracking unit to robot. The factors affecting the system accuracy can be expressed by the sum of target mapping error and robot positioning error. Then, the propagation model of target mapping error on the Euclidean motion group is established. At last, the simulations of the propagation model of target mapping error and the experiment of the system accuracy are carried out and the results show our proposed error propagation model is efficient and the system accuracy can satisfy the need of coagulation therapy for liver cancer.",2009,0,
582,583,Evaluating the Performance of Adaptive Fault-Tolerant Routing Algorithms for Wormhole-Switched Mesh Interconnect Networks,"One of the fundamental problems in parallel computing is how to efficiently perform routing in a faulty network each component of which fails with some probability. This paper presents a comparative performance study of ten prominent adaptive fault-tolerant routing algorithms in wormhole-switched 2D mesh interconnect networks. These networks carry a routing scheme suggested by Boppana and Chalasani as an instance of a fault-tolerant method. The suggested scheme is widely used in the literature to achieve high adaptivity and support inter-processor communications in parallel computer systems due to its ability to preserve both communication performance and fault-tolerant demands in these networks. The performance measures studied are the throughput, average message latency and average usage of virtual channels per node. Results obtained through simulation suggest two classes of presented routing schemes as high performance candidate in most faulty networks.",2007,0,
583,584,Analysis of Single-Phase-to-Ground Fault Generated Initial Traveling Waves,"Analysis of fault generated traveling waves is the base to implement traveling waves based protection and fault location. However, the structure at fault point is not asymmetrical under single-phase to ground fault condition in multi-phase power system, so that traveling waves analysis method of single circuit can not be applied. The paper at first analyzes initial traveling wave at fault point generated by the fault through resistance, according to superimposed theory and using phase-to-module transformation method, then considers the fault generated traveling waves' characteristics at the relay point. At last, EMTP is implemented to verify the correctness of analysis of single-phase-to-ground fault generated initial traveling waves",2005,0,
584,585,Real-time correction of distortion image based on FPGA,"Correcting infrared camera distortion is necessary in target tracking and object recognition system. The existent FPGA algorithm didn't utilize sufficiently the advantage of the parallel processing and leaded to the results that a great deal of the system resources were consumed and the running speed was slowed down. The paper analyzed the existing problems such as serial structure in other algorithms, proposed a new parallel algorithm and realized it with the lowest resources. The experiments carried on the chip-virtex5 produced by Xilinx company show that the proposed algorithm has a good real-time performance, use less resource than previous structure and realize the correction of distortion on FPGA on line.",2010,0,
585,586,Executable assertions for detecting data errors in embedded control systems,"In order to be able to tolerate the effects of faults, we must first detect the symptoms of faults, i.e. the errors. This paper evaluates the error detection properties of an error detection scheme based on the concept of executable assertions aiming to detect data errors in internal signals. The mechanisms are evaluated using error injection experiments in an embedded control system. The results show that using the mechanisms allows one to obtain a fairly high detection probability for errors in the areas monitored by the mechanisms. The overall detection probability for errors injected to the monitored signals was 74%, and if only errors causing failure are taken into account we have a detection probability of over 99%. When subjecting the target system to random error injections in the memory areas of the application, i.e., not only the monitored signals, the detection probability for errors that cause failure was 81%",2000,0,
586,587,Simultaneous optimization for wind derivatives based on prediction errors,"Wind power energy has been paid much attention recently for various reasons, and the production of electricity with wind energy has been increasing rapidly for a few decades. In this work, we will propose a new type of weather derivatives based on the prediction errors for wind speeds, and estimate their hedge effect on wind power energy businesses. At first, we will investigate the correlation of prediction errors between the power output and the wind speed in a Japanese wind farm. Then we will develop a methodology that will optimally construct a wind derivative based on the prediction errors using nonparametric regressions. A simultaneous optimization technique of the loss and payoff functions for wind derivatives is demonstrated based on the empirical data.",2008,0,
587,588,An empirical study of fault localization for end-user programmers,"End users develop more software than any other group of programmers, using software authoring devices such as e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been little research on finding ways to help these programmers with the dependability of their software. We have been addressing this problem in several ways, one of which includes supporting end-user debugging activities through fault localization techniques. This paper presents the results of an empirical study conducted in an end-user programming environment to examine the impact of two separate factors in fault localization techniques that affect technique effectiveness. Our results shed new insights into fault localization techniques for end-user programmers and the factors that affect them, with significant implications for the evaluation of those techniques.",2005,0,
588,589,A stochastic model of fault introduction and removal during software development,"Two broad categories of human error occur during software development: (1) development errors made during requirements analysis, design, and coding activities; (2) debugging errors made during attempts to remove faults identified during software inspections and dynamic testing. This paper describes a stochastic model that relates the software failure intensity function to development and debugging error occurrence throughout all software life-cycle phases. Software failure intensity is related to development and debugging errors because data on development and debugging errors are available early in the software life-cycle and can be used to create early predictions of software reliability. Software reliability then becomes a variable which can be controlled up front, viz, as early as possible in the software development life-cycle. The model parameters were derived based on data reported in the open literature. A procedure to account for the impact of influencing factors (e.g., experience, schedule pressure) on the parameters of this stochastic model is suggested. This procedure is based on the success likelihood methodology (SLIM). The stochastic model is then used to study the introduction and removal of faults and to calculate the consequent failure intensity value of a small-software developed using a waterfall software development",2001,0,
589,590,Detecting faults in technical indicator computations for financial market analysis,"Many financial trading and charting software packages provide users with technical indicators to analyze and predict price movements in financial markets. Any computation fault in technical indicator may lead to wrong trading decisions and cause substantial financial losses. Testing is a major software engineering activity to detect computation faults in software. However, there are two problems in testing technical indicators in these software packages. Firstly, the indicator values are updated with real-time market data that cannot be generated arbitrarily. Secondly, technical indicators are computed based on a large amount of market data. Thus, it is extremely difficult, if not impossible, to derive the expected indicator values to check the correctness of the computed indicator values. In this paper, we address the above problems by proposing a new testing technique to detect faults in computation of technical indicators. We show that the proposed technique is effective in detecting computation faults in faulty technical indicators on the MetaTrader 4 Client Terminal.",2010,0,
590,591,Dynamic node management and measure estimation in a state-driven fault injector,"The following topics were dealt with: visual querying and data exploration; graphs and hierarchies; taxonomies, frameworks and methodology; document visualization and collaborative visualization; algorithm visualization; and 3D navigation",2000,0,
591,592,Very high-resistance fault on a 525 kV transmission line - Case study,"This paper analyzes a 300 ohm primary ground fault, which is an unusually high value for a 525 kV transmission line in southeastern Brazil. This case study emphasizes the techniques used by the analysts. Considering that the fault impedance was larger than those usually observed in single-phase faults on extra-high-voltage (EHV) lines, this paper discusses the probable cause of the fault and mentions an analysis technique to evaluate such faults. The protective relaying community lacks information regarding the causes and values of fault resistances to ground on high-voltage (HV) and EHV transmission lines. The objectives of this paper are to stimulate research and contribute to the collection of very high-resistance fault information. The analysis techniques are presented using symmetrical components and fault calculations to arrive at fault parameter values that are very close to the ones provided by protective relays. The performance of the line protection is evaluated for the specific fault conditions, with calculation of the observed impedances and currents. The importance of the ground over- current directional protection on a pilot directional comparison scheme is shown. Speculation on the widespread use of differential protection for transmission lines should stimulate discussions of line protection philosophies and applications. The criteria for the resistive reach setting of the quadrilateral ground distance characteristic are presented to show an evolution of past criteria and to open discussion about the setting limits. The conclusions of this paper highlight the importance of present event report analysis techniques regarding fault calculation software and the need for appropriate settings criteria for the resistive ground distance element threshold. This paper also supports the use of ground directional overcurrent protection with a pilot scheme for HV and EHV transmission line protection, while proposing the widespread use of differential functions f- r transmission lines, even for the most extensive cases.",2009,0,
592,593,Circuit-level modeling of soft errors in integrated circuits,"This paper describes the steps necessary to develop a soft-error methodology that can be used at the circuit-simulation level for accurate nominal soft-error prediction. It addresses the role of device simulations, statistical simulation, analytical soft-error rate (SER) model development, and SER-model calibration. The resulting approach is easily automated and generic enough to be applied to any type of circuit for estimation of the nominal SER.",2005,0,
593,594,Development of a technique for calculation of the influence of generator design on power system balanced fault behaviour,"This paper presents the development of a method for quantitatively determining the potential impact that the design of a single generator may have upon the performance of power system under fault conditions. Initially it is illustrated that the impact that a single generator may have on network fault behaviour is limited by the configuration of the existing network to which the new generator is connected. These constraints are then used to develop a quantitative measure of variability in network-wide fault currents and the subsequent voltage disturbances that can be produced under balanced fault conditions by changing the design of a new generator, irrespective of its point of connection. Finally comparisons with the observed variation in network fault behaviour obtained from the simulation in PSS/E of a realistic 600-bus transmission network are used to demonstrate the technique's apparent effectiveness.",2002,0,
594,595,Fast Enhancement of Validation Test Sets for Improving the Stuck-at Fault Coverage of RTL Circuits,"A digital circuit usually comprises a controller and datapath. The time spent for determining a valid controller behavior to detect a fault usually dominates test generation time. A validation test set is used to verify controller behavior and, hence, it activates various controller behaviors. In this paper, we present a novel methodology wherein the controller behaviors exercised by test sequences in a validation test set are reused for detecting faults in the datapath. A heuristic is used to identify controller behaviors that can justify/propagate pre-computed test vectors/responses of datapath register-transfer level (RTL) modules. Such controller behaviors are said to be <i>compatible</i> with the corresponding precomputed test vectors/responses. The heuristic is fairly accurate, resulting in the detection of a majority of stuck-at faults in the datapath RTL modules. Also, since test generation is performed at the RTL and the controller behavior is predetermined, test generation time is reduced. For microprocessors, if the validation test set consists of instruction sequences then the proposed methodology also generates instruction-level test sequences.",2009,0,
595,596,Fault tolerant error coding and detection using reversible gates,"In recent years, reversible logic has emerged as one of the most important approaches for power optimization with its application in low power CMOS, quantum computing and nanotechnology. Low power circuits implemented using reversible logic that provides single error correction - double error detection (SEC-DED) is proposed in this paper. The design is done using a new 4times4 reversible gate called 'HCG' for implementing hamming error coding and detection circuits. A parity preserving HCG (PPHCG) that preserves the input parity at the output bits is used for achieving fault tolerance for the hamming error coding and detection circuits.",2007,0,
596,597,Design of fault simulation training system for a certain tank,"It is necessary to carry out simulation driving training before forces trainees conduct real vehicle driving training for tanks, which can save training funds and raise the level of military modernization. A fault simulation training system for a certain tank is designed and its hardware and software platform is introduced. The dynamics model of tank in linear motion is derived. The simulation shows that the system has good interaction between the trainees on the tank driving platform and the instructors on the ground master platform. The functions of driving simulation and fault exclusion have been realized initially.",2010,0,
597,598,On errors-in-variables regression with arbitrary covariance and its application to optical flow estimation,"Linear inverse problems in computer vision, including motion estimation, shape fitting and image reconstruction, give rise to parameter estimation problems with highly correlated errors in variables. Established total least squares methods estimate the most likely corrections Acirc and bcirc to a given data matrix [A, b] perturbed by additive Gaussian noise, such that there exists a solution y with [A + Acirc, b +bcirc]y = 0. In practice, regression imposes a more restrictive constraint namely the existence of a solution x with [A + Acirc]x = [b + bcirc]. In addition, more complicated correlations arise canonically from the use of linear filters. We, therefore, propose a maximum likelihood estimator for regression in the general case of arbitrary positive definite covariance matrices. We show that Acirc, bcirc and x can be found simultaneously by the unconstrained minimization of a multivariate polynomial which can, in principle, be carried out by means of a Grobner basis. Results for plane fitting and optical flow computation indicate the superiority of the proposed method.",2008,0,
598,599,Vehicle localization in outdoor woodland environments with sensor fault detection,"This paper describes a 2D localization method for a differential drive mobile vehicle on real forested paths. The mobile vehicle is equipped with two rotary encoders, Crossbow's NAV420CA inertial measurement unit (IMU) and a NAVCOM SF-2050M GPS receiver (used in StarFire-DGPS dual mode). Loosely-coupled multisensor fusion and sensor fault detection issues are discussed as well. An extended Kalman filter (EKF) is used for sensor fusion estimation where a GPS noise pre-filter is used to avoid introducing biased GPS data (affected by multi-path). Normalized innovation squared (NIS) tests are performed when a GPS measurement is incorporated to reject GPS data outliers and keep the consistency of the filter. Finally, experimental results show the performance of the localization system compared to a previously measured ground truth.",2008,0,
599,600,The design and implementation of a microcontroller-based single phase on- line uninterrupted power supply with power factor correction,"In this study, the design and implementation of a microcontroller-based single phase on-line UPS (Uninterrupted Power Supply) with PFC (Power Factor Correction) were made practically. SP-320-24 SMPS (Switch Mode Power Supply) module was used to correct the input power factor. Input power factor value was held at the desired value in uninterrupted power supply topologies. In the realized system, two PIC16F876 were used as microcontroller. One of them was used to generate sinusoidal PWM (Pulse Width Modulation) signals that are used to drive n-channel MOSFETs in push pull inverter and to assure feedback control. Other one was used to control and display units. Harmonics were eliminated and output filter was simplified by using sinusoidal PWM technology.",2009,0,
600,601,Fault-recovery Non-FPGA-based Adaptable Computing System Design,"Reconfigurability with fault-tolerance is one of the most desirable hardware combinations for space computing systems. This paper introduces an adaptable computing architecture that includes random and delay- fault recovering capability for avionics and space applications. A micro-architecture level fault handling and recovering scheme that can immunize random/delay errors is presented as a means of overcoming the limitations of gate-level fault tolerance. The fault- recovery flexible architecture was developed based on a pure-ASIC-based retargetable computing system. The retargetable system also offers sufficient flexibility without employing programmable devices. This adaptable system reasserts different signal patterns for random/delay faults by rerouting micro-operations of the operation that caused the faults. Different sequences of bit-pattern generated by the retargetable system avoid the same faulty situation in high-speed VLSI circuits, while continuously supporting seamless modification and migration of underlying hardware and software after fabrication of retargetable systems.",2007,0,
601,602,Soft errors in Flash-based FPGAs: Analysis methodologies and first results,"The paper presents the development of three different analysis methodologies in order to evaluate soft errors effects in flash-based FPGAs. They are complementary and can be used in different design stages, from the device characterization up to the design sensitiveness estimation. First results are very promising, proving that such methodologies are valid and open new ways of investigation. In particular, we are going to upgrade the experimental setup in order to support higher frequencies (up to 250 MHz) for further characterizing SEE effects. Moreover, a benchmark circuit should be defined in order to correctly predict the expected number of SETs for real circuits, taking into account other side effects, like broadening and logical masking. We expect that from the analysis results we will able to delight suitable hardening techniques that will undergo to both radiation test and prediction analysis.",2009,0,
602,603,"Reliable 3D surface acquisition, registration and validation using statistical error models","We present a complete data acquisition and processing chain for the reliable inspection of industrial parts considering anisotropic noise. Data acquisition is performed with a stripe projection system that was modeled and calibrated using photogrammetric techniques. Covariance matrices are attached individually to points during 3D coordinate computation. Different datasets are registered using a new multi-view registration technique. In the validation step, the registered datasets are compared with the CAD model to verify that the measured part meets its specification. While previous methods have only considered the geometrical discrepancies between the sensed part and its CAD model, we also consider statistical information to decide whether the differences are significant",2001,0,
603,604,Measuring application error rates for network processors,"Faults in computer systems can occur due to a variety of reasons. In many systems, an error has a binary effect, i.e. the output is either correct or it is incorrect. However, networking applications exhibit different properties. For example, although a portion of the code behaves incorrectly due to a fault, the application can still work correctly. Integrity of a network system is often unchanged during faults. Therefore, measuring the effects of faults on the network processor applications require new measurement metrics to be developed. In this paper, we highlight essential application properties and data structures that can be used to measure the error behavior of network processors. Using these metrics, we study the error behavior of seven representative networking applications under different cache access fault probabilities.",2004,0,
604,605,Study on the Features of Loudspeaker Sound Faults,"In this paper, the short-time Fourier transformation (STFT) is adopted to transform the loudspeaker sound signal. By STFT, the one-dimensional loudspeaker response signal is converted into two-dimensional time-frequency figure. Then, the figure is decomposed into a number of areas according to its harmonics distribution. The peak and mean values of every area are computed. Through observation and calculation, the features of loudspeaker defects are found. According to the experiment, this method is very effective and universal for different types of loudspeakers.",2009,0,
605,606,A Cellular Approach to Fault Detection and Recovery in Wireless Sensor Networks,"In the past few years wireless sensor networks have received a greater interest in application such as disaster management, border protection, combat field reconnaissance and security surveillance. Sensor nodes are expected to operate autonomously in unattended environments and potentially in large numbers. Failures are inevitable in wireless sensor networks due to inhospitable environment and unattended deployment. The data communication and various network operations cause energy depletion in sensor nodes and therefore, it is common for sensor nodes to exhaust its energy completely and stop operating. This may cause connectivity and data loss. Therefore, it is necessary that network failures are detected in advance and appropriate measures are taken to sustain network operation. In this paper we extend our cellular architecture and proposed a new mechanism to sustain network operation in the event of failure cause of energy-drained nodes. In our solution the network is partitioned into a virtual grid of cells to perform fault detection and recovery locally with minimum energy consumption. Specifically, the grid based architecture permits the implementation of fault detection and recovery in a distributed manner and allows the failure report to be forwarded across cells. The proposed failure detection and recovery algorithm has been compared with some existing related work and proven to be more energy efficient.",2009,0,
606,607,Engineering knowledge-based condition analyzers for on-board intelligent fault classification: A case study,"In this paper we describe the design of a knowledge-based condition analyzer that performs on-board intelligent fault classification. The system is designed to be deployed as a prototype on E414 locomotives, a series of downgraded highspeed vehicles that are currently employed in standard passenger service. Our goal is to satisfy the requirements of a development scenario in the Integrail project for a condition analyzer that leverages an ontology-based description of some critical E414 subsystems in order to classify faults considering mission and safety related aspects.",2008,0,
607,608,The Dangers of Failure Masking in Fault-Tolerant Software: Aspects of a Recent In-Flight Upset Event,"On 1 August 2005, a Boeing Company 777-200 aircraft, operating on an international passenger flight from Australia to Malaysia, was involved in a significant upset event while flying on autopilot. The Australian Transport Safety Bureau's investigation into the event discovered that ""an anomaly existed in the component software hierarchy that allowed inputs from a known faulty accelerometer to be processed by the air data inertial reference unit (ADIRU) and used by the primary flight computer, autopilot and other aircraft systems."" This anomaly had existed in original ADIRU software, and had not been detected in the testing and certification process for the unit. This paper describes the software aspects of the incident in detail, and suggests possible implications concerning complex, safety- critical, fault-tolerant software.",2007,0,
608,609,A fault line selection algorithm in non-solidly earthed network based on holospectrum,"The methods of line selection today focus on one target of the signal such as amplitude, frequency or phase. A novel method based on holospectrum algorithm to detect single-phase faults in distribution systems is proposed in this paper. After structuring analytic signals of zero sequence current and voltage, the holospectrum algorithm is applied. Thus the analysis of combined signal of amplitude, frequency and phase is realized. Compared with the use of single amplitude, frequency or phase, combined signal carries more details and information of transient signal. Theoretical analysis and simulation based on Simulink of MATLAB show that the presented method can exactly and effectively choose the faulty line in single-phase-to-ground fault.",2010,0,
609,610,Design of Integrated Fault Diagnostic System (FDS),"Early diagnosis of plant faults/deviations is a critical factor for optimized and safe plant operation. Although smart controllers and diagnosis systems are available and widely used in chemical plants, however, some faults couldn't be detected. Major reason is the lack of learning techniques that can learn from operational running data and previous abnormal cases. In addition, operator and maintenance engineer opinions and observations are not well used, and useful diagnosis knowledge is ignored. Providing link between operation management, maintenance management and fault diagnostic and monitoring systems will enable closing such gap where diagnostic and monitoring results can be used more effectively for real time operation support, and optimized plant maintenance. In addition, operation and maintenance findings and discovered knowledge can be used effectively for plant condition monitoring. This research work presents the framework and mechanism for such integrated fault diagnostic system, which is called FDS. The proposed idea will support operation and maintenance planning as well as overall plant safety",2006,0,
610,611,Develop on feed-forward real time compensation control system for movement error in CNC machining,"A theory model of feed-forward compensation controlling system is constructed by the method of precision compensation. A feed-forward compensation hardware control system is designed to MCS51CPU as the core and structure of compensation data processing program. Established components of linear contour error mathematical model, thus determine the amount of feed-forward compensation algorithm. CNC x-y experiment platform simulation results indicate that this design can effectively eliminate the phase lag and amplitude errors of the computer numerical control (CNC) system, and improve the general CNC machining accuracy on the part contour.",2010,0,
611,612,Non-inductive variable reactor design and computer simulation of rectifier type superconducting fault current limiter,"A rectifier type superconducting fault current limiter with noninductive reactor has been proposed by the authors. The concept behind this SFCL is that the high impedance generated during superconducting to normal state of the trigger coil limits the fault current. In the hybrid bridge circuit of the SFCL, two superconducting coils: a trigger coil and a limiting coil are connected in anti-parallel. Both the coils are magnetically coupled with each other and could have the same value of self inductance so that they can share the line current equally. At fault time when the trigger coil current reaches a certain level, the trigger coil changes from superconducting state to normal state. This super to normal transition of the trigger coil changes the current ratio of the coils and therefore the flux inside the reactor is no longer zero. So, the equivalent impedance of both the coils is increased and limits the fault current. We have carried out computer simulation using PSCAD/EMTDC and observed the results. Both the simulation and preliminary experiment shows good results. The advantage of using hybrid bridge circuit is that the SFCL can also be used as circuit breaker.",2005,0,
612,613,An FMO based error resilience method in H.264/AVC and its UEP application in DVB-H link layer,"Flexible Macroblock Ordering (FMO) is one of the new error resilience tools introduced in H264/AVC. Several slice grouping methods have been studied for improving error robustness using FMO. In this paper, a simple and fast slice grouping method for inter frames is introduced. Fast mode decision and early Skip Mode decision are applied for the first encoding pass, and only the features that are available at the stage of early Skip Mode decision are used for the classification. The computation time cost can be reduced by about 50% on average compared to traditional methods. The proposed scheme is tested under the proposed Unequal Error Protection scheme at the DVB-H link layer. The results are compared to the standard MPE-FEC EEP scheme using traditional FMO type `interleaved' at the DVB-H link layer. It is shown that the proposed scheme can provide improved error robustness for high error rate channels in a DVB-H system.",2010,0,
613,614,A PH complex control system built-in correction factor,"Besides the pH deployment process's non-linear, large hysteretic nature, the system's requirement of real-time and accuracy, the traditional control methods can not get to the high quality control results. The fuzzy control does not rely on a mathematical model of the object. It is very difficult to eliminate the steady-state deviation from the root. Because PI control has a very good scavenging effect of the steady-state, therefore the system uses a built-correction factor of the Fuzzy-PI composite control strategy.",2010,0,
614,615,Methodology to support laser-localized soft defects on analog and mixed-mode advanced ICs,"The soft defect localization on analog or mixed-mode ICs is becoming more and more challenging due to their increasing complexity and integration. New techniques based on dynamic laser stimulation are promising for analog and mixedmode ICs. Unfortunately, the considerable intrinsic sensitivity of this kind of devices under laser stimulation makes the defect localization results complex to analyze. As a matter of fact, the laser sensitivity mapping contains not only abnormal sensitive regions but also naturally sensitive ones. In order to overcome this issue by extracting the abnormal spots and therefore localize the defect, we propose in this paper a methodology that can improve the FA efficiency and accuracy. It consists on combining the mapping results with the electrical simulation of laser stimulation impact on the device. First, we will present the concept of the methodology. Then, we will show one case study on a mixed-mode IC illustrating the soft defect localization by using laser mapping technique & standard electrical simulations. Furthermore, we will argument the interest of a new methodology and we will show two simple examples from our experiments to validate it.",2009,0,
615,616,Empirical evaluation of the fault-detection effectiveness of smoke regression test cases for GUI-based software,"Daily builds and smoke regression tests have become popular quality assurance mechanisms to detect defects early during software development and maintenance. In previous work, we addressed a major weakness of current smoke regression testing techniques, i.e., their lack of ability to automatically (re)test graphical user interface (GUI) event interactions - we presented a GUI smoke regression testing process called daily automated regression tester (DART). We have deployed DART and have found several interesting characteristics of GUI smoke tests that we empirically demonstrate in this paper. We also combine smoke tests with different types of test oracles and present guidelines for practitioners to help them generate and execute the most effective combinations of test-case length and test oracle complexity. Our experimental subjects consist of four GUI-based applications. We generate 5000-8000 smoke tests (enough to be run in one night) for each application. Our results show that: (1) short GUI smoke tests with certain test oracles are effective at detecting a large number of faults; (2) there are classes of faults that our smoke test cannot detect; (3) short smoke tests execute a large percentage of code; and (4) the entire smoke testing process is feasible to do in terms of execution time and storage space.",2004,0,
616,617,Automatically translating dynamic fault trees into dynamic Bayesian networks by means of a software tool,"This paper presents a software tool allowing the automatic analysis of a dynamic fault tree (DFT) exploiting its conversion to a dynamic Bayesian network (DBN). First, the architecture of the tool is described, together with the rules implemented in the tool, to convert dynamic gates in DBNs. Then, the tool is tested on a case of system: its DFT model and the corresponding DBN are provided and analyzed by means of the tool. The obtained unreliability results are compared with those returned by other tools, in order to verify their correctness.",2006,0,
617,618,Average Error Performance of M-ary Modulation Schemes in Nakagami-q (Hoyt) Fading Channels,"Presented are exact-form expressions for the average error performance of various coherent, differentially coherent, and noncoherent modulation schemes in Nakagami-q (Hoyt) fading channels. The expressions are given in terms of the Lauricella hypergeometric function, F<sub>D</sub> <sup>(n)</sup>; for nges1, which can be evaluated numerically using its integral or converging series representation. It is shown that the derived expressions reduce to some existing results for Rayleigh fading as special cases",2007,0,
618,619,Analysis of the soft error effects on CAN network controller,"In this article, the effects of the single event upset on a Controller Area Network (CAN) controller and its effects on the network is being evaluated. The experiment is done using SINJECT fault injection tool in a simulation based environment. Three mail modules of the controller are used in three independent set of experiments in one of the CAN controllers of the network. The results show that the main cause of the network failure is the bit stream processor. 6.7% of the injected faults in the bit stream processor led to the network failure. On the other hand, the registers sub-module of the controller showed to be most fault tolerant. The experiment showed that 0.3% of the faults in the registers module results in network failure, and the bit timing module is responsible for the failure of the whole network in 3.2% of the injected single event upset faults.",2010,0,
619,620,Modeling of cable fault system,"Modeling is the essential part of implementing the prediction and location of three-phase cable fault. To predict and locate cable fault, a model of three-phase cable fault system is constructed based on a great deal of measured validation data by choosing BP neural network that has nonlinear characteristic and using the unproved BP algorithm, Levenberg-Marquardt data-optimized method. It is shown by the simulation using MATLAB software that the parameters of the model converge rapidly, and the simulated output of the neural network model and the measured output of cable fault system are approximately equal, and the mean value of the relatively predictive error of the fault distance is smaller than 0.3%, so that the model quality is reliable.",2004,0,
620,621,Asymmetries in soft-error rates in a large cluster system,"Early in the deployment of the ASC Q cluster supercomputer system, an unexpectedly high rate of soft errors were observed in the board-level cache subsystems of the constituent AlphaServer ES45 systems that make up the compute component of this large cluster. A series of tests and experiments was undertaken to validate the hypothesis that this frequency was consistent with the high level of terrestrial secondary cosmic-ray neutron flux resulting from the high elevation of its installation site. The overall success of this effort is reported elsewhere in this issue. This paper reports on three secondary phenomena that were observed during these tests and experiments: Error logs were collected from all servers during a representative period and examined for nonrandom event rates, which would indicate a systematic cause. The only significant result of this exploration was the discovery of a latent soft-error discovery effect, and a self-shielding effect, whereby the servers positioned physically higher in their racks suffered disproportionately higher soft-error rates. This excess was examined and found to be consistent with established shielding effect of the high-Z composition of the constituents of the overlying systems. Experiments with individual ES45 systems in an artificial neutron beam at the Los Alamos Neutron Science Center facility have established that the soft-error rates observed in the SRAM parts is significantly dependent on the incident direction of the neutrons in the beam. These asymmetries could be exploited as part of a strategy for mitigating the frequency of soft errors in future computer systems.",2005,0,
621,622,Sandra - A New Concept for Management of Fault Isolation in Aircraft Systems,"The embedded Fault Isolation functionality in the Saab JAS39 Gripen aircraft has been designed to accurately and reliably provide the technician with proposed maintenance procedures. A previously identified drawback and built in limitation has been the significant lead time for Fault Isolation functional changes based on aircraft operational statistics and line experience. With the Fault Isolation executing as compiled source code, changes and corrections require adaptation of the regular onboard systems computer software and careful planning of code and documentation releases, implying not only significant delays, but also high costs for necessary updates. The ""Sandra"" project aims at even further refine - and to introduce a state of the art - fault isolation maintenance concept for the Saab JAS39 Gripen aircraft. Based on an easy-to-use PC based graphical tool, Fault Isolation on dedicated aircraft monitoring and safety check result data is specified. Output in the form of design documentation artifacts, such as flowcharts and technical publications, is generated. The contained Fault Isolation object data is updated in parallel with the regular onboard computer software development process and the corresponding Loadable Data File will be delivered when convenient. The PC application constitutes the maintenance engineer's primary Fault Isolation design tool. The tool enables the maintenance engineer to select dedicated settings via a graphical user interface and use logical expressions to propose detailed and specific maintenance actions to be performed by the aircraft technician. The tool is capable of verifying a complete set of design documents towards the content of a generated loadable file. Thus, a generated output file with a minimum of additional verification can be delivered to be loaded into the aircraft. This new approach implies that the lead time for a Fault Isolation functional change can be reduced by as much as 80 %. The cost for the corresponding- functional change will decrease by more than 50 %.",2007,0,
622,623,Development of simulation model based on directed fault propagation graph,"A new method of simulation model is presented in this paper in order to deal with system based fault mode and effect analysis model in modern complex system with large structure. Directed fault propagation graph model based on fault influence degree is proposed and fault propagation model is put forward. With the definition of direct fault propagation influence degree and indirect fault propagation influence degree is introduced, the algorithm of propagation and search method for fault propagation model is discussed. Visualization simulation system based on directed fault propagation graph is developed with object oriented method according to the proposed fault analysis model. The Simulation system can used for fault propagation analysis and fault influence of exist complex system, simulation result can be validated and verified by control area network platform, the method is useful for fault diagnosis and analysis model in modern large complex system.",2010,0,
623,624,Research on Analyse and Compensation Approach Aimed at CNC Machine Geometrical and Kinematic Errors,"The geometrical error and kinematic error are regarded as the prime reasons to bring about CNC machine contour error, which confine the improvement of machining precision further. In the paper, the main error sources that produce geometrical error and kinematic error in CNC machine are researched in detail, and an analyse approach aimed at the main error sources is put forward which adopts the ""arc interpolation motion - arc image method"". Furthermore, the compensation approach aimed at error resources such as the mismatch of position loop servo gains and orthogonal axes out of the vertical is developed in CNC software. Finally, the analyse and compensation approach is tested on a CNC experiment table. The experimentation result reveals that the developed analyse and compensation approach aimed at the main error sources can enhance machine contour precision greatly. Consequently, the research is helpful to improve and keep CNC machine high precision long-time.",2009,0,
624,625,Hybrid Error Concealment with Automatic Error Detection for Transmitted MPEG-2 Video Streams over Wireless Communication Network,"This work presents a complete error concealment system, for overcoming visible distortions in video sequences which are transmitted over a lossy communication network. The system we propose provides an error concealment solution from the point of receiving the transmitted sequence by the decoder, until it is presented to viewers, without human interference. The system is composed of an automatic error detection algorithm, and a decision tree error concealment algorithm. The performance of the detection algorithm is estimated, along with a performance evaluation of the decision tree algorithm by comparing it to the other three error concealment methods. The results are evaluated using two quality measures. We show that our error concealment method achieves the highest quality compared to the other methods for most of the conducted tests.",2006,0,
625,626,On the Distribution of Software Faults,"The Pareto principle is often used to describe how faults in large software systems are distributed over modules. A recent paper by Andersson and Runeson again confirmed the Pareto principle of fault distribution. In this paper, we show that the distribution of software faults can be more precisely described as the Weibull distribution.",2008,0,
626,627,Accelerating learning from experience: avoiding defects faster,"All programmers learn from experience. A few are rather fast at it and learn to avoid repeating mistakes after once or twice. Others are slower and repeat mistakes hundreds of times. Most programmers' behavior falls somewhere in between: They reliably learn from their mistakes, but the process is slow and tedious. The probability of making a structurally similar mistake again decreases slightly during each of some dozen repetitions. Because of this a programmer often takes years to learn a certain rule-positive or negative-about his or her behavior. As a result, programmers might turn to the personal software process (PSP) to help decrease mistakes. We show how to accelerate this process of learning from mistakes for an individual programmer, no matter whether learning is currently fast, slow, or very slow, through defect logging and defect data analysis (DLDA) techniques",2001,0,
627,628,Towards high-precision lens distortion correction,"This paper points out and attempts to remedy a serious discrepancy in results obtained by global calibration methods: The re-projection error can be rendered very small by these methods, but we show that the optical distortion correction is far less accurate. This discrepancy can only be explained by internal error compensations in the global methods that leave undetected the inadequacy of the distortion model. This fact led us to design a model-free distortion correction method where the distortion can be any image domain diffeomorphism. The obtained precision compares favorably to the distortion given by state of the art global calibration and reaches a RMSE of 0.08 pixels. Nonetheless, we also show that this accuracy can still be improved.",2010,0,
628,629,Research on code pattern automata-based code error pattern automatic detection technique,"Nowadays, many defects, e.g., obscure error generation-scenario and lacking of formalization which is the basis for the automatic error detection, exist in field of code error research. Furthermore, the automation of error detection will greatly affect the quality and efficiency of software testing. Therefore, more deeply research on code errors need to be done. At first, this paper presents the definition of code error pattern based on definition of pattern. Secondly, it investigates the formalization description of code error pattern. Then, it studies the automatic error pattern detecting technique based on non-determinate finite state automata and treats the matching technique of error pattern as the key problem. Finally, some case studies are given. The preliminary results show the rationality of code error pattern definition and the effectiveness of error pattern formalization description and error pattern matching technique.",2009,0,
629,630,Multichamber Tunable Liquid Microlenses with Active Aberration Correction,"A design approach and new manufacturing technique for a novel type of stacked fluidic multi-chamber tunable lenses is presented. The design offers flexibility and extensibility, leading to fully functional miniature tunable optical lens systems with the ability for low order aberration control.",2009,0,
630,631,A New Fault-Information Model for Adaptive & Minimal Routing in 3-D Meshes,"In this paper, we rewrite the minimal-connected-component (MCC) model in 2-D meshes in a fully-distributed manner without using global information so that not only can the existence of a Manhattan-distance-path be ensured at the source, but also such a path can be formed by routing-decisions made at intermediate nodes along the path. We propose the MCC model in 3-D meshes, and extend the corresponding routing in 2-D meshes to 3-D meshes. We consider the positions of source & destination when the new faulty components are constructed. Specifically, all faulty nodes will be contained in some disjoint fault-components, and a healthy node will be included in a faulty component only if using it in the routing will definitely cause a non-minimal routing-path. A distributed process is provided to collect & distribute MCC information to a limited number of nodes along so-called boundaries. Moreover, a sufficient & necessary condition is provided for the existence of a Manhattan-distance-path in the presence of our faulty components. As a result, only the routing having a Manhattan-distance-path will be activated at the source, and its success can be guaranteed by using the information of boundary in routing-decisions at the intermediate nodes. The results of our Monte-Carlo-estimate show substantial improvement of the new fault-information model in the percentage of successful Manhattan-routing conducted in 3-D meshes.",2008,0,
631,632,Current fault management trends in NASA's planetary spacecraft,"Fault management for today's space missions is a complex problem, going well beyond the typical safing requirements of simpler missions. Recent missions have experienced technical issues late in the project lifecycle, associated with the development and test of fault management capabilities, resulting in both project schedule delays and cost overruns. Symptoms seem to become exaggerated in the context of deep space and planetary missions, most likely due to the need for increased autonomy and the limited communications opportunities with Earth-bound operators. These issues are expected to cause increasing challenges as the spacecraft envisioned for future missions become more capable and complex. In recognition of the importance of addressing this problem, the Discovery and New Frontiers Program Office hosted a Fault Management Workshop on behalf of NASA's Science Mission Directorate, Planetary Science Division, to bring together experts in fault management from across NASA, DoD, industry and academia. The scope of the workshop was focused on deep space and planetary robotic missions, with full recognition of the relevance of, and subsequent benefit to, Earth-orbiting missions. Three workshop breakout sessions focused the discussions to target three topics: 1) fault management architectures, 2) fault management verification and validation, and 3) fault management development practices, processes and tools. The key product of this three-day workshop is a NASA White Paper that documents lessons learned from previous missions, recommended best practices, and future opportunities for investments in the fault management domain. This paper summarizes the findings and recommendations that are captured in the white paper.",2009,0,
632,633,Extraction of Tectonic Faults of Longmen Mountain Based on DEM,"According to the analysis of the tectonic characteristics of thrust belt in the Longmen Mountain, the present study aims to build a methodology to extract liner fault structures in the study area. The methodology is an approach which includes automatic extraction of major faults based on combined calculation of landform factors from the SRTM-DEM and revision of the automatic extraction result according to remote sensing images and geologic data. Therein, these landform factors including elevation, slope, aspect and variation of aspect, slope of slope (SOS) and slope of aspect (SOA). The compound method, including the spatial analysis techniques based on SRTM-DEM, interpretation of remote sensing images, and some geosciences' researches, provides strong technical support to achieve the quantization of the morphotectonics research.",2009,0,
633,634,"A Comparison of Cascading Horizontal and Vertical Menus with Overlapping and Traditional Designs in Terms of Effectiveness, Error Rate and user Satisfaction","In this study, effectiveness, efficiency and user satisfaction of different menu designs were investigated. 24 graduate students voluntarily participated to the study. The results indicate that horizontal menus are more effective than vertical menus in terms of selecting sub menu items, overall task completion time is not related to menu design, horizontal overlapping menu design is the most effective one in terms of preventing user errors. Lastly, user satisfaction doesn't vary according to menu designs.",2007,0,
634,635,A new method in reducing the overcurrent protection response times at high fault currents to protect equipment from extended stress,"This paper describes a new method for protecting power equipment from extended stresses during high fault current conditions. This was achieved using a universal protection device with a software platform that can facilitate designing time-current characteristic (TCC) curves of different shapes, all in the same hardware. When combined, recloser control and relay response times are faster and more accurate than with conventional means. Reduced device response times are achieved by combining different overcurrent TCCs. A coordination example is presented for a typical distribution system loop scheme containing new multifunction relays and reclosers. The advantages of using integrated device functions over standard overcurrent relays and recloser controls are illustrated. A comparative analysis is presented to quantify the reduction in let-thru I<sup>2</sup>t values and equipment stress that can be realized using this method during high fault current conditions",2001,0,
635,636,Fault-tolerance for exascale systems,"Periodic, coordinated, checkpointing to disk is the most prevalent fault tolerance method used in modern large-scale, capability class, high-performance computing (HPC) systems. Previous work has shown that as the system grows in size, the inherent synchronization of coordinated checkpoint/restart (CR) limits application scalability; at large node counts the application spends most of its time checkpointing instead of executing useful work. Furthermore, a single component failure forces an application restart from the last correct checkpoint. Suggested alternatives to coordinated CR include uncoordinated CR with message logging, redundant computation, and RAID-inspired, in-memory distributed checkpointing schemes. Each of these alternatives have differing overheads that are dependent on both the scale and communication characteristics of the application. In this work, using the Structural Simulation Toolkit (SST) simulator, we compare the performance characteristics of each of these resilience methods for a number of HPC application patterns on a number of proposed exascale machines. The result of this work provides valuable guidance on the most efficient resilience methods for exascale systems.",2010,0,
636,637,Detection and correction of abnormal pixels in Hyperion images,"Hyperion images are currently processed to level 1a (from level 0 or raw data). These level 1a images are files of radiometrically corrected data in units of either watts/(sr  micron  m<sup>2</sup>)  40 for VNIR bands or watts/(sr  micron  m<sup>2</sup>)  80 for SWIR bands. Each distributed Hyperion level 1a image tape contains a log file, called ""(EO-1 identifier).fix.log"", that reports the bad or corrupted pixels (called known bad pixels) found during the pre-flight checking, and details how they were fixed. All bad pixels should be corrected in a level 1a image. However, bad pixels are still evident. In addition, there are dark vertical stripes in the image that are not reported in the log file. In this paper, we introduce a method to detect and correct the bad pixels and vertical stripes (we will refer to these occurrences as abnormal pixels). Images from the Greater Victoria Watershed and other EVEOSD test sites are used to determine how stationary the locations of the abnormal pixels are. After abnormal pixel correction a Hyperion image is ready for geometric correction, atmospheric correction, and further analysis.",2002,0,
637,638,Fault diagnosis for transformer based on fuzzy entropy,"Power transformers are one of the key equipments of the power system, so it is valuable to discover the incipient fault timely and truly. Code deficiency exists in the gas ratio method by the IEC/DEEE standard and fault diagnosis for power transformers. A model based on fuzzy entropy for power transformer faults diagnosis is put forward, which expand coding bound of original IEC three-ratio. At the same time, the method has some contain fault ability in a certain degree. It also shows the probability and disposes lost or false power transformer fault symptoms. That shows the validity of the method for power transformer fault diagnosis by dissolved gas-in-oil analysis.",2007,0,
638,639,Enhanced Fault Ride-Through Method for Wind Farms Connected to the Grid Through VSC-Based HVDC Transmission,"This paper describes a new control approach for secure fault-ride through of wind farms connected to the grid through a voltage source converter-based high voltage DC transmission. On fault occurrence in the high voltage grid, the proposed control initiates a controlled voltage drop in the wind farm grid to achieve a fast power reduction. In this way overvoltages in the DC transmission link can be avoided. It uses controlled demagnetization to achieve a fast voltage reduction without producing the typical generator short circuit currents and the related electrical and mechanical stress to the wind turbines and the converter. The method is compared to other recent FRT methods for HVDC systems and its superior performance is demonstrated by simulation results.",2009,0,
639,640,Error Resilient Video Coding Using B Pictures in H.264,"Since the quality of compressed video is vulnerable to errors, video transmission over unreliable Internet is very challenging today. Multi-hypothesis motion-compensated prediction (MHMCP) has been shown to have error resilience capability for video transmission, where each macroblock is predicted by a linear combination of multiple signals (hypotheses). B picture prediction is a special case of MHMCP. In H.264/AVC, the prediction of B pictures is generalized such that both of the two predictions can be selected from the past pictures or from the subsequent pictures. The multiple reference picture framework in H.264/AVC also allows previously decoded B pictures to be used as references for B picture coding. In this paper, we will discuss the error resilience characteristics of the generalized B pictures in H.264/AVC. Three prediction patterns of B pictures are analyzed in terms of their error-suppressing abilities. Both theoretical models (picture level error propagation) and simulation results are given for the comparison.",2009,0,
640,641,Managing Post-Development Fault Removal,"In this paper, we manage fault removal by classifying and prioritizing fault warnings reported by a static analysis tool. We present our findings from analyzing three cross-platform industrial code bases at Yahoo! totaling approximately 3.6+ MLOC. The tool found 1.2K potential fault warnings as follows: 52.29% true faults and 47.71% false/noise. The 52.29% correctly reported faults were prioritized based on severity. Additionally, we connected the tool classification to a standard software weakness schema, Common Weakness Enumeration (CWE) to standardized discourse. The results from creating a management system for post-development fault removal are intended to be shifted back into earlier stages of software development.",2009,0,
641,642,A Probabilistic Method for Aligning and Merging Range Images with Anisotropic Error Distribution,"This paper describes a probabilistic method of aligning and merging range images. We formulate these issues as problems of estimating the maximum likelihood. By examining the error distribution of a range finder, we model it as a normal distribution along the line of sight. To align range images, our method estimates the parameters based on the expectation maximization (EM) approach. By assuming the error model, the algorithm is implemented as an extension of the iterative closest point (ICP) method. For merging range images, our method computes the signed distances by finding the distances of maximum likelihood. Since our proposed method uses multiple correspondences for each vertex of the range images, errors after aligning and merging range images are less than those of earlier methods that use one-to-one correspondences. Finally, we tested and validated the efficiency of our method by simulation and on real range images.",2006,0,
642,643,New resonance type Fault Current Limiter,"This paper proposes a new parallel LC resonance type Fault Current Limiter (FCL). This structure has low cast because of using dry capacitor and non-superconducting inductor and fast operation. The proposed FCL is able to limit fault current in constant value near to pre-fault condition value against series resonance type FCL. In this way, the voltage of point of common coupling (PCC) will not change during fault. Analytical analysis is presented in detail and simulation results are involved to validate the effectiveness of this structure.",2010,0,
643,644,ConvexFit: an optimal minimum-error convex fitting and smoothing algorithm with application to gate-sizing,"Convex optimization has gained popularity due to its capability to reach global optimum in a reasonable amount of time. Convexity is often ensured by fitting the table data into analytically convex forms such as posynomials. However, fitting the look-up tables into the posynomial forms with minimum error itself may not be a convex optimization problem and hence excessive fitting errors may be introduced. In this paper, we propose to directly adjust the look-up table values into a numerically convex look-up table without explicit analytical form. We show that numerically ""convexifying"" the table data with minimum perturbation can be formulated as a convex semidefinite optimization problem and hence optimality can be reached in polynomial time. Without an explicit form limitation, we find that the fitting error is significantly reduced while the convexity is still ensured. As a result, convex optimization algorithms can still be applied. Furthermore, we also develop a ""smoothing"" algorithm to make the table data smooth and convex to facilitate the optimization process. Results from extensive experiments on industrial cell libraries demonstrate that our method reduces 30 fitting error over a well-developed posynomial fitting algorithm. Its application to circuit tuning is also presented.",2005,0,
644,645,Stereoscopic video error concealment for missing frame recovery using disparity-based frame difference projection,"At low bit-rate video communications, packet loss may easily cause whole-frame loss that, in return, leads to annoying frame drop phenomenon. In this paper, a novel error concealment algorithm is specifically developed for stereoscopic video, called the disparity-based frame difference projection (DFDP), to recover the lost frames at the decoder. The proposed DFDP contains three key components: 1) change detection; 2) disparity estimation; and 3) frame difference projection, which exploits both the intra-view frame difference from one view and interview correlation to estimate the lost frame in another view. The change region computed on the correctly received frame will be used to predict the change region between current missing frame and its previous frame through the estimated disparity, which is the summation of the estimated global disparity and the estimated local disparity. Experimental results have shown that the proposed stereoscopic video error concealment method can effectively restore the lost frames at the decoder and deliver attractive performance, in terms of objective measurement (in peak signal-to-noise ratio) and subjective visual quality.",2009,0,
645,646,Fault-tolerant static scheduling for grids,"While fault-tolerance is desirable for grid applications because of the distributed and dynamic nature of grid resources, it has seldom been considered in static scheduling. We present a fault-tolerant static scheduler for grid applications that uses task duplication and combines the advantages of static scheduling, namely no overhead for the fault-free case, and of dynamic scheduling, namely low overhead in case of a fault. We also give preliminary experimental results on our scheme.",2008,0,
646,647,Charge sharing and interaction depth corrections in a wide energy range for small pixel pitch CZT detectors,"The CSTD project aims at developing a high resolution pixel gamma detector based on CdZnTe for Compton imaging applications. Our research group has been working recently on the design and characterization of a new pixel detector with specifications focused at high energy SPECT for medical imaging applications. The detector pitch, 0.3 mm, and its thickness, 5 mm, allows to reach high spatial resolution and high detector efficiency. Non-ideal performance appears with more strength in small pixel pitch CdZnTe detectors, below 1 mm, affecting at the spectroscopic results. In order to recuperate the shared charge, the customized ASIC simultaneously collects the charge in the triggering pixel and its eight neighboring pixels per event. The detector design, readout electronics, acquisition software and data analysis have been completed at CIEMAT. Data has been taken by irradiating the CdZnTe detector with high and low energy gamma-ray sources. The high energy events of the <sup>137</sup>Cs source suffer from a great proportion of charge sharing in the neighboring pixels. Two <sup>137</sup>Cs spectra, with and without energy correction, are shown and compared. To obtain the corrected spectra offline, the collected charge at the neighboring pixels is added to the trigger pixel collected charge. The corrected spectra show that the 662 keV photopeak is reconstructed. Interaction depth correction follows to improve the energy resolution by data segmentation of the 662 keV energy peak according to fifty cathode to pixel ratios. The computed interaction depth correction profile is the inverse of the charge collection efficiency. Energy resolution can be improved discarding the segmented data which do not achieve an acceptable energy resolution. Several interaction depth correction profiles at 81, 356 and 662 keV are shown and reveal a second correlation between the charge collecting efficiency and the collecting energy.",2010,0,
647,648,Automatic Diagnosis of Defects of Rolling Element Bearings Based on Computational Intelligence Techniques,"This paper presents a method, based on classification techniques, for automatic detection and diagnosis of defects of rolling element bearings. We used vibration signals recorded by four accelerometers on a mechanical device including rolling element bearings: the signals were collected both with all faultless bearings and after substituting one faultless bearing with an artificially damaged one. We considered four defects and, for one of them, three severity levels. In all the experiments performed on the vibration signals represented in the frequency domain we achieved a classification accuracy higher than 99%, thus proving the high sensitivity of our method to different types of defects and to different degrees of fault severity. We also assessed the degree of robustness of our method to noise by analyzing how the classification performance varies on variation of the signal-to-noise ratio and using statistical classifiers and neural networks. We achieved very good levels of robustness.",2009,0,
648,649,Cloud Model-Based Security-Aware and Fault-Tolerant Job Scheduling for Computing Grid,"The uncertainties of grid nodes security are main hurdle to make the job scheduling secure, reliable and fault-tolerant. The fixed fault-tolerant strategy in jobs scheduling may utilize excessive resources. In this paper, the job scheduling decides which kinds of fault-tolerance strategy will be applied to each individual job for more reliable computation and shorter makespan. And we discuss the fuzziness or uncertainties between TL and SD attributes by the subjective judgment of human beings. Cloud model is a model of the uncertain transition between qualitative concept and its quantitative representation. Based on the cloud model, We propose a security-aware and fault-tolerant jobs scheduling strategy for grid (SAFT), which makes the assess of SD and SL to become more flexible and more reliable. Meanwhile, the different fault-tolerant strategy has been applied in grid job scheduling algorithm by the SD and job workload. Moreover, much more important, we are able to set up some rules and active each qualitative rule to select a suitable fault-tolerant strategy for a scheduling job by input value (the SD and job workload) to realize the uncertainty reasoning. The results demonstrate that our algorithm has shorter makespan and more excellent efficiencies on improving the job failure rate than the fixed fault-tolerant strategy selection.",2010,0,
649,650,SWIFT: software implemented fault tolerance,"To improve performance and reduce power, processor designers employ advances that shrink feature sizes, lower voltage levels, reduce noise margins, and increase clock rates. However, these advances make processors more susceptible to transient faults that can affect correctness. While reliable systems typically employ hardware techniques to address soft-errors, software techniques can provide a lower-cost and more flexible alternative. This paper presents a novel, software-only, transient-fault-detection technique, called SWIFT. SWIFT efficiently manages redundancy by reclaiming unused instruction-level resources present during the execution of most programs. SWIFT also provides a high level of protection and performance with an enhanced control-flow checking mechanism. We evaluate an implementation of SWIFT on an Itanium 2 which demonstrates exceptional fault coverage with a reasonable performance cost. Compared to the best known single-threaded approach utilizing an ECC memory system, SWIFT demonstrates a 51% average speedup.",2005,0,
650,651,On-line diagnosis of interconnect faults in FPGA-based systems,"This paper presents an on-line diagnosis approach for locating the interconnect faults in field programmable gate arrays (FPGAs)-based systems. The diagnosis proposed approach consists of two phases. Phase one is locating the faulty tile through partitioning the FPGA-based system into self-checking tiles. The faulty tile can be detected concurrently with the normal system operation. This operation is performed prior to scheduling and allocating the circuit. The proposed partitioning approach was applied on certain circuits as a case study, and has been implemented using Xilinx foundation CAD tool with FPGA chip XC4010. The simulation study proved that our partitioning scheme reduces the test complexity and produces lower overheads. Upon locating a faulty tile and by the aid of a proposed path-list file per tile created during the routing process, the second phase of the diagnosis approach is applied only on the utilized interconnection of that tile for locating the faulty wires and switches. Therefore, the diagnosis approach is considered to be simplified.",2004,0,
651,652,Application of Empirical Bayes Estimation in Error Model Identification of Two Orthometric Accelerometers,"The high accuracy accelerometer can be demarcated in multiposition tumbling experiment under 1g gravitational field. The g<sup>2</sup> observation model of Two Orthometric Accelerometers can eliminate the corner error. there is a serious multicollinearity exit in this system because some model coefficient mix together. In view of above question, this article has given the arithmetic of Empirical Bayes Estimation(EB), and applied this method in model which is mentioned above. The result of the simulation and the experiment shows that compared with the conventional least squares method and the generalized diagonal ridge estimation, the Empirical Bayes Estimation can overcome the influence of the multicollinearity and can separate two coefficients which are The Second-Order Terms and the cross-coupling terms.",2010,0,
652,653,Multi Gigabit Transceiver Configuration RAM Fault Injection Response,"High performance processing and memory systems require enormous amounts of I/O bandwidth. Wide parallel bus architectures have reached their practical limits for high bandwidth transport. High speed serial interfaces that support 10's of Gbps are now displacing wide shared bus architectures for many systems. Xilinx FPGAs serial links support this transition by providing more than 10 Gbps in their multi gigabit transceiver (MGT) I/Os. For space applications, these links are susceptible to single event effects (SEE). Many of these effects are due to upsets in the FPGAs configuration RAM that control the many features and functions of the I/O. This paper details the functional effects of configuration RAM upsets in Xilinx MGTs. These effects are realized by injecting upsets in the FPGA configuration RAM while monitoring MGT functional operation. Configuration RAM upset effects are described and functional upset rates due to configuration RAM upsets are calculated for an example orbit. The results of this work provide insight into the on-orbit upset rate and effects of Xilinx multigigabit transceivers",2005,0,
653,654,Statistical software debugging: From bug predictors to the main causes of failure,"Detecting latent errors is a key challenging issue in the software testing process. Latent errors could be best detected by bug predictors. A bug predictor manifests the effect of a bug on the program execution state. The aim has been to find the smallest reasonable subset of the bug predictors, manifesting all possible bugs within a program. In this paper, a new algorithm for finding the smallest subset of bug predictors is presented. The algorithm, firstly, applies a LASSO method to detect program predicates which have relatively higher effect on the termination status of the program. Then, a ridge regression method is applied to select a subset of the detected predicates as independent representatives of all the program predicates. Program control and data dependency graphs can be best applied to find the causes of bugs represented by the selected bug predictors. Our proposed approach has been evaluated on two well-known test suites. The experimental results demonstrate the effectiveness and accuracy of the proposed approach.",2009,0,
654,655,ERCOT's experience in identifying parameter and topology errors using State Estimator,"The State Estimator is an important tool that ERCOT relies on to monitor the real time state of power grid. As the parameter and topology errors are critical to the quality of state estimator results, operations engineers in ERCOT are using multiple tools to detect and identify the topology and parameter errors in ERCOT EMS network model. This paper will present ERCOT experience in detecting and identifying the topology and parameter errors using state estimator monitoring tools and by analyzing SE results.",2010,0,
655,656,Performance of fault-tolerant distributed shared memory on broadcast- and switch-based architectures,"This paper presents a set of distributed-shared-memory protocols that provide fault tolerance on broadcast-based and switch-based architectures with no decrease in performance. These augmented DSM protocols combine the data duplication required by fault tolerance with the data duplication that naturally results in distributed-shared-memory implementations. The recovery memory at each backup node is continuously maintained consistent and is accessible by all processes executing at the backup node. Simulation results show that the additional data duplication necessary to create fault-tolerant DSM causes no reduction in system performance during normal operation and eliminates most of the overhead at checkpoint creation. Data blocks which are duplicated to maintain the recovery memory are also utilized by the DSM protocol, reducing network traffic, and increasing the processor utilization significantly. We use simulation and multiprocessor address trace files to compare the performance of a broadcast architecture called the SOME-Bus to the performance of two representative switch architectures.",2005,0,
656,657,"Image steganalysis based on moments of characteristic functions using wavelet decomposition, prediction-error image, and neural network","In this paper, a general blind image steganalysis system is proposed, in which the statistical moments of characteristic functions of the prediction-error image, the test image, and their wavelet subbands are selected as features. Artificial neural network is utilized as the classifier. The performance of the proposed steganalysis system is significantly superior to the prior arts.",2005,0,
657,658,Supporting fault tolerance in a data-intensive computing middleware,"Over the last 2-3 years, the importance of data-intensive computing has increasingly been recognized, closely coupled with the emergence and popularity of map-reduce for developing this class of applications. Besides programmability and ease of parallelization, fault tolerance is clearly important for data-intensive applications, because of their long running nature, and because of the potential for using a large number of nodes for processing massive amounts of data. Fault-tolerance has been an important attribute of map-reduce as well in its Hadoop implementation, where it is based on replication of data in the file system. Two important goals in supporting fault-tolerance are low overheads and efficient recovery. With these goals, this paper describes a different approach for enabling data-intensive computing with fault-tolerance. Our approach is based on an API for developing data-intensive computations that is a variation of map-reduce, and it involves an explicit programmer-declared reduction object. We show how more efficient fault-tolerance support can be developed using this API. Particularly, as the reduction object represents the state of the computation on a node, we can periodically cache the reduction object from every node at another location and use it to support failure-recovery. We have extensively evaluated our approach using two data-intensive applications. Our results show that the overheads of our scheme are extremely low, and our system outperforms Hadoop both in absence and presence of failures.",2010,0,
658,659,Enhancing Motion Picture Lens Performance by Digital Calibration and Correction,"To some degree, all lenses used by the motion picture industry exhibit certain distortions which can detract from the ideal viewing experience. This paper presents a lens calibration and correction system which enables these problems to be resolved digitally in post production.  For some time lenses such as the Cook 4i and now 5i provide the necessary metadata on focus and aperture settings to enable digital post corrections to be applied. Cameras such the Alexa and RED are capable of capturing this metadata. Many other lenses however may be adapted to provide the necessary metadata by means of a simple encoder. The paper presents how this is achieved followed by a presentation of the digital correction system for enhancing such scenarios as extreme focus pulls. Using digital high definition camera systems the calibration of each individual lens is presented along with the automatic derivation of the required lens database. The full post production work flow through to final image generation is presented.",2010,0,
659,660,The study of fault diagnosis in rotating machinery,"This project presents a detail review of the subject fault diagnosis; feature extraction, dimensionality reduction and fault classification are being discussed. This project focuses on the faulty bearing which mainly caused by mass imbalance and axis misalignment. By analyzing the vibration signal obtained from the test rigs (rigs that are built to demonstrate the effect of faults in rotating machinery), it gives solid information concerning any faults within the rotating machinery.",2009,0,
660,661,Multiobjective Optimization for HTS Fault-Current Limiters Based on Normalized Simulated Annealing,"This paper presents an improved simulated annealing (SA) algorithm for multiobjective optimization, which is a positive approach in the design of high-temperature superconducting (HTS) fault-current limiters (SFCLs).The main goal of this paper is to achieve an effective and feasible approach in the structural design of HTS FCLs by means of multiobjective decision-making techniques, based on normalized SA. The combination of electrical and thermal models of a purpose-designed resistive-type HTS FCL is defined as a component in PSCAD/EMTDC simulations from which the proposed method will be used to optimize the selective parameters of the SFCL. The above requires the need of advanced numerical techniques for simulation studies by PSCAD on a sample distribution system for determining a global optimum HTS FCL, by considering individual parameters and accounting for the constraints, which is the main motivation for initiating this paper.",2009,0,
661,662,A Multi-agent System for Complex Vehicle Fault Diagnostics and Health Monitoring,"This paper presents a multi-agent system(MAS_VFD&HM) developed for complex vehicle fault diagnosis and health monitoring. The MAS_VFD&HM consists of signal diagnostic agents, special case agents, and a vehicle diagnostic/monitoring agent. A signal agent is responsible for the fault diagnosis or monitoring of one particular signal using either a single signal or multiple signals depending on the complexity of signal faults. Special case agents are those trained to detect specific component faults. All these agents are autonomous and report their results to the Vehicle System Agent. A computational framework is presented for agent learning and agent operation. The proposed MAS_VFD&HM is scalable, versatile, and has the capability of dealing complex problems such as multiple faults in a vehicle system. Although our focus was on the automotive diagnostics, the proposed MAS_VFD&HM is applicable to complex engineering diagnostic problems beyond vehicles.",2010,0,
662,663,Channel capacity and average error rates in generalised-K fading channels,"In the present study, the performance of digital communication systems operating over a composite fading channel modelled by the generalised-<i>K</i> distribution is analysed and evaluated. Novel closed-form expressions for the outage performance, the average bit error probabilities of several modulation schemes and the channel capacity under four different adaptive transmission schemes are derived. The analytical expressions are used to investigate the impact of different fading parameters of this composite fading channel model on the average bit error rate performance for a variety of digital modulation schemes and the spectral efficiency of different adaptive transmission policies.",2010,0,
663,664,Stochastic change detection based on an active fault diagnosis approach,The focus in this paper is on stochastic change detection applied in connection with active fault diagnosis (AFD). An auxiliary input signal is applied in AFD. This signal injection in the system will in general allow to obtain a fast change detection/isolation by considering the output or an error output from the system. The classical CUSUM (cumulative sum) method will be modified such that it will be able to detect change in the signature from the auxiliary input signal in the (error) output signal. It will be shown how it is possible to apply both the gain as well as the phase change of the output vector in the CUSUM test.,2007,0,
664,665,Identification of faulted section in TCSC transmission line based on DC component measurement,"This paper presents an analysis of possibility of detection of a fault position with respect to the compensating bank in a series compensating transmission line. The algorithm designed for this purpose is based on determining the contents of dc components in the distance relay input currents. Fuzzy logic technique is applied for making the decision whether a fault is in front of the compensating bank or behind it. The delivered algorithm has been tested and evaluated with use of the fault data obtained from versatile ATP-EMTP simulations of faults in the test power network containing the 400 kV, 300 km transmission line, compensated with the aid of TCSC (Thyristor Controlled Series Capacitor) bank installed at mid-line. The results of the evaluation are reported and discussed.",2009,0,
665,666,Research on Web-Based Multi-Agent System for Aeroengine Fault Diagnosis,"On the analysis of current state of aeroengine remote diagnosis, collaborative mechanism based on multi-agent was introduced to overcome the obstacles of conventional remote fault diagnosis. The model of aeroengine remote collaborative diagnosis based on multi-agent was put forward on analysis of the positional relationship of all agents in the collaborative environment and the relationship between collaborative agents and roles in the course of collaboration. Some key technologies such as coordination mechanism, task assignment mechanism, agent interaction mechanism, case-based reasoning (CBR) in treatment agent, and the analytic hierarchy process (AHP) in decision analysis were discussed and specific methods of realization were given concretely. Based on these, a Web-based prototype system for aeroengine fault diagnosis was developed on the JADE (Java Agent DEvelopment Framework) platform. The process of system implementation and a case example of fault diagnosis were presented to illustrate and prove the proposed system's applicability. Running results show the feasibility and reliability of the framework, which will be helpful to integrate the aeroengine diagnosis knowledge, improve the diagnosis efficiency effectively and decrease the aeroengine diagnosis cost remarkably.",2008,0,
666,667,A perceptual Sensitivity Based Redundant Slices Coding Scheme for Error-Resilient Transmission H.264/AVC Video,"In this paper, redundant slices feature of the H.264/AVC codec is evaluated. In order to trade off compression efficiency and error robustness of a H.264/AVC codec with redundant slice capability, we propose a novel perceptual sensitivity based redundant slices coding scheme. The perceptually sensitive regions are determined by using a simple yet effective perceptual sensitivity analysis technique, which analyzes both the motion and the texture structures in the original video sequence. The experimental results show that our proposed algorithm can remarkably improve the reconstructed video quality in the packet lossy network",2006,0,
667,668,Application of methods of 3D surface reconstruction for characterization of pitting defects,In this paper a possibility of application of two different methods of pitting visualization is discussed.,2009,0,
668,669,A State Machine for Detecting C/C++ Memory Faults,"Memory faults are major forms of software bugs that severely threaten system availability and security in C/C++ program. Many tools and techniques are available to check memory faults, but few provide systematic full-scale research and quantitative analysis. Furthermore, most of them produce high noise ratio of warning messages that require many human hours to review and eliminate false-positive alarms. And thus, they cannot locate the root causes of memory faults precisely. This paper provides an innovative state machine to check memory faults, which has three main contributions. Firstly, five concise formulas describing memory faults are given to make the mechanism of the state machine simple and flexible. Secondly, the state machine has the ability to locate the cause roots of the memory faults. Finally, a case study applying to an embedded software, which is written in 50 thousand lines of C codes, shows it can provide useful data to evaluate the reliability and quality of software",2005,0,
669,670,A different view of fault prediction,"We investigated a different mode of using the prediction model to identify the files associated with a fixed percentage of the faults. The tester could ask the tool to identify which files are likely to contain the bulks of faults, with the tester selecting any desired percentage of faults. Again the tool would return a list ordered in decreasing order of the predicted numbers of faults in the files the model expects to be most problematic. If the number of files identified is too large, the tester could reselect a smaller percentage of faults. This would make the number of files requiring particular scrutiny manageable. We expect both modes to be valuable to professional software testers and developers.",2005,0,
670,671,Insulation fault detection in a PWM controlled induction motor-experimental design and preliminary results,"To investigate feature extraction methods for early detection of insulation degradation in low voltage (under 600 V), 3-phase, PWM controlled induction motors, a series of seeded fault tests was planned on a 50 HP, 440 V motor. In this paper, the background and rationale for the test plan are described. The instrumentation and test plan are then detailed. Finally, preliminary test experiences are related",2000,0,
671,672,Clinic: A Service Oriented Approach for Fault Tolerance in Wireless Sensor Networks,"With the size and complexity of modern Wireless Sensor Networks (WSNs) systems, a system's ability to recover from faults is becoming more important. A self-healing system is one that has the capability to recover from faults without human intervention during execution. Since WSNs are inherently fault-prone and since their on-site maintenance is infeasible, scalable self-healing is crucial for enabling the deployment of large-scale sensor network applications. Previous work has typically dealt with single faults in isolation, has imposed constraints on systems, or required new protocol elements. In this paper, we attempt to solve some of these problems through the use of service-oriented architecture. We propose a service-oriented self-healing approach, called Clinic, that works with existing network components, e.g. routing protocols, and resources without adding extra overhead on the network. In Clinic, different network capabilities are viewed as services of the network instead of being isolated capabilities of individual nodes. This view of the network promotes collaboration among nodes and information reuse by sharing information collected by one service with other network services. Preliminary evaluation showed that Clinic achieved fault tolerance while keeping low communication overhead by reusing only the information collected by other network services to heal from faults.",2010,0,
672,673,Developing Fault Injection Environment for Complex Experiments,"The paper addresses the problem of creating a comprehensive fault injection environment, which integrates and improves various simulation and supplementary functions. This is illustrated with experimental results.",2008,0,
673,674,Neural network methods for error canceling in human-machine manipulation,"A neural network technique is employed to cancel hand motion error during microsurgery. A cascade-correlation neural network trained via extended Kalman filtering was tested on 15 recordings of hand movement collected from 4 surgeons. The neural network was trained to output the surgeon's desired motion, suppressing erroneous components. In experiments this technique reduced the root mean square error (rmse) of the erroneous motion by an average of 39.5%. This was 9.6% greater than the reduction achieved in earlier work, which followed the complementary approach of estimating the error rather than the desired component. Preliminary results are also presented from tests in which training and testing data were taken from different surgeons.",2001,0,
674,675,Exploration of beam fault scenarios for the Spallation Neutron Source target,"The Spallation Neutron Source (SNS) accelerator systems will provide a 1 GeV, 1.44 MW proton beam to a liquid mercury target for neutron production. In order to ensure adequate lifetime of the target system components, requirements on several beam parameters must be maintained. A series of error studies was performed to explore credible fault scenarios which can potentially violate the various beam-on-target parameters. The response of the beam-on-target parameters to errors associated with the phase-space painting process in the ring and field setpoint errors in all the ring-to-target beam transport line elements were explored and will be presented. The plan for ensuring beam-on-target parameters will also be described.",2003,0,
675,676,On undetectable faults in partial scan circuits,We provide a definition of undetectable faults in partial scan circuits under a test application scheme where a test consists of primary input vectors applied at-speed between scan operations. We also provide sufficient conditions for a fault to be undetectable under this test application scheme. We present experimental results on finite-state machine benchmarks to demonstrate the effectiveness of these conditions in identifying undetectable faults.,2002,0,
676,677,Analytical Modeling Approach to Detect Magnet Defects in Permanent-Magnet Brushless Motors,The paper presents a novel approach to detect magnet faults such as local demagnetization in brushless permanent-magnet motors. We have developed a new form of analytical model that solves the Laplacian/quasi-Poissonian field equations in the machine's air-gap and magnet element regions. We verified the model by using finite-element software in which demagnetization faults were simulated and electromotive force was calculated as a function of rotor position. We then introduced the numerical data of electromotive force into a gradient-based algorithm that uses the analytical model to locate demagnetized regions in the magnet as simulated in the finite-element package. The fast and accurate convergence of the algorithm makes the model useful in magnet fault diagnostics.,2008,0,
677,678,Error-rate analysis for multirate DS-CDMA transmission schemes,"We analyze and compare the error performance of a dual-rate direct-sequence code-division multiple-access (DS-CDMA) system using multicode (MCD) and variable-spreading gain (VSG) transmission in the uplink. Specifically, we present two sets of results. First, we consider an ideal additive white Gaussian noise channel. We show that the bit-error rate (BER) of VSG users is slightly lower than that of MCD users if the number of low-rate interferers is smaller than a specific threshold. Otherwise, they exhibit similar error performance. Second, we look at multipath fading channels. We show that with diversity RAKE reception, the VSG user suffers from a larger interference power than the MCD user if the channel delay spread is small. The reverse is true for a large delay spread. However, a larger interference power in this case does not necessarily lead to higher error probability. Essentially, our results for both cases show that: 1) in addition to the signal-to-interference ratio (SIR), the difference in error performance between the two systems strongly depends on the distributions of multiple-access and multipath interference; 2) for practical cellular communications, performances for both systems are expected to be similar most of the time.",2003,0,
678,679,An AS-DSP for forward error correction applications,"An application specific digital signal processor for channel coding is presented. The vector operations can improve both the performance of memory accesses and program code density. The special function units and datapaths for channel decoding accelerate the decoding speed and facilitate algorithm implementation. The processor had been fabricated in a 0.18 m CMOS 1P6M technology. The chip size is 7.73 mm<sup>2</sup> including 18k bits embedded memory, and the power consumption is 141 mW while decoding Reed-Solomon code and convolutional code. In contrast with general purpose processor designs, the results show this chip has at least 50% improvement in code density and 66% data rate enhancement.",2005,0,
679,680,Compensation of inertia error in brake dynamometer testing,"Loss in terms of windage and bearing friction is an important origin of inertia error to be compensated in brake dynamometer testing, acquisition of which has always been a troublesome problem. An indirect method of loss measurement using speed data under null pipeline pressure is described in this paper. Mathematical model of resistance torque or energy loss is calculated by regression of collected speed data using SPSS software. Error compensation of two inertia simulating methods, torque control method and energy compensation method, is discussed. Experiments of the former are conducted on NT11 brake dynamometer, which proves it to be effective in eliminating inertia error.",2009,0,
680,681,Rate-Distortion Optimal Video Transport Over IP with Bit Errors,"In this paper we propose a method for video delivery over bit error channels. In particular, we propose a rate distortion optimal method for slicing and unequal error protection (UEP) of packets over bit error channels. The proposed method performs full frame based search using a novel dynamic programming approach to determine the optimal slicing configuration in a practically short time. Also we propose a rate and distortion estimation technique that decreases the time to evaluate the objective function for a slice configuration. The proposed method can perform rate-distortion UEP that can be used over forward error correction (FEC) capable channels. We show that the proposed method successfully exploit the local dynamics of a video frame and perform more than 1 dB better than common methods.",2006,0,
681,682,Fault-Tolerant Overlay Protocol Network,"Voice over Internet Protocol (VoIP) and other time critical communications require a level of availability much higher than the typical transport network supporting traditional data communications. These critical command and control channels must continue to operate and remain available in the presence of an attack or other network disruption. Even disruptions of short duration can severely damage, degrade, or drop a VoIP connection. Routing protocols in use today can dynamically adjust for a changing network topology. However, they generally cannot converge quickly enough to continue an existing voice connection. As packet switching technologies continue to erode traditional circuit switching applications, some methodology or protocol must be developed that can support these traditional requirements over a packet-based infrastructure. We propose the use of a modified overlay tunneling network and associated routing protocols called the fault tolerant overlay protocol (FTOP) network. This network is entirely logical; the supporting routing protocol may be greatly simplified due to the overlays's ability to appear fully connected. Therefore, ensuring confidentiality and availability are much simpler using traditional cryptographic isolation and VPN technologies. Empirical results show for substrate networks, convergence time may be as high as six to ten minutes. However, the FTOP overlay network has been shown to converge in a fraction of a second, yielding an observed two order of magnitude convergence time improvement. This unique ability enhances availability of critical network services allowing operation in the face of substrate network disruption caused by malicious attack or other failure",2006,0,
682,683,Adaptive error protection for Scalable Video Coding extension of H.264/AVC,"This paper presents an adaptive error protection method which provides different packet correction capacities by using only one Reed-Solomon code. The proposed method can be applied separately for each data part in a bit stream. The adaption of the error correction capacity works on-the-fly and only based on the way of data interleaving. In this work, the error protection is applied unequally to data units in the Network Abstraction Layer (NAL) of the Scalable Video Coding (SVC) extension of H.264/AVC. Simulation results show that the video quality increases 6 dB in average with the total overhead of ca. 9%. The advantage of our method is the simpleness and flexibility to apply. Therefore, it is suitable for real-time streaming applications.",2008,0,
683,684,Power factor correction and efficiency investigation of AC-DC converters using forced commutation techniques,"In this paper the power factor and the efficiency of a suggested AC-DC converter topology is studied via Mathlab/Simulink simulation. This converter topology consists of four MOSFET elements in bridge form and: a) two antiparallel IGBT elements between the bridge and the AC grid, b) one MOSFET element between the bridge and the DC load. These switching elements control the conduction time intervals of the bridge by a hysteresis current controller in order to achieve an AC current waveform in phase with the AC voltage as well as a very low content of higher harmonics. This way the values of the power factor and the efficiency become very high (e.g. 0,98... 0,99).",2005,0,
684,685,The Application of Safety Simulation Technology in the Fault Diagnosis of the Chemical Process,"With the development of information and computational technology, the safety simulation technique is becoming more and more useful in the chemical process hazard assessment, hazard identification, and safety control system design and operating personnel training etc.The fault diagnosis of the gravity water tank is studied by using dynamic simulation of HYSYS (Hyprotech System for Engineers). The simulation results presents the method need not design problem-specific observer to estimate unmeasured state variables, and can identification and diagnosis faults simultaneously as well. The parameters of the chemical process are updated via on-line correction.",2008,0,
685,686,On the relation between design contracts and errors: a software development strategy,"When designing a software module or system, a systems engineer must consider and differentiate between how the system responds to external and internal errors. External errors cannot be eliminated and must be tolerated by the system, while the number of internal errors should be minimized and the resulting faults should be detected and removed. This paper presents a development strategy based on design contracts and a case study of an industrial project in which the strategy was successfully applied. The goal of the strategy is to minimize the number of internal errors during the development of a software system while accommodating external errors. A distinction is made between weak and strong contracts. These two types of contracts are applicable to external and internal errors, respectively. According to the strategy, strong contracts should be applied initially to promote the correctness of the system. Before releasing, the contracts governing external interfaces should be weakened and error management of external errors enabled. This transformation of a strong contract to a weak one is harmless to client modules",2002,0,
686,687,Defect control methods for SIMOX SOI wafer manufacture and processing,"The layered structure of thin film silicon-on-insulator (SOI) wafers introduces new considerations for defect detection, particularly for optical metrology tools used to characterize and control SOI wafer processing. Multi-layer interference, as well as subsurface features of the material, can complicate the detection of surface defects. Non-particle defect types which scatter light, such as mounds, pits (including so-called HF defects), and slip lines, can be efficiently detected and classified with advanced operating modes of state-of-the art optical metrology tools. Such capabilities facilitate improvements in the wafer manufacturing process, and result in improved defect detection capabilities and material quality. This work describes defect characterization of SIMOX-SOI wafers using the KLA-Tencor Surfscan 6420 and SP1<sup>TBI</sup>",2000,0,
687,688,Usage of Weibull and other models for software faults prediction in AXE,There are several families for software quality prediction techniques in development projects. All of them can be classified in several subfamilies. Each of these techniques has its own distinctive feature and it may not give correct prediction of quality for a scenario different from the one for which the technique was designed. All these techniques for software quality prediction are dispersed. One of them is statistical and probabilistic technique. The paper deals with software quality prediction techniques in development projects. Four different models based on statistical and probabilistic approach is presented and evaluated for prediction of software faults in very large development projects.,2008,0,
688,689,Fault Emulation for Dependability Evaluation of VLSI Systems,"Advances in semiconductor technologies are greatly increasing the likelihood of fault occurrence in deep-submicrometer manufactured VLSI systems. The dependability assessment of VLSI critical systems is a hot topic that requires further research. Field-programmable gate arrays (FPGAs) have been recently pro posed as a means for speeding-up the fault injection process in VLSI systems models (fault emulation) and for reducing the cost of fixing any error due to their applicability in the first steps of the development cycle. However, only a reduced set of fault models, mainly stuck-at and bit-flip, have been considered in fault emulation approaches. This paper describes the procedures to inject a wide set of faults representative of deep-submicrometer technology, like stuck-at, bit-flip, pulse, indetermination, stuck-open, delay, short, open-line, and bridging, using the best suitable FPGA- based technique. This paper also sets some basic guidelines for comparing VLSI systems in terms of their availability and safety, which is mandatory in mission and safety critical application contexts. This represents a step forward in the dependability benchmarking of VLSI systems and towards the definition of a framework for their evaluation and comparison in terms of performance, power consumption, and dependability.",2008,0,
689,690,Delay Constraint Error Control Protocol for Real-Time Video Communication,"Real-time video communication over wireless channels is subject to information loss since wireless links are error-prone and susceptible to noise. Popular wireless link-layer protocols, such as retransmission (ARQ) based 802.11 and hybrid ARQ methods provide some level of reliability while largely ignoring the latency issue which is critical for real-time applications. Therefore, they suffer from low throughput (under high-error rates) and large waiting-times leading to serious degradation of video playback quality. In this paper, we develop an analytical framework for video communication which captures the behavior of real-time video traffic at the wireless link-layer while taking into consideration both reliability and latency conditions. Using this framework, we introduce a delay constraint packet embedded error control (DC-PEEC) protocol for wireless link-layer. DC-PEEC ensures reliable and rapid delivery of video packets by employing various channel codes to minimize fluctuations in throughput and provide timely arrival of video. In addition to theoretically analyzing DC-PEEC, the performance of the proposed scheme is analyzed by simulating real-time video communication over ldquorealrdquo channel traces collected on 802.11 b WLANs using H.264/AVC JM14.0 video codec. The experimental results demonstrate performance gains of 5-10 dB for different real-time video scenarios.",2009,0,
690,691,Evolutionary design of lifting scheme wavelet-packet adaptive filters for elevator fault detection,"An evolutionary-based procedure for designing adaptive filters based on second-generation wavelet (lifting scheme) packet decomposition for industrial fault detection is presented. The proposed procedure is validated by an experimental case study for induction motor fault diagnosis in an elevator system. Preliminary results on two typologies of faults, broken rotor bars and static air gap eccentricity, are discussed by showing encouraging performance.",2010,0,
691,692,Job Migration and Fault Tolerance in SLA-Aware Resource Management Systems,"Contractually fixed service quality levels are mandatory prerequisites for attracting the commercial user to Grid environments. Service level agreements (SLAs) are powerful instruments for describing obligations and expectations in such a business relationship. At the level of local resource management systems, checkpointing and restart is an important instrument for realizing fault tolerance and SLA- awareness. This paper highlights the concepts of migrating such checkpoint datasets to achieve the goal of SLA- compliant job execution.",2008,0,
692,693,A particle swarm optimization approach for automatic diagnosis of PMSM stator fault,"Permanent magnet synchronous motors (PMSM) are frequently used to high performance applications. Accurate diagnosis of small faults can significantly improve system availability and reliability. This paper proposes a new scheme for the automatic diagnosis of interturn short circuit faults in PMSM stator windings. Both the fault location and fault severity are identified using a particle swarm optimization (PSO) algorithm. The performance of the motor under the fault conditions is simulated through lumped-parameter models. Waveforms of the machine phase currents are monitored, based on which a fitness function is formulated and PSO is used to identify the fault location and fault size. The proposed method is simulated in MATLAB environment. Simulation results provide preliminary verification of the diagnosis scheme",2006,0,
693,694,Efficient techniques for reducing error latency in on-line periodic BIST,"With transient and intermittent operational faults becoming a dominant failure mode in modern digital systems, the deployment of on-line test technology is becoming a major design objective. On-line periodic BIST is a testing method for the detection of operational faults in digital systems. The method applies a near-minimal deterministic test sequence periodically to the circuit under test and checks the circuit responses to detect the existence of operational faults. On-line periodic BIST is characterized by full error coverage, bounded error latency, moderate space and time redundancy. In this paper, we present various techniques to minimize the error latency without sacrificing the full error coverage. These techniques are primarily based on the reordering the test vectors or the selective repetition of test vectors. Our analytical and preliminary experimental results demonstrate that our techniques lead to a significant reduction in the error latency.",2009,0,
694,695,A multi-path routing protocol with fault tolerance in mobile ad hoc networks,"In recent years many researches have focused on ad-hoc networks, mainly because of their independence to any specific structure. These networks suffers from frequent and rapid topology changes that cause many challenges in their routing. Most of the routing protocols try to find a path between source and destination nodes because any path will expire, offer a short period, the path reconstruction may cause the network inefficiency. The proposed protocol build two paths between source and destination and create backup paths during the route reply process, route maintenance process and local recovery process in order to improve the data transfer and the fault tolerance. The protocol performance is demonstrated by using the simulation results obtain from the global mobile simulation software(Glomosim). The experimental results show that this protocol can decrease the packet loss ratio rather than DSR and SMR and it is useful for the applications that need a high level of reliability.",2009,0,
695,696,Cleansing Test Suites from Coincidental Correctness to Enhance Fault-Localization,"Researchers have argued that for failure to be observed the following three conditions must be met: 1) the defect is executed, 2) the program has transitioned into an infectious state, and 3) the infection has propagated to the output. Coincidental correctness arises when the program produces the correct output, while conditions 1) and 2) are met but not 3). In previous work, we showed that coincidental correctness is prevalent and demonstrated that it is a safety reducing factor for coverage-based fault localization. This work aims at cleansing test suites from coincidental correctness to enhance fault localization. Specifically, given a test suite in which each test has been classified as failing or passing, we present three variations of a technique that identify the subset of passing tests that are likely to be coincidentally correct. We evaluated the effectiveness of our techniques by empirically quantifying the following: 1) how accurately did they identify the coincidentally correct tests, 2) how much did they improve the effectiveness of coverage-based fault localization, and 3) how much did coverage decrease as a result of applying them. Using our better performing technique and configuration, the safety and precision of fault-localization was improved for 88% and 61% of the programs, respectively.",2010,0,
696,697,Videoendoscopic distortion correction and its application to virtual guidance of endoscopy,"Modern video based endoscopes offer physicians a wide-angle field of view (FOV) for minimally invasive procedures, Unfortunately, inherent barrel distortion prevents accurate perception of range. This makes measurement and distance judgment difficult and causes difficulties in emerging applications, such as virtual guidance of endoscopic procedures. Such distortion also arises in other wide FOV camera circumstances. This paper presents a distortion correction technique that can automatically calculate correction parameters, without precise knowledge of horizontal and vertical orientation. The method is applicable to any camera-distortion correction situation. Based on a least-squares estimation, the authors' proposed algorithm considers line fits in both FOV directions and gives a globally consistent set of expansion coefficients and an optimal image center. The method is insensitive to the initial orientation of the endoscope and provides more exhaustive FOV correction than previously proposed algorithms. The distortion-correction procedure is demonstrated for endoscopic video images of a calibration test pattern, a rubber bronchial training device, and real human circumstances. The distortion correction is also shown as a necessary component of an image-guided virtual-endoscopy system that matches endoscope images to corresponding rendered three-dimensional computed tomography views.",2001,0,
697,698,Test Generation and Diagnostic Test Generation for Open Faults with Considering Adjacent Lines,"In order to ensure high quality of DSM circuits, testing for the open defect in the circuits is necessary. However, the modeling and techniques for test generation for open faults have not been established yet. In this paper, we propose a method for generating tests and diagnostic tests based on a new open fault model. Firstly, we show a new open fault model with considering adjacent lines [9]. Under the open fault model, we reveal more about the conditions to excite the open fault. Next we propose a method for generating tests for open faults by using a stuck-at fault test with don't cares. We also propose a method for generating a diagnostic test that can distinguish the pair of open faults. Finally, experimental results show that (1) the proposed method is able to achieve 100% fault coverages for almost all benchmark circuits and (2) the proposed method is able to reduce the number of indistinguished open fault pairs.",2007,0,
698,699,Minimum Zone Evaluation of Sphericity Error Based on Ant Colony Algorithm,"In this paper, based on the analysis of existent evaluation methods for sphericity errors, an intelligent evaluation method is provided. The evolutional optimum model and the calculation process are introduced in detail. According to characteristics of sphericity error evaluation, ant colony optimization (ACO) algorithm is proposed to evaluate the minimum zone error. Compared with conventional optimum evaluation methods such as simplex search and Powell method, it can find the global optimal solution, and the precision of calculating result is very high. Then, the objective function calculation approaches for using the ACO algorithm to evaluate minimum zone error are formulated. Finally, the control experiment results evaluated by different method such as the least square, simplex search, Powell optimum methods and GA, indicate that the proposed method can provide better accuracy on sphericity error evaluation, and it has fast convergent speed as well as using computer expediently and popularizing application easily.",2007,0,
699,700,Development and evaluation of a model of programming errors,"Models of programming and debugging suggest many causes of errors, and many classifications of error types exist. Yet, there has been no attempt to link causes of errors to these classifications, nor is there a common vocabulary for reasoning about such causal links. This makes it difficult to compare the abilities of programming styles, languages, and environments to prevent errors. To address this issue, this paper presents a model of programming errors based on past studies of errors. The model was evaluated with two observational of Alice, an event-based programming system, revealing that most errors were due to attentional and strategic problems in implementing algorithms, language constructs, and uses of libraries. In general, the model can support theoretical, design, and educational programming research.",2003,0,
700,701,A Comparative Study of Voice Over Wireless Networks Using NS-2 Simulation with an Integrated Error Model,"Wireless communication is the fastest growing field and with the emergence of IEEE 802.11 based devices, wireless access is becoming more popular. Many multimedia applications for IP networks have been developed and thus the demand for quality of service (QoS) has increased. In this paper our primary objective is to evaluate 802.11e EDCF framework for video, voice and background traffic all at the same time. Our assessment is based on an error model called E-model, MOS for VoIP and PSNR for video. We also studied the effects of random uniform error model on various types of traffic. As expected, wireless networks are more prone to errors than wired networks",2006,0,
701,702,Adaptive Causal Models for Fault Diagnosis and Recovery in Multi-Robot Teams,"This paper presents an adaptive causal model method (adaptive CMM) for fault diagnosis and recovery in complex multi-robot teams. We claim that a causal model approach is effective for anticipating and recovering from many types of robot team errors, presenting extensive experimental results to support this claim. To our knowledge, these results show the first, full implementation of a CMM on a large multi-robot team. However, because of the significant number of possible failure modes in a complex multi-robot application, and the difficulty in anticipating all possible failures in advance, our empirical results show that one cannot guarantee the generation of a complete a priori causal model that identifies and specifies all faults that may occur in the system. Instead, an adaptive method is needed to enable the robot team to use its experience to update and extend its causal model to enable the team, over time, to better recover from faults when they occur. We present our case-based learning approach, called LeaF (for learning-based fault diagnosis), that enables robot team members to adapt their causal models, thereby improving their ability to diagnose and recover from these faults over time",2006,0,
702,703,Effects of clipping on the error performance of OFDM in frequency selective fading channels,"Previous studies on the effect of the clipping noise on the error performance of orthogonal frequency-division multiplexing (OFDM) systems in frequency selective fading channels provide pessimistic results. They do not consider the effect of channel fading on the clipping noise. The clipping noise is added at the transmitter and hence fades with the signal. Here, the authors show that the ""bad"" subcarriers that dominate the error performance of the OFDM system are least affected by the clipping noise and, as a result, the degradation in the error performance of OFDM system in fading channels is very small.",2004,0,
703,704,Master Defect Record Retrieval Using Network-Based Feature Association,"As electronic records (e.g., medical records and technical defect records) accumulate, the retrieval of a record from a past instance with the same or similar circumstances, has become extremely valuable. This is because a past record may contain the correct diagnosis or correct solution to the current circumstance. We refer to the two records of the same or similar circumstances as <i>master</i> and <i>duplicate</i> records. Current record retrieval techniques are lacking when applied to this special master defect record retrieval problem. In this study, we propose a new paradigm for master defect record retrieval using network-based feature association (NBFA). We train the master record retrieval process by constructing feature associations to limit the search space. The retrieval paradigm was employed and tested on a real-world large-scale defect record database from a telecommunications company. The empirical results suggest that the NBFA was able to significantly improve the performance of master record retrieval, and should be implemented in practice. This paper presents an overview of technical aspects of the master defect record retrieval problem, describes general methodologies for retrieval of master defect records, proposes a new feature association paradigm, provides performance assessments on real data from a telecommunications company, and highlights difficulties and challenges in this line of research that should be addressed in the future.",2010,0,
704,705,Fault-tolerant scheduling in distributed real-time systems,"In distributed systems, a real-time task has several subtasks which need to be executed at different nodes. Some of these subtasks can be executed in parallel on different nodes without violating their precedence relationships, if any, among them. To better exploit the parallelism, it becomes necessary to assign separate deadlines to subtasks and schedule them independently. We use three subtask deadline assignment policies which we have introduced earlier to develop a bidding-based fault-tolerant scheduling algorithm for distributed real-time systems. A local scheduler which resides on each node, tries to determine a schedule for each subtask according to the primary-backup approach. In this paper we discuss the algorithm and present the results of simulation studies conducted to establish the efficacy of our algorithm",2001,0,
705,706,The Error Reduced ADI-CPML Method for EMC Simulation,"In this paper, convolutional perfectly matched layer (CPML) is developed for the recently proposed error reduced (ER) ADI-FDTD method to solve electromagnetic compatibility problems efficiently. Its numerical results are examined and compared with the conventional ADI-CPML method. It is found that for a CFL number equal to 5, the reflection error of the ER- ADI-CPML is approximately 12 dB better than the conventional ADI-CPML method.",2007,0,
706,707,Identification of Errors in Power Flow Controller Parameters,"Transmission open access allows power transactions to take place between remote parts of an interconnected system. As a result, some parts of the transmission system may experience unusual power flows during certain power transactions. One way to circumvent possible congestion is to use power flow control devices. These devices which are also referred as flexible AC transmission system (FACTS) devices, allow rerouting of power flows in the system. The amount of power flowing through such a device can be controlled via device parameters. Hence, proper monitoring of these parameters is important for reliable operation and system security. In this paper, an identification method for detecting and identifying errors associated with power controller parameters will be presented. The method is based on the available measurements such as the power flows and injections which are used by the state estimators at the control center. Hence, the method can be implemented easily as part of the existing energy management functions",2006,0,
707,708,The use of historical defect imagery for yield learning,"The rapid identification of yield detracting mechanisms through integrated yield management is the primary goal of defect sourcing and yield learning. At future technology nodes, yield learning must proceed at an accelerated rate to maintain current defect sourcing cycle times despite the growth in circuit complexity and the amount of data acquired on a given wafer lot. As integrated circuit fabrication processes increase in complexity, it has been determined that data collection, retention, and retrieval rates will continue to increase at an alarming rate. Oak Ridge National Laboratory (ORNL) has been working with International SEMATECH to develop methods for managing the large volumes of image data that are being generated to monitor the status of the manufacturing process. This data contains an historical record that can be used to assist the yield engineer in the rapid resolution of manufacturing problems. To date there are no efficient methods of sorting and analyzing the vast repositories of imagery collected by off-line review tools for failure analysis, particle monitoring, line width control and overlay metrology. In this paper we will describe a new method for organizing, searching, and retrieving imagery using a query image to extract images from a large image database based on visual similarity",2000,0,
708,709,An iron core probe based inter-laminar core fault detection technique for generator stator cores,"A new technique for detecting incipient interlaminar insulation failure of laminated stator cores of large generators is proposed in this paper. The proposed scheme is a low flux induction method that employs a novel probe for core testing. The new probe configuration, which uses magnetic material and is scanned in the wedge depression area, significantly improves the sensitivity of fault detection as well as user convenience compared to existing methods. Experimental results from various test generators tested in factory, field and lab environments under a number of fault conditions are presented to verify the sensitivity and reliability of the proposed scheme.",2003,0,
709,710,Experiments on Fault-Tolerant Self-Reconfiguration and Emergent Self-Repair,"This paper presents a series of experiments on fault tolerant self-reconfiguration of the ATRON robotic system. For self-reconfiguration we use a previously described distributed control strategy based on meta-modules that emerge, move and stop. We perform experiments on three different types of failures: 1) Action failure: On the physical platform we demonstrate how roll-back of actions are used to achieve tolerance to collision with obstacles and other meta-modules. 2) Module failure: In simulation we show, for a 500 module robot, how different degrees of catastrophic module failure affect the robot's ability to shape-change to support an insecure roof. 3) Robot failure: In simulation we demonstrate how robot faults such as a broken robot bone can be emergent self-repaired by exploiting the redundancy of self-reconfigurable modules. We conclude that the use of emergent, distributed control, action roll-back, module redundancy, and self-reconfiguration can be used to achieve fault tolerant, self-repairing robots",2007,0,
710,711,Runtime Diversity against Quasirandom Faults,"Complex software based systems that have to be highly reliable, are increasingly confronted with fault types whose corresponding failures appear to be random, although they have a systematic cause. This paper introduces and defines these ""quasirandom"" faults. They have certain inconvenient common properties such as their difficulty to be reproduced, their strong state dependence and their likelihood to be found in operational systems after testing. However, these faults are also likely to be detected or tolerated with the help of diversity in software, and even low level diversity which can be achieved during runtime is a promising means against them. The result suggests, that runtime diversity can improve software reliability in complex systems.",2009,0,
711,712,"A secure modular exponential algorithm resists to power, timing, C safe error and M safe error attacks","This paper proposes a method for protecting public key schemes from timing and fault attacks. In general, this is accomplished by implementing critical operations using ""branch-less"" path routines. More particularly, the proposed method provides a modular exponentiation algorithm without any redundant computation does not have a store operation with non-certain destination so that it can protect the secret key from many known attacks.",2005,0,
712,713,Detailed radiation fault modeling of the Remote Exploration and Experimentation (REE) first generation testbed architecture,"The goal of the NASA HPCC Remote Exploration and Experimentation (REE) Project is to transfer commercial supercomputing technology into space. The project will use state of the art, low-power, non-radiation-hardened, COTS hardware chips and COTS software to the maximum extent possible, and will rely on software-implemented fault tolerance to provide the required levels of availability and reliability. We outline the methodology used to develop a detailed radiation fault model for the REE Testbed architecture. The model addresses the effects of energetic protons and heavy ions which cause single event upset and single event multiple upset events in digital logic devices and which are expected to be the primary fault generation mechanism. Unlike previous modeling efforts, this model will address fault rates and types in computer subsystems at a sufficiently fine level of granularity (i.e., the register level) that specific software and operational errors can be derived. We present the current state of the model, model verification activities and results to date, and plans for the future. Finally, we explain the methodology by which this model will be used to derive application-level error effects sets. These error effects sets will be used in conjunction with our Testbed fault injection capabilities and our applications' mission scenarios to replicate the predicted fault environment on our suite of onboard applications",2000,0,
713,714,Topology discovery for network fault management using mobile agents in ad-hoc networks,"Managing today's complex and increasingly heterogeneous networks requires in-depth knowledge and extensive training as well as collection of very large amount of data. Fault management is one of the functional areas of network management that entails detection, identification and correction of anomalies that disrupt services of a network. The task of fault management is even harder in ad-hoc networks where the topology of the network changes frequently. It is very inefficient if not impossible to discover the ad-hoc network topology using traditional practices of network discovery. We propose a mobile multi agent system for topology discovery that will allow fault management functions in ad-hoc network. Comparison to current mobile agent based topology discovery systems is also presented",2005,0,
714,715,Timing-based delay test for screening small delay defects,"The delay fault test pattern set generated by timing unaware commercial ATPG tools mostly affects very short paths, thereby increasing the escape chance of smaller delay defects. These small delay defects might be activated on longer paths during functional operation and cause a timing failure. This paper presents an improved pattern generation technique for transition fault model, which provides a higher coverage of small delay defect that lie along the long paths, using a commercial no-timing ATPG tool. The proposed technique pre-processes the scan flip-flops based on their least slack path and the detectable delay defect size. A new delay defect size metric based on the affected path length and required increase in test frequency is developed. We then perform pattern generation and apply a novel pattern selection technique to screen test patterns affecting longer paths. Using this technique will provide the opportunity of using existing timing unaware ATPG tools as slack based ATPG. The resulting pattern set improves the defect screening capability of small delay defects",2006,0,
715,716,Exploiting Memory Soft Redundancy for Joint Improvement of Error Tolerance and Access Efficiency,"Technology roadmap projects nanoscale multibillion- transistor integration in the coming years. However, on-chip memory becomes increasingly exposed to the dual challenges of device-level reliability degradation and architecture-level performance gap. In this paper, we propose to exploit the inherent memory soft (<i>transient</i>) <i>redundancy</i> for on-chip memory design. Due to the mismatch between fixed cache line size and runtime variations in memory spatial locality, many irrelevant data are fetched into the memory thereby wasting memory spaces. The proposed soft-redundancy allocated memory detects and utilizes these memory spaces for jointly achieving efficient memory access and effective error control. A runtime reconfiguration scheme is also proposed to further enhance the soft-redundancy allocation. Simulation results demonstrate 74.8% average error-control coverage ratio on the SPEC CPU2000 benchmarks with average of 59.5% and 41.3% reduction in memory miss rate and bandwidth usage, respectively, as compared to the existing memory techniques. Furthermore, the proposed technique is fully scalable with respect to various memory configurations.",2009,0,
716,717,Towards Identifying the Best Variables for Failure Prediction Using Injection of Realistic Software Faults,"Predicting failures at runtime is one of the most promising techniques to increase the availability of computer systems. However, failure prediction algorithms are still far from providing satisfactory results. In particular, the identification of the variables that show symptoms of incoming failures is a difficult problem. In this paper we propose an approach for identifying the most adequate variables for failure prediction. Realistic software faults are injected to accelerate the occurrence of system failures and thus generate a large amount of failure related data that is used to select, among hundreds of system variables, a small set that exhibits a clear correlation with failures. The proposed approach was experimentally evaluated using two configurations based on Windows XP. Results show that the proposed approach is quite effective and easy to use and that the injection of software faults is a powerful tool for improving the state of the art on failure prediction.",2010,0,
717,718,Fault tolerance of feed-forward artificial neural network architectures targeting nano-scale implementations,"Several circuit architectures have been proposed to overcome logic faults due to the high defect densities that are expected to be encountered in the first generations of nanoelectronic systems. How feed-forward artificial neural networks can possibly be exploited for the purpose of conceiving highly reliable Boolean gates is the topic of this paper. Computer simulations show that feed-forward artificial neural networks can be trained to absorb faults while implementing Boolean functions of various complexity. Using this approach, it can be shown that very high device failure rates (up to 20%) can be accommodated. The cost is to be paid in terms of hardware overhead, which is comparable to the area cost of conventional hardware redundancy measures.",2007,0,
718,719,Fault tolerance evaluation using two software based fault injection methods,"A silicon independent C-Based model of the TTP/C protocol was implemented within the EU-founded project FIT. The C-based model is integrated in the C-Sim simulation environment. The main objective of this work is to verify whether the simulation model of the TTP/C protocol behaves in the presence of faults in the same way as the existing hardware prototype implementation. Thus, the experimental results of the software implemented fault injection applied in the simulation model and in the hardware implementation of the TTP/C network have been compared. Fault injection experiments in both the hardware and the simulation model are performed using the same configuration setup, and the same fault injection input parameters (fault injection location, fault type and the fault injection time). The end result comparison has shown a complete conformance of 96.30%, while the cause of the different results was due to hardware specific implementation of the built-in-self-test error detection mechanisms.",2002,0,
719,720,Error detection and unit conversion,"The article discusses the accuracy mathematical modeling languages (MML) for biomedicine, for example in cardiac electrophysiology. Unit balance checking is showed that it can be automated. The implemented example is JSim (http://www.physiome.org/ jsim/), which is general and can be applied to other systems in which units can be specified and checked. ODE-based simulator Physiome CellML Environment is also discussed.",2009,0,
720,721,Implementation of Web-Based Fault Diagnosis Using Improved Fuzzy Petri Nets,"According to the current application and maintenance situation of numerical control equipment (NCE), a novel remote fault diagnosis expert system is designed to prevent fault occurrence and quicken the recovering process by online real-time monitoring the working state of NCEs. The article addresses the overall framework and relevant application technology of fault diagnosis system (FDS) and emphasizes on the establishment of fuzzy expert system (FES). Improved fuzzy Petri nets (FPNs) model and concurrent reasoning algorithm are applied to handle the fuzziness and concurrency of fault and inadequate and uncertain information. Utilization of simple matrix operation to realize complicated reasoning process that simplifies the diagnostic reasoning decision-making process. Meanwhile, it can be realized easily by computer programming. Finally, a practical fault instance is presented to demonstrate the feasibility and validity of this method.",2009,0,
721,722,An object-based approach to optical proximity correction,"As the feature sizes of integrated circuits have been continually reducing to below exposure wavelength, some correcting techniques, such as OPC and PSM are indispensable to compensate for the distortions on wafer images. In this paper, we describe an object-based approach to OPC named OPCM, which is a model-based OPC tool. Also, a rule-based OPC has been adopted to enhance the practicability of the software",2001,0,
722,723,Generator dynamics influence on currents distribution in fault condition,"Current flow calculation results along the elements of a complex power system are analyzed in this work, during a three-phase short-circuit taking into account relative rotor swing. Analysis is implemented on the examples when the infinite bus fault point is supplied by one or more generators. It is shown that generator swing neglected during short-circuits, essentially changing current distribution in the system, can lead to impermissible mistakes in calculation results.",2000,0,
723,724,"An automated fault analysis system for SP energy networks: Requirements, design and implementation","The proliferation of monitoring equipment on modern electrical power transmission networks is causing an increasing amount of monitoring data to be captured by transmission network operators. Traditional manual data analysis techniques fail to meet the analysis and reporting requirements of the utilities which have chosen to invest in monitoring. The volume of monitoring data, the complexities in analysing multiple related data sources and the preparation of internal reports based on that analysis, render timely manual analysis impractical, if not intractable. In 2006, the authors reported on the first online trials of the protection engineering diagnostics agents (PEDA) system, an automated fault diagnosis system which integrated legacy intelligent systems for the analysis of SCADA and digital fault recorder (DFR) data in order to provide automatic post fault assessment of protection system performance. In this paper the authors revisit the requirements of the TNO where PEDA was trialled. Based on a new formal specification of requirements carried out in 2008, the authors discuss the requirements met by the current version of PEDA and how PEDA could be augmented to meet these new requirements highlighted in this latest analysis of the utilities' requirements.",2009,0,
724,725,Impact of correlation errors on optimum Kalman filter matrices gains identification in multicoordinate systems,"This paper investigates the impact that errors in the innovation correlation calculations has upon the steady-state Kalman filter gain identification. This issue arises in all real time applications, where the correlations must he calculated from experimental data. The algorithm proposed by [L. Hong (1991)] is considered and equations describing the impact are established. Simulation results are presented and discussed. Finally, experimental results for the algorithm in [L. Hong (1991)], applied to estimate the states of a servo system, are presented.",2005,0,
725,726,Optimal Cluster-Cluster Design for Sensor Network with Guaranteed Capacity and Fault Tolerance,"Sensor networks have recently gained a lot of attention from the research community. To ensure scalability sensor networks are often partitioned into clusters, each managed by a cluster head. Since sensors self organize in the form of clusters within a hierarchal wireless sensor network, it is necessary for a sensor node to perform target tracking cooperating with a set of sensors that belong to another cluster. The increased flexibility allows for efficient and optimized use of sensor nodes. While most of the previous research focused on the optimal communication of sensors in one cluster, very little attention has been paid to the efficiency of cooperation among the clusters. This paper proposes a heuristic algorithm of designing optimal structure across clusters to allow the inter-cluster flow of communication and resource sharing under reliability constraints. Such a guarantee simultaneously provides fault tolerance against node failures and high capacity through multi-path routing.",2007,0,
726,727,Fault treatment with net condition/event systems: a first approach,"The paper presents a preliminary report on modeling parts of a modular production system, their dedicated controllers, and the appropriate methods of fault treatment on the level of net condition/event systems (NCES). To achieve practicability, NCES support a systematic and modular way of modeling more complex systems as well as concurrent and non-deterministic behavior which is highly beneficial for modeling and control of DES in failure situations as studies of existing methods show.",2001,0,
727,728,SLICED: Slide-based concurrent error detection technique for symmetric block ciphers,"Fault attacks, wherein faults are deliberately injected into cryptographic devices, can compromise their security. Moreover, in the emerging nanometer regime of VLSI, accidental faults will occur at very high rates. While straightforward hardware redundancy based concurrent error detection (CED) can detect transient and permanent faults, it entails 100% area overhead. On the other hand, time redundancy based CED can only detect transient faults with minimum area overhead but entails 100% time overhead. In this paper we present a general time redundancy based CED technique called SLICED for pipelined implementations of symmetric block cipher. SLICED SLIdes one encryption over another and compares their results for CED as a basis for protection against accidental faults and deliberate fault attacks.",2010,0,
728,729,A Support System for Teaching Computer Programming Based on the Analysis of Compilation Errors,"A system was developed to support teaching computer programming to a group of students who have common questions and make common mistakes on practice computer programs. The system extrapolates the causes and syntaxes of students' compilation errors by analyzing the trends of past compilation errors and presents the extrapolated result to the teacher in real time. By using the system, a teacher can understand in real time students' programming mistakes when they are writing computer programs, and can appropriately teach computer programming to a group of students who have common problems",2006,0,
729,730,The research on a new fault wave recording device in generator-transformer units,"This paper presents a new kind of distributed fault recorder including the design of the system structure, the hardware and software design of the recorder. The recorder adopts NI CompaceRIO series programmable automation controller (PAC) in the hardware while virtual instrument technology in the software. The network communication based on TCP/IP between the client and the server is adopted in the power plant. Moreover, an improved frequency tracking algorithm is presented in the monitoring of the electric quantity to improve the detection precision and the processing speed. The detection and operation results show that it has improved the performance greatly and realized authenticity, integrity and reliability and so on.",2009,0,
730,731,Rotor fault detection using the instantaneous power signature,"The aim of this paper is to present a method to detect broken rotor bar faults by estimating a global modulation index which corresponds to the contribution of all detected modulating frequencies in the stator current. We show that additional information carried by instantaneous power improves the detection of the sidebands and consequently the monitoring too. In fact, the instantaneous power method can be interpreted as a modulation operation in the time domain that translates the spectral components specific to the broken rotor bars to the band [0-50]Hz.",2004,0,
731,732,A system level approach in designing dual-duplex fault tolerant embedded systems,"This paper presents an approach for designing embedded systems able to tolerate hardware faults, defined as an evolution of our previous work proposing an hardware/software co-design framework for realizing reliable embedded systems. The framework is extended to support the designer in achieving embedded systems with fault tolerant properties minimizing overheads and limiting power consumption. A reference system architecture is proposed; the specific hardware/software implementation and reliability methodologies (to achieve the fault tolerance properties) are the result of an enhanced hw/sw partitioning process driven by the designer' constraints and by the reliability constraints, set at the beginning of the design process. By introducing also the reliability constraints during specification, the final system can benefit from the introduced redundancy also for performance gains, while limiting area, time, performance and power consumption overheads.",2002,0,
732,733,A 32-bit COTS-based fault-tolerant embedded system,"This paper presents a 32-bit fault-tolerant (FT) embedded system based on commercial off-the-shelf (COTS) processors. This embedded system uses two 32-bit Pentium processors with master/checker (M/C) configuration and an external watchdog processor (WDP) for implementing a behavioral-based error detection scheme called committed instructions counting (CIC). The experimental evaluation was performed using both power-supply disturbance (PSD) and software-implemented fault injection (SWIFI) methods. A total of 9000 faults have been injected into the embedded system to measure the coverage of error detection mechanisms, i.e., the checker processor and the CIC scheme. The results show that the M/C configuration is not enough for this system and the CIC scheme could cover the limitation of the M/C configuration.",2005,0,
733,734,A software fault tolerance method for safety-critical systems: effectiveness and drawbacks,"An automatic software technique suitable for on-line detection of transient errors due to the effects of the environment (radiation, EMC,...) is presented. The proposed approach, particularly well suited for low-cost safety-critical microprocessor-based applications, has been validated through fault injection experiments and radiation testing campaigns. The experimental results demonstrate the effectiveness of the approach in terms of fault detection capabilities. Undetected faults have been analyzed to point out the limitations of the method.",2002,0,
734,735,IFRA: Post-silicon bug localization in processors,"IFRA overcomes challenges associated with an expensive step in post-silicon validation of processors - pinpointing the bug location and the instruction sequence that exposes the bug from a system failure. On-chip recorders collect instruction footprints (information about flows of instructions, and what the instructions did as they passed through various design blocks) during the normal operation of the processor in a post-silicon system validation setup. Upon system failure, the recorded information is scanned out and analyzed off-line for bug localization. Special self-consistency-based program analysis techniques, together with the test program binary of the application executed during post-silicon validation, are used. Major benefits of using IFRA over traditional techniques for post-silicon bug localization are: 1. It does not require full system-level reproduction of bugs, and, 2. It does not require full system-level simulation. Simulation results on a complex super-scalar processor demonstrate that IFRA is effective in accurately localizing electrical bugs with very little impact on overall chip area.",2009,0,
735,736,Parallel computation of configuration space on reconfigurable mesh with faults,"A reconfigurable mesh (RMESH) can be used to compute robotic paths in the presence of obstacles, where the robot and obstacle images are represented and processed in mesh processors. For a non-point-like robot, we can compute the so-called configuration space to expand the obstacles, so that the robot can be reduced to a reference point to facilitate the robot's motion planning. In this paper, we present algorithms to compute the configuration space in a reconfigurable mesh that contains sparsely distributed faulty processors. Robots of rectangular and circular shapes are treated. It is seen that, in terms of computing the configuration space, a reconfigurable mesh can tolerate faulty processors without much extra cost-the computation takes the optimal O(1) time in both fault-free and faulty reconfigurable meshes",2000,0,
736,737,Nonstationary Motor Fault Detection Using Recent Quadratic TimeFrequency Representations,"As the use of electric motors increases in the aerospace and transportation industries where operating conditions continuously change with time, fault detection in electric motors has been gaining importance. Motor diagnostics in a nonstationary environment is difficult and often needs sophisticated signal processing techniques. In recent times, a plethora of new time-frequency distributions has appeared, which are inherently suited to the analysis of nonstationary signals while offering superior frequency resolution characteristics. The Zhao-Atlas-Marks distribution is one such distribution. This paper proposes the use of these new time-frequency distributions to enhance nonstationary fault diagnostics in electric motors. One common myth has been that the quadratic time-frequency distributions are not suitable for commercial implementation. This paper also addresses this issue in detail. Optimal discrete-time implementations of some of these quadratic time-frequency distributions are explained. These time-frequency representations have been implemented on a digital signal processing platform to demonstrate that the proposed methods can be implemented commercially.",2008,0,
737,738,Towards understanding the effects of intermittent hardware faults on programs,"Intermittent hardware faults are bursts of errors that last from a few CPU cycles to a few seconds. They are caused by process variations, circuit wear-out, and temperature, clock or voltage fluctuations. Recent studies show that intermittent fault rates are increasing due to technology scaling and are likely to be a significant concern in future systems. We study the propagation of intermittent faults to programs; in particular, we are interested in the crash behaviour of programs. We use a model of a program that represents the data dependencies in a fault-free trace of the program and we analyze this model to glean some information about the length of intermittent faults and their effect on the program under specific fault and crash models. The results of our study can aid fault detection, diagnosis and recovery techniques.",2010,0,
738,739,A Method of Detecting Vulnerability Defects Based on Static Analysis,"This paper proposes a method for detecting vulnerability defects caused by tainted data based on state machine. It first uses state machine to define various defect patterns. If the states of state machine is considered as the value propagated in dataflow analysis and the union operation of the state sets as the aggregation operation of dataflow analysis, the defect detection can be treated as a forward dataflow analysis problem. To reduce the false positives caused by intraprocedural analysis, the dynamic information of program was represented approximately by abstract value of variables, and then infeasible path can be identified when some variable's abstract value is empty in the state condition. A function summary method is proposed to get the information needed for performing interprocedural defect detection. The method proposed has been implemented in a defect testing tools.",2010,0,
739,740,Distortion correction of LDMOS power amplifiers using hybrid RF second harmonic injection/digital predistortion linearization,"An LDMOS RF power amplifier for RF multichannel wireless systems with improved IMD performance characteristics is presented. The application of two combined linearization methods is being tested with the help of circuit simulation software ADS. The injection of the fundamental signal's second harmonic in the RF amplifier, as well as a digital predistortion technique, is combined in order to achieve IMD improvement. By proper selection of phase and amplitude of the injected second harmonic signal, it is possible to reduce IMD products that have already been reduced by the well established method of digital predistortion",2006,0,
740,741,Identifying the root causes of memory bugs using corrupted memory location suppression,"We present a general approach for automatically isolating the root causes of memory-related bugs in software. Our approach is based on the observation that most memory bugs involve uses of corrupted memory locations. By iteratively suppressing (nullifying) the effects of these corrupted memory locations during program execution, our approach gradually isolates the root cause of a memory bug. Our approach can work for common memory bugs such as buffer overflows, uninitialized reads, and double frees. However, our approach is particularly effective in finding root causes for memory bugs in which memory corruption propagates during execution until an observable failure such as a program crash occurs.",2008,0,
741,742,Towards a Model of Fault Tolerance Technique Selection in Static and Dynamic Agent-Based Inter-Organizational Workflow Management Systems,"Research in workflow management systems design references the mobile agent computing paradigm where agents have been shown to increase the total capacity of a workflow system through the decoupling of execution management from a statically designated workflow engine, although coordinating fault tolerance mechanisms has been shown to be a downside due to increased overall execution times. To address this issue, we develop a model for comparing the effects of two fault tolerance techniques: local and remote checkpointing. The model enables an examination of fault tolerance coordination impacts on execution time while concomitantly taking into account the dynamic nature of a workflow environment. A proposed use for the model includes providing for selecting and configuring agent-based fault tolerance approaches based on changes in environmental variables - an approach that allows the owners of a workflow management system to reap the scaling efficiency benefits of the mobile agent paradigm without being forced to make trade-offs in execution performance.",2005,0,
742,743,Impact of configuration errors on DNS robustness,"During the past twenty years the Domain Name System (DNS) has sustained phenomenal growth while maintaining satisfactory user-level performance. However, the original design focused mainly on system robustness against physical failures, and neglected the impact of operational errors such as mis-configurations. Our measurement efforts have revealed a number of mis-configurations in DNS today: delegation inconsistency, lame delegation, diminished server redundancy, and cyclic zone dependency. Zones with configuration errors suffer from reduced availability and increased query delays up to an order of magnitude. The original DNS design assumed that redundant DNS servers fail independently, but our measurements show that operational choices create dependencies between servers. We found that, left unchecked, DNS configuration errors are widespread. Specifically, lame delegation affects 15% of the measured DNS zones, delegation inconsistency appears in 21% of the zones, diminished server redundancy is even more prevalent, and cyclic dependency appears in 2% of the zones. We also noted that the degrees of mis-configuration vary from zone to zone, with the most popular zones having the lowest percentage of errors. Our results indicate that DNS, as well as any other truly robust large-scale system, must include systematic checking mechanisms to cope with operational errors.",2009,0,
743,744,Detecting defects on planar circuits by using non-contacting magnetic probe,"Recently, the research of non-contacting measurement with magnetic coupling theorem mostly choose CPW(coplanar waveguide) loop-type circuits as probes. It has advantage of low cost, easy to fabricate and simple designing. While moving the probe, different relative position between planar circuit and magnetic probe cause different strength of coupling. Variation of resonance frequency due to changing magnetic coupling and electric coupling from metal strip outline can be observed. The relation between planar circuit and magnetic probe is analyzed by full-wave EM simulation and some simple measurement. Furthermore, the LC equivalent circuit has also been built for analyzing. At last, the possibility of doing quickly defect-detecting by sweeping the circuits at special frequency will be discussed.",2010,0,
744,745,An Application of Semantic Annotations to Design Errors,"As current engineered systems (e.g. aviation systems) have been equipped with automated and computer-based artefacts, human-system interaction (e.g. human computer interaction) has been an important issue. Design errors that are attributable to human-system interaction failures are not pure engineering design issues, but a multidisciplinary subject with related other areas such as management, psychology, physiology or ergonomics. To identify such design errors (called design-induced errors) in accident reports is important for designing more reliable systems. However, the lack of precise definitions of the concept of design-induced error and the diversity of expression of such failures make it difficult to retrieve relevant documents from accident reports. This paper describes how an ontology and annotation scheme can help to overcome such limitations. Engineering designers can be assisted by the developed ontology and annotation scheme to reason on the issues of design induced error",2006,0,
745,746,The effects of Gaussian weighting errors in hybrid SC/MRC combiners,"The paper examines the impact of Gaussian distributed weighting errors (in the channel gain estimates used for coherent combination) on the statistics of the output of hybrid selection/maximal-ratio (SC/MRC) receiver as well as the degradation of the average symbol error rate (ASER) performance from the ideal case. New expressions for the probability density function (PDF), cumulative distribution function (CDF) and moment generating function (MGF) of the coherent hybrid SC/MRC combiner output signal-to-noise ratio (SNR) are derived. The MGF is then used to derive exact closed form ASER formulas for binary and M-ary modulations employing a nonideal hybrid SC/MRC receiver in Rayleigh fading. Results for both SC and MRC are obtained as limiting cases. The effect of the weighting errors on the outage rate of error probability and the average combined SNR are also investigated. These analytical results provide some insights into the trade-off between diversity gain and combination losses with the increasing order of diversity branches in an energy-sharing communication system",2000,0,
746,747,An Extension of Differential Fault Analysis on AES,"In CHES 2006, M. Amir et al. introduced a generalized method of differential fault attack (DFA) against AES-128. Their fault models cover all locations before the 9th round in AES-128. However, their method cannot be applied to AES with other key sizes, such as AES-192 and AES-256. On the differential analysis, we propose a new method to extend DFA on AES with all key sizes. Our results in this study will also be beneficial to the analysis of the same type of other iterated block ciphers.",2009,0,
747,748,A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction,"In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: >75% percentage of correctly classified files, a recall of >80%, and a false positive rate <30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.",2008,0,
748,749,Smoothing Algorithm for Tetrahedral Meshes by Error-Based Quality Metric,"Smoothing or geometrical optimization is one of basic procedures to improve the quality of mesh. This paper first introduces an error-based mesh quality metric based on the concept of optimal Delaunay triangulations, and then examines the smoothing scheme which minimizes the interpolation error among all triangulations with the same number of vertices. Facing to its deficiency, a modified smoothing scheme and corresponding optimization model for tetrahedral mesh that avoid illegal elements are proposed. The optimization model is solved by integrating chaos search and BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm efficiently. Quality improvement for tetrahedral mesh is realized through alternately applying the smoothing approach suggested and topological optimization technique. Testing results show that the proposed approach is effective to improve mesh quality and suitable for combining with topological technique.",2010,0,
749,750,Fault diagnosis based on timed automata: Diagnoser verification,"The paper deals with the supervisory control problem based on vector synchronous product (VSP) of automata. A necessary and sufficient condition for the existence of such a controller is given, which is based on the notion of vs-controllability. Furthermore, a more general framework called vector synchronous product with communication is proposed. In addition, isomorph and homomorph of two VSPs are defined. Some simplified traffic examples are used to illustrate the notions and the result",2006,0,
750,751,Analogous view transfer for gaze correction in video sequences,"This paper provides a framework for doing facial gaze correction in video sequences. The proposed framework involves stages of face registration, face parameter mapping, and face synthesis. We introduce the concept of analogous views, and derive a novel formulation which extends view transfers based on epipolar geometry to cope with non-rigid motion. Additionally, a disparity mapping function is derived which is learned from training data and handles both spatial disparities as well as pixel-value changes. The disparity mapping function generalizes to facial expressions, illumination conditions and individuals not in the training set, as shown by the results obtained.",2002,0,
751,752,A learning-based approach for fault tolerance on grid resources scheduling,"While Grid environment has developed increasingly, unfortunately the importance of fault tolerance has not been remarkable in Grid resource management. On the other hand, the cost of computing by grid is important because grid is an economy-based system. Most organizations intend to spend little on their own computations by grid. Therefore, using a better approach to resource scheduling to avoid fault is necessary. This paper presents a new approach on fault tolerance mechanisms for the resource scheduling on grid by using Case-Based Reasoning technique in a local fashion. This approach applies a specific structure in order to prepare fault tolerance between executer nodes to retain system in a safe state with minimum data transferring. Certainly, this algorithm increases fault tolerant confidence therefore, performance of grid will be high.",2009,0,
752,753,More on general error locator polynomials for a class of binary cyclic codes,"Recently, the general error locator polynomials have been widely used in the algebraic decoding of binary cyclic codes. This paper utilizes the proposed general error locator polynomial to develop an algebraic decoding algorithm for a class of the binary cyclic codes. This general error locator polynomial differs greatly from the previous general error locator polynomial. Each coefficient of the proposed general error locator polynomial is expressed as a binary polynomial in the single syndrome and the degrees of nonzero terms in the binary polynomial satisfy at least one congruence relation.",2010,0,
753,754,A property oriented fault detection approach for link state routing protocol,"This paper proposes a new approach to fault detection for a link state routing system-property oriented analysis and detection (POD). A routing system is modeled as a set of distributed processes. A property is defined as state predicate(s) over system variables. For the link state routing protocol, the high-level overall converging property P is defined as the synchronization among routing information bases maintained by all processes. We decompose the routing protocol into different computation phases. For each phase, we use invariant state predicates (safety property) and the liveness property as our guide for observation and analysis. The goal of the detection algorithm is to construct a validation path based on the history to determine if the fault is natural or malicious once the stable property P is rendered invalid by faults. The contribution of this paper is twofold: first, a new detection approach is proposed that differs from traditional signature-based or profile-based intrusion detection paradigms in the sense that it utilizes the stable property as a starting point, and correlates the history and future to validate changes in the system; second, by exploring the primary concerned system properties, we show that detection effort can be conducted in a more focused and systematic fashion",2000,0,
754,755,On the 2-Adic Complexity and the k-Error 2 -Adic Complexity of Periodic Binary Sequences,"A significant difference between the linear complexity and the 2-adic complexity of periodic binary sequences is pointed out in this correspondence. Based on this observation, we present the concept of the symmetric 2-adic complexity of periodic binary sequences. The expected value of the 2-adic complexity is determined, and a lower bound on the expected value of the symmetric 2-adic complexity of periodic binary sequences is derived. We study the variance of the 2-adic complexity of periodic binary sequences, and the exact value for it is given. Because the k-adic complexity of periodic binary sequences is unstable, we present the concepts of the <i>kappa</i>-error 2-adic complexity and the k-error symmetric 2-adic complexity, and lower bounds on them are also derived. In particular, we give tighter upper and lower bounds for the minimum k-adic complexity of l-sequences by substituting two symbols within one period.",2008,0,
755,756,A Framework for Proactive Fault Tolerance,"Fault tolerance is a major concern to guarantee availability of critical services as well as application execution. Traditional approaches for fault tolerance include checkpoint/restart or duplication. However it is also possible to anticipate failures and proactively take action before failures occur in order to minimize failure impact on the system and application execution. This document presents a proactive fault tolerance framework. This framework can use different proactive fault tolerance mechanisms, i.e., migration and pause/un-pause. The framework also allows the implementation of new proactive fault tolerance policies thanks to a modular architecture. A first proactive fault tolerance policy has been implemented and preliminary experimentations have been done based on system-level virtualization and compared with results obtained by simulation.",2008,0,
756,757,Study of the Dispersion Characteristics of One Dimensional EBG with Defects,"In this paper we propose a simplified model for studying the Brillouin diagrams based on dielectric multilayers inside a parallel plate waveguide. The main objective of this work is the use of a simplified approach in modelling EBG structures with periodical defects. The effect of layer permittivity, defect length and periodicity were studied using simulation software with appropriated periodical boundary conditions. Physical insights and intuitive justifications for the simulation findings and concepts are also presented. It is shown that the two forbidden band-gaps can either be controlled independently by varying the permittivity or the size of the defects",2005,0,
757,758,Poisonedwater: an adaptive approach to reducing the reputation ranking error in P2P networks,"This paper preliminarily proposes a reputation ranking algorithm called ldquoPoisonedwaterrdquo to resist front peer attack - peers that gain high reputation values by always cooperating with other peers and then promote their malicious friends through passing most of their reputation values to those malicious peers. Specifically, we introduce a notion of Poisoned Water (PW) that iteratively floods from identified malicious peers in the reverse direction of the incoming trust links towards other peers. Furthermore, we propose the concept of spreading factor (SF) that is logistically correlated to each peer's PW level. Then, we design the new reputation ranking algorithm seamlessly integrated with peers' recommendation ability (represented as SF), to infer the more accurate reputation ranking for each peer. Simulation results show that, in comparison with Eigentrust, Poisonedwater can significantly reduce the ranking error ratio up to 20%, when P2P systems exist many malicious peers and front peers.",2009,0,
758,759,Distributed Fault Management for Computational Grids,"Grid resources having heterogeneous architectures, being geographically distributed and interconnected via unreliable network media, are at the risk of failure. Grid environment consists of unreliable resources; therefore, fault tolerant mechanisms can not be ignored. Some scientific jobs require long commitments of grid resources whose failures may not be overlooked. We need a flexible management of these failures by considering the failure of fault manager itself. In this paper we propose the concept of distributed management of failures without engaging the resources for this particular task exclusively. Resources performing the fault management may also participate in serving the long running user jobs. Each sub-job of the main user job is inspected by an individual resource. In case of failure inspector resource takes over in place of inspected resource. Contributions of this paper are: elimination of single point of failure and proposed concept's ability to be integrated with variety of grid middleware",2006,0,
759,760,Spherical Near-Field Antenna Measurements: A Review of Correction Techniques,"Following an introductory review of spherical near-field scanning measurements, with emphasis on the general applicability of the technique, we present a survey of the various methods to improve measurement accuracy by correcting the acquired data before performing the transform and by special processing of the resulting data following the transform. A post-processing technique recently receiving additional attention is the IsoFilter technique that assists in suppressing extraneous stray signals due to scattering from antenna range apparatus.",2007,0,
760,761,Filter Hardware Cost Reduction by Means of Error Feedback,"The article presents an uncommon application of the error feedback-improved IIR filter. A simple method to reduce the hardware cost (silicon area) of the biquadratic section implementation by means of error feedback (EF) is described. The optimization method utilizes the fact that the filter with an EF is more resistant to roundoff noise than a filter without it. An iterative method is used to reduce the occupied silicon area. First the standard IIR filter is designed with the requested quantization properties. Then the EF-improved biquadratic section is designed to attain the same roundoff noise properties. The occupied silicon areas of both solutions are compared then. Although implementation of EF results in more arithmetic components and more complex filter control, the resulting structure attaining the same quantization noise is smaller under defined circumstances (filter with poles close to the unit circle). Results show it is possible to spare up to 22% of the occupied silicon area. Our findings are valid for FPGA as well as ASIC implementation of the IIR filters. Our method has an advantage in using a standard and already verified filtering IP core which results in design time reduction.",2007,0,
761,762,The use of PSA for fault detection and characterization in electrical apparatus,"The monitoring of the actual condition of high voltage apparatus has become more and more important in the last years. One well established tool to characterize the actual condition of electric equipment is the measurement and evaluation of partial discharge data. Immense effort has been put into sophisticated statistic software-tools, to extract meaningful analyses out of data sets, without taking care of relevant correlations between consecutive discharge pulses. In contrast to these classical methods of partial discharge analysis the application of the Pulse Sequence Analysis allows a far better insight into the local defects. The detailed analysis of sequences of discharges in a voltage- and a current-transformer shows that the sequence of the partial discharge signals may change with time, because either different defects are active at different measuring times or a local defect may change with time as a consequence of the discharge activity. Hence for the evaluation of the state of degradation or the classification of the type of defect the analysis of short `homogeneous' sequences or sequence correlated data is much more meaningful than just the evaluation of a set of independently accumulated discharge data. This is demonstrated by the evaluation of measurements performed on different commercial hv-apparatus",2000,0,
762,763,Automated fault tree generation and risk-based testing of networked automation systems,"In manufacturing automation domain safety and availability are the most important factors to ensure productivity. In modern software intensive networked automation systems it became quite hard to ensure which non-functional requirements are related to these factors as well as whether these are satisfied or not. This is due to the prevalence of manual efforts in several analyses phases where complexity of the system often makes it hard to obtain comprehensive overview and thus makes it difficult to ascertain the presence of certain undesired consequences. Since design, development and following verification and validation activities are largely dependent upon the result of the analyses the product is largely affected. To address these problems automated fault tree generation is presented in this paper. It uses distinct modeling artifacts and information to automatically compose formal models of the system. Embedding hardware and network failures it is then ascertained through model checking whether the system satisfies certain safety and availability properties or not. This information is used to compose the fault tree. Proposed approach will improve completeness and correctness in fault trees and will consequently help in improving the quality of the system. Furthermore, it is also shown how the artifacts of this analysis can be used to produce test goals and test cases to validate the software constituents of the system and assure traceability between testing activity and safety requirements.",2010,0,
763,764,Quasi-cyclic generalized ldpc codes with low error floors,"In this paper, a novel methodology for designing structured generalized LDPC (G-LDPC) codes is presented. The proposed design results in quasi-cyclic G-LDPC codes for which efficient encoding is feasible through shift-register-based circuits. The structure imposed on the bipartite graphs, together with the choice of simple component codes, leads to a class of codes suitable for fast iterative decoding. A pragmatic approach to the construction of G-LDPC codes is proposed. The approach is based on the substitution of check nodes in the protograph of a low-density parity-check code with stronger nodes based, for instance, on Hamming codes. Such a design approach, which we call LDPC code doping, leads to low-rate quasi-cyclic G-LDPC codes with excellent performance in both the error floor and waterfall regions on the additive white Gaussian noise channel.",2008,0,
764,765,A new H.264/AVC error resilience model based on Regions of Interest,"Video transmission over the Internet can sometimes be subject to packet loss which reduces the end-user's quality of experience (QoE). Solutions aiming at improving the robustness of a video bitstream can be used to subdue this problem. In this paper, we propose a new region of interest-based error resilience model to protect the most important part of the picture from distortions. We conduct eye tracking tests in order to collect the region of interest (RoI) data. Then, we apply in the encoder an intra-prediction restriction algorithm to the macroblocks belonging to the RoI. Results show that while no significant overhead is noted, the perceived quality of the video's RoI, measured by means of a perceptual video quality metric, increases in the presence of packet loss compared to the traditional encoding approach.",2009,0,
765,766,The z990 first error data capture concept,"Superior availability is one of the outstanding features of modern zSeries machines, among the most highly rated of any existing computer platforms in this reqard. Many features contribute to this characteristic, some in hardware, some in software. This paper describes the first error data capture (FEDC) concept in the zSeries 990. The concept is used for both zSeries integration efficiency and its ability to gain field data for problem determination in the user environment. FEDC is not a single function, but part of all internal software (firmware) in the z990. This paper explains the overall concept and implementation details of the various internal functional layers (subsystems).",2004,0,
766,767,Enhanced server fault-tolerance for improved user experience,"Interactive applications such as email, calendar, and maps are migrating from local desktop machines to data centers due to the many advantages offered by such a computing environment. Furthermore, this trend is creating a marked increase in the deployment of servers at data centers. To ride the price/performance curves for CPU, memory and other hardware, inexpensive commodity machines are the most cost effective choices for a data center. However, due to low availability numbers of these machines, the probability of server failures is relatively high. Server failures can in turn cause service outages, degrade user experience and eventually result in lost revenue for businesses. We propose a TCP splice-based Web server architecture that seamlessly tolerates both Web proxy and backend server failures. The client TCP connection and sessions are preserved, and failover to alternate servers in case of server failures is fast and client transparent. The architecture provides support for both deterministic and non-deterministic server applications. A prototype of this architecture has been implemented in Linux, and the paper presents detailed performance results for a PHP-based Webmail application deployed over this architecture.",2008,0,
767,768,An adaptive distance relaying algorithm with a morphological fault detector embedded,"This paper presents an adaptive distance relaying algorithm (ADRA) for transmission line protection. In ADRA, a fault detector designed based on mathematical morphology (MM) is used to determine the occurrence of a fault. The Euclidean norm of the detector output is then calculated for fault phase selection and fault type classification. With respect to a specific type of fault scenario, an instantaneous circuit model applicable to a transient fault process is constructed to determine the position of the fault. The distance between the fault position and the relay is calculated by a differential equation of the instantaneous circuit model which is resolved in a recursive manner within each sampling interval. Due to the feature of recursive calculation, the protection zone of the relay varies from a small length to large, which increases as an augment in the sample window length. ADRA is evaluated on a transmission model based on PSCAD/EMDTC, under a variety of different fault distances, fault types, fault resistances and loading angles, respectively. The simulation results show that in comparison with conventional DFT-based protection methods, by which the fault distance is calculated using phasor measurements of voltage and current signals in a fixed-length window, ADRA requires much fewer samples to achieve a same degree of the accuracy of fault distance calculation, which enables much faster tripping, and its protection zone can be extended as more samples are used.",2009,0,
768,769,Fault-tolerant defect prediction in high-precision foundry,"High-precision foundry production is subjected to rigorous quality controls in order to ensure a proper result. Such exams, however, are extremely expensive and only achieve good results in a posteriori fashion. In previous works, we presented a defect prediction system that achieved a 99% success rate. Still, this approach did not take into account sufficiently the geometry of the casting part models, resulting in higher raw material requirements to guarantee an appropriate outcome. In this paper, we present here a fault-tolerant software solution for casting defect prediction that is able to detect possible defects directly in the design phase by analysing the volume of three-dimensional models. To this end, we propose advanced algorithms to recreate the topology of each foundry part, analyze its volume and simulate the casting procedure, all of them specifically designed for an robust implementation over the latest graphic hardware that ensures an interactive design process.",2010,0,
769,770,Bounds on the Decoding Error Probability of Binary Block Codes over Noncoherent Block AWGN and Fading Channels,"We derive upper bounds on the decoding error probability of binary block codes over noncoherent block additive white Gaussian noise (AWGN) and fading channels, with applications to turbo codes. By a block AWGN (or fading) channel, we mean that the carrier phase (or fading) is assumed to be constant over each block but independently varying from one block to another. The union bounds are derived for both noncoherent block AWGN and fading channels. For the block fading channel with a small number of fading blocks, we further derive an improved bound by employing Gallager's first bounding technique. The analytical bounds are compared to the simulation results for a coded block-based differential phase shift keying (B-DPSK) system under a practical noncoherent iterative decoding scheme proposed by Chen et al. We show that the proposed Gallager bound is very tight for the block fading channel with a small number of fading blocks, and the practical noncoherent receiver performs well for a wide range of block fading channels",2006,0,
770,771,ConfErr: A tool for assessing resilience to human configuration errors,"We present ConfErr, a tool for testing and quantifying the resilience of software systems to human-induced configuration errors. ConfErr uses human error models rooted in psychology and linguistics to generate realistic configuration mistakes; it then injects these mistakes and measures their effects, producing a resilience profile of the system under test. The resilience profile, capturing succinctly how sensitive the target software is to different classes of configuration errors, can be used for improving the software or to compare systems to each other. ConfErr is highly portable, because all mutations are performed on abstract representations of the configuration files. Using ConfErr, we found several serious flaws in the MySQL and Postgres databases, Apache web server, and BIND and djbdns name servers; we were also able to directly compare the resilience of functionally-equivalent systems, such as MySQL and Postgres.",2008,0,
771,772,Diagnostic and protection of inverter faults in IPM motor drives using wavelet transform,"This paper presents a novel faults diagnostic and protection technique for interior permanent magnet (IPM) motor drives using wavelet transform. The proposed wavelet based diagnostic and protection technique for inverter faults is developed and implemented in real-time for a voltage source inverter fed IPM motor. In the proposed technique, the motor currents of different faulted and unfaulted conditions of an IPM motor drive system are preprocessed by wavelet packet transform. The wavelet packet transformed coefficients of motor currents are used as inputs of a three-layer wavelet neural network. The performances of the proposed diagnostic and protection technique are investigated in simulation and experiments. The proposed technique is experimentally tested on a laboratory 1-hp IPM motor drive using the ds1102 digital signal processor board. The test results showed satisfactory performances of the proposed diagnostic and protection technique in terms of speed, accuracy and reliability.",2008,0,
772,773,FTDIS: A Fault Tolerant Dynamic Instruction Scheduling,"In this work, we target the robustness for controller scheduler of type Tomasulo for SEU faults model. The proposed fault-tolerant dynamic scheduling unit is named FTDIS, in which critical control data of scheduler is protected from driving to an unwanted stage using Triple Modular Redundancy and majority voting approaches. Moreover, the feedbacks in voters produce recovery capability for detected faults in the FTDIS, enabling both fault mask and recovery for system. As the results of analytical evaluations demonstrate, the implemented FTDIS unit has over 99% fault detection coverage in the condition of existing less than 4 faults in critical bits. Furthermore, based on experiments, the FTDIS has a 200% hardware overhead comparing to the primitive dynamic scheduling control unit and about 50% overhead in comparision to a full CPU core. The proposed unit also has no performance penalty during simulation. In addition, the experiments show that FTDIS consumes 98% more power than the primitive unit.",2010,0,
773,774,Single-switch power factor correction AC/DC converter with storage capacitor size reduction,"In universal line applications with hold-up time requirement, the single-stage PFC AC/DC converters may not be more attractive than the conventional two-stages approach if the size and cost of the storage capacitor are too high. Furthermore, computer related applications, in which the holdup time is a very important requirement, will have to comply with Class D limits of the low frequency harmonic regulation IEC 61000-3-2. Therefore, for these applications, a not very distorted line current will be required. In this paper, a new single-stage AC/DC converter suitable for universal line applications is proposed. The main difference with other solutions is the low voltage swing on the storage capacitor while the line varies within its universal range. This feature allows reducing the size and cost of the storage capacitor. Additional advantages of the proposed converter are topology simplicity (single-switch converter) and IEC 61000-3-2 Class D compliance. The experimental results confirms the above mentioned advantages.",2003,0,
774,775,A Java API for advanced faults management,"The paper proposes an alternative for modeling managed resources using Java and telecommunication network management standards. It emphasizes functions related to fault management, namely: diagnostic testing and performance monitoring. Based on Java management extension (JMX<sup>TM</sup>), specific extensions are proposed to facilitate diagnostic testing and performance measurements implementation. The new API also called Java fault management extension (JFMX) consists of managed objects that model real resources being tested or monitored and support objects defined for the need of diagnostic testing and performance measurements. The paper discusses four Java implementations of a 3-tier client/server scenario focusing on the SystemUnderTest package of the new API to instrument a minimalist managed system scenario. These implementations are respectively built on top of the following Java based communication infrastructures: JMX/JFMX, RMI, CORBA/Java, and Voyager<sup>TM</sup>. The paper extends the Voyager implementation with JMX/JFMX and uses their dynamic and advanced features to provide a highly efficient solution. The later implementation also uses the mobile agent paradigm to overcome well-known limitations of the RPC based implementations",2001,0,
775,776,Operating system function reuse to achieve low-cost fault tolerance,The aim of this article is to propose a new approach to fault tolerance in single processor embedded systems which is centred on the operating system. Particular attention is put on low-cost techniques that exploit functions already present in the system in a different than-usual way to achieve protection with little or no intervention at the application level. An example is given: the realization of a checkpoint and rollback scheme through the context switch function.,2004,0,
776,777,PSC-PWM in fault tolerant drive system for EMA operation,The introduction of EMA Systems requires the use of redundant inverters to drive the EMA and ensure reliability and safety. Redundant converters allow the implementation of fault tolerant control and high quality operation. Fault control has been implemented by means of redundant converter and fault detection system.,2010,0,
777,778,A fault-tolerant real-time supervisory scheme for an interconnected four-tank system,"In this paper, the implementation of a Command Governor (CG) strategy on a real-time computing system is described for the supervision of a laboratory four-tank test-bed. In particular, the real-time architecture has been developed on the RTAI/Linux operating system kernel and the CG module has been implemented in C++ on a general purpose off-the-shelf computing unit. An accurate model of the the four-tank process has been derived from both physical and experimental data and the applicability of the proposed method has been proved by means of real-time tests, which testified on the CG strategy ability to enforce the prescribed operative constraints even under unexpected adverse conditions, e.g. water pumps failures.",2010,0,
778,779,Evaluation of fault tolerance latency from real-time application's perspectives,"Information on Fault Tolerance Latency (FTL), which is defined as the total time required by all sequential steps taken to recover from an error, is important to the design and evaluation of fault-tolerant computers used in safety-critical real-time control systems with deadline information. In this paper, we evaluate FTL in terms of several random and deterministic variables accounting for fault behaviors and/or the capability and performance of error-handling mechanisms, while considering various fault tolerance mechanisms based on the trade-off between temporal and spatial redundancy, and use the evaluated FTL to check if an error-handling policy can meet the Control System Deadline (CSD) for a given real-time application",2000,0,
779,780,Research of Remote Fault Diagnosis System Based on Internet,"The technology of intelligent multi-agents is applied to design remote fault diagnosis system based on Internet. The system owns the kernel of the remote fault diagnosis platform, (RFDP) and the members of manufacturers and enterprise client. It is a distributed, remote monitoring and on-line diagnosis system. The system has overcome the function limitations of 2 kinds of traditional client/server architecture, equipment client-end remote diagnostic mode and manufacturer-end remote diagnostic mode. The platform has a rapid diagnosis response and makes it easy to realize information transmitting timely between platform and diagnosis members. For this reason, RFDP which holds a great ability for enabling on-line monitoring, general fault diagnosis, repairing service and updating knowledge base rapidly, can give a good service for remote distributed multi- equipments and multi-manufactures. As a common diagnosis platform, the system can be applied to various remote mechanical and electronic equipment diagnosis areas such as CNC and press machine diagnosis.",2007,0,
780,781,Application of compensation method in calculating symmetrical short circuit fault,"Based on compensation method, Symmetrical short-circuit fault current formula and nodal voltage formula are deduced in this paper. In the deduction process the time of calculating matrix inversion is eliminated for the node-admittance matrix being not modified when the fault occurs, however triangular matrix method is applied to calculate nodal impedance matrix at the program entrance in the process based on the original network nodal admittance matrix, thus the solution of the electrical network state variables is speed up with preparing data for fault calculation in advance. After further assumptions and simplification, short-circuit current formula is more efficient to estimate the size of short-circuit current, and it is very suitable for real-time online applications. At the same time, current coefficient power contributing which is different from power current distribution coefficient is put forward. A optimal node is found for new power supply to limit short-circuit current by judging the size of current coefficient power contributing.",2010,0,
781,782,On the design of error detection and correction cryptography schemes,"The paper introduces the method of modifying cryptography encryption and decryption units, which includes circuitry of checks that operations have been performed without errors. This technique is based on addition to storage devices, error correction codes, and module check of arithmetic and logic units operations",2000,0,
782,783,A platooning controller robust to vehicular faults,This paper presents a platooning controller for a four-wheel-driving four-wheel-steering vehicle to follow another. The controller is based on the full-state tracking theory and utilizes a vehicular model that makes it able to continue to operate when faults are detected at its steering systems or driving motors which are disabled accordingly. The unified controller is also able to track and follow the target either moving forward in front or moving backward in the back of the vehicle making the real-time implementation of different tracking modes simple. Tracking stability is secured by the proper selection of design parameters. Simulations show the proposed control scheme works properly even in the presence of faults at several different parts.,2004,0,
783,784,A new automated instrumentation for emulation-based fault injection,"Soft errors are an increasing threat in up-to-date technologies, so robustness evaluation has become an important part of digital circuit design. Emulation-based fault injection techniques have proved to be an efficient approach to perform such evaluations. In this paper, we propose new optimizations further improving the experimental duration and the instrumentation cost while maintaining the maximum flexibility for the dependability evaluation process.",2010,0,
784,785,Automatic Generation of Detection Algorithms for Design Defects,"Maintenance is recognised as the most difficult and expansive activity of the software development process. Numerous techniques and processes have been proposed to ease the maintenance of software. In particular, several authors published design defects formalising ""bad"" solutions to recurring design problems (e.g., anti-patterns, code smells). We propose a language and a framework to express design defects synthetically and to generate detection algorithms automatically. We show that this language is sufficient to describe some design defects and to generate detection algorithms, which have a good precision. We validate the generated algorithms on several programs",2006,0,
785,786,Analysis of fault-tolerant five-phase IPM synchronous motor,"The choice of a multi-phase motor is a potentially fault-tolerant solution and gives rise to many advantages, respect to the traditional three-phase motor drives. In this paper an high torque density five-phase IPM synchronous motor has been studied, and the motor performance have been evaluated in the case of healthy-mode and faulty-mode operation.",2008,0,
786,787,Space shuttle fault tolerance: Analog and digital teamwork,"The Space Shuttle control system (including the avionics suite) was developed during the 1970s to meet stringent survivability requirements that were then extraordinary but today may serve as a standard against which modern avionics can be measured. In 30 years of service, only two major malfunctions have occurred, both due to failures far beyond the reach of fault tolerance technology: the explosion of an external fuel tank, and the destruction of a launch-damaged wing by re-entry friction. The Space Shuttle is among the earliest systems (if not the earliest) designed to a ldquoFO-FO-FSrdquo criterion, meaning that it had to Fail (fully) Operational after any one failure, then Fail Operational after any second failure (even of the same kind of unit), then Fail Safe after most kinds of third failure. The computer system had to meet this criterion using a Redundant Set of 4 computers plus a backup of the same type, which was (ostensibly!) a COTS type. Quadruple redundancy was also employed in the hydraulic actuators for elevons and rudder. Sensors were installed with quadruple, triple, or dual redundancy. For still greater fault tolerance, these three redundancies (sensors, computers, actuators) were made independent of each other so that the reliability criterion applies to each category separately. The mission rule for Shuttle flights, as distinct from the design criterion, became ldquoFO-FS,rdquo so that a mission continues intact after any one failure, but is terminated with a safe return after any second failure of the same type. To avoid an unrecoverable flat spin during the most dynamic flight phases, the overall system had to continue safe operation within 400 msec of any failure, but the decision to shut down a computer had to be made by the crew. Among the interesting problems to be solved were ldquocontrol sliveringrdquo and ldquosync holes.rdquo The first flight test (Approach and Landing only) was the proof of the pudding: when a key wire harness solder - joint was jarred loose by the Shuttle's being popped off the back of its 747 mother ship, one of the computers ldquowent bananasrdquo (actual quote from an IBM expert).",2009,0,
787,788,Assessing the impact of active guidance for defect detection: a replicated experiment,"Scenario-based reading (SBR) techniques have been proposed as an alternative to checklists to support the inspectors throughout the reading process in the form of operational scenarios. Many studies have been performed to compare these techniques regarding their impact on the inspector performance. However, most of the existing studies have compared generic checklists to a set of specific reading scenarios, thus confounding the effects of two SBR key factors: separation of concerns and active guidance. In a previous work we have preliminarily conducted a repeated case study at the University of Kaiserslautern to evaluate the impact of active guidance on inspection performance. Specifically, we compared reading scenarios and focused checklists, which were both characterized as being perspective-based. The only difference between the reading techniques was the active guidance provided by the reading scenarios. We now have replicated the initial study with a controlled experiment using as subjects 43 graduate students in computer science at University of Bari. We did not find evidence that active guidance in reading techniques affects the effectiveness or the efficiency of defect detection. However, inspectors showed a better acceptance of focused checklists than reading scenarios.",2004,0,
788,789,A Reducing Transmission-Line Fault Current Method,"In this paper, a reducing transmission-line fault current method with capacitor compensators is proposed to limit the transmission-line fault current in power systems. In the normal mode of operation, the shunt capacitors banks as reactive power compensators that delivers reactive power to increase the power factor and used on medium-length and long transmission lines to increase line loadability and to maintain voltages near rated values. Their important effect is to reduce line-voltage drops and to increase the power factor and the steady-state stability limit. When faults states occurs, the capacitor another effect is to reduce transmission-line fault current peak value. Simulations performed in MATLAB/Simulink environment indicate that the proposed performance for capacitor compensators performs well to limit the fault currents of transmission lines and line-voltage drops.",2010,0,
789,790,Recovery of fault-tolerant real-time scheduling algorithm for tolerating multiple transient faults,"The consequences of missing deadline of hard real time system tasks may be catastrophic. Moreover, in case of faults, a deadline can be missed if the time taken for recovery is not taken into account during the phase when tasks are submitted or accepted to the system. However, when faults occur tasks may miss deadline even if fault tolerance is employed. Because when an erroneous task with larger execution time executes up to end of its total execution time even if the error is detected early, this unnecessary execution of the erroneous task provides no additional slack time in the schedule to mitigate the effect of error by running additional copy of the same task without missing deadline. In this paper, a recovery mechanism is proposed to augment the fault-tolerant real-time scheduling algorithm RM-FT that achieves node level fault tolerance (NLFT) using temporal error masking (TEM) technique based on rate monotonic (RM) scheduling algorithm. Several hardware and software error detection mechanisms (EDM), i.e. watchdog processor or executable assertions, can detect an error before an erroneous task finishes its full execution, and can immediately stops execution. In this paper, using the advantage of such early detection by EDM, a recovery algorithm RM-FT-RECOVERY is proposed to find an upper bound, denoted by Edm Bound, on the execution time of the tasks, and mechanism is developed to provide additional slack time to a fault-tolerant real-time schedule so that additional task copies can be scheduled when error occurs.",2007,0,
790,791,A Method to Evaluate Voltages to Earth During an Earth Fault in an HV Network in a System of Interconnected Earth Electrodes of MV/LV Substations,"An easy and swift method to evaluate, in a system of interconnected earth electrodes, earth potentials on earthing systems of medium-voltage/low-voltage (MV/LV) substations, in an event of single-line-to-earth fault inside a high-voltage/medium- voltage (HV/MV) station, is presented. The advantage of the method is the simplicity of the mathematical model for solving complex systems of any size with a sufficient accuracy for practical purposes. This paper shows the results of simulations, performed on networks with different extensions and characteristics, organized in easy-to-read graphs and tables. A comparison of these results with the values obtained according to the procedure explained in the IEC-Standard 60909-3, and a study on the accuracy of the method has been made. Moreover, some considerations on the inclusion of earth electrodes of HV/MV stations within global earthing systems are done.",2008,0,
791,792,Characterization of Upset-induced Degradation of Error-mitigated Highspeed I/O's Using Fault Injection,"Fault-injection experiments on Virtex-II FPGAs quantify failure and degradation modes in I/O channels incorporating triple modular redundancy (TMR). With increasing frequency (to 100 MHz), full TMR under both I/O standards investigated shows more configuration bits have a measurable performance effect.",2005,0,
792,793,Fault detection and protection system for the power converters with high-voltage IGBTs,"This paper addresses problems related to the design and implementation of a fault detection and protection system for high-voltage (HV) NPT IGBT-based converters. An isolated half-bridge power converter topology is investigated, which seems to be very attractive for the high-power electronic converters due to its overall simplicity, small component count and low realization costs. This converter is to be applied in rolling stock with its demanding reliability and safety requirements. Clearly, the robust control and protection system is essential.",2008,0,
793,794,All Bits Are Not Equal - A Study of IEEE 802.11 Communication Bit Errors,"In IEEE 802.11 Wireless LAN (WLAN) systems, techniques such as acknowledgement, retransmission, and transmission rate adaptation, are frame-level mechanisms designed for combating transmission errors. Recently sub-frame level mechanisms such as frame combining have been proposed by the research community. In this paper, we present results obtained from our bit error study for identifying sub-frame error patterns because we believe that identifiable bit error patterns can potentially introduce new opportunities in channel coding, network coding, forward error correction (FEC), and frame combining mechanisms. We have constructed a number of IEEE 802.11 wireless LAN testbeds and conducted extensive experiments to study the characteristics of bit errors and their location distribution. Conventional wisdom dictates that bit error probability is the result of channel condition and ought to follow corresponding distribution. However our measurement results identify three repeatable bit error patterns that are not induced by channel conditions. We have verified that such error patterns are present in WLAN transmissions in different physical environments and across different wireless LAN hardware platforms. We also discuss our current hypotheses for the reasons behind these bit error probability patterns and how identifying these patterns may help improving WLAN transmission robustness.",2009,0,
794,795,Evaluation of replication and fault detection in P2P-MPI,"We present in this paper an evaluation of fault management in the grid middleware P2P-MPI. One of P2P-MPI's objective is to support environments using commodity hardware. Hence, running programs is failure prone and a particular attention must be paid to fault management. The fault management covers two issues: fault-tolerance and fault detection. P2P-MPI provides a transparent fault tolerance facility based on replication of computations. Fault detection concerns the monitoring of the program execution by the system. The monitoring is done through a distributed set of modules called failure detectors. In this paper, we report results from several experiments which show the overhead of replication, and the cost of fault detection.",2009,0,
795,796,Testing of LUT delay aliasing faults in SRAM based FPGAs using half-frequencies,"In this paper, we present a technique for testing the delay aliasing faults associated with LUTs in SRAM based FPGAs. We compare the outputs of two identical LUTs when one is operated at half the frequency of the other. A Built in Self Test (BIST) circuitry consisting of a Test Pattern Generator, a Comparator, and the Circuit Under Test (CUT) is mapped on the FPGA. Application of input sequence vectors at half frequencies to the LUTs enable the detection of delay and aliasing faults which may go undetected by other techniques. The technique is verified using VHDL based simulations. The results are also experimentally verified using a Virtex II FPGA board.",2007,0,
796,797,Experimental studies on faults detection using residual generator,"In this paper one will develop the faults detection and localization method using residual vectors, in order to emphasize the noises, disturbances and faults on the outputs L<inf>1</inf> and L<inf>2</inf> of the control level plant with two coupled tanks Quanser Water Level Control Two Tank Module. The proposed method was theoretically developed and experimentally verified in this plant and allowed detection and localization of two faults created in a real plant. The experiments presented were realized using Matlab Simulink program.",2010,0,
797,798,Efficient Memory Error Coding for Space Computer Applications,"For the secure transaction of data between the central processing unit (CPU) of a satellite on board-computer and its local random access memory (RAM), the program memory has been usually designed with triple modular redundancy (TMR), which is a hardware implementation that includes replicated memory circuits and voting logic to detect and correct a faulty value. TMR error correction technique allows single correction of one error bit per stored word. For computers on board a satellite, there is however a definite risk of two error bits occurring within one byte of stored data. In this paper, the application of the quasi-cyclic codes to the routine error protection of SRAM program memory for satellites in low Earth orbit is described and implemented in field programmable gate array (FPGA) technology. The proposed device is transparent to the routine transfer of data between CPU and its local RAM",2006,0,
798,799,Immune Systems Inspired Approach to Anomaly Detection and Fault Diagnosis for Engines,"As more electronic devices are integrated into automobiles to improve the reliability, drivability and maintainability, automotive diagnosis becomes increasingly difficult to deal with. Unavoidable design defects, quality variations in the production process as well as different usage patterns make it is infeasible to foresee all possible faults that may occur to the vehicle. As a result, many systems rely on limited diagnostic coverage provided by a diagnostic strategy which tests only for a priori known or anticipated failures, and presumes the system is operating normally if the full set of tests is passed. To circumvent these difficulties and provide a more complete coverage for detection of any fault, a new paradigm for design of automotive diagnostic systems is needed. An approach inspired by the functionalities and characteristics of natural immune system is presented and discussed in the paper. The feasibility of the newly proposed paradigm is also partially demonstrated through application examples.",2007,0,
799,800,Research on Optimal Placement of Travelling Wave Fault Locators in Power Grid,"Taking the full network observability of power system operation state, maximum state measurement redundancy and minimum number of travelling wave fault location device (TFD) as objectives, an TFD optimal placement scheme for power grid fault location with travelling wave is presented in the paper. The scheme contains two steps: static processing and dynamic configuration. Terminal substations should install TFD. Then taking the terminal substations as starting point, the whole network is separated into several unattached branches. The branch which includes the most number of substations, via the longest line, and has not any loop, can install TFD at its both terminals. And then combining with the practical length of each line and coverage range of substations, the optimal disposition of TFD can successfully accomplish. A novel network-based fault location algorithm is also designed with travelling wave velocity on-line measuring and every TFD recorded travelling arrival times fusing. EMTP simulation results show that the TFD optimal placement scheme can use less TFDs to locate all faults in the whole power grid with economy and high reliability. The location error is no more than 100 m.",2008,0,
800,801,A Unity Power Factor Correction Preregulator with Fast Dynamic Response Based on a Low-Cost Microcontroller,"Low cost passive Power Factor Correction (PFC) and Single-Stage PFC converters cannot draw a sinusoidal input current and are only suitable solutions to supply low power levels. PFC preregulators based on the use of a multiplier solve such drawbacks, but a second stage DC/DC converter is needed to obtain fast output voltage dynamics. The output voltage response of PFC preregulators can be improved by increasing the corner frequency of the output voltage feedback loop. The main drawback to obtaining a faster converter output response is the distortion of the input current. This paper describes a simple control strategy to obtain a sinusoidal input current. Based on the static analysis of output voltage ripple, a modified sinusoidal reference is created using a low cost microcontroller in order to obtain a input sinusoidal current. This reference replaces the traditional rectified sinusoidal input voltage reference in PFC preregulators with multiplier control. Using this circuitry, PFC preregulator topologies with galvanic isolation are suitable solutions to design a power supply with fast output voltage response (10 ms or 8.33 ms) and low line current distortion. Finally, theoretical and simulated results are validated using a 500 W prototype.",2007,0,
801,802,Adaptive and Fault Tolerant Simulation of Relativistic Particle Transport with Data-Level Checkpointing,"Many scientific applications exhibit high demands on memory storage and computing capability. Improvements in commodity processors and networks have provided an opportunity to support such scientific applications within an everyday computing infrastructure. Good applications need the ability to work in constantly changing environments. Adaptability and fault tolerance are essential. Based on simulation of relativistic particle transport, this paper proposes a data-level checkpointing scheme for common scientific applications. This scheme takes advantage of the regular program layout, dominant computing loops, and fine-grained iterations. Without handling stack and heap segments directly, only application data is saved and restored as the computation state. Checkpointing interval can be dynamically adjusted to satisfy sensitivity and efficiency requirements for feasible fault tolerance. With this periodic but fixed-location checkpointing scheme, the MPI- based simulation system can be reconfigured by being shut down first and then restarted on same or different computer clusters. Application data can be redistributed for the new configuration. Experimental results have demonstrated this scheme's efficiency and effectiveness.",2008,0,
802,803,An intelligent FFT-analyzer with harmonic interference effect correction and uncertainty evaluation,"In the paper, the problem of the correction of harmonic interference effects on FFT results is discussed. A procedure for the effect evaluation and correction is proposed and implemented in an intelligent FFT-analyzer able also to provide the results with their uncertainty.",2003,0,
803,804,Forward error correction strategies for media streaming over wireless networks,"The success of next-generation mobile communication systems depends on the ability of service providers to engineer new added-value multimedia-rich services, which impose stringent constraints on the underlying delivery/transport architecture. The reliability of real-time services is essential for the viability of any such service offering. The sporadic packet loss typical of wireless channels can be addressed using appropriate techniques such as the widely used packet-level forward error correction. In designing channel-aware media streaming applications, two interrelated and challenging issues should be tackled: accuracy of characterizing channel fluctuations and effectiveness of application-level adaptation. The first challenge requires thorough insight into channel fluctuations and their manifestations at the application level, while the second concerns the way those fluctuations are interpreted and dealt with by adaptive mechanisms such as FEC. In this article we review the major issues that arise when designing a reliable media streaming system for wireless networks.",2008,0,
804,805,Tight exponential upper bounds on the ML decoding error probability of block codes over fully interleaved fading channels,"We derive tight exponential upper bounds on the decoding error probability of block codes which are operating over fully interleaved Rician fading channels, coherently detected and maximum-likelihood decoded. It is assumed that the fading samples are statistically independent and that perfect estimates of these samples are provided to the decoder. These upper bounds on the bit and block error probabilities are based on certain variations of the Gallager bounds. These bounds do not require integration in their final version and they are reasonably tight in a certain portion of the rate region exceeding the cutoff rate of the channel. By inserting interconnections between these bounds, we show that they are generalized versions of some reported bounds for the binary-input additive white Gaussian noise channel.",2003,0,
805,806,Fault location using traveling wave for power networks,"Fault location using traveling wave has been applied in extra-high voltage power grids successfully. Due to its complication and high cost, it is not easy for this technique to be accepted for use in distribution system. A new traveling wave fault location system is developed simply in a cost-effective way for power networks (especially for distribution system) in this paper. Two traveling wave sensors are developed to capture the current traveling wave flowing from the capacitive equipment to earth and the voltage traveling waves in all three phases. The outputs of the sensors are then applied to the trigger and time tagging by using Global Position System (GPS) receiver. The fault position is calculated by the traveling wave arrival times in every power station where only one fault locator is installed. The fault location system is tested in the power system. Testing results show that the fault locator has high precision and robustness.",2004,0,
806,807,Novel method for selective detection of earth faults in high impedance grounded distribution networks,"An elementary and reliable detection of earth faults in impedance grounded networks results in considerable benefits for the utility both in terms of outage duration and personal safety. This report describes an entirely new, only current measuring method; a method, which fulfils the standards of cost efficiency and reliability. Despite the seeming simplicity of the approach it is also demonstrated that it is an excellent method to detect arcing cable earth faults.",2005,0,
807,808,Fault Tolerance of Tornado Codes for Archival Storage,"This paper examines a class of low density parity check (LDPC) erasure codes called Tornado codes for applications in archival storage systems. The fault tolerance of Tornado code graphs is analyzed and it is shown that it is possible to identify and mitigate worst-case failure scenarios in small (96 node) graphs through use of simulations to find and eliminate critical node sets that can cause Tornado codes to fail even when almost all blocks are present. The graph construction procedure resulting from the preceding analysis is then used to construct a 96-device Tornado code storage system with capacity overhead equivalent to RAID 10 that tolerates any 4 device failures. This system is demonstrated to be superior to other parity-based RAID systems. Finally, it is described how a geographically distributed data stewarding system can be enhanced by using cooperatively selected Tornado code graphs to obtain fault tolerance exceeding that of its constituent storage sites or site replication strategies",2006,0,
808,809,Floating-point error analysis based on affine arithmetic,"During the development of floating-point signal processing systems, an efficient error analysis method is needed to guarantee the output quality. We present a novel approach to floating-point error bound analysis based on affine arithmetic. The proposed method not only provides a tighter bound than the conventional approach, but also is applicable to any arithmetic operation. The error estimation accuracy is evaluated across several different applications which cover linear operations, nonlinear operations, and feedback systems. The accuracy decreases with the depth of computation path and also is affected by the linearity of the floating-point operations.",2003,0,
809,810,A novel approach to faulted-phase selection using current traveling waves and wavelet analysis,"The early traveling wave faulted-phase selectors, due to lack of effective tool to process transient signals, had to directly used instantaneous values of signals so that they cannot overcome such bad influence as noise disturbance. Fortunately, wavelet analysis, with its time-frequency localization ability and the wavelet transform modulus maxima (WTMM) concept, is well suited to treat with singularity of fault-generated traveling waves in EHV/UHV transmission lines. This paper presents a novel approach to fast and accurate phase-selection, which used the WTMM of initial model current traveling waves according to the fault characteristic relations deduced from the boundary conditions of various types of faults. The criterion is explicit in characteristics and physical concepts, and is apt to be realized. A large number of EMTP simulations demonstrated the new faulted-phase selection algorithm.",2002,0,
810,811,A Case Study of Bias in Bug-Fix Datasets,"Software quality researchers build software quality models by recovering traceability links between bug reports in issue tracking repositories and source code files. However, all too often the data stored in issue tracking repositories is not explicitly tagged or linked to source code. Researchers have to resort to heuristics to tag the data (e.g., to determine if an issue is a bug report or a work item), or to link a piece of code to a particular issue or bug. Recent studies by Bird et al. and by Antoniol et al. suggest that software models based on imperfect datasets with missing links to the code and incorrect tagging of issues, exhibit biases that compromise the validity and generality of the quality models built on top of the datasets. In this study, we verify the effects of such biases for a commercial project that enforces strict development guidelines and rules on the quality of the data in its issue tracking repository. Our results show that even in such a perfect setting, with a near-ideal dataset, biases do exist - leading us to conjecture that biases are more likely a symptom of the underlying software development process instead of being due to the used heuristics.",2010,0,
811,812,Concurrent and simple digital controller of an AC/DC converter with power factor correction based on an FPGA,"Nowadays, most digital controls for power converters are based on DSPs. This paper presents a field programmable gate array (FPGA) based digital control for a power factor correction (PFC) flyback AC/DC converter. The main difference from DSP-based solutions is that FPGAs allow concurrent operation (simultaneous execution of all control procedures), enabling high performance and novel control methods. The control algorithm has been developed using a hardware description language (VHDL), which provides great flexibility and technology independence. The controller has been designed as simple as possible while maintaining good accuracy and dynamic response. Simulations and experimental results show the feasibility of the method, opening interesting possibilities in power converters control.",2003,0,
812,813,Feature set evaluation and fusion for motor fault diagnosis,"This paper proposes a novel approach to the feature fusion in motor fault diagnosis with the main aim of improving the performance and reliability of clustering and identification of the fault patterns. In addition, the significance of individual feature sets in specific fault scenarios, which is normally gained by engineers through experience, is investigated by using flexible Non-Gaussian modeling of the historical data. Furthermore the comparison is made by applying individual and fusion of feature sets to the probabilistic distributions of trained models using a Maximum a Posteriori (MAP) approach. To carry out the task, current waveforms are collected non-invasively from three-phase DC motors. Waveforms are then compressed into time, frequency and wavelet feature sets to form the input to the clustering algorithm. The result demonstrates the suitability of specific feature sets in different motor modes and the efficiency of fusion which is carried out with a Winner Takes All (WTA) approach.",2010,0,
813,814,High-speed serial communication with error correction using 0.25 m CMOS technology,"In this paper we propose a novel design for an autonomous high-speed serial off and on-chip communication system which incorporates impedance tuning, error correction with a packet transfer and a parallel asynchronous interface. The constructed transmitter-receiver pair has throughput of 5 Gbit/s. With error correction and packet transfer overhead accounted for this construct has bandwidth of 500 <bytes/s. The circuit has been simulated using HSpice with 0.25 m TMSC CMOS technology",2001,0,
814,815,"Attacking ""bad actor"" and ""no fault found"" electronic boxes","A the percentage of what are termed ""bad actor"" and no fault found (NFF) electronic box in military weapon systems is steadily growing. These are boxes that fail during operation, but test NFF during back shop testing, or, that fail during back shop testing and then test NFF at the depot repair facility. During operation, an electronic box is stressed by various environmental conditions which are normally absent on a test bench. If there are cold or cracked solder joints, corroded or dirty connector contacts, loose crimp joints, hairline cracks in a ribbon cable trace, or other intermittent conditions, the intermittency can occur while the box is under stress conditions, yet seldom occur while the box is on a test bench at room temperature. Very little concerted effort is currently focused on detecting, isolating and repairing these intermittent problems. Virtually all testing activity simply tests the unit for normal operation, one function, one circuit, or one set of circuits at a time. If an intermittent circuit is not displaying its intermittent nature at the instant it is being tested, the intermittency remains undetected. A three-pronged effort is currently underway to attack and repair bad actor and NFF electronic boxes. The first is to collect detailed repair data to identify which boxes are bad actor and NFF units. The second is to collect test data to determine which units yield inconsistent test results between back shop testing and depot testing, and why. The third is to employ a system that detects and isolates electronic box intermittent circuits. This paper describes the success realized to date by employing each of the three techniques described above, and how they are now effectively being employed together to reduce maintenance costs and improve avionics reliability for the F-16 weapon system.",2005,0,
815,816,Software-Implemented Fault Injection at Firmware Level,"Software-implemented fault injection is an established method to emulate hardware faults in computer systems. Existing approaches typically extend the operating system by special drivers or change the application under test. We propose a novel approach where fault injection capabilities are added to the computer firmware. This approach can work without any modification to operating system and / or applications, and can support a larger variety of fault locations. We discuss four different strategies in X86/X64 and Itanium systems. Our analysis shows that such an approach can increase portability, the non-intrusiveness of the injector implementation, and the number of supported fault locations. Firmware-level fault injection paves the way for new research directions, such as virtual machine monitor fault injection or the investigation of certified operating systems.",2010,0,
816,817,Application of neural networks and filtered back projection to wafer defect cluster identification,"During an electrical testing stage, each die on a wafer must be tested to determine whether it functions as it was originally designed. In the case of a clustered defect on the wafer, such as scratches, stains, or localized failed patterns, the tester may not detect all of the defective dies in the flawed area. To avoid the defective dies proceeding to final assembly, an existing tool is currently used by a testing factory to detect the defect cluster and mark all the defective dies in the flawed region or close to the flawed region; otherwise, the testing factory must assign five to ten workers to check the wafers and hand mark the defective dies. This paper proposes two new wafer-scale defect cluster identifiers to detect the defect clusters, and compares them with the existing tool used in the industry. The experimental results verify that one of the proposed algorithms is very effective in defect identification and achieves better performance than the existing tool.",2002,0,
817,818,Proactive fault management based on risk-augmented routing,"Carrier networks need to provide their customers with high availability of communication services. Unfortunately, failures are managed by recovery mechanisms getting involved only after the failure occurrence to limit the impact on traffic flows. However, there are often forewarning signs that a network device will stop working properly. We propose to take into account this risk exposure in order to improve the performance of the existing restoration mechanisms, in particular for IP networks. Based on an embedded and real-time risk-level assessment, we can perform a proactive fault-management and isolate the failing routers out of the routed topology, and thus totally avoid service unavailability. Our novel approach enables routers to preventively steer traffic away from risky paths by temporally tuning OSPF link cost.",2010,0,
818,819,Optimized resource allocation in grid networks using genetic algorithm with error rate factor,"Grid computing is an emerging computing paradigm that will have significant impact on the next generation information infrastructure. Due to the largeness and complexity of grid system, its quality of service, performance and reliability are difficult to model, analyze and evaluate. In real time evaluation, various noises will influence the model and which in turn accounts for increase in packet loss and Bit Error Rate (BER). Therefore, a novel optimization model for maximizing the expected grid service profit is mandatory. In our work, to achieve the improvement in the end to end grid network performance, an optimizer, which is based on Genetic Algorithm (GA) with Fitness Evaluation parameters considers BER and Service Execution Time, is designed in the RMS. This paper presents the novel tree structured model, is better than other existing models for grid computing performance and reliability analysis by not only considering data dependence and failure correlations, but also takes link failure, packet loss & BER real time parameters in account. The algorithm based on the Graph theory and Probability theory.",2009,0,
819,820,Improving SNR for DSM Linear Systems Using Probabilistic Error Correction and State Restoration: A Comparative Study,"Smaller feature sizes and lower supply voltages make DSM devices more susceptible to soft errors generated by alpha particles and neutrons as well as other sources of environmental noise. In this scenario, soft-error/noise tolerant techniques are necessary for maintaining the SNR of critical DSP applications. This paper studies linear DSP circuits and discusses two low cost techniques for improving the SNR of DSP filters. Both techniques use a single checksum variable for error detection. This gives a distance two code that is traditionally good for error detection but not correction. In this paper, such a code is used to improve SNR rather than perfectly remove the error. The first technique, 'checksum-based probabilistic error correction', uses the value indicated by the checksum variable to probabilistically correct the error and achieves up to 5 dB improvement in the SNR value. The second technique, 'state restoration', works well when the length of burst errors is small and the error magnitude is large. A general error statistics has been defined as a random process and the distribution of SNR is compared for the two proposed techniques",2006,0,
820,821,Simulation of Elman Neural network Extension Strategy Generator to Pattern Deformation Error in Flexibility Material Treating Field,"After analyzing flexibility material processing (such as quilting processing) influencing factor of pattern deformation, the edges of the original image and the deformation image are extracted. Then they are changed into coordinate. On top of it, the data are put into the Elman neural networks to train which has been built and the original image is used to as the teacher signal to tutoring. At last, the matter extenics model is built qua the database's data of the extension decision strategy generator by analysis the simulation result.",2008,0,
821,822,The dual parameterization approach to optimal least square FIR filter design subject to maximum error constraints,"This paper is concerned with the design of linear-phase finite impulse response (FIR) digital filters for which the weighted least square error is minimized, subject to maximum error constraints. The design problem is formulated as a semi-infinite quadratic optimization problem. Using a newly developed dual parameterization method in conjunction with the Caratheodory's dimensional theorem, an equivalent dual finite dimensional optimization problem is obtained. The connection between the primal and the dual problems is established. A computational procedure is devised for solving the dual finite dimensional optimization problem. The optimal solution to the primal problem can then be readily obtained from the dual optimal solution. For illustration, examples are solved using the proposed computational procedure",2000,0,
822,823,Model of Reliability of the Software with Coxian Distribution of Length of Intervals between the Moments of Detection of Errors,"The generalized software reliability model on the basis of nonstationary Markovian system of service is proposed. Approximation by distribution of Cox allows investigating growth of software reliability for any kinds of distribution of time between the moments of detection of errors and exponential distributions of time of their correction. The model allows receiving the forecast of important characteristics: the number of the corrected and not corrected errors, required time of debugging, etc. The diagram of transitions between states of the generalized model and system of the differential equations are presented. The example of calculation with use of the offered model is considered, research of influence of variation coefficient of Cox distribution of duration of intervals between the error detection moments on values of look-ahead characteristics is executed.",2010,0,
823,824,Managing Faults in the Service Delivery Process of Service Provider Coalitions,"In recent years, IT Service Management (ITSM) has become one of the most researched areas of IT. Incident Management and Problem Management form the basis of the tooling provided by an Incident Ticket System (ITS). As more compound or interdependent services are collaboratively offered by providers, the delivery of a service therefore becomes a responsibility of more than one provider's organization. In the ITS systems of various providers seemingly unrelated tickets are created and the connection between them is not realized automatically. The introduction of automation will reduce human involvement and time required for incident resolution.In this paper we consider a collaborative service delivery model that supports both per-request services and continuous high-availability services. In the case of high availability service the information stored in the ITS of the provider often includes information on the outage of a particular service rather than on the failure of a particular request. In this paper we offer an information model that consolidates and supports inter-organizational incident management and probabilistic model for fault discovery.",2009,0,
824,825,Adaptive partition size temporal error concealment for H.264,"Existing temporal error concealment methods for H.264 often decide the partition size of the lost macroblock (MB) before recovering the motion information, without actual quality comparison between different partition modes. In this paper, we propose to select the best partition mode by minimizing the Weighted Double-Sided External Boundary Matching Error (WDS-EBME), which jointly measures the inter-MB boundary discontinuity, inter-partition boundary discontinuity and intrapartition block artifacts in the recovered MB. The proposed method estimates the best motion vectors for each of the candidate partition modes, calculates the overall WDS-EBME values for them, and selects the partition mode with the smallest overall WDS-EBME to recover the lost MB. We also propose a progressive concealment order for the 4times4 partition mode. Test results show that the adaptive partition size method always outperforms the fixed partition size methods. Both the adaptive and fixed partition size methods are much superior to the temporal error concealment (TEC) method in the H.264 reference software.",2008,0,
825,826,A fault-tolerance mechanism in grid,"Grid appears as an effective technology coupling geographically distributed resources for solving large-scale problems in the wide area network. Fault tolerance in grid system is a significant and complex issue to secure a stable and reliable performance. Until now, various techniques exist for detecting and correcting faults in distributed computing systems. Unfortunately, few energy focus on fault-tolerance in grid environment, especially with the emergence of OGSA. A new fault-tolerant mechanism is needed to detect and recover service faults and nodes crash. Based on our previous work on Java threads state capturing and existing mobile agent techniques, we put forward a fault-tolerant mechanism providing effective fault-handling and recovering methods.",2003,0,
826,827,Building a Transformer Defects Database for UHF Partial Discharge Diagnostics,"In the case of a defective transformer, when a partial discharge is detected and recorded, critical information can be deduced from its pattern, such as the type of defect, its criticality or even information on the level of degradation of the insulation. This information can help to determine the remaining life of the transformer and thus provide criteria for its maintenance and operation. In this paper different artificial PD patterns will be recorded in the laboratory, representative of specific transformer defects, in order to build a database for comparison purposes when measuring on-line. This can greatly improve the recognition and identification of the defect and thus help take some important life assessment conclusions on the transformer.",2007,0,
827,828,RMS bounds and sample size considerations for error estimation in linear discriminant analysis,"The validity of a classifier depends on the precision of the error estimator used to estimate its true error. This paper considers the necessary sample size to achieve a given validity measure, namely RMS, for resubstitution and leave-one-out error estimators in the context of LDA. It provides bounds for the RMS between the true error and both the resubstitution and leave-one-out error estimators in terms of sample size and dimensionality. These bounds can be used to determine the minimum sample size in order to obtain a desired estimation accuracy, relative to RMS. To show how these results can be used in practice, a microarray classification problem is presented.",2010,0,
828,829,Icon based error concealment for JPEG and JPEG 2000 images,"This paper describes methods to recover the useful data in JPEG and JPEG 2000 compressed images and to estimate data for those portions of the image where correct data cannot be recovered. These techniques are designed to handle the loss of hundreds of bytes in the file. No use is made of restart markers or other optional error detection features of JPEG and JPEG 2000, but an uncorrupted low resolution version of the image, such as an icon, is assumed to be available. These icons are typically present in Exif or JFIF format JPEG files.",2003,0,
829,830,An efficient spatial domain error concealment method for H.264 video,"This paper presents an efficient spatial domain error concealment method for the forthcoming video coding standard H.264. In H.264, a frame is divided into 44 blocks during the encoding procedure. For natural image signal, the blocks are smoothly connected with each other. Based on this property, a linear smoothness constraint equation that describes the connection of the lost block and its neighboring blocks can be constructed. By solving this equation, the coefficients of lost block can be recovered. Because the reconstructed high frequency coefficients may be affected by noise, the recovered center pixel may have obvious error. To eliminate the error, we use the recovered pixels that are on the boundaries of the lost block and average pixel difference to interpolate the center pixels. The implementation is simple and is suitable for real-time video application. Experimental results show our method has better recovery result than conventional approach.",2003,0,
830,831,Fault-tolerant voltage-fed PWM inverter AC motor drive systems,This paper shows how to integrate fault compensation strategies into two different types of configurations of induction motor drive systems. The proposed strategies provide compensation for open-circuit and short-circuit failures occurring in the converter power devices. The fault compensation is achieved by reconfiguring the power converter topology with the help of isolating and connecting devices. These devices are used to redefine the post-fault converter topology. This allows for continuous free operation of the drive after isolation of the faulty power switches in the converter. Experimental results demonstrate the validity of the proposed systems.,2004,0,
831,832,"""That one's gotta work"" Mars Odyssey's use of a fault tree driven risk assessment process","The Odyssey project was the first mission to Mars after the failures of Mars Climate Orbiter and Mars Polar Lander. In addition to incorporating the results of those failure review boards and responding to external ""Red Team"" reviews, the Odyssey project itself implemented a risk assessment process. This paper describes that process and its use of fault trees as an enabling tool. These trees were used to break the mission down into the functional elements needed to make it a success. By determining how each function could be prevented from executing, a list of failure modes was created. Each fault was individually assessed as to what mitigations could prevent the fault from occurring, as well as what methods should be used to explicitly verify that mitigation. Fault trees turned out to be an extremely useful tool in both identifying risks as well as structuring the development of mitigations.",2002,0,
832,833,3D shape from multi-camera views by error projection minimization,"Traditional shape from silhouette methods compute the 3D shape as the intersection of the back-projected silhouettes in the 3D space, the so called visual hull. However, silhouettes that have been obtained with background subtraction techniques often present miss-detection errors (produced by false negatives or occlusions) which produce incomplete 3D shapes. Our approach deals with miss-detections and noise in the silhouettes. We recover the voxel occupancy which describes the 3D shape by minimizing an energy based on an approximation of the error between the shape 2D projections and the silhouettes. The energy also includes regularization and takes into account the visibility of the voxels in each view in order to handle self-occlusions.",2009,0,
833,834,Investigating no fault found in the aerospace industry,This paper describes a package of work to investigate the root cause of no fault found (NFF) events within the aerospace industry. The project focus is to develop practical guidance for designers and project managers to facilitate a reduction in NFF removal events for both current products and new designs. This investigation forms part of the second phase of the REMM (Reliability Enhancement Methodology and Modelling) project and comprises three diverse investigation activities: (i) examination of NFF issues at a system level that can highlight common areas of concern for all partner companies and across the Aerospace industry; (ii) classification and root cause analysis of service-data collected by partner companies; (iii) system modelling the 'softer' NFF issues to determine the effects of intervention. This paper describes the formulation of the work package strategy and details the progress made during the first year of this three-year project.,2003,0,
834,835,Fault Diagnosis of Mine Hoist Braking System Based on Wavelet Packet and Support Vector Machine,"This paper concerns mine hoist braking system fault diagnosis with the combination of wavelet packet and support vector machine. It is motivated by the scarce of fault samples in mine hoist such requiring very high security system. A novel approach is presented in order to diagnose blockage piston in cylinder, a typical fault of mine hoist braking system. This method mainly consists of three steps: (1) apply 3 levels wavelet package to construct and reconstruct signal of brake distance-time , extract fault feature vectors (2) set up training samples (3) establish a SVM fault classifier to complete fault diagnosis. Experimental results show that SVM method can effectively accomplish the blockage piston in cylinder fault diagnosis of braking system and has a high adaptability for fault diagnosis in the case of smaller number of samples.",2006,0,
835,836,Fault contribution trees for product families,"Software fault tree analysis (SFTA) provides a structured way to reason about the safety or reliability of a software system. As such, SFTA is widely used in mission-critical applications to investigate contributing causes to possible hazards or failures. In this paper we propose an approach similar to SFTA for product families. The contribution of the paper is to define a top-down, tree-based analysis technique, the fault contribution tree analysis (FCTA), that operates on the results of a product-family domain analysis and to describe a method by which the FCTA of a product family can serve as a reusable asset in the building of new members of the family. Specifically, we describe both the construction of the fault contribution tree for a product family (domain engineering) and the reuse of the appropriately pruned fault contribution tree for the analysis of a new member of the product family (application engineering). The paper describes several challenges to this approach, including evolution of the product family, handling of subfamilies, and distinguishing the limits of safe reuse of the FCTA, and suggests partial solutions to these issues as well as directions for future work. The paper illustrates the techniques with examples from applications to two product families.",2002,0,
836,837,Application of Fault Tolerant Control Using Sliding Modes With On-line Control Allocation on a Large Civil Aircraft,This paper describes an on-line sliding mode control allocation scheme for fault tolerant control of the lateral and longitudinal axes of the non-linear B747 aircraft. The effectiveness level of the actuators is used by the control allocation scheme to redistribute the control signals to the functioning actuators when a fault or failure occurs. The simulation results on the non-linear B747 model show good performance when tested on different fault and even certain total actuator failure scenarios without reconfiguring the controller.,2007,0,
837,838,Analysis of the effects of real and injected software faults: Linux as a case study,The application of fault injection in the context of dependability benchmarking is far from being straightforward. One decisive issue to be addressed is to what extent injected faults are representative of actual faults. This paper proposes an approach to analyze the effects of real and injected faults.,2002,0,
838,839,Anomaly Detection Support Vector Machine and Its Application to Fault Diagnosis,"We address the issue of classification problems in the following situation: test data include data belonging to unlearned classes. To address this issue, most previous works have taken two-stage strategies where unclear data are detected using an anomaly detection algorithm in the first stage while the rest of data are classified into learned classes using a classification algorithm in the second stage. In this study, we propose anomaly detection support vector machine (ADSVM) which unifies classification and anomaly detection. ADSVM is unique in comparison with the previous work in that it addresses the two problems simultaneously. We also propose a multiclass extension of ADSVM that uses a pairwise voting strategy. We empirically present that ADSVM outperforms two-stage algorithms in application to an real automobile fault dataset, as well as to UCI benchmark datasets.",2008,0,
839,840,Study on Fault Diagnosis Expert System for Power Supply Circuit Board on Vxi Bus,A high-tech information electronic equipment of some given type is designed in order to proceed automatically fault detection and improve the efficiency and accuracy of diagnosis. This thesis which is a part of the program introduces the research of algorithm of fault diagnose expert system of a power supply circuit board of an electronic device and algorithm realization and example proving on the hardware platform. It's quicker and more convenient to locate fault on the circuit boards with this equipment. It's proved that this expert system can solve the problems of high cost and long intervals of maintenance and keep the equipment in a stable status,2006,0,
840,841,"Software-based, low-cost fault detection for microprocessors","The PSW-NOP is a low-cost solution to the error detection problem because multiple versions of code or hardware redundancy are not needed. The approach is useful in creating dependable software, especially in deep-submicron ICs. It also supplements to existing approaches of control-flow error detection.",2008,0,
841,842,The Diagnosis System of Mechanical Fault Based on LabVIEW Platform and Its Application,"This paper introduced the content and the status quo of study of the mechanical fault diagnosis, detailed the construction method and the overall structure of mechanical fault diagnosis system based on LabVIEW, and carried out applied research of rotating mechanical fault diagnosis by the use of virtual instrument technology and the use of mechanical fault diagnosis system based on the LabVIEW platform according to rotating machinery and its characteristics.",2009,0,
842,843,Error Vector Magnitude Measurement On Cascaded Butler Matrices System,"This paper describes error vector magnitude (EVM) measurement on vehicle communication system that employs low noise amplifiers (LNAs) and cascading Butler Matrices in producing broad beam high linearity and high gain narrow beam system. The output signals from the first Butler Matrix that have high gain and narrow beam width can be used for long distance communication while the outputs from the second Butler Matrix, which have high linearity, and broad beam width can be used for short range communications.",2007,0,
843,844,Fault Tolerant Control for Nonlinear Systems: Sum-of-Squares Optimization Approach,"In this paper, the fault tolerant control problem of nonlinear systems against actuator failures is considered. By representing the open-loop nonlinear systems in a state dependent linear-like polynomial form and implementing a special class of Lyapunov functions, the above problem can be formulated in terms of state dependent linear polynomial inequalities. Semidefinite programming relaxations based on the sum of squares decomposition are then used to efficiently solve such inequalities.",2007,0,
844,845,Increasing data TLB resilience to transient errors,"This paper first demonstrates that a large fraction of data TLB entries are dead (i.e., not used again before being replaced) for many applications at any given time during execution. Based on this observation, it then proposes two alternate schemes that replicate actively accessed data TLB entries in these dead entries to increase the resilience of the TLB against transient errors.",2005,0,
845,846,An approach for analysing the propagation of data errors in software,"We present a novel approach for analysing the propagation of data errors in software. The concept of error permeability is introduced as a basic measure upon which we define a set of related measures. These measures guide us in the process of analysing the vulnerability of software to find the modules that are most likely exposed to propagating errors. Based on the analysis performed with error permeability and its related measures, we describe how to select suitable locations for error detection mechanisms (EDMs) and error recovery mechanisms (ERMs). A method for experimental estimation of error permeability, based on fault injection, is described and the software of a real embedded control system analysed to show the type of results obtainable by the analysis framework. The results show that the developed framework is very useful for analysing error propagation and software vulnerability and for deciding where to place EDMs and ERMs.",2001,0,
846,847,MPEG-2 error concealment based on block-matching principles,"The MPEG-2 compression algorithm is very sensitive to channel disturbances due to the use of variable-length coding. A single bit error during transmission leads to noticeable degradation of the decoded sequence quality, in that part or an entire slice information is lost until the next resynchronization point is reached. Error concealment (EC) methods, implemented at the decoder side, present one way of dealing with this problem. An error-concealment scheme that is based on block-matching principles and spatio-temporal video redundancy is presented in this paper. Spatial information (for the first frame of the sequence or the next scene) or temporal information (for the other frames) is used to reconstruct the corrupted regions. The concealment strategy is embedded in the MPEG-2 decoder model in such a way that error concealment is applied after entire frame decoding. Its performance proves to be satisfactory for packet error rates (PER) ranging from 1% to 10% and for video sequences with different content and motion and surpasses that of other EC methods under study",2000,0,
847,848,Accelerated functional modeling of aircraft electrical power systems including fault scenarios,"The more-electric aircraft concept is a fast-developing trend in modern aircraft power systems and will result in an increase in electrical loads fed by power electronic converters. Finalizing the architectural bus paradigm for the next generation of more-electric aircraft involves extensive simulations ensuring power system integrity. Since the possible number of loads in an on-board power system can be very large, the development of accurate, effective and computational time-saving models is of great importance. This paper focuses on development of a modeling approach based-on functional representation of individual power system units. This provides for possibility of fast simulation of a full generator-load power system under both normal and fault conditions. The paper describes the modeling principle, illustrates the acceleration attainable and shows how the functional representation can handle fault scenarios.",2009,0,
848,849,A Flexible Fault-Tolerance Mechanism for the Integrade Grid Middleware,"Computer grids have attracted great attention of both academic and enterprise communities, becoming an attractive alternative for the execution of applications that demand huge computational power, allowing the integration of computational resources spread through different administrative domains. The dynamic nature of the grid infrastructure, its high scalability, and great heterogeneity exacerbates the likelihood of errors occurrence, imposing fault tolerance as a major requirement for grid middlewares. This paper describes a flexible fault-tolerance mechanism implemented on integrate grid middleware that allows the customization of several fault tolerance parameters and the combination of different fault tolerance techniques. This paper also presents several experiments that measure the benefits of our approach, considering several different execution environments scenarios.",2007,0,
849,850,PMSM Bearing Fault Detection by means of Fourier and Wavelet transform,This paper presents a study of permanent magnet synchronous machines (PMSM) with bearing fault using a two-dimensional (2-D) finite element analysis (FEA). Fourier fast and wavelet transform were used to fault detection of bearing damage under stationary and non stationary working conditions. Simulation were carried out and compared with experimental results.,2007,0,
850,851,Testing for missing-gate faults in reversible circuits,"Logical reversibility occurs in low-power applications and is an essential feature of quantum circuits. Of special interest are reversible circuits constructed from a class of reversible elements called k-CNOT (controllable NOT) gates. We review the characteristics of k-CNOT circuits and observe that traditional fault models like the stuck-at model may not accurately represent their faulty behavior or test requirements. A new fault model, the missing gate fault (MGF) model, is proposed to better represent the physical failure modes of quantum technologies. It is shown that MGFs are highly testable, and that all MGFs in an N-gate k-CNOT circuit can be detected with from one to [N/2] test vectors. A design-for-test (DFT) method to make an arbitrary circuit fully testable for MGFs using a single test vector is described. Finally, we present simulation results to determine (near) optimal test sets and DFT configurations for some benchmark circuits.",2004,0,
851,852,Analysis of arcing fault models,"The objective of this paper is to present, discuss and compare, in some detail, the arc models used to represent an arcing fault, in order to determine which of them is the most precise for this purpose. At the beginning of the paper a brief explanation of arcing faults is given, bringing out the importance of a realistic simulation of them. A theoretical description of the black box equations to model arc is explained. The paper concludes with a comparison of the most representative arc models. Results will be useful not only in arcing fault detection but also in the design of autoreclosure schemes on transmission lines. Implementation and simulation method are based on ATP/EMTP.",2008,0,
852,853,A Fault-Tolerant Attitude Determination System Based on COTS Devices,"In this paper we present a low cost fault-tolerant attitude determination system to a scientific satellite using COTS devices. We related our experience in developing the attitude determination system, where we combine proven fault tolerance techniques to protect the whole system composed only by COTS from the effects produced by transient faults. We detailed the failure cases and the detection, reconfiguration and recovery schemes that assure the fault-tolerant condition. A testbed system was used to inject faults, evaluate the recovery capability of the fault-tolerant system and validate the solution proposed.",2008,0,
853,854,Secure and fault-tolerant voting in distributed systems,"Concerns about both security and fault-tolerance have had an important impact on the design and use of distributed information systems in the past. As such systems become more prevalent, as well as more pervasive, these concerns will become even more immediately relevant. We focus on integrating security and fault-tolerance into one, general-purpose protocol for secure distributed voting. Distributed voting is a well-known fault-tolerance technique. For the most part, however, security had not been a concern in systems that used voting. More recently, several protocols have been proposed to shore up this lack. These protocols, however, have limitations which make them particularly unsuitable for many aerospace applications, because those applications require very flexible voting schemes (e.g., voting among real-world sensor data). We present a new, more general voting protocol that reduces the vulnerability of the voting process to both attacks and faults. The algorithm is contrasted with the traditional 2-phase commit protocols typically used in distributed voting and with other proposed secure voting schemes. Our algorithm is applicable to exact and inexact voting in networks where atomic broadcast and predetermined message delays are present, such as local area networks. For wide area networks without these properties, we describe yet another approach that satisfies our goals of obtaining security and fault tolerance for a broad range of aerospace information systems",2001,0,
854,855,Fault-based attack of RSA authentication,"For any computing system to be secure, both hardware and software have to be trusted. If the hardware layer in a secure system is compromised, not only it would be possible to extract secret information about the software, but it would also be extremely hard for the software to detect that an attack is underway. In this work we detail a complete end-to-end fault-attack on a microprocessor system and practically demonstrate how hardware vulnerabilities can be exploited to target secure systems. We developed a theoretical attack to the RSA signature algorithm, and we realized it in practice against an FPGA implementation of the system under attack. To perpetrate the attack, we inject transient faults in the target machine by regulating the voltage supply of the system. Thus, our attack does not require access to the victim system's internal components, but simply proximity to it. The paper makes three important contributions: first, we develop a systematic fault-based attack on the modular exponentiation algorithm for RSA. Second, we expose and exploit a severe flaw on the implementation of the RSA signature algorithm on OpenSSL, a widely used package for SSL encryption and authentication. Third, we report on the first physical demonstration of a fault-based security attack of a complete microprocessor system running unmodified production software: we attack the original OpenSSL authentication library running on a SPARC Linux system implemented on FPGA, and extract the system's 1024-bit RSA private key in approximately 100 hours.",2010,0,
855,856,Novel method for fault section identification,"From stability point of view fault section identification is an important task for a transmission systems. In this paper, a novel method for section identification using extended Kalman filter (EKF) has been proposed. Subsynchronous frequency as indicator of fault section identification is estimated by EKF algorithm. Several tests as different fault locations have been performed to show performance of the method. Simulation results reveal high performance of the method. In all tests, the proposed algorithm detects accurately presence of subsynchronous frequency.",2009,0,
856,857,A Dynamic Temporal Error Concealment Algorithm for H.264,"Packet losses or errors of high compressed video stream during transmission over error-prone channel may cause serious decline in video quality. Error concealment (EC) at decoder side is an effective technology to reduce this video degradation. This paper proposes a Dynamic Temporal Error Concealment (DTEC) algorithm for H.264, which chooses different error concealment approach according to the variance of motion vectors of available macro-blocks (MBs) around the lost MB. Furthermore, a recovery method based Directional Temporal Boundary Match Algorithm (DTBMA) is proposed. Experimental results show that the proposed algorithm not only increases PSNR but also improves subjective video quality compared with conventional temporal error concealment algorithms in the case of the same packet loss rate.",2010,0,
857,858,Steward: Scaling Byzantine Fault-Tolerant Replication to Wide Area Networks,"This paper presents the first hierarchical byzantine fault-tolerant replication architecture suitable to systems that span multiple wide-area sites. The architecture confines the effects of any malicious replica to its local site, reduces message complexity of wide-area communication, and allows read-only queries to be performed locally within a site for the price of additional standard hardware. We present proofs that our algorithm provides safety and liveness properties. A prototype implementation is evaluated over several network topologies and is compared with a flat byzantine fault-tolerant approach. The experimental results show considerable improvement over flat byzantine replication algorithms, bringing the performance of byzantine replication closer to existing benign fault-tolerant replication techniques over wide area networks.",2010,0,
858,859,Modeling of hydraulic systems tailored to diagnostic fault detection systems,"The consistent and reliable operation of hydraulic componentry is paramount for many systems. From spacecraft to the most basic automotive bottle jack an undetected failure can have significant consequences if not noticed in time. Many hydraulic systems have diagnostics capabilities but these are normally very limited in scope. They can detect events only related to specifically monitored components or general system failures. Typically these diagnostic systems are designed after the fact and tuned to meet goals. When hydraulic systems are designed, simulation models are frequently used to gain some idea of the finished systems performance. Rarely is the simulation model designed to accommodated and optimize a diagnostic capability. The number and placement of diagnostic sensors can have a significant effect on the ability of a diagnostic system to resolve faults early in their evolution cycle. This paper describes a technique developed at the Penn State Applied Research Laboratories Systems Operations and Automation Department for the design of hydraulic simulation models that pre-incorporate fault diagnostic advanced design features. This technique is being applied to a US Army M1120 heavy expanded mobility tactical truck (HEMTT) load handling system (LHS) supply vehicle that utilizes a palletized hydraulic loading system. The test vehicle has a hydraulic system that was fully instrumented with sensors for this work. This paper addresses a piece in the diagnostic puzzle that has until now been looked at. Namely what additional features in a simulation model allow for optimal placement and monitoring of the hydraulic system. The researchers have found that this typically leads the readdressing or removing certain engineering assumptions that are typically made when designing simulation models. When this is done the models are more flexible when it comes to diagnostic implementation",2006,0,
859,860,Magnets faults characterization for Permanent Magnet Synchronous Motors,"Nowadays Permanent Magnet Synchronous Motor (PMSM) are an attractive alternative to induction machines for a variety of applications due to their higher efficiency, power density and wide constant power speed range. In this context the condition monitoring of magnets status is receiving more and more attention since is critical for industrial applications. This paper presents a characterization of rotor faults for such a motor due to local and uniform demagnetization by means of two dimensional (2-D) Finite Element Analysis (FEA) and proposes a new non-invasive method for their detection by means of a Fourier transform of the back-EMF. The proposed approach is then validated for three permanent magnet synchronous motors with different winding configurations.",2009,0,
860,861,Evaluation of Respiratory Motion Effect on Defect Detection in Myocardial Perfusion SPECT: A Simulation Study,"The objective of this study is to investigate the effects of respiratory motion (RM) on defect detection in Tc-99m sestamibi myocardial perfusion SPECT (MPS) using a phantom population that includes patient variability. Three RM patterns are included, namely breath-hold, slightly enhanced normal breathing, and deep breathing. For each RM pattern, six 4-D NCAT phantoms were generated, each with anatomical variations. Anterior, lateral and inferior myocardial defects with different sizes and contrasts were inserted. Noise-free SPECT projections were simulated using an analytical projector. Poisson noise was then added to generate noisy realizations. The projection data were reconstructed using the OS-EM algorithm with 1 and 4 subsets/iteration and at 1, 2, 3, 5, 7, and 10 iterations. Short-axis images centered at the centroid of the myocardial defect were extracted, and the channelized Hotelling observer (CHO) was applied for the detection of the defect. The CHO results show that the value of the area under the receiver operating characteristics (ROC) curve (AUC) is affected by the RM amplitude. For all the defect sizes and contrasts studied, the highest or optimal AUC values indicate maximum detectability decrease with the increase of the RM amplitude. With no respiration, the ranking of the optimal AUC value in decreasing order is anterior then lateral, and finally inferior defects. The AUC value of the lateral defect drops more severely as the RM amplitude increases compared to other defect locations. Furthermore, as the RM amplitude increases, the AUC values of the smaller defects drop more quickly than the larger ones. We demonstrated that RM affects defect detectability of MPS imaging. The results indicate that developments of optimal data acquisition methods and RM correction methods are needed to improve the defect detectability in MPS.",2009,0,
861,862,A Simulation Environment for the On-Line Monitoring of a Fault Tolerant Flight Control Computer,"An approach of designing a simulation environment for the on-line monitoring of a fault tolerant flight control computer is presented in this paper. The simulation environment is designed to evaluate an improved on-line monitoring technique for processors with a built-in cache. This technique assumes that a monitor checks on-line whether the execution of a program is in accordance with the control flow graph created for the program off-line by a preprocessor. The simulation environment consists of the target processor and the monitor, but also includes carefully chosen benchmark programs, fault injection modules and the preprocessor.",2009,0,
862,863,A New Topology of Fault-current Limiter and its control strategy,"In this paper a new type of fault current limiter based on DC reactor with using superconductor are presented. In normal operation condition the limiter has no obvious effect on loads. When fault happens, the bypass AC reactor and series resistor will insert the fault line automatically to limit the short circuit current, when the control circuit detects a short circuit fault, the solid state bridge in fault line works as an inverter and is closed as soon as possible. Subsequently the fault current is fully limited by the bypass AC reactor and series resistor. The magnitude of L<sub>ac</sub> and r<sub>ac</sub> must be equal with protected load. By using the electro-magnetic transients in DC systems which are the simulator of electric networks (EMTDC) software we carried out analysis of the voltage and current waveforms for fault conditions. Waveforms are considered in calculating the voltage drop at substation during the fault. The analysis used in selecting an appropriate inductance value for designing",2006,0,
863,864,Fault correction profiles,"In general, software reliability models have focused on modeling and predicting the failure detection process and have not given equal priority to modeling the fault correction process. However, it is important to address the fault correction process in order to identify the need for process improvements. Process improvements, in turn, will contribute to achieving software reliability goals. We introduce the concept of a fault correction profile "" a set of functions that predict fault correction events as a function of failure detection events. The fault correction profile identifies the need for process improvements and provides information for developing fault correction strategies. Related to the fault correction profile is the goal fault correction profile. This profile represents the fault correction goal against which the achieved fault correction profile can be compared. This comparison motivates the concept of fault correction process instability, and the attributes of instability. Applying these concepts to the NASA Goddard Space Flight Center fault correction process and its data, we demonstrate that the need for process improvement can be identified, and that improvements in process would contribute to meeting product reliability goals.",2003,0,
864,865,Cross layer error-control scheme for video quality support over 802.11b wireless LAN,"Mitigating the impact of errors on video quality over wireless network has been a major issue of concern which requires highly efficient and effective scheme. The dynamic and heterogeneous nature of the wireless network requires highly sophisticated approach to mitigate the impact of transmission error on video quality. The trade-off between delay and video quality should be considered while designing such applications to reasonably maintain video quality in wireless channel. In order to significantly reduce the impact of high error bit and error burst on transmitted video, more efficient error correction scheme are needed. In this paper, we paper presents an approach using forward error correction and cross layer mechanism which dynamically adapts with the channel condition to recover the loss packets in order to enhance the perceived video quality. The scenario has been simulated using NS-2 and it shows more dramatic improvement in video quality.",2009,0,
865,866,Cycle error correction in asynchronous clock modeling for cycle-based simulation,"As the complexity of SoCs is increasing, hardware/software co-verification becomes an important part of system verification. C-level cycle-based simulation could be an efficient methodology for system verification because of its fast simulation speed. The cycle-based simulation has a limitation in using asynchronous clocks that causes inherent cycle errors. In order to reuse the output of a C-level cycle-based simulation for the verification of a lower level model, the C-level model should be cycle-accurate with respect to the lower level model. In this paper, a cycle error correction technique is presented for two asynchronous clock models. An example design is devised to show the effectiveness of the proposed method. Our experimental results show that the fast speed of cycle-based simulation can be fully exploited without sacrificing the cycle accuracy",2006,0,
866,867,The same is not the same - postcorrection of alphabet confusion errors in mixed-alphabet OCR recognition,"Character sets for Eastern European languages typically contain symbols that are optically almost or fully identical to Latin letters. When scanning documents with mixed Cyrillic-Latin or Greek-Latin alphabets, even high-quality OCR-software is often not able to correctly separate between Cyrillic (Greek) and Latin symbols. This effect leads to an error rate that is far beyond the usual error rates observed when recognizing single-alphabet documents. In this paper we first survey similarities between Latin and Cyrillic (Greek) letters and words for distinct languages and fonts. After briefly introducing a new and public corpus collected by our groups for evaluating OCR-technology over mixed-alphabet documents, we describe how to adapt general algorithms and tools for postcorrection of OCR results to the new context of mixed-alphabet recognition. Experimental results on Bulgarian documents from the corpus and from other sources demonstrate that a drastic reduction of error rates can be achieved.",2005,0,
867,868,Single-phase power-factor-correction AC/DC converters with three PWM control schemes,"Three pulse-width modulation (PWM) control schemes for a single-phase power-factor-correction (PFC) AC/DC converter are presented to improve the power quality. A diode bridge with two power switches is employed as a PFC circuit to achieve a high power factor and low line current harmonic distortion. The control schemes are based on look-up tables with hysteresis current controller (HCC) to generate two-level or three-level PWM on the DC side of diode rectifier. Based on the proposed three control schemes, the line current is driven to follow the sinusoidal current command which is in phase with the supply voltage, and two capacitor voltages on the DC bus are controlled to be balanced. The simulation and experimental results of a 1 kW converter with load as well as line voltage variation and shown to verify the proposed control schemes. It is shown that unity PFC is achieved using a simple control circuit and the measured line current harmonics satisfy the IEC 1000-3-2 requirements",2000,0,
868,869,New EMTP-RV Equivalent Circuit Model of Core-Shielding Superconducting Fault Current Limiter Taking Into Account the Flux Diffusion Phenomenon,"In order to successfully integrate superconducting fault current limiters (SFCL) into electric power system networks, accurate and fast simulation models are needed. This led us to develop a generic electric circuit model of an inductive SFCL, which we implemented in the EMTP-RV software. The selected SFCL is of shielded-core type, i.e. a HTS hollow cylinder surrounds the central leg of a magnetic core, and is located inside a primary copper winding, generating an AC magnetic field proportional to the line current. The model accounts for the highly nonlinear flux diffusion phenomenon across the superconducting cylinder, governed by the Maxwell equations and the non-linear E-J relationship of HTS materials. The computational efficiency and simplicity of this model resides in a judicious 1-D approximation of the geometry, together with the use of an equivalent electric circuit that reproduces accurately the actual magnetic behavior for the flux density (B) inside the walls of the HTS cylinder. The HTS properties are not restricted to the simple power law model, but instead, any resistivity function depending on J, B and T can be used and inserted directly in the model through a non-linear resistance appearing in the equivalent circuit.",2009,0,
869,870,Safety assessment for safety-critical systems including physical faults and design faults,"Two types of faults, design faults and physical faults, are discussed in this paper. Since they are two mutually exclusive and complete fault types on the fault space, the safety assessment of safety-critical computer systems in this paper considers the hazard contribution from both types. A three-state Markov model is introduced to model safety-critical systems. Steady state safety and mean time to unsafe failure (MTTUF) are the two most important metrics for safety assessment. Two homogenous Markov models are derived from the three-state Markov model to estimate the steady state safety and the MTTUF. The estimation results are generalized given the fault space is divided by M mutually exclusive and complete types of faults",2006,0,
870,871,Fault Detection System Based on Embedded Platform,"This paper introduced a gear-box fault detection system that based on PC/104 embedded platform. Gear-box is almost as important as engine in a vehicle, so it is necessary to make sure that it is not out of order when driving. By collecting the vibrant signal from the gear, axes and bearing in gear-box, then processing and analysis the data, we can get acquirement of the working condition of the gear-box. And we can store the information in PC/104 embedded module for a future analysis or send the data to control center or upper PC by network or serial port, then achieving remote monitoring in real-time. This system can help safely driving, and it can reduce the risk of accident that caused by the fault of gear-box.",2008,0,
871,872,The Design and Implementation of Checkpoint/Restart Process Fault Tolerance for Open MPI,"To be able to fully exploit ever larger computing platforms, modern HPC applications and system software must be able to tolerate inevitable faults. Historically, MPI implementations that incorporated fault tolerance capabilities have been limited by lack of modularity, scalability and usability. This paper presents the design and implementation of an infrastructure to support checkpoint/restart fault tolerance in the Open MPI project. We identify the general capabilities required for distributed checkpoint/restart and realize these capabilities as extensible frameworks within Open MPI's modular component architecture. Our design features an abstract interface for providing and accessing fault tolerance services without sacrificing performance, robustness, or flexibility. Although our implementation includes support for some initial checkpoint/restart mechanisms, the framework is meant to be extensible and to encourage experimentation of alternative techniques within a production quality MPI implementation.",2007,0,
872,873,Early error detection in systems-on-chip for fault-tolerance and at-speed debugging,"In this paper we propose a new method for the design of duplex fault-tolerant systems with early error detection and high availability. All the scannable memory elements (flip-flops) of the duplicated system are implemented as multimode memory elements according to Singh et al. (1999), thus allowing during normal operation the accumulation of a signature of its states in its scan-paths. By continuously comparing a 1-bit sequence of the compacted scan-out outputs of the accumulated signatures of the duplicated systems an error can be already detected and a recovery procedure started before an erroneous result appears at the system outputs when a computations is completed. The accumulation of a signature during normal operation can also be used for debugging at-speed. For this application the system need not be duplicated",2001,0,
873,874,"Intelligent, Fault Tolerant Control for Autonomous Systems","We present a methodology for intelligent control of an autonomous and resource constrained embedded system. Geared towards mastering permanent and transient faults by dynamic reconfiguration, our approach uses rules for describing device functionality, valid environmental interactions, and goals the system has to reach. Besides rules, we use functions that characterize a goal's target activity profile. The target activity profile controls the frequency our system uses to reach the corresponding goal. In the paper we discuss a first implementation of the given methodology, and introduce useful extensions. In order to underline the feasibility and effectiveness of the presented control system, we present a case study that has been carried out on a prototype system.",2007,0,
874,875,Low Cost Differential GPS Receivers (LCD-GPS): The Differential Correction Function,"Wireless Sensor Networks (WSN) are used in many applications such as environmental data collection, smart home, smart care and intelligent transportation system. Sensor nodes composing the WSN cooperate together in order to monitor physical entities such as temperature, humidity, sound, atmospheric pressure, motion or pollutants at different locations. To have location information, it is possible to configure nodes with their locations, in small deployments, but in large-scale deployments or when the nodes are mobile, the use of GPS is very interesting. However, the current accuracy of standard civil GPS is not sufficient for all WSN applications. Indeed, GPS measurements suffer from many errors especially in city. To improve GPS accuracy the differential mode (DGPS) has been introduced. In this paper, we present a WSN used to provide a DGPS solution. It's consisting of a set of low cost standard civil GPS communicating receivers. We present the design, implementation and some experimental results of this solution.",2008,0,
875,876,Ground distance relaying algorithm for high resistance fault,"This study proposes a new fault impedance estimation algorithm of phase-ground fault for ground distance relaying based on the negative-, zero- and comprehensive negative-zero-sequence current component. The principle is based on the assumption that fault path is purely resistive, and the phase angle of fault point voltage and fault path current is equal to construct the fault impedance estimating equations for ground distance relay, which can eliminate the effect of fault path resistance, load current and power swing. PSCAD software simulations show the accuracy of proposed algorithm.",2010,0,
876,877,A PIN-Based Dynamic Software Fault Injection System,"Fault injection plays a critical role in the verification of fault-tolerant mechanism, software testing and dependability benchmarking for computer systems. In this paper, according to the characteristics of software faults, we propose a new fault injection design pattern based on the PIN framework provided by Intel company, and develop a PIN-based dynamic software fault injection system (PDSFIS). Faults can be injected by PDSFIS without the source code of target applications under assessment, nor does the injection process involve interruption or software traps. Experimental assessment results of an Apache Web server obtained by the dependability benchmarking are presented to demonstrate the potentials of PDSFIS.",2008,0,
877,878,Evaluating the Fault Tolerance of Stateful TMR,"Module redundancy is often used in the construction of reliable systems. Triple Module Redundancy (TMR) is a method for improving reliability through module redundancy, although it does not give the correct results when two out of three modules fail. We, therefore, proposed a new voting architecture known as Stateful TMR, which uses both the results of TMR and the history of states to select the most reliable module. Through simulations, we evaluate the reliability of a module using both TMR and Stateful TMR, and show that for both transient and permanent failures, Stateful TMR achieves higher reliability than TMR.",2010,0,
878,879,A design of the low-pass filter using the novel microstrip defected ground structure,"A new defected ground structure (DGS) for the microstrip line is proposed in this paper. The proposed DGS unit structure can provide the bandgap characteristic in some frequency bands with only one or more unit lattices. The equivalent circuit for the proposed defected ground unit structure is derived by means of three-dimensional field analysis methods. The equivalent-circuit parameters are extracted by using a simple circuit analysis method. By employing the extracted parameters and circuit analysis theory, the bandgap effect for the provided defected ground unit structure can be explained. By using the derived and extracted equivalent circuit and parameters, the low-pass filters are designed and implemented. The experimental results show excellent agreement with theoretical results and the validity of the modeling method for the proposed defected ground unit structure",2001,0,
879,880,Notice of Retraction<BR>Material inner defect detection by a vibration spectrum analysis,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In this paper is described possibility of non-destructive diagnostics of solid objects by software analysis of vibration spectrum. With using platform MATLAB, we can process and evaluate information from accelerometer, which is placed on the measured object. The analog signal is digitized by special I/O device dSAPCE CP-1104 and then processed offline with FFT (Fast Fourier Transform). The power spectrum is then examined by developed evaluating procedures and individual results are displayed in bar graph. When we take a look on results, there is an evidently correlation between spectrum and examining object (inner defects).",2010,0,
880,881,Use of DGPS corrections with low power GPS receivers in a post SA environment,"With the removal of the dithering effects of Selective Availability (SA), use of Differential GPS (DGPS) corrections can now be applied for extended periods of time allowing enhanced performance for low power configurations of a Si RF based GPS receiver. The software selectable low power settings, implemented by Si RF, employ three states; track, navigate and trickle. During track and trickle states there is no UART communication making reception of DGPS correction unavailable. During the NAV state (when the navigation calculation is performed), corrections may be received. Previously, SA induced error shortened the viable extrapolation time to less than 30 seconds; else significant navigation error would build up between measurements. Additionally, the need to return to a full power state every 30 seconds significantly increased the overall average power dissipation over standard TricklePower<sup>TM</sup> operation. Now that SA (the dominate error source of the DGPS correction) has been removed, the time limit that a DGPS correction can be applied has been extended from 30 seconds to several minutes without significant degradation in navigation performance. This opens up opportunity for low power GPS receiver operation to make use of the DGPS correction to improve navigation without severely impacting the average power requirements. Si RF's implementations of low power operation, leverages off its unique architecture that allows 100 ms signal reacquisition allowing a pseudorange measurement to as little 200 ms. The chipset is then shut down for 800 ms, significantly reducing the power consumption, while still maintaining 1 Hz navigation updates",2001,0,
881,882,A Labview based rotor fault diagnostics tool for inverter fed induction machines by means of the Vienna monitoring method at variable speed,"The Vienna monitoring method (VMM) is a fault detection technique for squirrel cage induction machines. It is based on the comparison of the calculated torque values of two machine models with different model structure. Till now, steady state operation has been investigated only. This contribution deals with an exploitation for variable speed drives under dynamic conditions. The introduced configuration is due to a Labview application, based on a portable personal computer system",2000,0,
882,883,High performance error concealment algorithm by motion vector refinement for MPEG-4 video,A new error concealment algorithm is proposed using recursive motion vector refinement. The proposed method utilizes the top/bottom motion vectors of lost macroblocks in current and reference frames and refines motion vectors recursively. Simulation results based on the MPEG-4 codec present a superior subjective and objective performance of the proposed technique compared with conventional temporal concealment techniques.,2005,0,
883,884,Temperature Correction of PSP Measurement for Low-Speed Flow Using Infrared Camera,"Pressure-Sensitive Paint (PSP) system combined with an infrared (IR) camera has been developed at 2 m x 2 m low-speed wind tunnel at WINTEC/JAXA. The temperature correction of PSP was conducted using both temperature image acquired by the IR camera and wind-off images immediately after the wind tunnel shutdown. As a verification test, the pressure distribution on a supersonic transfer (SST) model was measured by the PSP/IR combined system. The measurement accuracy was fairly improved compared to the previous method, i.e., the temperature correction of PSP using only wind-off PSP images immediately after wind tunnel shutdown.",2005,0,
884,885,Research of the Middleware Based Fault Tolerance for the Complex Distributed Simulation Applications,"With the rapid development of computer simulation technology, the Radar simulation applications scale up increasingly. More and more Radar simulation applications adopt distributed structure to improve system performance and availability. Hence, how to enhance the robustness and efficiency of these complex distributed simulation systems is a hot point. At the same time, fault tolerance middleware makes the applications more robust, available and reliable. Therefore, we strengthen the functionalities of existing fault tolerant middleware and integrate our middleware with the complex distributed simulation systems to provide efficient fault tolerance with balanced workload allocation among different replicas for the distributed simulation applications.",2009,0,
885,886,Joint evaluation of performance and robustness of a COTS DBMS through fault-injection,"Presents and discusses observed failure modes of a commercial off-the-shelf (COTS) database management system (DBMS) under the presence of transient operational faults induced by SWIFI (software-implemented fault injection). The Transaction Processing Performance Council (TPC) standard TPC-C benchmark and its associated environment is used, together with fault-injection technology, building a framework that discloses both dependability and performance figures. Over 1600 faults were injected in the database server of a client/server computing environment built on the Oracle 8.1.5 database engine and Windows NT running on COTS machines with Intel Pentium processors. A macroscopic view on the impact of faults revealed that: (1) a large majority of the faults caused no observable abnormal impact in the database server (in 96% of hardware faults and 80% of software faults, the database server behaved normally); (2) software faults are more prone to letting the database server hang or to causing abnormal terminations; (3) up to 51% of software faults lead to observable failures in the client processes",2000,0,
886,887,Finding liveness errors with ACO,"Model checking is a well-known and fully automatic technique for checking software properties, usually given as temporal logic formulae on the program variables. Most of model checkers found in the literature use exact deterministic algorithms to check the properties. These algorithms usually require huge amounts of memory if the checked model is large. We propose here the use of an algorithm based on ACOhg, a new kind of ant colony optimization model, to search for liveness property violations in concurrent systems. This algorithm has been previously applied to the search for safety errors with very good results and we apply it here for the first time to liveness errors. The results state that our algorithmic proposal, called ACOhg-live, is able to obtain very short error trails in faulty concurrent systems using a low amount of resources, outperforming by far the results of nested-DFS, the traditional algorithm used for this task in the model checking community and implemented in most of the explicit state model checkers. This fact makes ACOhg-live a very suitable algorithm for finding liveness errors in large faulty concurrent systems, in which traditional techniques fail because of the model size.",2008,0,
887,888,A 5 GHz class-AB power amplifier in 90 nm CMOS with digitally-assisted AM-PM correction,This paper presents a technique for correcting AM-PM distortion in power amplifiers. The technique uses a varactor as part of a tuned circuit to introduce a phase shift that counteracts the AM-PM distortion of the PA. The varactor is controlled by the amplitude of the IQ baseband data in a feedforward fashion. The technique has been demonstrated in a class-AB CMOS power amplifier designed for WEAN applications and implemented in a 90 nm CMOS process. The PA delivers 10.5 dBm of average power while transmitting at 54 Mbps (64 QAM). The proposed technique is shown to improve the efficiency of the PA by a factor of 2,2005,0,
888,889,Enhanced detection of electrode placement/connection errors,Lead connection and electrode positional errors are a common problem in ECG recording. This study set out to review the sensitivity and specificity of existing criteria in the Glasgow program using an older (1997) version of the software and to produce enhancements where required for incorporation into the current version of the program still in development. 50 volunteers were recruited to the study. Arm and leg lead connection errors were introduced as were V1/V2 and V2/V3 connection reversals. It was shown that detection of arm lead connection errors could be enhanced from 64% to 88% at 100% specificity. Chest lead misconnections were detected with improved sensitivity. V1 and V2 reversal was much more easily detected than V2 and V3 reversal while maintaining high specificity.,2008,0,
889,890,Distance errors correction for the time of flight (ToF) cameras,One of the most important distance measurement errors is produced by light reflections. These errors can't be avoided and black are more affected than white objects. The measured distance by the ToF camera to an object in the scene changes if surrounding objects are moved. The distance error can be greater than 50% and camera calibration is useless if objects are moved. The calibration method we propose can be performed in any conditions not only in the laboratory. The distance errors for all objects in the scene can be corrected if on the objects are attached white or black tags/ labels. The ToF cameras can be improved using an active illumination with structured light. The improvement will eliminate the distance errors produced by light reflections.,2008,0,
890,891,Lessons learned in building a fault-tolerant CORBA system,"The Eternal system pioneered the interception approach to providing transparent fault tolerance for CORBA, which allows it to make a CORBA application reliable with little or no modification to the application or the ORB. The design and implementation of the Eternal system has influenced industrial practices by providing the basis for the specifications of the fault-tolerant CORBA standard that the Object Management Group adopted. We discuss our experience in developing the Eternal system, with particular emphasis on the challenges that we encountered and the lessons that we learned.",2002,0,
891,892,A Practical Framework of Realizing Actuators for Autonomous Fault Management in SOA,"Due to the key features of service-oriented architecture (SOA); blackbox-nature of services, heterogeneity, service dynamism, and service evolvability, fault management in SOA is known to be more challenging than conventional system management. An efficient way of managing faults in SOA is to apply principles of autonomic computing (AC), of which process is specified in MAPE. The first two phases of MAPE are to monitor target systems and diagnose faults to determine underlying cause. The other two phases are to plan healing/actuation methods and to execute them. Devising methods to remedy service faults which can run in autonomous manner is a hard problem, mainly due to the remoteness and the limited visibility and controllability. In this paper, we present a practical framework to design actuators which can be invoked autonomously. By considering the relationships among fault, cause, and actuator, we derive the abstract and concrete actuators. For some essential concrete actuators, we present their algorithms which can be implemented in practice. We believe our proposed service actuation framework makes the realization of autonomous service management more feasible.",2009,0,
892,893,Fault-tolerant data delivery for multicast overlay networks,"Overlay networks represent an emerging technology for rapid deployment of novel network services and applications. However, since public overlay networks are built out of loosely coupled end-hosts, individual nodes are less trustworthy than Internet routers in carrying out the data forwarding function. Here we describe a set of mechanisms designed to detect and repair errors in the data stream. Utilizing the highly redundant connectivity in overlay networks, our design splits each data stream to multiple sub-streams which are delivered over disjoint paths. Each sub-stream carries additional information that enables receivers to detect damaged or lost packets. Furthermore, each node can verify the validity of data by periodically exchanging Bloom filters, the digests of recently received packets, with other nodes in the overlay. We have evaluated our design through both simulations and experiments over a network testbed. The results show that most nodes can effectively detect corrupted data streams even in the presence of multiple tampering nodes.",2004,0,
893,894,Behavioral analysis of a fault-tolerant software system with rejuvenation,"In recent years, considerable attention has been devoted to continuously running software systems whose performance characteristics are smoothly degrading in time. Software aging often affects the performance of a software system and eventually causes it to fail. A novel approach to handle transient software failures due to software aging is called software rejuvenation, which can be regarded as a preventive and proactive solution that is particularly useful for counteracting the aging phenomenon. In this paper, we focus on a high assurance software system with fault-tolerance and preventive rejuvenation, and analyze the stochastic behavior of such a highly critical software system. More precisely, we consider a fault-tolerant software system with two-version redundant structure and random rejuvenation schedule, and evaluate quantitatively a dependability measure like the steady-state system availability based on the familiar Markovian analysis. In numerical examples, we examine the dependence of two system diversity techniques; design and environment diversity techniques, on the system dependability measure.",2005,0,
894,895,FISCADE - A Fault Injection Tool for SCADE Models,"This paper presents the FISCADE fault injection tool which has been developed as a plug-in to SCADE (Safety-Critical Application Development Environment). The tool automatically replaces original operators with fault injection nodes (FINs). A FIN is a node that encapsulates the original operator so the operator can be replaced or the operator output can be manipulated. During execution of the generated source code, FISCADE controls the SCADE simulator to execute the model, inject the fault, and log the results. The tool allows the user to inject errors (activated faults) in all signals in the model. Furthermore FISCADE can simulate specification of design errors by automatically replacing operators with fault injection nodes, as well as simulating transient, intermittent or permanent faults affecting memories and CPU registers. The tool automatically performs a pre-injection analysis to reduce the number of fault injection experiments needed and supports the work of configuring and carrying out automated fault injection campaigns.",2007,0,
895,896,Perceptually Unequal Packet Loss Protection by Weighting Saliency and Error Propagation,"We describe a method for achieving perceptually minimal video distortion over packet-erasure networks using perceptually unequal loss protection (PULP). There are two main ingredients in the algorithm. First, a perceptual weighting scheme is employed wherein the compressed video is weighted as a function of the nonuniform distribution of retinal photoreceptors. Secondly, packets are assigned temporal importance within each group of pictures (GOP), recognizing that the severity of error propagation increases with elapsed time within a GOP. Using both frame-level perceptual importance and GOP-level hierarchical importance, the PULP algorithm seeks efficient forward error correction assignment that balances efficiency and fairness by controlling the size of identified salient region(s) relative to the channel state. PULP demonstrates robust performance and significantly improved subjective and objective visual quality in the face of burst packet losses.",2010,0,
896,897,Detection of Rotor Faults in Brushless DC Motors Operating Under Nonstationary Conditions,"There are several applications where the motor is operating in continuous nonstationary operating conditions. Actuators and servo motors in the aerospace and transportation industries are examples of this kind of operation. Detection of faults in such applications is, however, challenging because of the need for complex signal processing techniques. Two novel methods using windowed Fourier ridges and Wigner-Ville-based distributions are proposed for the detection of rotor faults in brushless dc motors operating under continuous nonstationarity. Experimental results are presented to validate the concepts and illustrate the ability of the proposed algorithms to track and identify rotor faults. The proposed algorithms are also implemented on a digital signal processor to study their usefulness for commercial implementation",2006,0,
897,898,The check-pointed and error-recoverable MPI JAVA library of agent teamwork grid computing middleware,"We are implementing a fault-tolerant mpiJava API on top of the AgentTeamwork grid-computing middleware system. Our mpiJava implementation consists of the mpiJava API, the GridTcp socket library, and the user program wrapper, each providing a user with the standard mpiJava functions, facilitating message-recording/error-recovering socket connections, and monitoring a user process. This paper presents the application framework, mpiJava implementation, and communication performance in AgentTeamwork.",2005,0,
898,899,"ACE: an aggressive classifier ensemble with error detection, correction and cleansing","Learning from noisy data is a challenging and reality issue for real-world data mining applications. Common practices include data cleansing, error detection and classifier ensembling. The essential goal is to reduce noise impacts and enhance the learners built from the noise corrupted data, so as to benefit further data mining procedures. In this paper, we present a novel framework that unifies error detection, correction and data cleansing to build an aggressive classifier ensemble for effective learning from noisy data. Being aggressive, the classifier ensemble is built from the data that has been preprocessed by the data cleansing and correcting techniques. Experimental comparisons will demonstrate that such an aggressive classifier ensemble is superior to the model built from the original noisy data, and is more reliable in enhancing the learning theory extracted from noisy data sources, in comparison with simple data correction or cleansing efforts",2005,0,
899,900,Hybrid intelligent fault diagnosis based on granular computing,"To solve the problem of lacking hybrid modes and common algorithms in hybrid intelligent diagnosis, this paper presents a new approach to hybrid intelligent fault diagnosis of the mechanical equipment based on granular computing. The hybrid intelligent diagnosis model based on neighborhood rough set is constructed in different granular levels, and the results of support vector machines (SVMS) and artificial neural network (ANN) in granular levels are combined by criterion matrix algorithm as output of hybrid intelligent diagnosis. Finally, the proposed model is applied to fault diagnosis in roller bearings of high-speed locomotive. The applied results show that the classification accuracy of hybrid model reaches to 97.96%, which is 8.49% and 39.12% higher than the classification accuracy of SVMS and ANN respectively. It shows that the proposed model as a new common algorithm can reliably recognize different fault categories and effectively enhance robustness of the hybrid intelligent diagnosis model.",2009,0,
900,901,Fault Detection by Means of HilbertHuang Transform of the Stator Current in a PMSM With Demagnetization,"This paper presents a novel method to diagnose demagnetization in permanent-magnet synchronous motor (PMSM). Simulations have been performed by 2-D finite-element analysis in order to determine the current spectrum and the magnetic flux distribution due to this failure. The diagnostic just based on motor current signature analysis can be confused by eccentricity failure because the harmonic content is the same. Moreover, it can only be applied under stationary conditions. In order to overcome these drawbacks, a novel method is used based upon the Hilbert-Huang transform. It represents time-dependent series in a 2-D time-frequency domain by extracting instantaneous frequency components through an empirical-mode decomposition process. This tool is applied by running the motor under nonstationary conditions of velocity. The experimental results show the reliability and feasibility of the methodology in order to diagnose the demagnetization of a PMSM.",2010,0,
901,902,Localizing Software Faults Simultaneously,"Current automatic diagnosis techniques are predominantly of a statistical nature and, despite typical defect densities, do not explicitly consider multiple faults, as also demonstrated by the popularity of the single-fault Siemens set. We present a logic reasoning approach, called Zoltar-M(ultiple fault), that yields multiple-fault diagnoses, ranked in order of their probability. Although application of Zoltar-M to programs with many faults requires further research into heuristics to reduce computational complexity, theory as well as experiments on synthetic program models and two multiple-fault program versions from the Siemens set show that for multiple-fault programs this approach can outperform statistical techniques, notably spectrum-based fault localization (SFL). As a side-effect of this research, we present a new SFL variant, called Zoltar-S(ingle fault), that is provably optimal for single-fault programs, outperforming all other variants known to date.",2009,0,
902,903,Interface faults injection for component-based integration testing,"This paper presents a simple and improved technique of interface fault insertion for conducting component integration testing through the use of aspect-oriented software development (AOSD). Taking the advantage of aspect's cross-cutting features, this technique only requires additional codes written in AspectJ rather than having a separate tool to perform this operation. These aspect codes act as wrappers around interface services and perform operations such as disabling the implementation of the interface services, raising exceptions or corrupting the inputs and outputs of interface services. Interface faults are inserted into the system under test to evaluate the quality of the test cases by ensuring not only that they detect errors due to the interactions between components, but they are also able to handle exceptions raised when interface faults are triggered.",2006,0,
903,904,Combinational fault diagnosis in a monitored environment by a wireless sensor network,"This paper studies a combinational algorithm of a limit-trend checking, plausibility test and model-based method to attain a secure fault diagnosis in a wireless sensor network. It has been implemented based on a new theoretical identification method. The sensor nodes of the network have been distributed inside an intelligent container to monitor environmental parameters (temperature and relative humidity). It employs measured parameters, residuals and a developed model of the environment to introduce a topology, applicable in several applications of fault diagnosis area.",2009,0,
904,905,Modeling a fault-tolerant distributed system,A C-based simulation model of the time-triggered protocol (TTP/C) has been designed and implemented as a tool for verifying the properties of a system designed on the basis of it. The model has been provided with a user-friendly interface to allow easy visualization and evaluation of the results. The functionality of this general-purpose model is demonstrated on a simple TTP/C cluster application running under the influence of fault injection. The first round of experiments shows that the system is tolerant toward some typical transient faults like memory data distortion.,2001,0,
905,906,Induction machines fault simulation based on FEM modelling,Induction machines operated by inverter use to present efficiency decrease and sometimes present additional rotor and stator fault. FEM has been used for faulty motor simulation and shows results of motor fault effects. Experimental results corroborate the simulation and theoretical effects.,2007,0,
906,907,"Corrections to On the Suitability of a High-<formula formulatype=""inline""> <img src=""/images/tex/18651.gif"" alt=""k""> </formula> Gate Dielectric in Nanoscale FinFET CMOS Technology","In the above titled paper (ibid., vol. 55, no. 7, pp. 1714-1719), the propagation delays in Table III are incorrect, being too long by a factor of two. Furthermore, there was a typo in the table title. The corrected table and title are presented here.",2009,0,
907,908,Techniques for fast transient fault grading based on autonomous emulation [IC fault tolerance evaluation],"Very deep submicron and nanometer technologies have increased notably integrated circuit (IC) sensitivity to radiation. So errors are currently appearing in ICs working at the Earth's surface. Hardened circuits are currently required in many applications where fault tolerance (FT) was not a requirement in the very near past. The use of platform FPGAs for the emulation of single-event upset effects (SEU) is gaining attention in order to speed up the FT evaluation. In this work, a new emulation system for FT evaluation with respect to SEU effects is proposed, providing shorter evaluation times by performing all the evaluation process in the FPGA and avoiding emulator-host communication bottlenecks.",2005,0,
908,909,The utility of hybrid error-erasure LDPC (HEEL) codes for wireless multimedia,"Traditional wireless communication protocols do not relay corrupted packets towards the application layer and neither do they forward such packets over multiple hops. Such an approach can lead to a significant number of packet drops and thus a severe deterioration in performance of high bandwidth applications. Cross-layer protocols which do relay and forward corrupted packets have exhibited substantial promise to mitigate the above problem and thus their utility for wireless multimedia needs to be explored further. Moreover, there is a need to identify efficient channel coding methods for the cross-layer channel. Unlike the traditional schemes, where the channel observed at the application layer is a pure erasure channel, in the cross-layer schemes the application layer channel exhibits hybrid erasure-error impairments. Thus in this paper, we use a rather abstract link-layer model on the basis of which we compare the performance of cross-layer and conventional schemes. We identify the modifications required to be made to RS and LDPC based FEC schemes in order to use them over hybrid erasure-error channels. Finally we compare the considered schemes in terms of video quality using the emerging H.264 video standard. Our video analysis is based on employing a hybrid error-erasure channel coding FEC for the cross-layer schemes versus employing erasure recovery FEC for the traditional protocols. We show that cross-layer schemes can lead to a significant improvement in video quality.",2005,0,
909,910,Digital processing of touch signal - error probability,"A new algorithm for digital processing of a touch signal is proposed. We consider the calculation of the error probability in the process of deciding on the existence of the optical infrared beam between the infrared transmitter and receiver. The error probability is calculated in the function of: (a) the ambient illumination simulated by the reflector placed a certain distance in front of the center of the touch interface; (b) window functions implemented in the algorithm for processing of the touch signal. The error probability is calculated for some classical, time-symmetrical as well as for some original, time-asymmetrical window functions.",2001,0,
910,911,Evaluation of a Monte Carlo scatter correction in clinical 3D PET,"Phantom and patient data were used to compare performance of a one-iteration Monte Carlo scatter correction (MC-SC-1i) for 3D PET, a vendor-supplied one-iteration single scatter model-based correction (SSS-1i) for 3D PET, unscatter-corrected 3D PET (No-SC), a SSS-1i followed by Monte Carlo scatter correction as a second iteration (MC-SSS) for 3D PET, and a convolution-subtraction scatter correction for 2D PET in terms of quantitative accuracy and lesion detectability. ROI analysis showed 2D PET images were more accurate than 3D, particularly for large phantoms, and MC-SSS corrected 3D PET images were more accurate than SSS-1i corrected 3D PET images for this data set. 2D and 3D PET images were reconstructed from 59 patient data sets. Bias of 3D PET images with respect to 2D images was determined using Corresponding Intensity Variance. 3D PET uncorrected images overestimated activity by 50% (smallest patients) to 150% (largest patients). The average absolute bias of SSS-1i corrected images (16%) was twice that of MC-SSS (8%) and more dependent on patient size. Lesion detection sensitivity in these patient images was evaluated using a Channelized Hotelling Observer. Scatter corrected 3D PET images performed 10% better than uncorrected 3D PET images for smaller patients. Slightly better lesion sensitivity was seen for large patients in images reconstructed using SSS-1i (CHO-SNR=2.230.29) compared to MC-SSS (2.080.27) and uncorrected images (2.020.23).",2003,0,
911,912,Harmonic resistance emulator technique for three-phase unity power factor correction,"In this paper, a new technique for three-phase power factor correction, using the typical three-phase line side active front-end converter, is proposed. The proposed technique is capable of simplifying the three-phase power factor correction algorithms to a greater extent. As a consequence, the sampling time will reduce considerably and switching frequency of the converter can be pushed further. The proposed scheme is suitable for the sine-triangle PWM (pulse width modulation) implementation but it completely eliminates the need of frame-synchronization. It also avoids the forward and backward d-q reference-frame transformations. Moreover, presetting of the two orthogonal references is also not required. Simulation results are presented.",2005,0,
912,913,On the value of static analysis for fault detection in software,"No single software fault-detection technique is capable of addressing all fault-detection concerns. Similarly to software reviews and testing, static analysis tools (or automated static analysis) can be used to remove defects prior to release of a software product. To determine to what extent automated static analysis can help in the economic production of a high-quality product, we have analyzed static analysis faults and test and customer-reported failures for three large-scale industrial software systems developed at Nortel Networks. The data indicate that automated static analysis is an affordable means of software fault detection. Using the orthogonal defect classification scheme, we found that automated static analysis is effective at identifying assignment and checking faults, allowing the later software production phases to focus on more complex, functional, and algorithmic faults. A majority of the defects found by automated static analysis appear to be produced by a few key types of programmer errors and some of these types have the potential to cause security vulnerabilities. Statistical analysis results indicate the number of automated static analysis faults can be effective for identifying problem modules. Our results indicate static analysis tools are complementary to other fault-detection techniques for the economic production of a high-quality software product.",2006,0,
913,914,Adaptive noise canceller using LMS algorithm with codified error in a DSP,"In this paper we present an implementation of a digital adaptive filter on the digital signal processor TMS320C6713, using a variant of the LMS algorithm, which consists in error codification, thus the speed of convergence is increased and the complexity of design for its implementation in digital adaptive filters is reduced, because the resulting codified error is composed of integer values. The LMS Algorithm with codified error (ECLMS), was tested in an environmental noise canceller and the results demonstrate an increase in the convergence speed, and a reduction of processing time.",2009,0,
914,915,Polish N-Grams and Their Correction Process,"Word n-gram statistics collected from over 1 300 000 000 words are presented. Eventhough they were collected from various good sources, they contain several types of errors. The paper focuses on the process of partly supervised correction of the n- grams. Types of errors are described as well as our software allowing efficient and fast corrections.",2010,0,
915,916,Manga University: web-based correction system for artistic design education,"In artistic design education, the teacher instructs each student individually face to face. As a result, it is difficult to share coaching with a third party or to teach distantly. To solve these problems, we have developed Manga University, which is a web-based application to aid artistic design education. It enables distant teaching or shared teaching and provides a learning portfolio for collaboration. It is applicable to several other types of artistic design education, such as fashion design or GUI design for software or the web.",2002,0,
916,917,"A Case Study of an Electrolytic Tinning Line, with an Analysis of Faults in the Power Rectifiers",A model is represented in this paper to simulate the tin plating coating process. The control of the output current provided by high current rectifiers (HCR) is the best way to assure the quality of the final product. The model includes the interaction of power rectifiers and thickness of the final coating. ARMAX model has been used for this purpose. An application of the model is presented to identify faults in the set of power rectifiers,2005,0,
917,918,Wavelet analysis based protection for high impedance ground fault in supply systems,"Many high impedance ground faults (HIGF) that happen in a low voltage (LV) system often cause loss of customer supply, fire and human safety hazards. Traditional ground fault protection is provided by residual current circuit breaker (RCCB). The RCCB often causes nuisance tripping and it is difficult to detect HIGF. Wavelet analysis based HIGF protection is developed in the paper. The wavelet transform is applied to filter out some frequency bands of harmonics from residual current and line current. The root mean square (RMS) value of harmonics are calculated using their wavelet coefficients directly. HIGF is identified from disturbance by the RMS difference between the residual current and the line current. The digital protection scheme is designed. EMTP simulation results show that the new protection is able to detect HIGF and prevent electric shock with high sensitivity and robustness.",2002,0,
918,919,A fault tolerance infrastructure for high-performance COTS-based computing in dependable space systems,"A fundamental solution that allows the use of high-performance, but poorly checked processors in dependable space systems is the use of a generic, hierarchical, fault-tolerant hardware infrastructure (FTI). This FTI is a software-independent innermost defense for an autonomous, fault-tolerant long-life system that may also employ other, especially software-based , fault tolerance techniques. The entire FTI is fault-tolerant and contains no software, thus being immune to malicious software intrusions.",2004,0,
919,920,A fault-tolerant structure for reliable multi-core systems based on hardware-software co-design,"To cope with the soft errors and make full use of the multi-core system, this paper gives an efficient fault-tolerant hardware and software co-designed architecture for multi-core systems. And with a not large number of test patterns, it will use less than 33% hardware resources compared with the traditional hardware redundancy (TMR) and it will take less than 50% time compared with the traditional software redundancy (time redundant).Therefore, it will be a good choice for the fault-tolerant architecture for the future high-reliable multi-core systems.",2010,0,
920,921,Error sensitivity data structures and retransmission strategies for robust JPEG 2000 wireless imaging,"In this paper we address the problem of JPEG 2000 imaging in a wireless environment. We first define a flexible and efficient data structure for the description of the error sensitivity of different parts of a JPEG 2000 codestream or file format; the data structure is designed in such a way that it can seamlessly integrated as payload of a JPEG 2000 marker segment or file format box. Moreover, we investigate ARQ policies for robust packet-based JPEG 2000 image transmission over 3G mobile communication systems, and highlight how the proposed data structure can be exploited to improve the end-to-end performance.",2003,0,
921,922,The framework of a web-enabled defect tracking system,"This paper presents an evaluation and investigation of issues to implement a defect management system; a tool used to understand and predict software product quality and software process efficiency. The scope is to simplify the process of defect tracking through a web-enabled application. The system will enable project management, development, quality assurance and software engineer to track and manage problem specifically defects in the context of software project. A collaborative function is essential as this will enable users to communicate in real time mode. This system makes key defect tracking coordination and information available disregards the geographical and time factor.",2004,0,
922,923,"Operation, Design and Testing of Generator 100% Stator Earth Fault Protection Using Low Frequency Injection","This paper describes the development of a new 100% generator stator earth fault protection scheme, based on low frequency injection principle. The design of a new analogue input module and the digital filtering technique are presented. Results of the simulation and site testing are also discussed.",2008,0,
923,924,Toward fault-tolerant and reconfigurable digital microfluidic biochips,"Microfluidics-based biochips are revolutionizing high-throughput sequencing, parallel immunoassays, blood chemistry for clinical diagnostics, and drug discovery. These devices enable the precise control of nanoliter volumes of biochemical samples and regents. They combine electronics with biology, and they integrate various bioassay operations, such as sample preparation, analysis, separation, and detection. This survey paper provides an overview of droplet-based digital microfluidic biochips. It describes emerging techniques for designing fault-tolerant and reconfigurable digital microfluidic biochips. Recent advances in fault modeling, testing, diagnosis and reconfiguration techniques are presented. These quality-driven techniques ensure that biochips can be used reliably during liquid-based biochemical assays.",2010,0,
924,925,A simulation technique for the evaluation of random error effects in time-domain measurement systems,"While many papers deal with time-domain network analyzer calibration procedures for the correction of systematic errors, little work has been published about the treatment of random errors. This paper is focused on the evaluation of random error effects in time-domain measurement systems. As a first step, an experimental identification of the measurement system random errors is achieved. Random errors addressed are jitter, vertical noise, and fast time drifts. Based on this identification, mathematical models are developed to simulate random errors. At a second step, time-domain measurements are simulated with these random errors. These simulations are used to predict measurement system repeatability and dynamic range. Then, as an application example, simulations of the measurement of the complex propagation coefficient and S parameters of a lossy mismatched microstrip line are achieved. By comparison with real measurements, it is shown that random error effects can be accurately predicted by Monte Carlo simulations",2001,0,
925,926,Bayesian Calibration of a Lookup Table for ADC Error Correction,"This paper presents a new method for the correction of nonlinearity errors in analog-to-digital converters (ADCs). The method has been designed to allow a self-calibration in systems where an internal signal can be generated, such as base stations for mobile communications. The method has been implemented and tested in simulation on the behavioral model of commercial ADCs and on a hardware setup composed by a data acquisition board and a distorting circuit",2007,0,
926,927,A Novel Fault Observer Design and Application in Flight Control System,"For estimating the fault signals of the discrete flight control system with unknown senor faults and actuator faults simultaneously, a discrete proportional integral observer (PIO) is presented. The proposed PIO uses the augment states to estimate the sensor faults of the flight control system. Moreover, this observer uses an additionally introduced integral term of the output error to obtain the estimation of actuator faults. The convergence of the PIO is proved. The proposed fault observer is applied in flight control system. Simulation results are given to demonstrate the effectiveness of the proposed fault observer.",2009,0,
927,928,Fault detection of open-switch damage in voltage-fed PWM motor drive systems,This paper investigates the use of different techniques for fault detection in voltage-fed asynchronous machine drive systems. With the proposed techniques it is possible to detect and identify the power switch in which the fault has occurred. Such detection requires the measurement of some voltages and is based on the analytical model of the voltage source inverter. Simulation and experimental results are presented to demonstrate the correctness of the proposed techniques. The results obtained so far indicate that it is possible to embed some fault-tolerant properties for the voltage-fed asynchronous machine drive system.,2003,0,
928,929,Fault Tolerant Control in NCS Medium Access Constraints,"This paper deals with the problem of fault-tolerant control of a Network Control System (NCS) for the case in which the sensors, actuators and controller are inter-connected via various Medium Access Control protocols which define the access scheduling and collision arbitration policies in the network and employing the so-called periodic communication sequence. A new procedure for controlling a system over a network using the concept of an NCS-Information-Packet is described which comprises an augmented vector consisting of control moves and fault flags. The size of this packet is used to define a <i>Completely Fault Tolerant NCS.</i> The fault-tolerant behaviour and control performance of this scheme is illustrated through the use of a process model and controller. The plant is controlled over a network using Model-based Predictive Control and implemented via MATLABcopy and LABVIEWcopy software.",2007,0,
929,930,Modeling and simulation of inner defect in impulse storage capacitor,"Because of big capability and small volume, impulse storage capacitor was found that the fast impulse would do much damage to capacitor insulation. Based on the electrical discharge mechanism, several classical defects of capacitor were put forward in this paper. To estimate the status of insulation, the electrical field distribution of defects should be analyzed carefully. As the most popular defect in storage capacitor, inner defect models had been designed for FEA (finite element analysis). Through simulation and analysis, the result proved that the different size and location of inner defect in insulation would result in dissimilar partial concentration and aberrance of electrical field distribution.",2005,0,
930,931,Six-phase brushless DC motor for fault tolerant electric power steering systems,"This paper is focused on the development of a multiphase fault tolerant BLDC machine, oriented towards the stator windings, number of phases and commutation sequences. As a first step, a six-phase BLDC machine will be modeled and simulated, using JMAG-studio software, for no-load regime. The induced emfs, cogging torque, magnetic field density map and distribution will be processed. The results of simulation, in terms of induced emfs, will give the possibility to develop, in the second step, the commutation sequences in order to get optimal torque quality.",2007,0,
931,932,Segmented attenuation correction using <sup>137</sup>Cs single photon transmission,"Using a <sup>137</sup>Cs single photon transmission source for transmission scanning allows a higher photon flux and thus, better transmission statistics compared to coincidence transmission scanning. However, <sup>137</sup>Cs suffers from a high scatter fraction as well as emission contamination, both leading to an underestimation of the attenuation values. On our NaI- and GSO-systems this is currently compensated by subtracting emission contamination, scatter-scaling and re-mapping. Histogram based segmentation, widely used to shorten the scan time on <sup>68</sup>Ge devices, inherently is capable to compensate for a potential bias in the attenuation values. We have investigated segmented attenuation correction for <sup>137</sup>Cs transmission scans with NaI(Tl) PET scanners in previous work, and came to the conclusion that our current processing was superior to the formerly used segmentation routine. In this paper we re-investigate segmentation, however, using a more sophisticated algorithm. Our focus was mainly to improve the accuracy of our transmission scans rather than shorten the scan times. However, the potential to reduce the scan duration was investigated as well",2001,0,
932,933,Fault tolerant switched reluctance machine for fuel pump drive in aircraft,"As switched reluctance motors (SRM) generally offer a simple and robust design, they are very suitable for an aircraft main engine fuel pump drive which needs to be actuated by fault tolerant drives. Based on analytical comparison the merits and demerits of some different machine topologies including redundancies, a six phase 12/8 fault-tolerant SRM is proposed and designed for fuel pump drive application. The finite element model based on field-circuit coupling is established, results indicate that this machine meets the needs of demanding fuel pump drive system in aerospace environments.",2009,0,
933,934,A Mac-error-warning method for SCTP congestion control over high BER wireless network,"The problem of high BER (bit error rate) usually plagues the wireless connection, especially for some real time applications such as VoIP (voice over IP) and some military uses. The newly developed transport layer protocol SCTP (stream control transmission protocol) also has to face this problem. Though equipped with many new features, SCTP congestion control mechanism fails to distinguish wireless loss from congestion loss, thus its performance over high BER wireless network suffers from unnecessary congestion window decreasing. To improve the performance of SCTP in such a scenario, a Mac-error-warning method is proposed in this paper. Simulation experiments conducted through extended ns-2 validated that the proposed method could achieve higher throughput. The throughput improvement arrives at 946.22% when BER is 0.0005.",2005,0,
934,935,Comparison of Euclidean distance based neural networks for analog Integrated Circuits fault recognition- LVQS &SOM,"The advent of integrated circuits (ICs) and hence the subsequent miniaturization of electronic circuitry has brought out considerable difficulties encountered during identification of faults in integrated circuits during the testing phase of manufacturing and subsequent mass production. Artificial neural network (ANN) augurs well in handling such complex tasks in such systems as it generalizes well without the need to explicitly define the relationship between variables. There has been resurgence in interest among researchers in utilizing ANN for recognizing faults in analog circuits. This work aims at analyzing the role played by the various training parts of both the Euclidean distance based ANNs namely, the self organizing feature maps(SOM) and various versions of learning vector quantization neural network (LVQNN)i.e., LVQ1, LVQ 2, LVQ 2.1 and LVQ. Extensive studies have been conducted to ascertain the role played by learning rate and other unique parameters such as the role played by normalization as a part of preprocessing technique and the number of iterations for convergence. Moreover the results have been compared with the generalized multilayer feedforward network with back propagation algorithm. The best combination of network parameters was also determined. For this purpose an analog filter circuit with 1 fault free and 10 single hard faults was simulated using SPICE simulation software. Experimental results demonstrate the high classification accuracy and the adaptability of both the Euclidean classifiers and its suitability for fault recognition in analog circuits.",2007,0,
935,936,Fault Tolerant Control of a Civil Aircraft Using a Sliding Mode Based Scheme,"This paper presents a sliding mode control scheme for reconfigurable control of a civil aircraft. The controller is based around a state-feedback sliding mode scheme where the nonlinear unit vector term is allowed to adaptively increase when the onset of a fault is detected. Compared to other fault tolerant controllers which have been implemented on this model, the controller proposed here is relatively simple and yet is shown to work across the entire `up and away' fight envelope. Unexpected deviation of the switching variables from their nominal condition triggers the adaptation mechanism.",2005,0,
936,937,Fault detection based on H<inf></inf> states observer for networked control systems,"The influence of random short time-delay to networked control systems (NCS) is changed into an unknown bounded uncertain part. Without changing the structure of the system, an H<inf></inf> states observer is designed for NCS with short time-delay. Based on the designed states observer, a robust fault detection approach is proposed for NCS. In addition, an optimization method for the selection of the detection threshold is introduced for better tradeoff between the robustness and the sensitivity. Finally, some simulation results demonstrate that the presented states observer is robust and the fault detection for NCS is effective.",2009,0,
937,938,User-Centered Interface Reconfiguration for Error Reduction in Human-Computer Interaction,"Human-computer interaction (HCI) is greatly influenced by findings in psychological research concerning interaction with complex technical processes. Psychological concepts like perception and comprehension of information, as well as projection of future system states can be summarized as situation awareness. Poor situation awareness can increase the probability of human error during interaction, caused by erroneous recall and interpretation of percept information through an interface that does not match the user's understanding and mental capabilities. Therefore, a new approach to human-centric reconfiguration of user interfaces will be presented to support situation awareness and, in this way, reduce human errors that occur during interaction.",2010,0,
938,939,Demagnetization Analysis of Permanent Magnet Synchronous Machines under Short Circuit Fault,"Sudden symmetrical short circuit is a serious fault when the entire demagnetization or partial demagnetization of permanent magnet can occur. The aim of this paper is to analyze the demagnetization phenomenon of permanent magnet synchronous machines (PMSM) based on FEM and analytic method. Firstly, the transient FEM is utilized to analyze the demagnetization operating point of permanent magnet when symmetrical short circuit occurs. The computed time evolution of the maximum partial demagnetization operating point is derived. Secondly, the synthesized magnetomotive force (MMF) of the short circuit current is analyzed by analytical method. Through the analysis some characters are obtained. At last several factors that affect the demagnetization operating point are summarized and several measures are put forward to improve the max demagnetization operating point of permanent magnet.",2010,0,
939,940,FuSE - a hardware accelerated HDL fault injection tool,"The ongoing miniaturization of digital circuits makes them more and more susceptible to faults which also complicates the design of fault tolerant systems. In this context fault injection plays an important role in the process of fault tolerance validation. As a result many fault injection tools have emerged during the last decade. However these tools only operate on specific domains and can therefore be referred to as hardware- or software-, simulation- or emulation based techniques. In this paper we present FuSE, a single fault injection tool which covers multiple domains as well as different fault injection purposes. FuSE has been designed for usage with the SEmulator<sup>reg</sup>-an FPGA-based hardware accelerator. The created tool set has been fully automated for the fault injection process and only requires a VHDL description and a test bench of the circuit under test. FuSE can then perform fault injection experiments with a diagnostic resolution that is known from simulation-based approaches, but at a speed that even handles long running experiments with ease.",2009,0,
940,941,Dealing with dormant faults in an embedded fault-tolerant computer system,"Accumulation of dormant faults is a potential threat in a fault tolerant system, especially because most often fault tolerance is based on the single-fault assumption. We investigate this threat by the example of an automotive steer-by-wire application based on the Time-Triggered Architecture (TTA). By means of a Markov model we illustrate that the effect of fault dormancy can degrade the MTTF of a system by several orders of magnitude. We study potential remedies, of which transparent online testing proves to be the most powerful one, while taking a hot spare offline temporarily to test it provides a more feasible solution, though with tight constraints regarding the test duration.",2003,0,
941,942,"Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems","Biological structures and organizations in nature, from gene, molecular, immune systems, and biological populations, to ecological communities, are built to stand against perturbations and biological robustness is therefore ubiquitous. Furthermore, it is intuitively obvious that the counterpart of bio-robustness in engineered systems is fault tolerance. With the objective to stimulate inspiration for building reliable and survivable computer networks, this paper reviews the state-of-the-art research on bio-robustness at different biological scales (level) including gene, molecular networks, immune systems, population, and community. Besides identifying the biological/ecological principles and mechanisms relevant to biological robustness, we also review major theories related to the origins of bio-robustness, such as evolutionary game theory, self-organization and emergent behaviors. Evolutionary game theory, which we present in a relative comprehensive introduction, provides an ideal framework to model the reliability and survivability of computer networks, especially the wireless sensor networks. We also present our perspectives on the reliability and survivability of computer networks, particularly wireless sensor and ad hoc networks, based on the principles and mechanisms of bio-robustness reviewed in the paper. Finally, we propose four open questions including three in engineering and one in DNA code robustness to demonstrate the bidirectional nature of the interactions between bio-robustness and engineering fault tolerance.",2008,0,
942,943,A Systematic Robot Fault-Tolerance Approach,"This paper introduces a systematic approach to suggesting proper fault-tolerance techniques for robots. We classified general robot faults into six types, and developed a UML profile to organize and model a dependancy structure of existing fault-tolerance techniques. In future, we will organize fault-tolerance techniques into the form of the UML profile and determine the relationships between each fault and techniques.",2009,0,
943,944,Fault Diagnosis Way Based on RELAX Algorithms in Frequency Domain for the Squirrel Cage Induction Motors,"The traditional method of spectrum analysis on the current signal via FFT is hard to diagnose the fault of the broken rotor bars. This paper presents a fault diagnosis method using the RELAX algorithm in frequency domain. It can estimate the amplitude and phase values of various frequency components using coarse and fine estimation according to the criterion of minimum energy. Finally the fundamental component can be eliminated in frequency domain after the expressions of the above frequency components are constructed. Compared with the method of eliminating the fundamental component in time domain, this method has the advantage of computing faster although less accurate. However, it has been proved that the algorithm can highlight the fault characteristic and value a lot early in the motor fault diagnosis.",2010,0,
944,945,Ionospheric corrections from a prototype operational assimilation and forecast system,"This paper describes an operational system, sponsored by the US Air Force, for generating and distributing near real-time three-dimensional ionospheric electron densities and corresponding GPS propagation delays. The core ionospheric model solves plasma dynamics and composition equations governing evolution of density, velocity and temperature for ion species on a fixed grid in magnetic coordinates. It uses a realistic model of the Earth's magnetic field and solar indices obtained in real time from NOAA's Space Environment Center. At the present time the model computes real-time ion and electron densities at a grid of more than one million points. Higher resolutions are anticipated in the future. While the core model is capable of delivering realistic results, its accuracy can be significantly improved by employing a special set of numerical techniques known as data assimilation. These techniques originated and are currently used for numerical weather forecasting. The core ionospheric model is constantly fed real-time observational data from a network of reference GPS ground stations. This improves both the nowcast and the forecast of electron densities. Web-based access to the system is provided to early users for validation and exploration purposes at: http://fusionnumerics.com/ionosphere.",2004,0,
945,946,Efficiently utilization of redundancy backup server by forming dynamic clustering in Distributed Systems for tolerating faults,"This paper proposes a novel REDENDENCY THROUGH BACKUP PROCESS with CLUSTERING scheme which aims to address the following concerns: reliable, effective with high maintenance backup can be achieved by constructing a multi-node clustering backbone with a small number of backup cluster-heads for redundancy filling system through backup process for FAULT TOLRANCE DISTRIBUTED SYSTEMS. We can successfully reduce the overhead of backup servers and enhance the speed of backup delivering in an allowable time span compared to other redundancy fault tolerance distributed operating systems for both WAN and LAN networks. Next, we utilize the CLUSTER STRUCHING MECHANISM which determines the cluster size according to the leaving frequency of cluster-members. As the number of leaving events is reduced, the cluster topology is more stable and the BACKUP may also be available in a short and manageable time span before all the distributed system becomes failed. We have done our simulation using MATLAB to show cluster formation with a back-up server election and have investigated the performance of our system for different scenarios.",2010,0,
946,947,McC++/Java: Enabling Multi-core Based Monitoring and Fault Tolerance in C++/Java,"Monitoring and fault tolerance are important approaches to give high confidence that long-running online software systems run correctly. But these approaches will certainly cause high overhead cost, i.e. the loss of efficiency. Multi-core platforms can make such cost acceptable because of the advantage of the parallel performance. For allowing ordinary software developers without any knowledge of multi-core platforms to handle such programming tasks more efficiently, we propose an approach to enable multi-core based monitoring and fault tolerance in C++/Java.",2010,0,
947,948,A New Weak Fault Component Reactance Distance Relay Based on Voltage Amplitude Comparison,"A new weak fault component reactance distance relay is proposed in this paper. By adaptive setting of the compensated voltage, the scheme synthesizes the performance of the impedance distance relay and the reactance distance relay. The distance protection relay on the receiving end will misoperate when the fault resistance is larger than the critical resistance. So a new switching criterion is applied to eliminate this disadvantage. Based on that, the proposed scheme can detect the fault with the high fault resistance in the setting coverage, regardless of whether the relay is located at the receiving end or the sending end. Test results from the simulation and experimental conditions show that the new scheme is successful in detecting the internal fault. It has higher sensitivity and selectivity during different conditions than the traditional fault component protection schemes.",2008,0,
948,949,Differential busbar protection current circuits transients features during nonsimultaneous short faults,The paper deals with special features of transients in current transformer groups of differential busbar and busway protection. The unfaulted phases currents nature is described. The methods of determining of current transformer deep saturation condition and of these protections input currents extreme distortion condition are proposed. The methods described allow initial information obtaining necessary for stable protections algorithms development.,2003,0,
949,950,"An outlook on the dynamic error ""blind"" correction for the time-varying measurement channel","The paper presents the measuring system, which allows for correction of dynamic error caused by the analogue signal transducers, whose dynamic characteristics are changing with a rate approximate to the rate of change of the measured signal. Three methods for self-identification of the coefficients of the transducers' dynamics model, using exclusively the measured signal at the transducers' operating locations, are proposed. Analytical justification for the correctness of the proposed methods is presented for both: the special case of measuring periodic signals and for the general case when the measured signals are nonperiodic. The self-identification and correction procedures are performed as the algorithms processing the data collected from transducers.",2004,0,
950,951,Software Defect Content Estimation: A Bayesian Approach,"Software inspection is a method to detect errors in software artefacts early in the development cycle. At the end of the inspection process the inspectors need to make a decision whether the inspected artefact is of sufficient quality or not. Several methods have been proposed to assist in making this decision like capture recapture methods and Bayesian approach. In this study these methods have been analyzed and compared and a new Bayesian approach for software inspection is proposed. All of the estimation models rely on an underlying assumption that the inspectors are independent. However, this assumption of independence is not necessarily true in practical sense, as most of the inspection teams interact with each other and share their findings. We, therefore, studied a new Bayesian model where the inspectors share their findings, for defect estimate and compared it with Bayesian models in the literature, where inspectors examine the artefact independently. The simulations were carried out under realistic software conditions with a small number of difficult defects and a few inspectors. The models were evaluated on the basis of decision accuracy and median relative error and our results suggest that the dependent inspector assumption improves the decision accuracy (DA) over the previous Bayesian model and CR models",2006,0,
951,952,Broadband measurements of nanofiber devices: Repeatability and random error analysis,"On-wafer, broadband measurements of two-port nanofiber devices were made in order to test the short-term repeatability of a widely used measurement approach that builds on established on-wafer calibration techniques. The test devices used in this study consist of Pt nanowire and Au microbridge structures incorporated into two-port coplanar waveguides. Based on repeated measurements of these test structures, we computed statistical (Type A) uncertainties. The standard deviation (k=1) of five repeated measurements of a Pt nanowire device was less than 50 S. The analysis suggests refinements to the measurement process depending on the desired output of the measurements, e.g. the broadband response itself or the extraction of circuit model parameters.",2010,0,
952,953,Prioritizing Tests for Software Fault Localization,"Test prioritization techniques select test cases that maximize the confidence on the correctness of the system when the resources for quality assurance (QA) are limited. In the event of a test failing, the fault at the root of the failure has to be localized, adding an extra debugging cost that has to be taken into account as well. However, test suites that are prioritized for failure detection can reduce the amount of useful information for fault localization. This deteriorates the quality of the diagnosis provided, making the subsequent debugging phase more expensive, and defeating the purpose of the test cost minimization. In this paper we introduce a new test case prioritization approach that maximizes the improvement of the diagnostic information per test. Our approach minimizes the loss of diagnostic quality in the prioritized test suite. When considering QA cost as the combination of testing cost and debugging cost, on the Siemens set, the results of our test case prioritization approach show up to a 53% reduction of the overall QA cost, compared with the next best technique.",2010,0,
953,954,Automatic detection of head refixation errors in fractionated stereotactic radiotherapy (FSR),Patient surface images are acquired using a novel 3D camera when the patient is at the CT-simulation position and after setup for fractionated stereotactic treatment. The simulation and treatment images are aligned through an initial registration using several feature points followed by a refined automatic matching process using an iterative-closest-point mapping-align algorithm. All of the video-surface images could be automatically transformed to the machine coordinate according to the calibration file obtained from a template image. Phantom tests have demonstrated that we can capture surface images of patients in a second with spatial resolution of submillimeter. A millimeter shift and one-degree rotation relative to the treatment machine can be accurately detected. The entire process takes about two minutes. Our primary result on patients involved in a clinical trial is very promising. This research is partially supported by INH Grant 1R43CA91690-01 and NIH CA88843.,2004,0,
954,955,Error Rate Expression for Perpendicular Magnetic Recording,"An expression for estimating error rate in a perpendicular magnetic recording system is developed. The probability of error is estimated for a dominant dibit error event. Noise includes stationary white Gaussian head/electronics noise and nonstationary colored medium transition noise. Error rate is determined for a variety of parameter changes. In particular, it is shown how two systems with the same total signal-to-noise ratio can have different error rates. Expansion of the model to include additional signal and noise effects as well as the evaluation of different error events is discussed.",2008,0,
955,956,A New Method for Dynamic Fault Diagnosis of Electric Appliance,"A new method of fault diagnosis by dynamic modeling is put forward to obtain the diagnosis parameters of intelligent appliance in different running stages. The model identification approach using support vector regression (SVR) and immune clone selection algorithm (ICSA) is presented in this paper. The relation between process status and the temperature change rate is analyzed in the paper. For appliance fault with uncertainty, the way of fuzzy inference is applied for actualizing inference engine of fault diagnosis. Experimental results prove that the fault diagnosis method for intelligent appliance is credible in the accuracy.",2009,0,
956,957,"State-of-the-art, single-phase, active power-factor-correction techniques for high-power applications - an overview","A review of high-performance, state-of-the-art, active power-factor-correction (PFC) techniques for high-power, single-phase applications is presented. The merits and limitations of several PFC techniques that are used in today's network-server and telecom power supplies to maximize their conversion efficiencies are discussed. These techniques include various zero-voltage-switching and zero-current-switching, active-snubber approaches employed to reduce reverse-recovery-related switching losses, as well as techniques for the minimization of the conduction losses. Finally, the effect of recent advancements in semiconductor technology, primarily silicon-carbide technology, on the performance and design considerations of PFC converters is discussed.",2005,0,
957,958,Towards fault-tolerant software architectures,"""Software engineering has produced no effective methods to eradicate latent software faults. "" This sentence is, of course, a stereotype, but it is as true as a stereotype can get. And yet, it begs some questions. If it is not possible to construct a large software system without residual faults, is it at least possible to construct it to degrade gracefully if and when a latent fault is encountered? This paper presents the approach adopted on CAATS (Canadian Automated Air Traffic System), and argues that OO design and certain architectural properties are the enabling elements towards a true fault-tolerant software architecture",2001,0,
958,959,Techniques and experience in on-line transformer condition monitoring and fault diagnosis in ElectraNet SA,"With evolving maintenance strategies in the electricity industry internationally, there has been increasing pressure to develop improved techniques for condition monitoring. Specifically there has been a trade off between the speed and accuracy of testing. Traditionally, transformer condition monitoring involved high accuracy tests, which due to their duration, could only be performed on a discrete periodic basis. ElectraNet SA has experienced many limitations associated with this form of condition monitoring, and there has been a trend towards high speed on-line monitoring techniques for power transformers. Though these new techniques do not provide the level of accuracy found in traditional forms of testing, they overcome many of their limitations. This paper, describes ElectraNet SA's techniques and experience with power transformer monitoring",2000,0,
959,960,Assessment of fault location algorithms in transmission grids,"The increased accuracy into the fault's detection and location make an easier task for maintenance, this being the reason to develop new possibilities to a precise estimation of the fault location. The paper presents the results of the implementation of two fault location algorithms in ATP-EMTP program. Some ATP-MODELS modules were associated to the ATP model of different transmission grids, these modules being developed on basis of Takagi algorithm applied in two-machine systems and on basis of one algorithm processing synchronized positive-sequence phasor quantities on both transmission lines' terminals. DFT and A3 type filters were used to calculate the fundamental frequency phasors of the transient voltages and currents. There are presented some simulations', the considered parameters of the presented analysis being: line's load, fault's type and resistance and fault position along the overseen line.",2009,0,
960,961,On the Automation of Software Fault Prediction,"This paper discusses the issues involved in building a practical automated tool to predict the incidence of software faults in future releases of a large software system. The possibility of creating such a tool is based on the authors' experience in analyzing the fault history of several large industrial software projects, and constructing statistical models that are capable of accurately predicting the most fault-prone software entities in an industrial environment. The emphasis of this paper is on the issues involved in the tool design and construction and an assessment of the extent to which the entire process can be automated so that it can be widely deployed and used by practitioners who do not necessarily have any particular statistical or modeling expertise",2006,0,
961,962,"Modelling, calibration and correction of nonlinear illumination dependent fixed pattern noise in logarithmic CMOS image sensors","At present, most CMOS image sensors use an array of pixels with a linear response. However, logarithmic CMOS sensors are also possible, which are capable of imaging high dynamic range scenes without saturating. Unfortunately, logarithmic sensors suffer from fixed pattern noise (FPN). Work reported in the literature generally assumes the FPN is independent of illumination. This paper develops a nonlinear model y=a+bln(c+x) of the pixel response y to an illuminance z showing that FPN arises from variation of the offset a, gain b and bias c. Equations are derived which can be used to extract these parameters by calibration against a uniform illuminance of varying intensity. Experimental results, demonstrating parameter calibration and FPN correction, show that the nonlinear model outperforms outputs previous models that assume either only offset or offset and gain variation",2001,0,
962,963,Stator Windings Fault Diagnostics of Induction Machines Operated From Inverters and Soft-Starters Using High-Frequency Negative-Sequence Currents,"This paper studies the application of high-frequency voltage excitation-based stator winding diagnostic methods to three-phase ac machines operated from power converters that create the necessary high-frequency excitation as part of their normal operation. This paper focuses on two specific operating modes: 1) machines operated from inverters in the overmodulation region and 2) machines operated from soft-starters during startup. In both cases, high-frequency (in the range of the hundred hertz) voltage components at well-defined frequencies are created. The negative-sequence currents induced from these high-frequency voltages are shown to contain accurate information on the level of asymmetry (fault) in the machine. This information is significantly richer than exists in other modes of operation, i.e., inverters working in the linear modulation region or soft-starters in the steady state, and provides interesting opportunities to complement other diagnostic methods.",2009,0,
963,964,High throughput Byzantine fault tolerance,"This paper argues for a simple change to Byzantine fault tolerant (BFT) state machine replication libraries. Traditional BFT state machine replication techniques provide high availability and security but fail to provide high throughput. This limitation stems from the fundamental assumption of generalized state machine replication techniques that all replicas execute requests sequentially in the same total order to ensure consistency across replicas. We propose a high throughput Byzantine fault tolerant architecture that uses application-specific information to identify and concurrently execute independent requests. Our architecture thus provides a general way to exploit application parallelism in order to provide high throughput without compromising correctness. Although this approach is extremely simple, it yields dramatic practical benefits. When sufficient application concurrency and hardware resources exist, CBASE, our system prototype, provides orders of magnitude improvements in throughput over BASE, a traditional BFT architecture. CBASE-FS, a Byzantine fault tolerant file system that uses CBASE, achieves twice the throughput of BASE-FS for the IOZone micro-benchmarks even in a configuration with modest available hardware parallelism.",2004,0,
964,965,Intelligent fault-tolerant CORBA service on real-time CORBA,"Distributed object applications can be made fault tolerant by replicating their constituent objects, and by distributing these replicas across the different computers in the network. The idea behind object replication is that the failure of one replica of an object can be masked from a client of the object because the other replicas can continue to perform any operation that the client requires. We propose IFTS (Intelligent Fault Tolerant CORBA Service) for handling faults of server object replica using a replication concept to support fault tolerance. It can choose the fastest primary replica using the multicast mechanism. It also introduces passive replication for secure fault tolerance. Furthermore, we propose the design and implementation of IFTS service to provide reliability and faster service using multicast technology by extending existing CORBA ORB",2001,0,
965,966,Application of BPNN and CBR on Fault Diagnosis for Missile Electronic Command System,"Based on the complexity of the mobile missile electronic command system (MMECS), applying the single method in system fault diagnosis can hardly achieve satisfactory results. The fault diagnosis system combining the BP neural network (BPNN) method and the case-based reasoning (CBR) method was presented. The framework of the mixed neural network and the case presentation was put forward. The question of redundancy reasoning was solved, moreover, it can interpret the diagnoses by providing the successful case. Finally, with the example of voice interrupt, the system's correctness and validity was proved. It is shown that the system is suitable for both the operators training and online decision making for the army",2006,0,
966,967,Harmonic suppression with photonic bandgap and defected ground structure for a microstrip patch antenna,"This letter presents a microstrip patch antenna integrated with two-dimensional photonic bandgap (PBG) and one-dimensional defected ground structure (DGS) jointly in ground plane. It is demonstrated that application of both PBG and DGS eliminates the second and third harmonics and improves the return loss level. Moreover, the combination use of PBG and DGS decreases the occupied area by 70% compared to the conventional PBG patch antenna.",2005,0,
967,968,A newly developed web-based fault locating technology for transmission lines and its experience in the field,"This paper describes a newly developed fault locating (FL) system using Internet technology. The method using current and voltage data at one terminal for FL has been applied, and good results have been obtained. But this method has theoretical error in multi-terminal transmission lines because of the influence of the fault current from the terminal without FL equipment. It is necessary for precise fault location to gather the data from all of the terminals connected to the faulted fine. Internet technology has progressed remarkably and enabled us to gather the data easily from distributed multi-terminals. The proposed FL system uses current, voltage data and status information of power apparatus of multi-terminals via Internet communication. By using these data, fault-locating accuracy of proposed FL system improves remarkably. The network for transmitting information is constituted by general network devices. To perform fault location, a general PC is used. Moreover, for the acquisition of real-time information from power system, compact terminal devices are used. These devices have micro web servers for realizing the data transmission over HTTP. This combination of Internet technology and real time processing technology realized low-cost and high-performance FL systems. The proposed system has been in practical use in Chubu Electric Power Co. Inc.. and excellent results have been obtained.",2002,0,
968,969,Fault-tolerant output tracking control for a flexible air-breathing hypersonic vehicle,"This paper addresses the problem of guaranteed cost fault-tolerant output tracking control against actuator faults for a flexible air-breathing hypersonic vehicle. Firstly, using the parameters of the trim condition, a linearized model is established around the trim point for a nonlinear, dynamically coupled simulation model. Secondly, the control objective and models of actuator faults are presented. Thirdly, the performance analysis condition is proposed in the frame of convex optimization problems via Lyapunov functional approach. Then, the stand controller and fault-tolerant controller are designed such that the resulting closed-loop system is asymptotically stable and satisfies a prescribed performance cost respectively. Finally, the simulation results are given to show the effectiveness of the proposed control method, which is verified by excellent reference altitude and velocity tracking performance.",2010,0,
969,970,Estimating Error-probability and its Application for Optimizing Roll-back Recovery with Checkpointing,"The probability for errors to occur in electronic systems is not known in advance, but depends on many factors including influence from the environment where the system operates. In this paper, it is demonstrated that inaccurate estimates of the error probability lead to loss of performance in a well known fault tolerance technique, Roll-back Recovery with checkpointing (RRC). To regain the lost performance, a method for estimating the error probability along with an adjustment technique are proposed. Using a simulator tool that has been developed to enable experimentation, the proposed method is evaluated and the results show that the proposed method provides useful estimates of the error probability leading to near-optimal performance of the RRC fault-tolerant technique.",2010,0,
970,971,Support Vector Machine for Mechanical Faults Diagnosis,"Aiming at the difficulty that Support Vector Machine (SVM) model selection of classification algorithm affect classification accuracy, it research relevant factors that influence the precision of fault classifiers based on the typical fault data samples obtained by experimental setup of rotor-bearing systems. The results show that different SVM classifiers, in which different kernel functions and different kernel functions parameters are adopted, will influence the precision of fault classifiers in conditions that fault data samples is small. It can be conveniently applied to choose appropriate kernel functions and kernel functions parameters in engineering application.",2010,0,
971,972,Frequency error measurement in GMSK signals in a multipath propagation environment,"The paper presents an efficient method for evaluating the carrier frequency in GMSK communication systems. This method operates in a nonintrusive way. It utilizes the learning vector quantisation neural network based demodulator for reconstructing the transmitted phases. From these and the expected phases is estimated the carrier frequency error. The method is able to operate both in static and multipath propagation cases and it does not require a high frequency sampling rate because the base-band signal is processed. In order to apply the method two procedures, PSP (Procedure for Static Propagation) and PMP (Procedure for Multipath Propagation), are set-up. Tests performed on GMSK signals show that the method is quite attractive, fast and more accurate if compared with other approaches",2001,0,
972,973,A Novel Red-Eye Correction Method Based on AdaBoost Algorithm for Mobile Phone and Digital Camera,"Caused by light reflected off the subject's retina, red-eye is a troublesome problem in consumer photography. Correction of red eyes without any human intervention is an important task. There are some algorithms existing for red-eye detection, but almost all of them have less accuracy, in addition, they cannot support both high pixel and single red-eye. In this paper, a novel approach is proposed to eliminate red-eyes in the digital images automatically with a satisfactory result. This method gets the face region first by AdaBoost algorithm and then detects the red-eye on the top part of the face region, corrects the red-eye in the eye region for recovering the image's original color at last. Experiments in the platform of mobile phone and digital camera show that this method can eliminate the red-eye with high accuracy of 87%, which is higher than the best known technology of face detection base on complexion by 7%, and it can also support 8 million pixels image, moreover, the method has advantages of robustness and real-time computability.",2009,0,
973,974,Bug reports retrieval using Self-organizing Map,"An important process when implementing complex software systems consist of documenting the bugs found in that software. However, since many developers are working at the same time on the project, a bug may easily be reported multiple times, resulting in duplicated bug reports. Therefore, developers responsible for fixing bugs may spend time and effort reading and trying to understand two bugs that actually are the same. This way, we propose in this paper an approach for identifying duplicated bug reports that combines document indexing and self-organizing maps (SOM). The results of our experiments show that at most 69% of duplicated bug reports were identified, representing saving of time and effort for the developers.",2008,0,
974,975,Fuzzy ART neural network algorithm for classifying the power system faults,"This paper introduces advanced pattern recognition algorithm for classifying the transmission line faults, based on combined use of neural network and fuzzy logic. The approach utilizes self-organized, supervised Adaptive Resonance Theory (ART) neural network with fuzzy decision rule applied on neural network outputs to improve algorithm selectivity for a variety of real events not necessarily anticipated during training. Tuning of input signal preprocessing steps and enhanced supervised learning are implemented, and their influence on the algorithm classification capability is investigated. Simulation results show improved algorithm recognition capabilities when compared to a previous version of ART algorithm for each of the implemented scenarios.",2005,0,
975,976,"Improving fault-tolerance in intelligent video surveillance by monitoring, diagnosis and dynamic reconfiguration","In this paper, we present an approach for improving fault-tolerance and service availability in intelligent video surveillance (IVS) systems. A typical IVS system consists of various intelligent video sensors that combine image sensing with video analysis and network streaming. System monitoring and fault diagnosis followed by appropriate dynamic system reconfiguration mitigate effects of faults and therefore enhance the system's fault-tolerance. The applied monitoring and diagnosis unit (MDU) allows the detection of both node- and system-level faults. Lacking redundant hardware such reconfigurations are established by graceful degradation of the overall application. An optimizer module that performs multi-criterion optimization is used to compute a new degraded system configuration by trading off quality of service (QoS), energy consumption, and service availability. We demonstrate the functionality of our approach by an illustrative example.",2005,0,
976,977,A Trustworthy Network Fault Diagnosis Approach,"Trustworthy network fault diagnosis approach is one critical management item to enhance network trustworthiness. Aiming at gaining highly trustworthy of fault diagnosis in Internet, we present a trustworthy fault diagnosis approach based on integration of Artificial Neural Network and Rule-based Reasoning. Supported by the topology of hierarchical and distributed in multi-domains, reasoning rule matrix and its operation are studied to acquire parallel reasoning capability. Moreover, the quantitative trustworthy degree is defined and information entropy is applied to define threshold function marked on arcs and nodes in the Artificial Neural Network. Our approach possesses higher parallel capability guaranteeing by matrix operation and trustworthiness by trustworthy degree definition and calculation using Artificial Neural Network. Further, it is general so that it can be transplanted into various application fields.",2009,0,
977,978,Data Mining Applied to the Electric Power Industry: Classification of Short-Circuit Faults in Transmission Lines,"Data mining can play a fundamental role in modern power systems. However, the companies in this area still face several difficulties to benefit from data mining. A major problem is to extract useful information from the currently available non-labeled digitized time series. This work focuses on automatic classification of faults in transmission lines. These faults are responsible for the majority of the disturbances and cascading blackouts. To circumvent the current lack of labeled data, the alternative transients program (ATP) simulator was used to create a public comprehensive labeled dataset. Results with different preprocessing (e.g., wavelets) and learning algorithms (e.g., decision trees and neural networks) are presented, which indicate that neural networks outperform the other methods.",2007,0,
978,979,Adding fault tolerance mechanisms to Interbus-S,"Field bus technology is now a reality in industrial environments. There are many field bus systems commercially available, and each is suitable for particular kinds of applications. In this scenario the Interbus-S system is playing a leading role, due to the efficiency of its protocol. However, a drawback of this communication system is the centralisation of the mono-master arbitration scheme. The presence of a single device to co-ordinate communication activities makes the Interbus-S protocol vulnerable to fault occurrences in the master. Maintaining full compatibility with the existing standard, the authors have defined a protocol extension which allows the whole communication system to continue working after the occurrence of a fault in the master node",2000,0,
979,980,Effects of carrier phase error on EGC receivers in correlated Nakagami-m fading,"The effects of incoherently combining on dual-branch equal-gain combining (EGC) receivers in the presence of correlated, but not necessarily identical, Nakagami-m fading and additive white Gaussian noise are studied. Novel closed-form expressions for the moments of the output signal-to-noise ratio (SNR) are derived. Based on these expressions, the average output SNR and the amount of fading are obtained in closed-form. Moreover, the outage and the average bit error probability for binary and quadrature phase-shift keying are also studied using the moments-based approach. Numerical and computer simulation results clearly depict the effect of the carrier phase error, correlation coefficient, and fading severity on the EGC performance. An interesting finding is that higher values of the correlation coefficient results to lower irreducible error floors.",2005,0,
980,981,The influence of fault distribution on stochastic prediction of voltage sags,"This paper analyzes the influence of modeling of fault distribution along transmission line on the assessment of number and characteristics of voltage sags. The generic distribution network was used in all simulations. Different types of transformer winding connections were modeled and different (symmetrical and asymmetrical) types of faults were simulated. A line was selected from the previously identified area of vulnerability for a given bus and different faults having different distributions along the line were simulated. It was shown that depending on the fault distribution (uniform, normal, exponential) along the line, different numbers and characteristics of voltage sags could be expected at the selected bus.",2005,0,
981,982,Time domain phase noise correction for OFDM signals,"We introduce an algorithm for compensating for carrier phase noise in an OFDM communication system. Through the creation of a linearized parametric model for phase noise, we generate a least squares (LS) estimate of the transmitted symbol. Using digitized DVB-T RF signals created in a laboratory and a DVB-T compliant receiver model, simulation results are presented to evaluate the effectiveness of the algorithm in practical environments.",2002,0,
982,983,Using Register Lifetime Predictions to Protect Register Files against Soft Errors,"To increase the resistance of register files to soft errors, this paper presents the ParShield architecture. ParShield is based on two observations: (i) the data in a register is only useful for a small fraction of the register's lifetime, and (ii) not all registers are equally vulnerable. ParShield selectively protects registers by generating, storing, and checking the ECCs of only the most vulnerable registers while they contain useful data. In addition, it stores a parity bit for all the registers, re-using the ECC circuitry for parity generation and checking. ParShield has no SDC AVF and a small average DUE AVF of 0.040 and 0.010 for the integer and floating-point register files, respectively. ParShield consumes on average only 81% and 78% of the power of a design with full ECC for the SPECint and SPECfp applications, respectively. Finally, ParShield has no performance impact and little area requirements.",2007,0,
983,984,Unequal error protected transmission with dynamic classification in H.264/AVC,"As an efficient error resilient tool in H.264, FMO (Flexible Macroblock Ordering) still has 2 disadvantages: (1) unacceptable bitrate overhead, and (2) unsuitable for widely used UEP (unequal error protected) transmission. In this paper, to overcome these 2 disadvantages, a dynamic FMO classification (DFMOC) is proposed. For disadvantage(l), in DFMOC since lots of MBs in the same slice are placed together, thus the bitrate overhead is smaller. For disadvantage^), DFMOC generates 2 slices and each of them takes unequal priority in transmission by the large and small motion area extraction. After employing LDPC coding for UEP transmission strategy, experiment shows DFMOC has a better error robustness while still keeps less bitrate overhead compared with traditional FMO mode: the PSNR has 1 to 2 db outperforming and the bitrate overhead keeps no more than 5% which is about a half of traditional FMO overhead.",2007,0,
984,985,Wafer Topography-Aware Optical Proximity Correction,"Depth of focus is the major contributor to lithographic process margin. One of the major causes of focus variation is imperfect planarization of fabrication layers. Presently, optical proximity correction (OPC) methods are oblivious to the predictable nature of focus variation arising from wafer topography. As a result, designers suffer from manufacturing yield loss as well as loss of design quality through unnecessary guardbanding. In this paper, the authors propose a novel flow and method to drive OPC with a topography map of the layout that is generated by chemical-mechanical polishing simulation. The wafer topography variations result in local defocus, which the authors explicitly model in the OPC insertion and verification flows. In addition, a novel topography-aware optical rule check to validate the quality of result of OPC for a given topography is presented. The experimental validation in this paper uses simulation-based experiments with 90-nm foundry libraries and industry-strength OPC and scattering bar recipes. It is found that the proposed topography-aware OPC (TOPC) can yield up to 67% reduction in edge placement errors. TOPC achieves up to 72% reduction in worst case printability with little increase in data volume and OPC runtime. The electrical impact of the proposed TOPC method is investigated. The results show that TOPC can significantly reduce timing uncertainty in addition to process variation",2006,0,
985,986,Design and Implementation of Inference Engine in Safety Risk Assessment Expert System in Petrochemical Industry Based on Fault Tree,"The project in petrochemical industry is complex and risky. For this feature, we established a safety risk assessment (SRA) expert system based on fault tree in petrochemical industry. In this paper, we studied the design and implementation of infer engine in SRA expert system. we adopted the method of fault tree analysis (FTA) to acquire expert knowledge and the fault tree established is to be the basis of inference. The knowledge in petrochemical industry was divided into shallow knowledge and deep knowledge and the method of KR(knowledge representation) adopted in this paper is production rule combined with the framework. On the basis of good representation and organization of knowledge, we adopted infer control strategy of forward reason combined with depth-first search which improve the validity and accuracy of the SRA expert system to a certain extent.",2010,0,
986,987,The sensor of traveling-wave for fault location in power systems,"The fault-generated high frequency signals contain much information which can be used to accurately locate the fault point in power systems. For capturing the high frequency signals, two specially designed traveling-wave sensors are developed in the paper. One is the current sensor installed on the earth line of capacitive equipment (such as: CVT, capacitive CT, transformer bushing, wall bushing) to capture the current traveling-waves flowing from the equipment to earth. The other is the voltage sensor installed at the zero sequence winding of a voltage transformer to capture the voltage traveling waves in three-phase generated by faults. The fault location system with the traveling wave sensors is also developed simply. The sensors and the fault location system have been tested on an 110 kV power system. Results show that the sensors have good performance to capture traveling-waves and the error of fault location is no more than 120 m.",2004,0,
987,988,Fault-Adaptive Control for Robust Performance Management of Computing Systems,"This paper introduces a fault-adaptive control approach for the robust and reliable performance management of computing systems. Fault adaptation involves the detection and isolation of faults, and then taking appropriate control actions to mitigate the fault effects and maintain control.",2007,0,
988,989,Error reduction in non-electric measurement by interpolation combined with loop transformation method,"In this paper, we used second order interpolation method via three succeeding data points in narrow area, combined with loop transformation method and used samples to build an algorithm for processing of measurement data to reduce non-linear errors and transformation errors of non-electric measuring devices. Simulation and experimental results shown the ability to reduce errors of measurement devices.",2010,0,
989,990,Virtual sensor for fault detection and isolation in flight control systems - fuzzy modeling approach,"A virtual sensor for normal acceleration has been developed and implemented in the flight control system of a small commercial aircraft. The inputs of the virtual sensor are the consolidated outputs of dissimilar sensor signals. The virtual sensor is a fuzzy model of the Takagi-Sugeno type and it has been identified from simulated data, using a detailed, realistic Matlab/Simulink<sup>TM</sup> model used by the aircraft manufacturer. This virtual sensor can be applied to identify a failed sensor in the case that only two real sensors are available and even to detect a failure of the last available sensor",2000,0,
990,991,A modified Dendritic Cell Algorithm for on-line error detection in robotic systems,"The immune system is a key component in the maintenance of host homeostasis. Key actors in this process are cells known as dendritic cells (DCs). An artificial immune system based on DCs (known as the Dendritic Cell Algorithm: DCA) is well established in the literature and has been applied in a number of applications. Work in this paper is concerned with the development of an integrated homeostatic system for small, autonomous robotic systems, implemented on a resource limited micro-controller. As a first step, we have modified the DCA to operate in both simulated robotic units, and a resource constrained micro-controller that can operate in an on-line manner. Errors can be introduced into the robotic unit during operation, and these can be detected and then circumvented by the modified DCA.",2009,0,
991,992,Evaluating the Post-Delivery Fault Reporting and Correction Process in Closed-Source and Open-Source Software,"Post-delivery fault reporting and correction are important activities in the software maintenance process. It is worthwhile to study these activities in order to understand the difference between open-source and closed-source software products from the maintenance perspective. This paper proposes three metrics to evaluate the post-delivery fault reporting and correction process, the average fault hidden time, the average fault pending time, and the average fault correction time. An empirical study is further performed to compare the fault correction processes of NASA Ames (closed-source) projects and three open-source projects: Apache Tomcat, Apache Ant, and Gnome Panel.",2007,0,
992,993,Quasi-active power factor correction using transformer-assisted driving voltage,"This paper presents a novel simple input current shaper based on a quasi-active power factor correction (PFC) scheme. In this method high power factor and low harmonic content are achieved by providing an auxiliary PFC circuit with a driving voltage derived from a third winding of the transformer of a DC-DC flyback converter. It eliminates the use of active switch and control circuit for PFC. Operating principles, analysis, and simulation results of the proposed method are presented.",2009,0,
993,994,An Energy Based Approach of Electromagnetism Applied to Adaptive Meshing and Error Criteria,"In order to improve the finite-element modeling of macroscopic eddy currents (associated with motion and/or a time-varying electrical excitation), an original error criterion for adaptive meshing, based on a local power conservation, is proposed. Then, the importance of the order element in the error computation is investigated. Finally, the criterion is coupled to a ldquobubblerdquo mesh generator, and an adaptive meshing of a 2D induction heating case is performed.",2008,0,
994,995,Fault Tolerance of Multiprocessor-Structured Control System by Hardware and Software Reconfiguration,"Since the traditional redundancy for fault tolerance of a control system is complex in structure and expensive, a novel method for fault tolerance of multiprocessor-structured control system by hardware and software reconfiguration is presented. Based on the fact that the control system is composed of several processors, this method performs fault detection by self-diagnosis implemented in each processor and validation of exchanged information between the processors, tolerates fault by hardware and software reconfiguration carried out by monitoring and configuring device. Security strategy and operation mode were presented. The principle of the monitoring and configuring device was discussed in detail. The method was proved by a control system of dc motor and got a satisfied result.",2007,0,
995,996,Joint write policy and fault-tolerance mechanism selection for caches in DSM technologies: Energy-reliability trade-off,"Write-through caches potentially have higher reliability than write-back caches. However, write-back caches are more energy efficient. This paper provides a comparison between the write-back and write-through policies based on the combination of reliability and energy consumption criteria. In the experiments, SIMPLESCALAR tool and CACTI model are used to evaluate the characteristics of the caches. The results show that a write-through cache with one parity bit per word is as reliable as a write-back cache with SEC-DED code per word. Furthermore, the results show that the energy saving of the write-through cache over the write-back cache increases if any of the following changes happens: i) a decrease in the feature size, ii) a decrease in the L2 cache size, and iii) an increase in the L1 cache size. The results also show that when feature size is bigger than 32 nm, the write-back cache is usually more energy efficient. However, for 32 nm and smaller feature sizes the write-through cache can be more energy efficient.",2009,0,
996,997,The study of single line to ground fault line selection in non-direct ground power system based on DSP device,"The middle-low voltage distribution networks mostly adopt neuter point not-valid grounding(be so called to the small current groundding system).The distribution wire fault particularly the fast and accurate location that the single grounding fault, not only to the repair wire and promise dependable power supply, and circulate to the safe stability and economy that promises the whole electric power system to all have a very important function. With reference of actual conditions for single phase earthing fault, the paper puts forward the principle on single phase earthing faulty line selection, and with the support of digital signal processing device(DSP)capable in floating point computation, to design a new type of low current single phase earthing faulty line selection device for power distribution system.",2010,0,
997,998,Fault-Tolerant Sensor Coverage for Achieving Wanted Coverage Lifetime with Minimum Cost,"We study how to select and arrange multiple types of wireless sensors to build a star network that meets the coverage, the lifetime, the fault-tolerance, and the minimum-cost requirements, where the network lifetime, the acceptable failure probability of the network, and the failure rate of each type of sensors are given as parameters. This problem is NP-hard. We model this problem as an integer linear programming minimization problem. We then present an efficient approximation algorithm to find a feasible solution to the problem, which provides a sensor arrangement and a scheduling. We show that, through numerical experiments, our approximation provides solutions with approximation ratios less than 1.4.",2007,0,
998,999,Just-in-time statistical process control for flexible fault management,"A new fault detection and identification method is proposed to solve the problems that obstruct industrial applications of multivariate statistical process control (MSPC) techniques. The proposed method, referred to as just-in-time statistical process control (JIT-SPC), can realize flexible, adaptive, high-performance process monitoring. In addition, fault identification can be done through contribution plot in the framework of JIT-SPC. The usefulness of JIT-SPC is demonstrated through a numerical example, which conventional methods cannot cope with, and a case study of the vinyl acetate monomer production plant.",2010,0,
999,1000,Local/global fault diagnosis of event-driven controlled systems based on probabilistic inference,"This paper presents a new local/global fault diagnosis strategy for the event-driven controlled systems such as the programmable logic controller (PLC). First of all, the controlled plant is decomposed into some subsystems, and the global diagnosis is formulated using the Bayesian network (BN), which represents the causal relationship between the fault and observation in subsystems. Second, the local diagnoser is developed using the conventional timed Markov model (TMM), and the local diagnosis results are used to specify the conditional probability assigned to each arc in the BN. By exploiting the local/global diagnosis architecture, the computational burden for the diagnosis can be drastically reduced. As the result, large scale diagnosis problems in the practical situation can be solved. Finally, the usefulness of the proposed strategy is verified through some experimental results of an automatic transfer line.",2007,0,
1000,1001,The overview of fiber fault localization technology in TDM-PON network,"In this paper, we discussed the mechanism of optical fiber break in time division multiplexing passive optical network (TDM-PON) and the upwardly or downwardly monitoring issues with conventional fiber fault localization technique by using optical time domain reflectometer (OTDR). We also studied the previous fault localization technology that had been recommended. Finally, we proposed a centralized inline monitoring and network testing system named as centralized failure detection system (CFDS). CFDS will be installed with optical line terminal (OLT) at the central office (CO) or network operation center to centrally monitoring each optical fiber line's status and detecting the failure location that occurs in the multi-line drop region of fiber-to-the-home (FTTH) access network downwardly from CO towards the customer premises to improve the service reliability and reduce the restoration time and maintenance cost.",2008,0,
1001,1002,Design of quasi-cyclic Tanner codes with low error floors,"Tanner codes represent a broad class of graph-based coding schemes, including low-density parity-check (LDPC) and turbo codes. Whereas many different classes of LDPC and turbo codes have been proposed and studied in the past decade, very little work has been performed on the broader class of Tanner codes. In this paper we propose a design technique which leads to efficiently encodable quasi-cyclic Tanner codes based on both Hamming and single parity check (SPC) nodes. These codes are characterized by fast message-passing decoders and can be encoded using shift-register-based circuits. The resulting schemes exhibit excellent performance in both the error floor and waterfall regions on the additive white Gaussian noise channel.",2006,0,
1002,1003,Low Area Adaptive Fail-Data Compression Methodology for Defect Classification and Production Phase Prognosis,"With the shrinking technology and increasing statistical defects, multiple design respins are required based on yield learning. Hence, a solution is required to efficiently diagnose the failure types of memory during production in the shortest time frame possible. This paper introduces a novel method of fault classification through image based prognosis of predefined fail signature dictionary. In contrary to the existing Bitmap Diagnosis methodologies, this method predicts the compressed failure map without generating and transferring complete Bitmap to the tester. The proposed methodology supports testing through a very low cost ATE. This architecture is partitioned to achieve sharing among various memories and at-speed testing.",2007,0,
1003,1004,Error-injection-based failure characterization of the IEEE 1394 bus,This paper investigates the behavior of the IEEE 1394 bus in the presence of transient errors in the hardware layers of the protocol. Software-implemented error injection is used to introduce errors into the internals of the 1394 bus hardware chipset. Results from this study indicate that the IEEE 1394 bus protocol provides robust network communication in the presence of single-bit errors in the chipset.,2003,0,
1004,1005,Design and emulate on motor fault diagnosis system,"In the traditional motor fault diagnosis, only a certain type of motor fault diagnosis was diagnosed. The less amount of information leads to diagnostic conclusions unreliable. In this article, a new fault diagnosis method was put forward. Information fusion technology, stator current and rotor vibration signals as a diagnostic characteristics input signal were introduced into the motor fault diagnosis. Neural network method was applied to the fault identification. In order to improve the diagnostic precision, the input signs were divided into the stator current signal related and the rotor vibration signal related. They separately adopt a diagnosis sub-network to complete different aspects of fault diagnosis. Finally, each sub-network diagnostic results information fusion were carried out and the final diagnosis results were got. The simulation of the diagnostic method showed that it is feasible that the neural network data fusion applied to the motor fault diagnosis.",2010,0,
1005,1006,Deformation correction in ultrasound images using contact force measurements,"During an ultrasound scan, contact between the probe and the skin deforms the underlying tissue. This can be considered a feature (as in elastography), but is in general undesirable, particularly for 3D scanning. In this paper we propose a novel system to correct this deformation by measuring the contact force at the time of the ultrasound scan and then using an elastic model to predict the tissue deformation. The inverse of this deformation is then applied to the image, generating the image that would have been seen had there been no contact with the probe. A prototype system has been implemented using an a priori finite element model to predict the deformation. This has been tested on gelatine phantoms and shown to remove the contact deformation and so give improved 3D reconstructions",2001,0,
1006,1007,"An Effective Approach for the Diagnosis of Transition-Delay Faults in SoCs, based on SBST and Scan Chains","In this paper, a Software-Based Diagnosis (SBD) procedure suitable for SoCs is proposed to tackle the diagnosis of transition-delay faults. The illustrated methodology takes advantage of an initial Software-Based Self-Test (SBST) test set and of the scan-chains included in the final SoC design release. In principle, the proposed methodology consists in partitioning the considered SBST test set in several slices, and then proceeding to the evaluation of the diagnostic ability owned by each slice with the aim of discarding diagnosis-ineffective test programs portions. The proposed methodology is aimed to provide precise feedback to the failure analysis process focusing the systematic timing failures characteristic of new technologies. Experimental results show the effectiveness and feasibility of the proposed approach on a suitable SoC test vehicle including an 8-bit microcontroller, 4 SRAM memories and an arithmetic core, manufactured by STMicroelectronics, whose purpose is to provide precise information to the failure analysis process. The reached diagnostic resolution is up to the 99.75%, compared to the 93.14% guaranteed by the original SBST procedure.",2007,0,
1007,1008,Introduction to fault attacks on smartcard,We present what can be achieved by attacks through faults induction on smart cards. We first describe the different means to perform fault attacks on chips and explain how fault attacks on cryptographic algorithms are used to recover secret keys. We next study the impact of fault attacks when focused on the disruption of the functional software layer. We conclude with the overall impact of this type of attacks on the smartcard environment and the need for software countermeasures and their limits.,2005,0,
1008,1009,Efficient method for correction and interpolation signal of magnetic encoders,"Magnetic encoders are widely used for speed or position measurement. This research presents a suitable method to correct the quadratic signal from a magnetic sensor. A new quadrature all digital phased-locked loop (QADPLL) method is presented. The method minimizes the effect of amplitude imbalance, noise, phase shift, and signal offsets. It also can solve waveform distortion and time-lag problems. Moreover, this paper proposes an interpolation technique to improve the accuracy of position information. By deriving the high-order signal from a sinusoidal signal, a high-resolution position can be obtained from a low-resolution encoder. Simulation and experiment on a linear motor were conducted. The results verify the performance of the proposed methods.",2008,0,
1009,1010,Improving robustness of gene ranking by resampling and permutation based score correction and normalization,"Feature ranking, which ranks features via their individual importance, is one of the frequently used feature selection techniques. Traditional feature ranking criteria are apt to produce inconsistent ranking results even with light perturbations in training samples when applied to high dimensional and small-sized gene expression data. A widely used strategy for solving the inconsistencies is the multi-criterion combination. But one problem encountered in combining multiple criteria is the score normalization. In this paper, problems in existing methods are first analyzed, and a new gene importance transformation algorithm is then proposed. Experimental studies on three popular gene expression datasets show that the multi-criterion combination based on the proposed score correction and normalization produces gene rankings with improved robustness.",2010,0,
1010,1011,An immune inspired fault diagnosis system for analog circuits using wavelet signatures,This work focuses on fault diagnosis of electronic analog circuits. A fault diagnosis system for analog circuits based on wavelet decomposition and artificial immune systems is proposed. It is capable of detecting and identifying faulty components in analog circuits by analyzing its impulse response. The use of wavelet decomposition for preprocessing of the impulse response drastically reduces the size of the detector used by the Real-valued Negative Selection Algorithm (RNSA). Results have demonstrated that the proposed system is able to detect and identify faults in a Sallen-Key bandpass filter circuit.,2004,0,
1011,1012,Timing for the Loran-C signal by Beidou satellite and error correction for the transmission time,"The capability of positioning of Beidou/Loran-C integrated navigation system is restricted by the transmitting precision of Loran-C signal. In this paper, the wavelet shrinkage de-noising method of correction for the transmission time error of Loran-C signal, which is timed by the Beidou satellite, is discussed. Getting the random offset from the time difference, and then, adopting the soft-threshold function and the SUREShrink estimation in the de-noising. The results demonstrated that the transmission time error was decreased about 30 ns, and the performance of three dimension positioning was improved evidently through the method in the experiment.",2008,0,
1012,1013,Research on information engineering surveillance risk evaluation based on fault tree analysis,"Information security risk analysis method is now a hot issue of information security management field. Fault tree analysis method has proposed since the 1960s, obtain the widespread application in many large-scale complicated system's security fall-safe analyses. It's recognized as an effective method for the complex system reliability, security analysis. The basic principle, qualitative analysis and quantitative analysis of fault tree analysis method are introduced. And this article introduced briefly the information security risk analysis method, and to has carried on the exhaustive elaboration based on fault tree's risk analysis's modeling way and the analysis principle. Finally, an example is introduced to tome to the conclusion whether the project is feasible.",2010,0,
1013,1014,A software-implemented fault injection methodology for design and validation of system fault tolerance,"Presents our experience in developing a methodology and tool at the Jet Propulsion Laboratory (JPL) for software-implemented fault injection (SWIFI) into a parallel-processing supercomputer which is being designed for use in next-generation space exploration missions. The fault injector uses software-based strategies to emulate the effects of radiation-induced transients occurring in the system hardware components. JPL's SWIFI tool set, which is called JIFI (JPL's Implementation of a Fault Injector), is being used in conjunction with an appropriate system fault model to evaluate candidate hardware and software fault tolerance architectures, to determine the sensitivity of applications to faults, and to measure the effectiveness of fault detection, isolation and recovery strategies. JIFI has been validated to inject faults into user-specified CPU registers and memory regions with a uniform random distribution in location and time. Together with verifiers, classifiers and run scripts, JIFI enables massive fault injection campaigns and statistical data analysis.",2001,0,
1014,1015,Fault Testing And Diagnosis System Of Armored Vehicle Based On Information Fusion Technology,"This paper introduces a fault testing and diagnosis system of bearing in armored vehicle. It can realize real-time testing and precise fault diagnosis of bearing on vehicle chassis. In the hardware, it adopts PCL1800 data acquisition card to acquire sample data and send to the software. In the software, we compile fault testing and diagnosis system of bearing in armored vehicle on a portable computer, its core diagnostic method based on virtual instrument and multi-sensor information fusion technology. Virtual testing subsystem has various functions, including on-line data acquiring, signal displaying, different kinds of signals analyzing and data management. It only monitors the status of bearing and warning alarm. The further work to judge the type and the severity factor of the faults lies on fault diagnosis subsystem based on neural network information fusion technology.",2007,0,
1015,1016,Design and realization on the fault diagnostic flat based on virtual instrument for warship equipment,"Based on virtual instrument (VI) technology, Delphi and database, etc., the fault diagnostic flat for shipboard equipment is developed in order to avoid various abuses that conventional methods brought. The modularization and universalization are proposed in its database-based design concept, realized the design of software and hardware. It broke through conventional check diagnosis patterns for warships equipment, resolved difficult to overcome problems brought on adopting existing conventional fashions examined and repaired shipboard equipment, greatly shortened the cycle of maintenance for naval warships equipment. It was proved by experiments that the flat system has merits, such as operation simpleness, high testing precision, strong flexibility and reliability and extensibility, and economical practicability. Also it is of some values for developing the other fault diagnostic instrument.",2010,0,
1016,1017,Detection of Turn to Turn Faults in Stator Winding with Axial Magnetic Flux in Induction Motors,"Induction motors play a very important part in the safe and efficient running of any industrial plant. Early detection of abnormalities in the motor helps to avoid costly breakdown. Accordingly, this work presents a technique for the diagnosis of turn to turn faults in stator winding in induction motors, with use of axial magnetic flux. Axial leakage flux generates voltages in flux coil sensor and their time based waveforms and frequency presentations are subsequently analyzed. Turn to turn failures in stator windings are studied with load and unload effects. Experimental results prove the efficiency of employed method.",2007,0,
1017,1018,On error detection and error synchronization of reversible variable-length codes,"Reversible variable-length codes (RVLCs) are not only prefix-free but also suffix-free codes. Due to the additional suffix-free condition, RVLCs are usually nonexhaustive codes. When a bit error occurs in a sentence from a nonexhaustive RVLC, it is possible that the corrupted sentence is not decodable. The error is said to be detected in this case. We present a model for analyzing the error detection and error synchronization characteristics of nonexhaustive VLCs. Six indices, the error detection probability, the mean and the variance of forward error detection delay length, the error synchronization probability, the mean and the variance of forward error synchronization delay length are formulated based on this model. When applying the proposed model to the case of nonexhaustive RVLCs, these formulations can be further simplified. Since RVLCs can be decoded in backward direction, the mean and the variance of backward error detection delay length, the mean and the variance of backward error synchronization delay length are also introduced as measures to examine the error detection and error synchronization characteristics of RVLCs. In addition, we found that error synchronization probabilities of RVLCs with minimum block distance greater than 1 are 0.",2005,0,
1018,1019,Biologically-Based Signal Processing Chips with Emphasis on Telecommunication Defect Tracking and Reliability Estimation,"This paper provides observations and motivations to mimic biological information processing. Alternative bio-inspired systems definitions, basics, approaches, algorithms, and chip implementations will be illustrated to offer a base of choice for bio-based Intelligent Information Processing (IIP) systems. Hybrid biological and bio-based IIP are briefly presented. Two specific applications follow with embedded bio-based systems: Bio-chemical sensing and detection E-nose; and Track improvements In the reliability of the software used in telecommunication network deployments. The biologically-based processing discoveries gleaned from observing the spikes in the brain activity of monkeys, introduced the concept of plasticity in synapses used in our embedded Spiking Neural Network (SNN) system for the E-Nose The mathematical construct of a defect tracking classifier is nonlinear, and the event to be recognized involves a sequentially varying or non-stationary phenomenon for telecommunication defect tracking and reliability estimation. Thus, Adaptive Recurrent Dynamic Neural Network (ARDNN) system using wavelet function as the basis improved the failure event estimation of software defect tracking in telecommunications and reduced the error from 88% to L25-8%.",2007,0,
1019,1020,Age-related Neural Changes during Memory Conjunction Errors,"Human behavioral studies demonstrate that healthy aging is often accompanied by increases in memory distortions or errors. Here we used event-related fMRI to examine the neural basis of age-related memory distortions. We used the memory conjunction error paradigm, a laboratory procedure known to elicit high levels of memory errors. For older adults, right parahippocampal gyrus showed significantly greater activity during false than during accurate retrieval. We observed no regions in which activity was greater during false than during accurate retrieval for young adults. Young adults, however, showed significantly greater activity than old adults during accurate retrieval in right hippocampus. By contrast, older adults demonstrated greater activity than young adults during accurate retrieval in right inferior and middle prefrontal cortex. These data are consistent with the notion that age-related memory conjunction errors arise from dysfunction of hippocampal system mechanisms, rather than impairments in frontally mediated monitoring processes.",2010,0,
1020,1021,Selective ground-fault protection using an adaptive algorithm model in neutral ungrounded power systems,"The selective ground-fault protection is greatly valued for the safe and reliable operation of power systems. In order to eliminate the effect of zero-sequence current transformer's phase character on selective ground-fault protection devices, this paper proposes a new adaptive principle of selective ground-fault protection, and gives an algorithm model of action criterion based on the half-wave Fourier algorithm. The simulation results show that this criterion will possess very good selectivity to ground faults",2000,0,
1021,1022,Enhanced DO-RE-ME based defect level prediction using defect site aggregation-MPG-D,"Predicting the final value of the defective part level after the application of a set of test vectors is not a simple problem. In order for the defective part level to decrease, both the excitation and observation of defects must occur. This research shows that the probability of exciting an as yet undetected defect does indeed decrease exponentially as the number of observations increases. In addition, a new defective part level model is proposed which accurately predicts the final defective part level (even at high fault coverages) for several benchmark circuits and which continues to provide good predictions even as changes are made an the set of test patterns applied",2000,0,
1022,1023,Delivering error detection capabilities into a field programmable device: the HORUS processor case study,"Designing a complete SoC or reuse SoC components to create a complete system is a common task nowadays. The flexibility offered by current design flows offers the designer an unprecedented capability to incorporate more and more demanded features like error detection and correction mechanisms to increase the system dependability. This is especially true for programmable devices, were rapid design and implementation methodologies are coupled with testing environments that are easily generated and used. This paper describes the design of the HORUS processor, a RISC processor augmented with a concurrent error mechanism, the architectural modifications needed on the original design to minimize the resulting performance penalty.",2002,0,
1023,1024,Fault Analysis of a MEMS Tuneable Bandpass Filter,"The availability of Micro-Electro-Mechanical Systems (MEMS) switches has enabled the design of a high Q-factor but low insertion loss tuneable bandpass filter. This paper investigates the potential faults that could occur during fabrication and long term operation of a tuneable bandpass filter using MEMS switches. The causes of the filter defects and the resulting filter response will be identified, simulated and co- related, with the final aim of being able to identify the defects by measuring the faulty responses in the future. The different defects are simulated using SONNET to obtain the response of the faulty filter. Parameters such as insertion loss and return loss of the tuneable filter vary for different faults. In the future study, the defects will be recreated and tested experimentally to corroborate simulation findings. Eventually, a relationship between defects and the filter response will be developed.",2007,0,
1024,1025,Bi-direction Motion Vector retrieval based error concealment scheme for H.264/AVC,"As the newest video coding standard, H.264/AVC adopts the high-efficiently predictive coding and variable length entropy coding to achieve high compression efficiency. On the other side, transmission errors become the major problem faced by video broadcasting service providers. Error concealment (EC) here is adopted to handle slices with huge conjunctive corrupted areas inside. Considering error propagation from corrupted slice to succeeding ones is the key factor affecting the video quality, this paper proposes a novel temporal EC scheme including the bi-direction motion vector (MV) retrieval method and an adaptive EC ordering basing on it. Background and motional steady shift part of slice will be given top and second priority, respectively. Combined with our proposed improved boundary matching algorithm (IBMA) which provides more accurate distortion function, experiments results show that our proposal achieves better performance under different error rate channel, compared with EC algorithm adopted in H.264 reference software.",2009,0,
1025,1026,Effects of Systematic and Stochastic Errors on Estimated Failure Probability of Anisotropic Conductive Film,"This paper analyzes the effects of systematic and stochastic errors on the failure probability of anisotropic-conductive-film (ACF) assemblies estimated using the V-shaped-curve method. It is shown that the effect of systematic errors varies as a function of the volume fraction and the volume-fraction bias. The effects of stochastic errors are investigated by using an in-house software program to generate random conductive-particle distributions in the pad and inter pad regions of the ACF package for the given volume fractions and package geometries. The dependences of the coefficient of variation (CV), essentially the degree of uniformity of the particle distribution, and the failure probability on the volume fraction are examined, and the corresponding results are used to derive the correlation between the stochastic error and the CV for a given volume fraction. In general, the current results indicate that the effects of systematic errors on the accuracy of the estimated failure probability can be controlled by improving the accuracy with which the resin and conductive-particle components of the ACF-compound material are weighed during the ACF fabrication process. However, the effects of stochastic errors cannot be controlled and vary as a function of the volume fraction and the degree of nonuniformity of the particle distribution. Nevertheless, the results indicate that the effects of both systematic and stochastic errors can be suppressed by specifying the volume fraction as the value corresponding to the tip of the V-shaped curve when designing the ACF compound.",2007,0,
1026,1027,A Robust High Speed Serial PHY Architecture With Feed-Forward Correction Clock and Data Recovery,"This paper describes a robust architecture for high speed serial links for embedded SoC applications, implemented to satisfy the 1.5 Gb/s and 3 Gb/s Serial-ATA PHY standards. To meet the primary design requirements of a sub-system that is very tolerant of device variability and is easy to port to smaller nanometre CMOS technologies, a minimum of precision analog functions are used. All digital functions are implemented in rail-to-rail CMOS with maximum use of synthesized library cells. A single fixed frequency low-jitter PLL serves the transmit and receive paths in both modes so that tracking and lock time issues are eliminated. A new oversampling CDR with a simple feed-forward error correction scheme is proposed which relaxes the requirements for the analog front-end as well as for the received signal quality. Measurements show that the error corrector can almost double the tolerance to incoming jitter and to DC offsets in the analog front-end. The design occupies less than 0.4 mm<sup>2</sup> in 90 nm CMOS and consumes 75 mW.",2009,0,
1027,1028,An approach to reliability growth planning based on failure mode discovery and correction using AMSAA projection methodology,"Exact expressions for the expected number of surfaced failure modes and system failure intensity as functions of test time are presented under the assumption that the surfaced modes are mitigated through corrective actions. These exact expressions depend on a large number of parameters. Functional forms are derived to approximate these quantities that depend on only a few parameters. Such parsimonious approximations are suitable for developing reliability growth plans and portraying the associated planned growth path. Simulation results indicate that the functional form of the derived parsimonious approximations can adequately represent the expected reliability growth associated with a variety of patterns for the failure mode initial rates of occurrence. A sequence of increasing MTBF target values can be constructed from the parsimonious MTBF projection approximation based on the following: (1) planning parameters that determine the parsimonious approximation; (2) corrective action mean lag time with respect to implementation and; (3) test schedule that gives the number of planned reliability, availability, and maintainability (RAM) test hours per month and specifies corrective action implementation periods",2006,0,
1028,1029,GPRS-based fault monitoring for distribution grid,"The GPRS is a useful mean to solve the communication problem in the automatic system of power distribution network system. This paper designs a GPRS-based real-time system to monitor the on-off states of the switching station in the power distribution system. The system is consisting of a low-power controller of MSP430F149, the module of GR64 from WAVECOM company and detection circuits of on-off states. The IEC60870-5-101 protocol is used in the system. Therefore, it can conveniently communicate with other SCADA systems. It is reliable and stable running on site.",2010,0,
1029,1030,Fixed point error analysis of CORDIC processor based on the variance propagation,"The effects of angle approximation and rounding in the CORDIC processor have been intensively studied for the determination of design parameters. However, the conventional analyses provide only the error bound which results in large discrepancy between the analysis and the actual implementation. Moreover, some of the signal processing architectures require the specification in terms of the mean squared error (MSE) as in the design specification of FFT processor for OFDM. This paper proposes a fixed point MSE analysis based on the variance propagation for more accurate error expression of the CORDIC processor. It is shown that the proposed analysis can also be applied to the modified CORDIC algorithms. As an example of application, an FFT processor for OFDM using the CORDIC processor is presented. The results show close match between the analysis and simulation.",2003,0,
1030,1031,Information Retrieval Based on OCR Errors in Scanned Documents,"An important proportion of documents are document images, i.e. scanned documents. For their retrieval, it is important to recognize their contents. Current technologies for optical character recognition (OCR) and document analysis do not handle such documents adequately because of the recognition errors. In this paper, we describe an approach that integrates the detection of errors in scanned texts without relying on a lexicon, and this detection is integrated in the research process. The proposed algorithm consists of two basic steps. In the first step, we apply editing operations on OCR words that generate a collection of error-grams and correction rules. The second step uses query terms, error-grams, and correction rules to create searchable keywords, identify appropriate matching terms, and determine the degree of relevance of retrieved document images. Algorithms has been tested on 979 document images provided by Media-team databases from Washington University, and the experimental results obtained show the effectiveness of our method and indicate improvement in comparison with the standard methods such as exact or partial matching, N-gram overlaps, and Q-gram distance.",2003,0,
1031,1032,A novel ray-space based color correction algorithm for multi-view video,"In multi-view video, color inconsistency among different views always exists because of imperfect camera calibration, CCD noise, etc. Since color inconsistency greatly reduces the coding efficiency and rendering quality of multi-view video, a novel ray-space based color correction algorithm is proposed in this paper. Firstly, for each epipolar plane image (EPI) in ray-space domain, feature points are extracted to form a corresponding feature EPI (FEPI). Secondly, radon transform is applied to each FEPI to detect corresponding points from different views and the average color is calculated from the detected corresponding points. Finally, for each viewpoint image, the optimal color correction matrix is calculated by minimizing the error energy between the color of the current view and the average color based on the least square error criteria. Experimental results show that the proposed algorithm greatly improves the color consistency among different views. Moreover, the coding efficiency of the corrected multi-view images is greatly improved compared to that of the original ones and the ones corrected by histogram matching method.",2009,0,
1032,1033,Automated red-eye detection and correction in digital photographs,"Caused by light reflected off the subject's retina, red-eye is a troublesome problem in consumer photography. Although most of the cameras have the red-eye reduction mode, the result reality is that no on-camera system is completely effective. In this paper, we propose a fully automatic approach to detecting and correcting red-eyes in digital images. In order to detect red-eyes in a picture, a heuristic yet efficient algorithm is first adopted to detect a group of candidate red regions and then an eye classifier is utilized to confirm whether each candidate region is a human eye. Thereafter, for each detected redeye, we can correct it by the correction algorithm. In case that a red-eye cannot be detected automatically, another algorithm is also provided to detect red-eyes manually with the user's interaction by clicking on an eye. Experimental results on about 300 images with various red-eye appearances demonstrate that the proposed solution is robust and effective.",2004,0,
1033,1034,Forward link capacity evaluation for W-CDMA with amplitude limiter and forward error correction,"In this paper, the influence of the amplitude limiter of combined multiuser signals in the base station of the wideband code division multiple access (W-CDMA) system is described. The transmission performance under Rayleigh fading and interference from an adjacent 6-cell environment are evaluated by computer simulation; the tool of its simulation is MATLAB software. The relationship between limiter level and capacity degradation is clarified. Furthermore, it is shown that the introduction of the limiter is effective in reducing the dynamic range of the power amplifier. The effect of forward error correction (FEC) is also described. The results show that it is effective to use FEC in compensating the degradation by the amplitude limiter.",2002,0,
1034,1035,Defect recognition algorithm based on curvelet moment and support vector machine,"In this paper, a new recognition algorithm based on curvelet moment and support vector machine(SVM) is proposed for chip defect recognition. The proposed recognition method is implemented through a reference comparison method. First the defect regions of chips are extracted through preprocessing, and then the curvelet moment feature of the defect region is computed as the input of SVM classifier, the output of the trained SVM classifier is the result of defect recognition. The algorithm combines the good properties of curvelet moment and SVM classifier, the former can provide multi-scale, local details and orientation information of the defect region, and the latter is suitable to solve the small samples, nonlinear and high dimensions pattern recognition problem. Experimental results show that the algorithm has higher recognition rate compared with PCA based method and can solve the complex defects recognition problem effectively.",2010,0,
1035,1036,A rate-distortion approach to wavelet-based encoding of predictive error frames,"We develop a framework for efficiently encoding predictive error frames (PEF) as part of a rate scalable, wavelet-based video compression algorithm. We investigate the use of rate-distortion analysis to determine the significance of coefficients in the wavelet decomposition. Based on this analysis, we allocate the bit budget assigned to a PEF to the coefficients that yield the largest reduction in distortion, while maintaining the embedded and rate scalable properties of our video compression algorithm",2000,0,
1036,1037,"A comparative analysis of network dependability, fault-tolerance, reliability, security, and survivability","A number of qualitative and quantitative terms are used to describe the performance of what has come to be known as information systems, networks or infrastructures. However, some of these terms either have overlapping meanings or contain ambiguities in their definitions presenting problems to those who attempt a rigorous evaluation of the performance of such systems. The phenomenon arises because the wide range of disciplines covered by the term information technology have developed their own distinct terminologies. This paper presents a systematic approach for determining common and complementary characteristics of five widely-used concepts, dependability, fault-tolerance, reliability, security, and survivability. The approach consists of comparing definitions, attributes, and evaluation measures for each of the five concepts and developing corresponding relations. Removing redundancies and clarifying ambiguities will help the mapping of broad user-specified requirements into objective performance parameters for analyzing and designing information infrastructures.",2009,0,
1037,1038,Study on the Neural Network Model for Shield Construction Faults Diagnosis,"In order to solve the problem of establishing the mathematic model for shield construction faults diagnosis, an approach to the mathematic model by using BP neural network is presented in this paper. The BP neural network model for diagnosing three familiar shield construction faults based on the data of shield excavation parameters was built. The inputs of the model are respectively nine shield excavation parameters which are correlative with shield construction faults. The outputs of the model are three shield construction faults which are respectively the spewing at screw conveyer, the wear of disc-cutters and the jamming of shield. The case study of a shield project validated that the structure of the established model is practical, the diagnostic results are right and the diagnosis method is effective. The conclusion provides the beneficial guidance for the design of the online diagnosis system of shield construction faults based on the data of shield excavation parameters.",2010,0,
1038,1039,A novel fault tolerant design and an algorithm for tolerating faults in digital circuits,"This paper proposes a novel fault tolerant algorithm for tolerating stuck-at-faults in digital circuits. We consider in this paper single stuck-at type faults, occurring either at a gate input or at a gate output. A stuck-at-fault may adversely affect on the functionality of the user implemented design. A novel fault tolerant design based on hardware redundancy (replication) is presented here for single fault model to tolerate transient as well as permanent faults. The design is also suitable to be used for highly dependable systems implemented by means of Field Programmable Gate Arrays (FPGAs) at RTL level. This approach offers the possibility of using larger and more cost effective devices that contain interconnect defects without compromising on performance or configurability. The algorithm presented here demonstrates the fault tolerance capability of the design and is implemented for a full adder circuit but can be generalized for any other digital circuit. Using exhaustive testing the functioning of all the three full adders can be easily verified. In case of occurrence of stuck-at-faults; the circuit will configure itself to select the other fault free outputs. We have evaluated our novel fault tolerant technique (NFT) in five different circuits: full adder, encoder, counter, shift register and microprocessor. The proposed design approach scales well to larger digital circuits also and does not require fault detection. We have also presented and compared the results of triple modular redundancy (TMR) method with our technique. All possible faults are tested by injecting the faults using a multiplexer.",2008,0,
1039,1040,Planar Microwave Bandpass Filters with Defected Ground Resonators,"In this paper a study of some planar microwave bandpass filters composed of defected ground resonators is presented. Different kinds of couplings between two defected ground resonators are investigated: electric coupling, magnetic coupling and two different mixed couplings. The values of the coupling coefficients are extracted from the simulation results, obtained by using full-wave EM-field simulation software. On the basis of this study, some second-order coupled planar microwave bandpass Chebyshev filters are designed. The simulated performances of these novel filter structures are very close to the filter requirements, validating this way the design method and its results",2006,0,
1040,1041,The construction of a novel agent fault-tolerant migration model,"In agent migration process, malicious hosts can compromise the agent. To solve the problem, the paper introduces the measure of agent integrity verification and constructs a novel agent fault-tolerant migration model. The model avoids much agent replicas in migration process. By simulation experiment, the results prove that the model provided by the paper is feasible and efficient, and can save network resource much than other relative works.",2004,0,
1041,1042,"Fault detection, diagnosis and control in a tactical aerospace vehicle",In this paper we propose a fault-tolerant control ( FTC ) scheme using multiple controller switching. The performance of this scheme is studied on a tactical aerospace vehicle. A parity space (PS) based residual generation approach is used to detect the fault. Once a fault is detected the diagnosis scheme identifies the faulty actuator. Using this information on-line reconfiguration of the controller is done based on the configuration of the existing healthy actuator. To implement this scheme no modification were done in hardware (H/W) configuration and only existing redundancies were utilised. Simulation with nonlinear 6-degree of freedom (6-DoF) model shows that the above fault tolerant control approach is able to reduce the probability of failure due to actuators.,2003,0,
1042,1043,A Multi-Perspective Taxonomy for Systematic Classification of Grid Faults,Classification turns chaotic knowledge into regularity by systematizing a domain and providing a common vocabulary. Currently there is a lack of systematic and comprehensive studies in organization and classification of Grid faults. We address this gap with a multi-perspective Grid fault taxonomy describing an incident using eight different characteristics. It is hard to define a taxonomy of broad validity and acceptance that satisfies the vast number of requirements of the many Grid user communities. Nevertheless we proof that our taxonomy can serve as a solid basis for defining project-specific custom classification schemes by giving a concrete example created for a state-of-the-art Grid middleware environment.,2008,0,
1043,1044,Three-phase synchronous PWM for flyback converter with power-factor correction using FPGA ASIC design,The design and development of a synchronous pulsewidth modulation (PWM) generator suitable for the three-phase flyback converter with transformer isolated and power-factor correction using a field-programmable gate array is proposed. The proposed three-phase synchronous PWM makes it possible for the converter to obtain the sinusoidal supply currents with a near-unity power factor. A high-frequency transformer is considered in the design to provide galvanic isolation and serves the dual role of inductor and transformer. Results are provided to demonstrate the effectiveness of the design.,2004,0,
1044,1045,Fault Diagnosis on Hermetic Compressors Based on Sound Measurements,"A fault identification study is made to identify five common faults in hermetic compressors manufactured in a large plant. Sound power level is used as raw data. Sound measurements were made in a room where microphones were located at different places of a virtual hemi-sphere, designed according to international standards. Obtained data is analyzed using the artificial neural networks method, where the multilayer perceptron model is used. Two different analysis approaches are carried out. In the first approach, only the summary data that emanated from the information coming from all microphones are used. In the second approach, all data coming from all microphones are used. The results indicate that the first approach is partially successful and the second is successful.",2007,0,
1045,1046,Reflective fault-tolerant systems: from experience to challenges,"This paper presents research work performed on the development and the verification of dependable reflective systems based on MetaObject Protocols (MOPS). We describe our experience, we draw the lessons learned from both a design and a validation viewpoint, and we discuss some possible future trends on this topic. The main originality of this work relies on the combination of both design and validation issues for the development of reflective systems, which has led to the definition of a reflective framework for the next generation of fault-tolerant systems. This framework includes: 1) the specification of a MetaObject Protocol suited to the implementation of fault-tolerant systems and 2) the definition of a general test strategy to guide its verification. The proposed approach is generic and solves many issues related to the use and evolution of system platforms with dependability requirements. Two different instances of the specified MOP have been implemented in order to study the impact of the MOP architecture in the development of a reflective fault-tolerant system. As far as the test strategy is concerned, a different testing level is associated with each reflective mechanism defined in the MOP. For each testing level, we characterize the test objectives and the required test environments. According to this experience, several new research challenges are finally identified.",2003,0,
1046,1047,Implementation and verification of the Amplitude Recovery Method algorithm with the faults diagnostic system on induction motors,"The implementation and verification of the amplitude recovery method algorithm (A.R.M.), which was first presented in ICEMS2008, are displayed in this paper. The mathematics deduction, application and test results show that the A.R.M. can extract directly the energy of the harmonics of other orders (including high orders and fractional orders) in the tested original signals of three phase stator currents of induction motors even though the harmonics elements are much smaller than the fundamental one.",2009,0,
1047,1048,Influence of window width selection in fault diagnosis of loudspeaker based on Short-time Fourier Transform,"It is important that selects window function and width to Short-time Fourier Transform(STFT). Especially, when diagnosing fault loudspeaker, the different analysis window function and width can influent the result of analysis on respond signal of loudspeaker. So, it proposed a selecting analysis window function and width method based on the energy correction factor, the maximum side lobe and main lobe peak value in the frequency domain. Through reasonable selecting analysis window function and width, it can reduce the influence of truncation of signal caused by the Gibbs phenomenon and resolve the frequency resolution problem on STFT. Therefore, it can be more accurately to extract the fault feature of loudspeaker, and improve the loudspeaker on-line automatic fault detection accuracy.",2010,0,
1048,1049,Bridge fault diagnosis using stuck-at fault simulation,A new diagnostic fault simulator is described that diagnoses both feedback and nonfeedback bridge faults in combinational circuits while using information from fault simulation of single stuck-at faults. A realistic fault model is used which considers the existence of the Byzantine Generals problem. Sets representing nodes possibly involved in a defect are partitioned based on logic and fault simulation of failing vectors. The approach has been demonstrated for two-line bridge faults on several large combinational benchmark circuits containing Boolean primitives and has achieved over 98% accuracy for nonfeedback bridge faults and over 85% accuracy for feedback bridge faults with good diagnostic resolution,2000,0,
1049,1050,"Correction Technique for Cascade Gammas in I-124 Imaging on a Fully-3D, Time-of-Flight PET Scanner","It has been shown that I-124 PET imaging can be used for accurate dose estimation in radio-immunotherapy techniques. However, I-124 is not a pure positron emitter, leading to two types of coincidence events not typically encountered: increased random coincidences due to non-annihilation cascade photons, and true coincidences between an annihilation photon and primarily a coincident 602 keV cascade gamma (true coincidence gamma-ray background). The increased random coincidences are accurately estimated by the delayed window technique. Here we evaluate the radial and time distributions of the true coincidence gamma-ray background in order to correct and accurately estimate lesion uptake for I-124 imaging in a time-of-flight (TOF) PET scanner. We performed measurements using a line source of activity placed in air and a water-filled cylinder, using F-18 and I-124 radio-isotopes. Our results show that the true coincidence gamma-ray backgrounds in I-124 have a uniform radial distribution, while the time distribution is similar to the scattered annihilation coincidences. As a result, we implemented a TOF-extended single scatter simulation algorithm with a uniform radial offset in the tail-fitting procedure for accurate correction of TOF data in I-124 imaging. Imaging results show that the contrast recovery for large spheres in a uniform activity background is similar in F-18 and I-124 imaging. There is some degradation in contrast recovery for small spheres in I-124, which is explained by the increased positron range, and reduced spatial resolution, of I-124 compared to F-18. Our results show that it is possible to perform accurate TOF based corrections for I-124 imaging.",2009,0,
1050,1051,Application of Bayesian belief networks to fault detection and diagnosis of industrial processes,"In industrial processes, to confide the success of planed operation, implementing early and accurate method for recognizing abnormal operating conditions, known as faults, is essential. Effective method for fault detection and diagnosis helps reducing impact of these faults, extols the safety of operation, minimizes down time and reduces manufacturing costs. In this paper, application of BBNs is studied for a benchmark chemical industrial process, known as, Tennessee Eastman in order to achieve early fault detection and accurate probable diagnosis of their causes. Application of Bayesian belief networks for fault detection and diagnosis of Tennessee Eastman process in the graphical context description has not been tested yet. Success of this feature confirms capability and ease use of it as a diagnostic system in actual industrial processes.",2010,0,
1051,1052,Detection of rotor faults in torque controlled induction motor drives,"In the supervision of electrical equipment, the task of diagnostic system is to detect an upcoming machine fault as soon as possible, in order to save expensive manufacturing processes or to replace fault parts. An important issue in such effort is the modelling of the induction machine (IM) including rotor bar and end-ring faults, with a minimum of computational complexity. In this paper, a simpler method is employed in the simulation of an induction motor with rotor asymmetries. Simulation of classical and dynamic space vector models, Finite Element Analysis and experimental results are presented to support the proposed model. The need for detection of rotor faults at an earlier stage has pushed the development of monitoring methods with increasing sensitivity and noise immunity. Addressing diagnostic techniques based on current signatures analysis (MCSA), the characteristic components introduced by specific faults in the current spectrum are investigated and a diagnosis procedure correlates the amplitudes of such components to the fault extent. The impact of feedback control on asymmetric rotor cage induction machine behavior is also analyzed. It is shown that the variables usually employed in diagnosis procedures assuming open-loop operation are no longer effective under closed-loop operation. Simulation results show that signals already present at the drive are suitable to effective diagnostic procedure. The utilization of the current regulator error signals in rotor failure detection are the aim of the present work. The use of a band-pass filter bank to detect the presence of sidebands is also proposed.",2007,0,
1052,1053,Analysis of Hyperion data with the FLAASH atmospheric correction algorithm,"A combination of good spatial and spectral resolution make visible to shortwave infrared spectral imaging from aircraft or spacecraft a highly valuable technology for remote sensing of the Earth's surface. Many applications require the elimination of atmospheric effects caused by molecular and particulate scattering; a process known as atmospheric correction, compensation, or removal. The Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes (FLAASH) atmospheric correction code derives its physics-based algorithm from the MODTRAN4 radiative transfer code. A new spectra; recalibration algorithm, which has been incorporated into FLAASH, is described. Results from processing Hyperion data with FLAASH are discussed.",2003,0,
1053,1054,Transient Fault Prediction Based on Anomalies in Processor Events,"Future microprocessors will be highly susceptible to transient errors as the sizes of transistors decrease due to CMOS scaling. Prior techniques advocated full scale structural or temporal redundancy to achieve fault tolerance. Though they can provide complete fault coverage, they incur significant hardware and/or performance cost. It is desirable to have mechanisms that can provide partial but sufficiently high fault coverage with negligible cost. To meet this goal, we propose leveraging speculative structures that already exist in modern processors. The proposed mechanism is based on the insight that when a fault occurs, it is likely that the incorrect execution would result in abnormally higher or lower number of mispredictions (branch mispredictions, L2 misses, store set mispredictions) than a correct execution. We design a simple transient fault predictor that detects the anomalous behavior in the outcomes of the speculative structures to predict transient faults",2007,0,
1054,1055,Comparison of accelerated DRAM soft error rates measured at component and system level,"Single event upsets from terrestrial cosmic rays (i.e. high-energy neutrons) are more important than alpha particle induced soft errors in modern DRAM devices. A high intensity broad spectrum neutron source from the Los Alamos Neutron Science Center (LANSCE) was used to characterize the nature of these upsets in DRAM technologies ranging from 180 nm down to 70 nm from several vendors at the DIMM component level using a portable memory tester. Another set of accelerated neutron beam tests were made with DRAM DIMMs mounted on motherboards. Soft errors were characterized using these two methods to determine the influence of neutron angle, frequency, data patterns and process technology. The purpose of this study is to analyze the effects of these differences on DRAM soft errors.",2008,0,
1055,1056,Error analysis of the moment method,"Because of the widespread use of the Method of Moments for simulation of radiation and scattering problems, analysis and control of solution error is a significant concern in computational electromagnetics. The physical problem to be solved, its mesh representation, and the numerical method all impact accuracy. Although empirical approaches such as benchmarking are used almost exclusively in practice for code validation and accuracy assessment, a number of significant theoretical results have been obtained in recent years, including proofs of convergence and solution-error estimates. This work reviews fundamental concepts such as types of error measures, properties of the problem and numerical method that affect error, the optimality principle, and basic approximation error estimates. Analyses are given for surface-current and scattering-amplitude errors for several scatterers, including the effects of edge and corner singularities and quadrature error. We also review results on ill-conditioning due to resonance effects and the convergence rates of iterative linear-system solutions.",2004,0,
1056,1057,Fault management in ECLIPSE,"ECLIPSE is a next-generation virtual telecommunication network that provides multimedia services that integrate voice, video, text and images. ECLIPSE facilitates the modular decomposition of new telecommunication services. In this paper, we sketch the challenges we face in making ECLIPSE highly available when running on top of a heterogenous and widely distributed system. We describe how we approach these challenges using the modular decomposition of services provided by ECLIPSE, together with its novel failure detection and recovery strategies",2001,0,
1057,1058,The state of documentation practice within corrective maintenance,"Consistent, correct and complete documentation is an important vehicle for the maintainer to gain understanding of a software system, to ease the learning and/or relearning processes, and to make the system more maintainable. Former studies have shown that documentation is one of the most neglected process issues within software engineering today. The authors check the current state of documentation practice within corrective maintenance in Sweden",2001,0,
1058,1059,What Types of Defects Are Really Discovered in Code Reviews?,"Research on code reviews has often focused on defect counts instead of defect types, which offers an imperfect view of code review benefits. In this paper, we classified the defects of nine industrial (C/C++) and 23 student (Java) code reviews, detecting 388 and 371 defects, respectively. First, we discovered that 75 percent of defects found during the review do not affect the visible functionality of the software. Instead, these defects improved software evolvability by making it easier to understand and modify. Second, we created a defect classification consisting of functional and evolvability defects. The evolvability defect classification is based on the defect types found in this study, but, for the functional defects, we studied and compared existing functional defect classifications. The classification can be useful for assigning code review roles, creating checklists, assessing software evolvability, and building software engineering tools. We conclude that, in addition to functional defects, code reviews find many evolvability defects and, thus, offer additional benefits over execution-based quality assurance methods that cannot detect evolvability defects. We suggest that code reviews may be most valuable for software products with long life cycles as the value of discovering evolvability defects in them is greater than for short life cycle systems.",2009,0,
1059,1060,Regularized B-spline deformable registration for respiratory motion correction in PET images,"A major challenge in respiratory motion correction of gated PET images is their low signal to noise ratios (SNR). This particularly affects the accuracy of image registration. This paper presents an approach to overcoming this problem using a deformable registration algorithm which is regularized using a Markov random field (MRF). The deformation field is represented using B-splines and is assumed to form a MRF. A regularizer is then derived and introduced to the registration, which penalizes noisy deformation fields. Gated PET images are aligned using this registration algorithm and summed. Experiments with simulated data show that the regularizer effectively suppresses the noise in PET images, yielding satisfactory deformation fields. After motion correction, the PET images have significantly better image quality.",2008,0,
1060,1061,Refined Spatial Error Concealment with Directional Entropy,"A refined error concealment method for intra frame in H.264 is proposed in this work. Directional entropy of neighboring edges is used to classify the content of the lost block. Some techniques aimed to shorten the computation time without degrading quality of reconstructed video are presented to enforce multi-directional interpolation. As for block with high texture, either bilinear or multi-directional interpolation alone is not so well to recover, we integrate the two methods to get the final result. Experimental results demonstrate the efficiency and better performance of proposed method as compared to other interpolation based methods.",2009,0,
1061,1062,Adaptive error control scheme for multimedia applications in integrated terrestrial-satellite wireless networks,"This paper presents an adaptive error control (AEC) scheme for multimedia applications in integrated terrestrial-satellite wireless networks. The AEC protocol supports both real-time and non-real-time applications. In the AEC protocol, we propose new adaptive FEC (AFEC) and hybrid ARQ (HARQ) schemes for real-time and non-real-time traffic, respectively. Throughput performance for non-real-time application shows that the proposed AEC protocol outperforms hybrid ARQ (HARQ) protocols with the same code used. Under real-time application, the AEC protocol outperforms the static FEC (SFEC) protocols with respect to packet miss probability",2000,0,
1062,1063,Analysis of the ABS Wheel Speed Signal Error and Method of Equal Period Sampling,"The measuring error of wheel speed signal of anti-lock braking system (ABS) has been analyzed in this paper. It has been concluded that trigger error is the main factor which influences the wheel speed measurement limitation, and the correlation between trigger error and the wheel speed signal are surveyed by experiments. It is necessary in ABS to transform the wheel speed signal sampled in the form of equiangular to the form of equal period. In the low speed measurement situation of lacking wheel speed signal, a prediction algorithm based on polynomial order two fit has been utilized to estimate wheel speed. Simulation results verified the algorithm.",2006,0,
1063,1064,Evaluation of several Efron bootstrap methods to estimate error measures for software metrics,"A narrow confidence interval of a sample statistic or a model parameter implies low variability of that statistic, and permits a strong conclusion to be made about the underlying population. Conversely, the analysis should be considered inconclusive if the confidence interval is wide. Efron's (1992) bootstrap statistical analysis appears to address the fact that many statistics used in software metrics analysis do not come with theoretical formulas to allow accuracy assessment. In this paper we will present preliminary results on an empirical analysis of the reliability of several Efron nonparametric bootstrap methods in assessing the accuracy of sample statistics in the context of software metrics. In particular, we focus on the standard errors and 90% confidence intervals of five basic statistics as a tool to evaluate the Bootstrap. It was found confidence intervals for mean and median were accurately estimated, those for variance grossly under-estimated with skewness and kurtosis grossly over-estimated.",2002,0,
1064,1065,Fault-Tolerant CORBA: From Specification to Reality,"Based on 10 years' personal experience implementing academic prototypes of FT-CORBA, helping to establish the FT-CORBA standard, and subsequently developing and selling its commercial implementations, this critique provides an overview of the FT-CORBA standard's specifications from the viewpoint of realizing its concrete implementation for real-world applications",2007,0,
1065,1066,Interleaving for combating bursts of errors,"To ensure data fidelity, a number of random error correction codes (ECCs) have been developed. ECC is, however, not efficient in combating bursts of errors, i.e., a group of consecutive (in one-dimensional (1-D) case) or connected (in two- and three- dimensional (2-D and 3-D) case) erroneous code symbols owing to the bursty nature of errors. Interleaving is a process to rearrange code symbols so as to spread bursts of errors over multiple code-words that can be corrected by ECCs. By converting bursts of errors into random-like errors, interleaving thus becomes an effective means to combat error bursts. In this article, we first illustrate the philosophy of interleaving by introducing a 1-D block interleaving technique. Then multi-dimensional (M-D) bursts of errors and optimality of interleaving are defined. The fundamentals and algorithms of the state of the art of M-D interleaving - the t-interleaved array approach by Blaum, Bruck and Vardy and the successive packing approach by Shi and Zhang-are presented and analyzed. In essence, a t-interleaved array is constructed by closely tiling a building block, which is solely determined by the burst size t. Therefore, the algorithm needs to be implemented each time for a different burst size in order to maintain either the error burst correction capability or optimality. Since the size of error bursts is usually not known in advance, the application of the technique is somewhat limited. The successive packing algorithm, based on the concept of 2  2 basis array, only needs to be implemented once for a given square 2-D array, and yet it remains optimal for a set of bursts of errors having different sizes. The performance comparison between different approaches is made. Future research on the successive packing approach is discussed. Finally, applications of 2-D/3-D successive packing interleaving in enhancing the robustness of image/video data hiding are presented as examples of practical utilization of interleaving.",2004,0,
1066,1067,Research on domestic PV module structure based on fault detection,"As to detecting the location of fault in photovoltaic (PV) module structure, this paper presents a new type of PV arrays connection: CTCT structure (complex-total-cross-tied array). In the array of CTCT-type PV cells, by adding a certain number of current sensors and comparing the detected current, the Hot Spot cells can be located to avoid their damage to PV panels. This paper derives a formula to demonstrate the relationship between the total number of PV panels in parallel, the resolution and the total number of current sensors in the PV systems. And the algorithm has been verified correctly in a 3*9 board of PV panel. With the certain number of PV panels, selecting the reasonable value of resolution for determining the number of sensors needed, we can detect the precise location of PV cells that have Hot Spot by using current sensors as few as possible.",2010,0,
1067,1068,On the Threat of Metastability in an Asynchronous Fault-Tolerant Clock Generation Scheme,"Due to their handshake-based flow control, asynchronous circuits generally do not suffer from metastability issues as much as synchronous circuits do. We will show, however, that fault effects like single-event transients can force (sequential) asynchronous building blocks such as Muller C-Elements into a metastable state. At the example of a fault-tolerant clock generation scheme, we will illustrate that metastability could overcome conventional error containment boundaries, and that, ultimately, a single metastable upset could cause even a multiple Byzantine fault-tolerant system to fail. In order to quantify this threat, we performed analytic modeling and simulation of the elastic pipelines, which are at the heart of our physical implementation of the fault-tolerant clocks. Our analysis results reveal that only transient pulses of some very specific width can trigger metastable behavior. So even without consideration of other masking effects the probability of a metastable upset to propagate through a pipeline is fairly small. Still, however, a thorough metastability analysis is mandatory for circuits employed in high-dependability applications.",2009,0,
1068,1069,"Using program analysis to identify and compensate for nondeterminism in fault-tolerant, replicated systems","Fault-tolerant replicated applications are typically assumed to be deterministic, in order to ensure reproducible, consistent behavior and state across a distributed system. Real applications often contain nondeterministic features that cannot be eliminated. Through the novel application of program analysis to distributed CORBA applications, we decompose an application into its constituent structures, and discover the kinds of nondeterminism present within the application. We target the instances of nondeterminism that can be compensated for automatically, and highlight to the application programmer those instances of nondeterminism that need to be manually rectified. We demonstrate our approach by compensating for specific forms of nondeterminism and by quantifying the associated performance overheads. The resulting code growth is typically limited to one extra line for every instance of nondeterminism, and the runtime overhead is minimal, compared to a fault-tolerant application with no compensation for nondeterminism.",2004,0,
1069,1070,Neural network approach to diagnose faults in linear antenna array,"A novel approach using artificial neural network (ANN) is proposed to identify the faulty elements present in a non uniform linear array. The input to the neural network is amplitude of radiation pattern and output of neural network is the location of faulty elements. In this work, ANN is implemented with two algorithms; radial basis function neural network (RBF) and probabilistic neural network and their performance is compared. The network is trained with some of the possible faulty radiation patterns and tested with various measurement errors. It is proved that the method gives a high success rate.",2008,0,
1070,1071,Stator winding turn-fault detection for closed-loop induction motor drives,"Sensorless diagnostics for line-connected machines is based on extracting fault signatures from the spectrum of the line currents. However, for closed-loop drives, the power supply is a regulated current source and, hence, the motor voltages must also be monitored for fault information. In this paper, a previously proposed neural network scheme for turn-fault detection in line-connected induction machines is extended to inverter-fed machines, with special emphasis on closed-loop drives. Experimental results are provided to illustrate that the method is impervious to machine and instrumentation nonidealities, and that it requires lesser data memory and computation requirements than existing schemes, which are based on data lookup tables.",2003,0,
1071,1072,Fault injection approach based on dependence analysis,"Fault injection is used to validate a system in the presence of faults. Jaca, a software injection tool developed in previous work, is used to inject faults at interfaces between classes of a system written in Java. We present a strategy for fault injection validation based on dependence analysis. The dependence analysis approach is used to help in reducing the number of experiments necessary to cover the system's interfaces. For the experiments we used a system that consists of two integrated components, an ODBMS performance test benchmark, Wisconsin 007 and an ODBMS, Ozone. The results of some experiments and their analysis are presented.",2005,0,
1072,1073,Analysis of the practical capacity of multi-valued hetero-associator considering fault tolerance,"Presents a method of pattern recognition using the multi-valued polynomial bidirectional hetero-associator (PBHA). This network can be used for the industrial application of optical character recognition. According to detailed simulations, the PBHA has a higher capacity for pattern pair storage than that of the conventional bidirectional associative memories and fuzzy memories. Meanwhile, the practical capacity of a PBHA considering fault tolerance is discussed. The fault tolerance requirement leads to the discovery of the attraction radius of the basin for each stored pattern pair. The PBHA takes advantage of multi-valued characteristics in evolution equations such that the signal-noise-ratio is significantly increased. We apply the result of this research to pattern recognition problems. The practical capacity of the multi-valued data recognition using the PBHA considering fault tolerance in the worst case is also estimated. Simulation results are presented to verify the derived theory",2001,0,
1073,1074,A DSP based controller for power factor correction (PFC) in a rectifier circuit,In this paper a digital signal processor (DSP) based power factor correction (PFC) scheme is presented. A dual-loop controller is designed to control the average input AC current as well as DC bus voltage. The DSP controller is implemented and tested. Design methodologies and trade-offs such as discrete-time implementation methods are also presented,2001,0,
1074,1075,Total Sensitivity Index Calculation via Error Propagation Equation,"This paper presents a new and convenient method to calculate the total sensitivity indices defined by variance-based sensitivity analysis. By decomposing the output variance using error propagation equations, this method can transform the ""double-loop"" sampling procedure into ""single-loop"" one and obviously reduce the computation cost of analysis. In contrast with Sobol and Fourier amplitude sensitivity test (FAST) method, which is limited in non-correlated variables, new approach is suitable for correlated input variables. An application in semiconductor assemble and test manufacturing (ATM) factory indicates that this approach has a good performance in additive model and simple non-additive mathematical model.",2007,0,
1075,1076,Accurate Bit-Error-Rate Analysis of Bandlimited Cooperative OSTBC Networks Under Timing Synchronization Errors,"The distributed multiple-input-multiple-output (MIMO) system (e.g., intercluster communication via cooperating nodes in a wireless sensor network) is a topic of emerging interest. Many previous studies have assumed perfect synchronization among cooperating nodes and identically distributed communication links. Such assumptions are rarely valid in practical operating scenarios. This paper develops an analytical framework for computing the average bit error rate (ABER) of a distributed multiple-input-single-output (MISO) space-time-coded system with binary phase shift keying (BPSK) modulation affected by timing synchronization errors. The cooperating nodes use data pulse-shaping filters for transmission over generalized frequency-nonselective fading channels. As an illustrative example, the performance evaluation of a 2 times 1 MISO system that uses distributed orthogonal space-time block coding (OSTBC) is presented, although this approach can be readily extended to analyze distributed transmit diversity with a larger number of cooperating nodes. We show that under certain conditions, a distributed MISO system with time synchronization errors can still outperform a perfectly synchronized single-input-single-output (SISO) system.",2009,0,
1076,1077,Software defect detection and process improvement using personal software process data,"Statistical evidence has shown that programmers perform better when following a defined, repeatable process such as the Personal Software Process (PSP).Anecdotal and qualitative evidence from industry indicates that two programmers working side-by-side at one computer, collaborating on the same design, algorithm, code, or test, perform substantially better than the two working alone i.e. pair programming. Bringing these two ideas together, a new Software Process has been formulated. The Hybrid Personal Software Process (HPSP) is a defined, repeatable process for two programmers working collaboratively. The development time and cost of the product are reduced in HPSP when compared with PSP programming.",2010,0,
1077,1078,Fuzzy Expert System for Defect Classification for Non-Destructive Evaluation of Petroleum Pipes,"In this paper, an expert system has been outlined to classify the defects in metallic petroleum pipelines using acoustic techniques with non-destructive evaluation (NDE) protocols, the proposed system maps the quantitative defect data through a novel perception-based kernel that has its roots in multidimensional fuzzy set theory to map the relative weights given to various features; mathematical or statistical, to the decision surface to deduce the type of the defect. The system has a centralized database which holds the defect information in the form of known and calculated features. The known features and their quantitative representations are used to initialize the database. Then experiments are conducted on known defects and the collected experimental data is then modeled into autoregressive process models using state of the art l<sub>tinfin</sub> deconvolution algorithm. With each feature set, a classifier tag is associated that assigns a class number to that defect. The classifier tag is then used to classify any new data using the fuzzy classifier.",2007,0,
1078,1079,"A Model Driven Architecture approach to fault tolerance in Service Oriented Architectures, a performance study","In modern service oriented architectures (SoA) identifying the occurrences of failure is a crucial task, which can be carried out by the creation of diagnosers to monitor the behavior of the system. Model driven architecture (MDA) can be used to automatically create diagnosers and to integrate them into the system to identify if a failure has occurred. There are different methods of incorporating a diagnoser into a group of interacting services. One option is to modify the BPEL file representing services to incorporate the diagnoser. Another option is to implement the diagnoser as a separate service which interacts with the existing services. Moreover, the interaction between the diagnoser and the services can be either orchestration or choreography. As result, there are four options for the implementation of the diagnoser into the SoA via MDA. This paper reports on an Oracle JDeveloper plugin tool developed which applies MDA to create these four possible implementations and compares the performance of them with the help of a case study.",2008,0,
1079,1080,Rigorous development of an embedded fault-tolerant system based on coordinated atomic actions,"Describes our experience using coordinated atomic (CA) actions as a system structuring tool to design and validate a sophisticated and embedded control system for a complex industrial application that has high reliability and safety requirements. Our study is based on an extended production cell model, the specification and simulator for which were defined and developed by FZI (Forschungszentrum Informatik, Germany). This ""fault-tolerant production cell"" represents a manufacturing process involving redundant mechanical devices (provided in order to enable continued production in the presence of machine faults). The challenge posed by the model specification is to design a control system that maintains specified safety and liveness properties even in the presence of a large number and variety of device and sensor failures. Based on an analysis of such failures, we provide details of: (1) a design for a control program that uses CA actions to deal with both safety-related and fault tolerance concerns and (2) the formal verification of this design based on the use of model checking. We found that CA action structuring facilitated both the design and verification tasks by enabling the various safety problems (involving possible clashes of moving machinery) to be treated independently. Even complex situations involving the concurrent occurrence of any pairs of the many possible mechanical and sensor failures can be handled simply yet appropriately. The formal verification activity was performed in parallel with the design activity, and the interaction between them resulted in a combined exercise in ""design for validation""; formal verification was very valuable in identifying some very subtle residual bugs in early versions of our design which would have been difficult to detect otherwise",2002,0,
1080,1081,An Error Resilient Coding Scheme for Video Transmission Based on Pixel Line Decimation,"The loss of packets is unavoidable when compressed video data is transmitted over error prone channels. In this study, an error resilient coding scheme based on pixel lines decimation is proposed to enhance performance of error concealment for both intra and inter frames. At the encoder, an input picture is decimated by pixel lines into two similar sub-pictures and then they are merged together before encoding. At the decoder, the high correlation of two sub-pictures is employed to facilitate error concealment. For an intra frame, the lost pixel lines in a sub-picture can be spatial concealed by interpolation between spatial neighbor correct pixel lines in the other sub-picture in a short distance. For an inter frame, a corrupt macroblock can be temporal concealed by utilizing motion vectors of its co-located macroblock in the other sub-picture. Experimental results demonstrate that the proposed scheme can significantly improve both subjective and objective quality of the reconstructed picture.",2008,0,
1081,1082,Diagnosing arbitrary defects in logic designs using single location at a time (SLAT),"A new form of logic diagnosis is described that is suitable for diagnosing fails in combinational logic. It can diagnose defects that can affect arbitrarily many elements in the integrated circuit. It operates by first identifying patterns during which only one element is affected by the defect, and then diagnosing the fails observed during the application of such patterns, one pattern at a time. Single stuck-at faults are used for this purpose, and the aggregate of stuck-at fault locations thus identified is then further analyzed to obtain the most accurate estimate of the identities of those elements that can be affected by the defect. This approach to logic diagnosis is as effective as that of classical stuck-at fault-based diagnosis, when the latter applies, but is far more general. In particular, it can diagnose fails caused by bridges and opens as well as fails caused by regular stuck-at faults.",2004,0,
1082,1083,Minimizing temperature drift errors of conditioning circuits using artificial neural networks,"Temperature drift errors are a problem that affect the accuracy of measurement systems. When small amplitude signals from transducers are considered and environmental conditions of conditioning circuits exhibit a large temperature range, the temperature drift errors have a real impact in systems accuracy. In this paper, a solution to overcome the problem of temperature drift errors of conditioning circuits is proposed. As an example, a thermocouple-based temperature measurement system is considered, and the stability of its conditioning circuit (AD595) is analyzed in two cases: with and without temperature drift error compensation. An Artificial Neural Network (ANN) is used for data optimization and a Virtual Instrument, using GPIB instrumentation, is used to collect experimental data. Final results show a significant improvement in the accuracy of the system when the proposed temperature drift error compensation technique is applied to compensate errors caused by temperature variations",2000,0,
1083,1084,"A fault tolerant, peer-to-peer replication network","We propose a fault tolerant, peer-to-peer replication network for synchronizing files across multiple hosts. The proposed topology is constructed by applying existing technologies and tools to ensure that files are kept synchronized even after subsequent modifications. One of its main advantages lies in the fact that there is no central authority to coordinate the process, hosts are connected in a peer-to-peer fashion, thus avoiding a single point of failure. Our proposal is intended for use in networks of personal computers where a small number of hosts have to be synchronized.",2010,0,
1084,1085,A formal specification of fault-tolerance in prospecting asteroid mission with Reactive Autonomie Systems Framework,"The NASA's Autonomous Nano Technology Swarm (ANTS) is a generic mission architecture consisting of miniaturized, autonomous, self-similar, reconfigurable, and addressable components forming structures. The Prospecting Asteroid Mission (PAM) is one of ANTS applications for survey of large dynamic populations. In this paper, we propose a formal approach based on Category Theory to specify the fault-tolerance property in PAM by Reactive Autonomie Systems Framework.",2010,0,
1085,1086,Analytical redundancy for sensor fault isolation and accommodation in public transportation vehicles,"The paper discusses an instrument fault detection, isolation, and accommodation procedure for public transportation vehicles. After a brief introduction to the topic, the rule set implementing the procedure with reference to the kinds of sensors usually installed on public transportation vehicles is widely discussed. Particular attention is paid to the description of the rules aimed at allowing the vehicle to continue working regularly even after a sensor fault develops. Finally, both the estimated diagnostic and dynamic performances in the off-line processing of the data acquired in several drive tests are then analyzed and commented upon.",2004,0,
1086,1087,An error tolerance scheme for 3D CMOS imagers,"A three-dimensional (3D) CMOS imager constructed by stacking a pixel array of backside illuminated sensors, an analog-to-digital converter (ADC) array, and an image signal processor (ISP) array using micro-bumps (ubumps) and through-silicon vias (TSVs) is promising for high throughput applications. However, due to the direct mapping from pixels to ISPs, the overall yield relies heavily on the correctness of the ubumps, ADCs and TSVs - a single defect leads to the information loss of a tile of pixels. This paper presents an error tolerance scheme for the 3D CMOS imager that can still deliver high quality images in the presence of bump, ADC, and/or TSV failures. The error tolerance is achieved by properly interleaving the connections from pixels to ADCs so that the corrupted data, if any, can be recovered in the ISPs. A key design parameter, the interleaving stride, is decided by analyzing the employed error correction algorithm. Architectural simulation results demonstrate that the error tolerance scheme enhances the effective yield of an exemplar 3D imager from 46% to 99%.",2010,0,
1087,1088,Fault Tolerant Active Rings for Structured Peer-to-Peer Overlays,"Algorithms by which peers join and leave structured overlay networks can be classified as passive or active. Passive topology maintenance relies on periodic background repair of neighbor pointers. When a node passively leaves the overlay, subsequent lookups may fail silently. Active maintenance has been proven only for fault-free networks. We develop an active topology maintenance algorithm for practical, fault-prone networks. Unlike prior work, it a) maintains ring continuity during normal topology changes and b) guarantees consistency and progress in the presence of faults. The latter property is inherited by novel extension of the Paxos commit algorithm. The topology maintenance algorithm is formally developed using the B method and its event-driven extensions for dynamic systems. Messaging and storage overheads are quantified",2005,0,
1088,1089,On the error distribution for randomly-shifted lattice rules,"Randomized quasi-Monte Carlo (RQMC) methods estimate the expectation of a random variable by the average of n dependent realizations of it. In general, due to the strong dependence, the estimation error may not obey a central limit theorem. Analysis of RQMC methods have so far focused mostly on the convergence rates of asymptotic worst-case error bounds and variance bounds, when n  , but little is known about the limiting distribution of the error. Here we examine this limiting distribution for the special case of a randomly-shifted lattice rule, when the integrand is smooth. We start with simple one-dimensional functions, where we show that the limiting distribution is uniform over a bounded interval if the integrand is non-periodic, and has a square root form over a bounded interval if the integrand is periodic. In higher dimensions, for linear functions, the distribution function of the properly standardized error converges to a spline of degree equal to the dimension.",2009,0,
1089,1090,Consistent detection of global predicates under a weak fault assumption,"We study the problem of detecting general global predicates in distributed systems where all application processes and at most t<m monitor processes may be subject to crash faults, where m is the total number of monitor processes in the system. We introduce two new observation modalities called negotiably and discernibly (which correspond to possibly and definitely in fault-free systems) and present detection algorithms for them which work under increasingly weak fault assumptions",2000,0,
1090,1091,A fault-tolerant P-Q decoupled control scheme for static synchronous series compensator,"Control of nonlinear devices in power systems relies on the availability and the quality of sensor measurements. Measurements can be corrupted or interrupted due to sensor failure, broken or bad connections, bad communication, or malfunction of some hardware or software (referred to as missing sensor measurements in this paper). This paper proposes a fault-tolerant control scheme (FTCS) for a static synchronous series compensator (SSSC). This FTCS consists of a sensor evaluation and (missing sensor) restoration scheme (SERS) cascaded with a P-Q decoupled control scheme (PQDC). It is able to provide effective control to the SSSC when single or multiple crucial sensor measurements are unavailable. Simulation studies are carried out to examine the validity of the proposed FTCS. During the simulations, single and multiple phase current sensors are assumed to be missing, respectively. Results show that the SERS restores the missing data correctly during steady and transient states, including small and large disturbances, and unbalanced three-phase operation. Thus, the FTCS continuously provides effective control to the SSSC with and without missing sensor measurements",2006,0,
1091,1092,BER Performance of FSO Links over Strong Atmospheric Turbulence Channels with Pointing Errors,"In this letter, we investigate the error rate performance of free-space optical (FSO) links over strong turbulence fading channels together with misalignment (pointing error) effects. First, we present a novel closed-form expression for the distribution of a stochastic FSO channel model which takes into account both atmospheric turbulence-induced fading and misalignment-induced fading. Then, we evaluate the average bit-error rate in closed form of a FSO system operating in this channel environment, assuming intensity modulation/direct detection with on-off keying. Numerical examples are further provided to collaborate on the derived analytical expressions.",2008,0,
1092,1093,Deterministic high-speed simulation of complex systems including fault-injection,"FAUmachine is a virtual machine for the highly detailed simulation of standard PC hardware together with an environment. FAUmachine comes with fault injection capabilities and an automatic experiment controller facility. Due to its use of just-in-time compiler techniques, it offers good performance. This tool description introduces the new feature of FAUmachine to simulate systems deterministically. This will enable developers to design and test complex systems for fault tolerance by running identically reproducible automated tests in reasonable time and thus even allow testing for real time constraints.",2009,0,
1093,1094,Fault Diagnosis Using a Timed Discrete-Event Approach Based on Interval Observers: Application to Sewer Networks,This paper proposes a fault diagnosis method using a timed discrete-event approach based on interval observers that improves the integration of fault detection and isolation tasks. The interface between fault detection and fault isolation considers the activation degree and the occurrence time instant of the diagnostic signals using a combination of several theoretical fault signature matrices that store the knowledge of the relationship between diagnostic signals and faults. The fault isolation module is implemented using a timed discrete-event approach that recognizes the occurrence of a fault by identifying a unique sequence of observable events (fault signals). The states and transitions that characterize such a system can directly be inferred from the relation between fault signals and faults. The proposed fault diagnosis approach has been motivated by the problem of detecting and isolating faults of the Barcelona's urban sewer system limnimeters (level meter sensors). The results obtained in this case study illustrate the benefits of using the proposed approach in comparison with the standard fault detection and isolation approach.,2010,0,
1094,1095,A Zero Module Current Obtaining Approach Based on Magnetic Induction for Single Phase Grounding Fault,"A new approach based on the magnetic field induction is presented to obtain transient zero module current for single phase grounding gault of overhead lines. The paper analyses the characteristics of magnetic field around the overhead lines and presents the magnetic field under the lines is proportional to the zero module current, and the zero module current can be measured by inducting the magnetic field. The paper proposes a zero-module current obtaining approach using a hall sensor to induct magnetic field, and elaborates the solution to the key issues in practical applications, at last simulation and experiment results demonstrate the feasibility of the approach.",2010,0,
1095,1096,Similarity-Based Bayesian Learning from Semi-structured Log Files for Fault Diagnosis of Web Services,"With the rapid development of XML language which has good flexibility and interoperability, more and more log files of software running information are represented in XML format, especially for Web services. Fault diagnosis by analyzing semi-structured and XML like log files is becoming an important issue in this area. For most related learning methods, there is a basic assumption that training data should be in identical structure, which does not hold in many situations in practice. In order to learn from training data in different structures, we propose a similarity-based Bayesian learning approach for fault diagnosis in this paper. Our method is to first estimate similarity degrees of structural elements from different log files. Then the basic structure of combined Bayesian network (CBN) is constructed, and the similarity-based learning algorithm is used to compute probabilities in CBN. Finally, test log data can be classified into possible fault categories based on the generated CBN. Experimental results show our approach outperforms other learning approaches on those training datasets which have different structures.",2010,0,
1096,1097,Path Splicing with Guaranteed Fault Tolerance,"This paper addresses the problem of exploring the fault tolerance potential of the routing primitive called path splicing. This routing mechanism has been recently introduced in order to improve the reliability level of networks. The idea is to provide for each destination node in a network several different routing trees, called slices, by running different routing protocols simultaneously. The possibility for the traffic to switch between different slices at any hop on the way to the destination makes it possible to achieve a level of reliability that is close to the ideal level achieved by the underlying network. In this work we show that there is a method for computing just two slices that achieves fault tolerance against all single-link failures that do not disconnect the underlying network. We present an experimental evaluation of our approach, showing that for a number of realistic topologies our method of computing the slices achieves the same level of fault tolerance that is achieved by a much larger number of slices using the previously proposed method.",2009,0,
1097,1098,Simulated Fault Injection for Quantum Circuits Based on Simulator Commands,"This paper addresses the problem of evaluating the fault tolerance algorithms and methodologies (FTAMS) designed for quantum circuits, by making use of fault injection techniques. These techniques are inspired from their rich classical counterparts, and were adapted to the quantum computation specific features, including the available error models. Because of their wide spectrum of application, including quantum circuit simulation, and their flexibility in circuit representation (i.e. both behavioral and structural descriptions of a circuit are possible), the hardware description languages (HDLs) appear as the best choice for our simulation experiments. The techniques employed for fault injection are based on simulator commands. The simulation results for the fault-affected circuit were compared to the outputs of a Gold circuit (a faultless circuit) in order to compute the quantum failure rate.",2007,0,
1098,1099,Speeding up Fault Injection for Asynchronous Logic by FPGA-Based Emulation,"While stability and robustness of synchronous circuits becomes increasingly problematic due to shrinking feature sizes, delay-insensitive asynchronous circuits are supposed to provide inherent protection against various fault types. However, results on experimental evaluation and analysis of these fault tolerance properties are scarce, mainly due to the lack of suitable prototyping platforms. Using a soft-core processor as an example, this paper shows how an off-the-shelf FPGA can be used for asynchronous four state logic designs, on which future fault injection experiments will be conducted.",2009,0,
1099,1100,The effect of the specification model on the complexity of adding masking fault tolerance,"In this paper, we investigate the effect of the representation of safety specification on the complexity of adding masking fault tolerance to programs - where, in the presence of faults, the program 1) recovers to states from where it satisfies its (safety and liveness) specification and 2) preserves its safety specification during recovery. Specifically, we concentrate on two approaches for modeling the safety specifications: 1) the bad transition (BT) model, where safety is modeled as a set of bad transitions that should not be executed by the program, and 2) the bad pair (BP) model, where safety is modeled as a set of finite sequences consisting of at most two successive transitions. If the safety specification is specified in the BT model, then it is known that the complexity of automatic addition of masking fault tolerance to high atomicity programs - where processes can read/write all program variables in an atomic step) - is polynomial in the state space of the program. However, for the case where one uses the BP model to specify safety specification, we show that the problem of adding masking fault tolerance to high atomicity programs is NP-complete. Therefore, we argue that automated synthesis of fault-tolerant programs is likely to be more successful if one focuses on problems where safety can be represented in the BT model.",2005,0,
1100,1101,Fault detection and isolation based on system feedback,"This paper present a method to detect the transducers fault in the close loop control systems. The necessities imposed for the fault detection algorithm are: rapid answer in the case of fault, which comes out; the diminution of the risk to come out false alarms; lower effort calculation. In the paper are presented the equations of fault detection structure that suggest the software algorithms. In last part of the paper, the algorithm was verified on the steam overhead equations, developed in this paper.",2008,0,
1101,1102,Analysis and management system for power system fault information based on intranet network,"Using Internet network technique, database technique and object-oriented design method, a comprehensive analysis and management system for power system fault information is developed and a feasible network connection project for fault information connection is put forward. The fault analysis functions, e.g., fault diagnosis and location, supervisory and judgment to equipment operation, harmonic analysis and waveform treatment, etc. are developed. Fault information management system is also set up, and easy inquiry and browse based on the Web is developed. The practical application shows that the proposed system makes the information be wide share and promotes operation automation.",2002,0,
1102,1103,Using simulation for assessing the real impact of test-coverage on defect-coverage,"The use of test-coverage measures (e.g., block-coverage) to control the software test process has become an increasingly common practice. This is justified by the assumption that higher test-coverage helps achieve higher defect-coverage and therefore improves software quality. In practice, data often show that defect-coverage and test-coverage grow over time, as additional testing is performed. However, it is unclear whether this phenomenon of concurrent growth can be attributed to a causal dependency, or if it is coincidental, simply due to the cumulative nature of both measures. Answering such a question is important as it determines whether a given test-coverage measure should be monitored for quality control and used to drive testing. Although it is no general answer to this problem, a procedure is proposed to investigate whether any test-coverage criterion has a genuine additional impact on defect-coverage when compared to the impact of just running additional test cases. This procedure applies in typical testing conditions where the software is tested once, according to a given strategy, coverage measures are collected as well as defect data. This procedure is tested on published data, and the results are compared with the original findings. The study outcomes do not support the assumption of a causal dependency between test-coverage and defect-coverage, a result for which several plausible explanations are provided",2000,0,
1103,1104,Performance analysis of three-phase induction motor drives under inverter fault conditions,"This paper presents a comparative analysis involving several fault tolerant operating strategies applied to three-phase induction motor drives. The paper exploits the advantages and the inconveniences of using remedial operating strategies under different control techniques, such as the field oriented control and the direct torque control. Global results are presented concerning the analysis of some key parameters like efficiency, motor line currents harmonic distortion, among others.",2003,0,
1104,1105,Tool Support for Fault Localization Using Architectural Models,"Locating software faults is a problematic activity in many systems. Existing tool approaches usually work close to the system implementation, requiring the developer to perform tedious code analyses in which the amount of information she must manage is usually overwhelming. This problem calls for approaches able to work at higher abstraction levels than code. In this context, we present a tool approach, called FLABot, to assist fault-localization tasks. A novelty of FLABot is that it reasons about faults using software architecture information. Based on Use-case-maps and system logs, FLABot performs a heuristic search for possible faulty functions in the architecture, and then maps these functions to code sections. This allows the developer to quickly navigate large systems and spot code regions that may contain faults, which can be further debugged using conventional techniques. Our preliminary experiments have shown that FLABot is practical and reduces the efforts for discovering faults.",2009,0,
1105,1106,Performance Improvement of the IPMSM Position Sensor-less Vector Control System by the On-line Motor Parameter Error Compensation and the Practical Dead-time Compensation,"This paper proposes the performance improvement methods for the IPMSM position sensor-less vector control system. The stability of the position sensor-less control is influenced by the motor parameter error and dead time compensation error. Especially, it becomes problem in case of the extreme temperature variation and at low speed. The influence on the position estimation is analyzed for the proposed method which using an armature current flux adaptive observer. For performance improvement and simplification of control system, the offline parameter measurement of a stator resistance, permanent magnet flux, d,q-axis inductances, and dead-time compensation are proposed. And the on-line parameter error compensation method is applied to reduce the position error, which uses only the observer signal without any additional sensor or signal injection. In addition, the practical dead-time compensation method is proposed based on the experimental measurement. Experimental results of the test system using the proposed methods showed good performance.",2007,0,
1106,1107,Path vs. subpath vs. link restoration for fault management in IP-over-WDM networks: performance comparisons using GMPLS control signaling,"We investigate three restoration techniques (path, subpath, and link restoration) for fault management in an IP-over-WDM network. We have implemented all of these techniques on the ns-2 simulation platform using generalized multiprotocol label switching (GMPLS) control signaling. These techniques can handle practical situations such as simultaneous multiple fiber failures, which are difficult to design for and recover from by nonrestoration techniques. We then present performance measurement results for the three restoration techniques by applying them to a typical nationwide mesh network running IP over WDM. We investigate interesting trade-offs in the performance of the restoration techniques on restoration success rate, average restoration time, availability, and blocking probability.",2002,0,
1107,1108,Error Models for the Transport Stream Packet Channel in the DVB-H Link Layer,"Digital video broadcasting for hand held terminals (DVB-H) is a broadcast system designed for high-speed data transmission in highly dispersive mobile channel conditions. In this paper, methods of reproducing the statistical properties of measured DVB-H packet error traces are presented. Statistical and finite-state modeling approaches are found to be suitable for simulating the error performance of a DVB-H system operating in typical urban channel conditions. Evaluation of these models focuses on the accuracy of the models in replicating the high-order statistical properties of measured DVB-H transport stream error traces. Also, the effect of these error statistics on the DVB-H link layer frame error rate is considered.",2006,0,
1108,1109,Operating system supports to enhance fault tolerance of real-time systems,"The virtual memory functions in real-time operating systems have been used in real-time systems. The virtual memory functions of real-time operating systems enhance the fault-tolerance of real-time systems because their memory protection mechanism isolates faulty real-time tasks. Recent RISC processors provide virtual memory support through software-managed translation lookaside buffer (TLB) in software. In real-time systems, managing TLB entries is the most important issue because overhead at TLB miss time greatly affects overall performance of the system. In this paper, we propose several virtual memory management algorithms by comparing overheads at task switching times and TLB miss times.",2003,0,
1109,1110,Effect of errors in the system matrix on iterative image reconstruction,"Statistically based iterative image reconstruction is widely used in emission tomography. One important component in iterative image reconstruction is the system matrix, which defines the mapping from the image space to the data space. Several groups have demonstrated that an accurate system matrix can improve image quality in both SPECT and PET. While iterative methods are amenable to arbitrary and complicated system models, the true system response is never known exactly. In practice, one also has to sacrifice the accuracy of the system model because of limited computing and imaging resources. This paper analyzes the effect of errors in the system matrix on iterative image reconstruction. We derived a theoretical expression for calculating artifacts in a reconstructed image that are caused by errors in the system matrix. Using this theoretical expression, we can address the question of how accurate the system matrix needs to be. Computer simulations were conducted to validate theoretical results.",2004,0,
1110,1111,Design and Verification of Internet Service Automatic Fault-Heal System,"Based on the current situation of Internet service management, we put forward an Internet service automatic fault-heal system. Combined with autonomic computing and services probes components, we put forward organization model and autonomic computing model of the system. Simultaneously, with the use of timed automata and UPPAAL - a model checking tool for timed automatons, we modeled the system and simulated it. Then we made a detailed description and analysis of each component of this model. Finally, combined with the verifier of UPPAAL and the TCTL formula, we verified the deadlock, safety and feasibility of the system.",2009,0,
1111,1112,Rolling element bearing fault classification using soft computing techniques,"This paper presents a method, based on classification techniques, for automatically detecting and diagnosing various types of defects which may occur on a rolling element bearing. In the experiments we have used vibration signals coming from a mechanical device including more than ten rolling element bearings monitored by means of four accelerometers: the signals have been collected both with all faultless bearings and substituting one faultless bearing with an artificially damaged one: four different defects have been taken into account. The proposed technique considers all the aspects of classification: feature selection, different base classifiers (two statistical classifiers, namely LDC and QDC, and MLP neural networks) and classifier fusion. Experiments, performed on the vibration signals represented in the frequency domain, have shown that the proposed classification method is highly sensitive to different types of defects and to different severity degrees of the defects.",2009,0,
1112,1113,Study of solid state fault interruption device for medium voltage distribution systems with distributed generators,"Solid state fault interruption devices (FID) can interrupt fault currents much faster than the presently available circuit breakers. Due to their current inability to block system level voltages, presently available semiconductor devices are connected in series to increase blocking capability of the fault interrupting device. To verify the ability of the interruption device for use in a medium voltage distribution system, a FID model is subjected to simulated tests for continuous current carrying capability, rated fault current interruption, and lightning impulse withstand tests. A simulation to demonstrate the ability of the FID to interrupt fault currents in a 7.2kV distribution system and to study the effect on system voltages is shown.",2010,0,
1113,1114,Error analysis of Chinese text segmentation using statistical approach,"The Chinese text segmentation is important for the indexing of Chinese documents, which has significant impact on the performance of Chinese information retrieval. The statistical approach overcomes the limitations of the dictionary based approach. The statistical approach is developed by utilizing the statistical information about the association of adjacent characters in Chinese text collected from the Chinese corpus. Both known words and unknown words can be segmented by the statistical approach. However, errors may occur due to the limitation of the corpus. In this work, we have conducted the error analysis of two Chinese text segmentation techniques using statistical approach, namely, boundary detection and heuristic method. Such error analysis is useful for the future development of the automatic text segmentation of Chinese text or other text in oriental languages. It is also helpful to understand the impact of these errors on the information retrieval system in digital libraries.",2004,0,
1114,1115,Synthesis of Flexible Fault-Tolerant Schedules with Preemption for Mixed Soft and Hard Real-Time Systems,"In this paper we present an approach for scheduling with preemption for fault-tolerant embedded systems composed of soft and hard real-time processes. We are interested to maximize the overall utility for average, most likely to happen, scenarios and to guarantee the deadlines for the hard processes in the worst case scenarios. In many applications, the worst-case execution times of processes can be much longer than their average execution times. Thus, designs for the worst-case can be overly pessimistic, i.e., result in low overall utility. We propose preemption of process executions as a method to generate flexible schedules that maximize the overall utility for the average case while guarantee timing constraints in the worst case. Our scheduling algorithms determine off-line when to preempt and when to resurrect processes. The experimental results show the superiority of our new scheduling approach compared to approaches without preemption.",2008,0,
1115,1116,"Assessing and Estimating Corrective, Enhancive, and Reductive Maintenance Tasks: A Controlled Experiment","This paper describes a controlled experiment of student programmers performing maintenance tasks on a C++ program. The goal of the study is to assess the maintenance size, effort, and effort distribution of three different maintenance types and to describe estimation models to predict the programmer's effort on maintenance tasks. The results of our study suggest that corrective maintenance is much less productive than enhancive and reductive maintenance. Our study also confirms the previous results which conclude that corrective and reductive maintenance requires large proportions of effort on program comprehension activity. Moreover, the best effort model we obtained from fitting the experiment data can estimate the time of 79% of the programmers with the error of 30% or less.",2009,0,
1116,1117,A New Algorithm for the Detection of Inter-Turn Stator Faults in Doubly-Fed Wind Generators,Steady state analysis techniques (i.e. Motor Current Signature Analysis) cannot be applied to variable speed wind generators since their operation is predominately in the transient. A new non-stationary fault detection technique is proposed to detect inter-turn stator faults in doubly-fed wind generators. The technique is a combination of the extended Park's vector approach and a new adaptive algorithm. It will be shown that the new technique can unambiguously detect inter-turn stator faults under transient conditions while providing insight into the degree of severity of the fault,2006,0,
1117,1118,Automatic Recognition of Defect Signatures and Notification of Tool Malfunctions - IECON'06,"We have developed an automatic method that efficiently detects the defect signatures of substrates and identifies possible problems pertaining to LSI/TFT-LCD manufacturing processes and tools. This system, which has no built-in libraries, can be applied to the mass production line of thin film devices. This method is useful to quickly detect problems that have been overlooked thus far",2006,0,
1118,1119,antenna-pattern correction for near-field-to-far field RCS transformation of 1D linear SAR measurements,"In a previous AMTA paper (B. E. Fischer, et al.), we presented a first-principles algorithm, called wavenumber migration (WM), for estimating a target's far-field RCS and/or far-field images from extreme near-field linear (one-dimensional) or planar (two-dimensional) SAR measurements, such as those collected for flight-line diagnostics of aircraft signatures. However, the algorithm assumes the radar antenna has a uniform, isotropic pattern for both transmitting and receiving. In this paper, we describe a modification to the (one-dimensional) linear SAR wavenumber migration algorithm that compensates for nonuniform antenna-pattern effects. We also introduce two variants to the algorithm that eliminate certain computational steps and lead to more efficient implementations. The effectiveness of the pattern compensation is demonstrated for all three versions of the algorithm in both the RCS and the image domains using simulated data from arrays of simple point scatterers.",2004,0,
1119,1120,Error resilience of EZW coder for image transmission in lossy networks,We investigate the effect of network errors on Embedded Zerotree Wavelet (EZW) encoded images and propose modifications to the EZW coder to increase error resilience in bursty packet loss conditions. A hybrid-encoding scheme that uses data interleaving to spread correlated information into independently processed groups and layered encoding to protect significant information within each group is presented. Simulation results for various packet loss percentages show the improved error resiliency of our scheme in random and bursty packet loss environments.,2002,0,
1120,1121,Optimum Machine Performance in Fault-Tolerant Networked Control Systems,"This paper investigates the effect of failures on the productivity of fault-tolerant networked control systems under varying loads. Higher speeds of operation are sometimes used to increase production and compensate for down time due to component failures. Improved Markov models are developed and used to calculate system probabilities. When these probabilities are combined with the maximum speed of operation in each system state, the average speed of operation is obtained. If machines cannot be run at maximum speed all the time, the Markov models are used again to find the best speed mix that would yield maximum output capacity",2005,0,
1121,1122,A novel algorithm of wide area backup protection based on fault component comparison,"This paper proposes a novel wide area back-up protection algorithm that measures synchronized information from different buses in region. The amplitude of voltage fault component from different buses is compared, and the bus with maximum magnitude will be selected. Hence, suspected fault line set could be established according to the sub-graph and complete incidence matrix of selected buses. Then, voltage fault component amplitude at two sides of each suspected line could be calculated from another side, and the amplitude comparison between computed and measured value could be implemented. The ratio would be 1 when external fault occurs, and it would be greater than 1 when internal fault occurs. Thus, the fault line could be identified finally. The technique doesn't need high precision synchronization of wide area information, and could response to different faults. The simulation of 10-unit and 39-bus New England system using PSCAD/EMTDC illustrates the effectiveness of this method.",2010,0,
1122,1123,Fault tolerant mechanism in grid based on Backup Node,"Grid is a very efficient technology to performing heavy processing with distributed resources. These resources at geographically distributed widely and without central control unit. One of the important challenges in grid is fault tolerant, because the grid resources in this environment are not reliable. And to avoid duplication with processing done by a resource, the Check Pointing mechanism has been proposed. The state of application at particular point of time can be store and in case of failure the status information can restore on new node and computation can continue. Check Pointe storage location is a major challenge in the fault tolerant techniques. In this paper we presented a new method by using a backup node. Adding a component to GRAM to improve efficiency and fault tolerant. With clear Backup Node the Check point information can save on it. This method increases the efficiency and system reliability.",2010,0,
1123,1124,Embedded Implementation of a SIP Server Gateway with Forward Error Correction to a Mobile Network,"The emergence of the Voice over Internet Protocol (VoIP) services requires interconnection between different technologies. In this paper we present an embedded Session Initiation Protocol (SIP) Proxy Gateway (GW) to a Mobile Network with Forward Error Corrections (FEC). Our server registers, locates and forwards the calls of an end user providing intelligent routing for user tracking. In addition, FEC along with a Markov-Chain loss model has been integrated into the server to perform a range of tests. Results prove that under high packet loss rate the embedded SIP server GW improves the speech quality.",2010,0,
1124,1125,Information theoretic fault detection,In this paper we propose a novel method of fault detection based on a clustering algorithm developed in the information theoretic framework. A mathematical formulation for a multi-input multi-output (MIMO) system is developed to identify the most informative signals for the fault detection using mutual information (MI) as the measure of correlation among various measurements on the system. This is a model-independent approach for the fault detection. The effectiveness of the proposed method is successfully demonstrated by employing MI-based algorithm to isolate various faults in 16-cylinder diesel engine in the form of distinct clusters.,2005,0,
1125,1126,The Fault Diagnosis of a Class of Nonlinear Stochastic Time-delay systems,"This paper presents a new fault detection algorithm for a class of nonlinear stochastic time-delay systems. Different from the classical fault detection design, a fault detection filter with an output observer and a consensus filter is constructed for fault detecting. Simulations are provided to show the efficiency of the proposed approach.",2006,0,
1126,1127,Three-Stage Error Concealment for Distributed Speech Recognition (DSR) with Histogram-Based Quantization (HQ) Under Noisy Environment,"In this paper, a three-stage error concealment (EC) framework based on the recently proposed histogram-based quantization (HQ) for distributed speech recognition (DSR) is proposed, in which noisy input speech is assumed and both the transmission errors and environmental noise are considered jointly. The first stage detects the erroneous feature parameters at both the frame and subvector levels. The second stage then reconstructs the detected erroneous subvectors by MAP estimation, considering the prior speech source statistics, the channel transition probability, and the reliability of the received subvectors. The third stage then considers the uncertainty of the estimated vectors during Viterbi decoding. At each stage, the error concealment (EC) techniques properly exploit the inherent robust nature of histogram-based quantization (HQ). Extensive experiments with AURORA 2.0 testing environment and GPRS simulation indicated the proposed framework is able to offer significantly improved performance against a wide variety of environmental noise and transmission error conditions.",2007,0,
1127,1128,Error-free arithmetic for discrete wavelet transforms using algebraic integers,"A novel encoding scheme is introduced with applications to error-free computation of discrete wavelet transforms (DWT) based on Daubechies wavelets. The encoding scheme is based on an algebraic integer decomposition of the wavelet coefficients. This work is a continuation of our research into error-free computation of DCTs and IDCTs, and this extension is timely since the DWT is part of the new standard for JPEG2000. This encoding technique eliminates the requirements to approximate the transformation matrix elements by obtaining their exact representations. As a result, we achieve error-free calculations up to the final reconstruction step where we are free to choose an approximate substitution precision based on a hardware/accuracy trade-off.",2003,0,
1128,1129,Identifying efficient combinations of error detection mechanisms based on results of fault injection experiments,"We introduce novel performance ratings for error detection mechanisms. Given a proper setup of the fault injection experiments, these ratings can be directly computed from raw readout data. They allow the evaluation of the overall performance of arbitrary combinations of mechanisms without the need for further experiments. With this means we can determine a minimal subset of mechanisms that still provides the required performance",2002,0,
1129,1130,Immune-Inspired Adaptable Error Detection for Automated Teller Machines,"This paper presents an immune-inspired adaptable error detection (AED) framework for automated teller machines (ATMs). This framework has two levels: one is local to a single ATM, while the other is network-wide. The framework employs vaccination and adaptability analogies of the immune system. For discriminating between normal and erroneous states, an immune-inspired one-class supervised algorithm was employed, which supports continual learning and adaptation. The effectiveness of the proposed approach was confirmed in terms of classification performance and impact on availability. The overall results are encouraging as the downtime of ATMs can de reduced by anticipating the occurrence of failures before they actually occur.",2007,0,
1130,1131,Fault Tolerance and Recovery in Grid Workflow Management Systems,"Complex scientific workflows are now commonly executed on global grids. With the increasing scale complexity, heterogeneity and dynamism of grid environments the challenges of managing and scheduling these workflows are augmented by dependability issues due to the inherent unreliable nature of large-scale grid infrastructure. In addition to the traditional fault tolerance techniques, specific checkpoint-recovery schemes are needed in current grid workflow management systems to address these reliability challenges. Our research aims to design and develop mechanisms for building an autonomic workflow management system that will exhibit the ability to detect, diagnose, notify, react and recover automatically from failures of workflow execution. In this paper we present the development of a Fault Tolerance and Recovery component that extends the ActiveBPEL workflow engine. The detection mechanism relies on inspecting the messages exchanged between the workflow and the orchestrated Web Services in search of faults. The recovery of a process from a faulted state has been achieved by modifying the default behavior of ActiveBPEL and it basically represents a non-intrusive checkpointing mechanism. We present the results of several scenarios that demonstrate the functionality of the Fault Tolerance and Recovery component, outlining an increase in performance of about 50% in comparison to the traditional method of resubmitting the workflow.",2010,0,
1131,1132,Quantization errors in digital motor control systems,"When implementing a motor control drive scheme digitally, the quantization errors always exist in the system. Two major sources of quantization errors are analog-to-digital (A-D) process and numerical calculation in the fixed-point computing device. Typically, the effects of quantization errors due to A-D conversion contribute less than one produced by numerical calculation. This paper studies the quantization errors in a sensorless direct vector control system of induction motor system using a 32-bit fixed-point digital signal processor (DSP) from Texas Instruments (TMS320x28xx series). The investigation of quantization errors produced by numerical computation in such DSP is focused. Both simulation and experiment are carried out within DSP itself in three kinds of data formats; 16-bit fixed-point, 32-bit fixed-point, and floating-point. By comparing the results between floating-point and fixed-point implementation on one machine, numerical issues related to quantization errors can be verified and resolved. As a result, the system performance and behavior can be affected by quantization errors in 16-bit word length while it is not significant in the 32-bit word length.",2004,0,
1132,1133,Fault containment and error detection in the time-triggered architecture,"This paper investigates the fault-containment and error-detection mechanisms of distributed safety-critical time-triggered systems. The following critical failure modes of a fault-containment region are introduced and analyzed in detail: babbling idiot failures, masquerading failures, slightly-off-specification (SOS) failures, crash/omission (CO) failures, and massive transient disturbances. After a short description of the two time-triggered protocols TTP/C and FlexRay this paper tries to show how these two protocols handle the listed failure modes at the architecture level.",2003,0,
1133,1134,How a cyber-physical system can efficiently obtain a snapshot of physical information even in the presence of sensor faults,We present a distributed algorithm for cyber-physical systems to obtain a snapshot of sensor data. The snapshot is an approximate representation of sensor data; it is an interpolation as a function of space coordinates. The new algorithm exploits a prioritized medium access control (MAC) protocol to efficiently transmit information of the sensor data. It scales to a very large number of sensors and it is able to operate in the presence of sensor faults.,2008,0,
1134,1135,Bit-Error-Rate (BER) for modulation technique using Software defined Radio,"Software-defined radio technologies are attractive for future mobile communication systems because of reconfigurable and multimode operation capabilities. The reconfigurable feature is useful for enhancing functions of equipment with out replacing hardware. Multimode operation is essential for future wireless terminals because a number of wireless communication standards will still coexist. The transceiver is modeled in Matlab and consists of a BPSK transmitter, an additive white Gaussian noise (AWGN) channel, and a BPSK receiver. In this paper, we have considered basics modulation techniques used in mobile and wireless systems. Based on this analysis a PSK modulation scheme for SDR is proposed to pick the constellation size that offers the best reconstructed signal quality for each average SNR. Simulation results of signal transmissions confirm the effectiveness of our proposed PSK modulation scheme. The performance of the modulation technique is evaluated when the system is subjected to noise and interference in the channel. Computer simulation tool, MATLAB, is used to evaluate Bit-Error-Rate (BER) for Software defined Radio.",2009,0,
1135,1136,Modeling Depth Estimation Errors for Side Looking Stereo Video Systems,"In the EU funded Integrated Project APROSYS, a side pre-crash sensing system will be set up consisting of a stereo video rig and a radar network. A second goal of APROSYS is to provide tools for the efficient development of future products based on such a sensing system. If a stereo rig points to the side of a moving road vehicle, then maximum angular velocities in azimuth are typically very large. Synchronous operation of the stereo video cameras therefore becomes highly important for correct depth estimation, and hence crucial for pre-crash sensing. This paper proposes a tool for automated verification of the synchronicity of a stereo rig. It consists of a running light clock and automatic image analysis. An error model relates synchronization errors to depth estimation errors. For a given pair of cameras, the tool is applied to give an upper bound to the resulting depth estimation error for the APROSYS application scenario. The tool can be used as a standard for quality control of future product developments",2006,0,
1136,1137,Fault-Tolerant Real-Time Scheduling Algorithm for Tolerating Multiple Transient Faults,"The influence of computer systems in human life is increasing and thereby increases the need for having reliable, robust and real-time services of computer systems. Avoidance of any catastrophic consequences due to faults in such systems is one of the main objectives. This paper presents a fault-tolerant realtime scheduling algorithm, RM-FT, by extending the rate monotonic (RM) scheduling for real-time systems. The main approach used is employing temporal error masking (TEM) technique to achieve node level fault tolerance (NLFT) within the least common multiple of periods of a set of pre-emptively scheduled periodic tasks with at most f transient faults.",2006,0,
1137,1138,Hybrid Fault Diagnosis Scheme Implementation for Power Distribution Systems Automation,"Power distribution automation and control are important tools in the current restructured electricity markets. Unfortunately, due to its stochastic nature, distribution systems faults are hardly avoidable. This paper proposes a novel fault diagnosis scheme for power distribution systems, composed by three different processes: fault detection and classification, fault location, and fault section determination. The fault detection and classification technique is wavelet based. The fault-location technique is impedance based and uses local voltage and current fundamental phasors. The fault section determination method is artificial neural network based and uses the local current and voltage signals to estimate the faulted section. The proposed hybrid scheme was validated through Alternate Transient Program/Electromagnetic Transients Program simulations and was implemented as embedded software. It is currently used as a fault diagnosis tool in a Southern Brazilian power distribution company.",2008,0,
1138,1139,Emulation of software faults by educated mutations at machine-code level,"This paper proposes a new technique to emulate software faults by educated mutations introduced at the machine-code level and presents an experimental study on the accuracy of the injected faults. The proposed method consists of finding key programming structures at the machine code-level where high-level software faults can be emulated. The main advantage of emulating software faults at the machine-code level is that software faults can be injected even when the source code of the target application is not available, which is very important for the evaluation of COTS components or for the validation of software fault tolerance techniques in COTS based systems. The technique was evaluated using several real programs and different types of faults and, additionally, it includes our study on the key aspects that may impact on the technique accuracy. The portability of the technique is also addressed. The results show that classes of faults such as assignment, checking, interface, and simple algorithm faults can be directly emulated using this technique.",2002,0,
1139,1140,Understanding earthquake fault systems using QuakeSim analysis and data assimilation tools,"We are using the QuakeSim environment to model interacting fault systems. One goal of QuakeSim is to prepare for the large volumes of data that spaceborne missions such as DESDynI will produce. QuakeSim has the ability to ingest distributed heterogenous data in the form of InSAR, GPS, seismicity, and fault data into various earthquake modeling applications, automating the analysis when possible. Virtual California simulates interacting faults in California. We can compare output from long time-history Virtual California runs with the current state of strain and the strain history in California. In addition to spaceborne data we will begin assimilating data from UAVSAR airborne flights over the San Francisco Bay Area, the Transverse Ranges, and the Salton Trough. Results of the models are important for understanding future earthquake risk and for providing decision support following earthquakes. Improved models require this sensor web of different data sources, and a modeling environment for understanding the combined data.",2009,0,
1140,1141,Commercial fault tolerance: a tale of two systems,"This paper compares and contrasts the design philosophies and implementations of two computer system families: the IBM S/360 and its evolution to the current zSeries line, and the Tandem (now HP) NonStop Server. Both systems have a long history; the initial IBM S/360 machines were shipped in 1964, and the Tandem NonStop System was first shipped in 1976. They were aimed at similar markets, what would today be called enterprise-class applications. The requirement for the original S/360 line was for very high availability; the requirement for the NonStop platform was for single fault tolerance against unplanned outages. Since their initial shipments, availability expectations for both platforms have continued to rise and the system designers and developers have been challenged to keep up. There were and still are many similarities in the design philosophies of the two lines, including the use of redundant components and extensive error checking. The primary difference is that the S/360-zSeries focus has been on localized retry and restore to keep processors functioning as long as possible, while the NonStop developers have based systems on a loosely coupled multiprocessor design that supports a ""fail-fast"" philosophy implemented through a combination of hardware and software, with workload being actively taken over by another resource when one fails.",2004,0,
1141,1142,Application of immune-based optimization method for fault-section estimation in a distribution system,"In this paper, an immune algorithm-based (IA-based) optimization approach for the fault-section estimation of a distribution system is proposed. To apply the method to solve this estimation problem, each section of a power system model can be considered as an antibody. Through the immunology evolution, an antibody that most fits the antigen of concern becomes the solution. An affinity calculation has been employed in this computation process to measure the combination intensity. As this method can operate the population of antibodies simultaneously, the process stagnation can be better prevented. The proposed approach has been tested on Taiwan Power System (Taipower) through the utility data. Test results demonstrated the feasibility and effectiveness of the method for the applications.",2002,0,
1142,1143,Intrinsic Error Sources of Neural Networks,"The nature of radial basis function (RBF) networks necessitates some types of errors which can never be removed by traditional training algorithms. This paper is an attempt to introduce the natural error sources of neural networks such as bias error, iteration-restricted error, and Gibbs' error. Moreover, a new method is introduced, called post-training, to reduce these errors as far as desired",2006,0,
1143,1144,ISOMAP Algorithm-Based Feature Extraction for Electromechanical Equipment Fault Prediction,"As for the difficult problem of sensitive feature extraction during fault prediction for nonlinear electromechanical equipment, nonlinear dimensionality reduction ISOMAP (isometric feature mapping) algorithm is introduced based on the comprehensive analysis of the operation state data to reduce the dimensionality of high dimensional operation data and acquire sensitive fault features, furthermore, the ISOMAP dimensionality reduction result is verified by compiling algorithm programs based on MATLAB platform. The suitability of applying ISOMAP algorithm to dimensionality reduction of high dimensional data is discussed in terms of algorithm principle in this paper, and calculation steps of the algorithm are provided. According to MATLAB tests, ISOMAP algorithm is able to reduce dimensionality greatly and find out the essential data structure so as to provide a new method for extracting sensitive features of electromechanical equipment fault from another point of view.",2009,0,
1144,1145,Error Estimation Models Integrating Previous Models and Using Artificial Neural Networks for Embedded Software Development Projects,"In an earlier paper, we established 9 models for estimating errors in a new project. In this paper, we integrate the 9 models into 5 by investigating similarities among the models. In addition, we establish a new model using an artificial neural network (ANN). It is becoming increasingly important for software-development corporations to ascertain how to develop software efficiently, whilst guaranteeing delivery time and quality, and keeping development costs low. Estimating the manpower required by new projects and guaranteeing the quality of software are particularly important, because the estimation relates directly to costs while the quality reflects on the reliability of the corporations. In the field of embedded software, development techniques, management techniques, tools, testing techniques, reuse techniques, real-time operating systems and so on, have already been studied. However, there is little research on the relationship between the scale of the development and the number of errors using data accumulated from past projects. Hence, we integrate the previous models and establish a new model using an artificial neural network (ANN). We also compare the accuracy of the ANN model and the regression analysis models. The results of these comparisons indicate that the ANN model is more accurate than any of the 5 integrated models.",2008,0,
1145,1146,Multiple-antenna-aided OFDM employing genetic-algorithm-assisted minimum bit error rate multiuser detection,"The family of minimum bit error rate (MBER) multiuser detectors (MUD) is capable of outperforming the classic minimum mean-squared error (MMSE) MUD in terms of the achievable bit-error rate (BER) owing to directly minimizing the BER cost function. In this paper, we will invoke genetic algorithms (GAs) for finding the optimum weight vectors of the MBER MUD in the context of multiple-antenna-aided multiuser orthogonal frequency division multiplexing (OFDM) . We will also show that the MBER MUD is capable of supporting more users than the number of receiver antennas available, while outperforming the MMSE MUD.",2005,0,
1146,1147,Development of an expert system to fault diagnosis of three phase induction motor drive system,"Power electronic systems are considered as one of the most important components in many applications, such as nuclear reactors, aerospace, military applications and life saving machines. In such applications the system should be high reliable, a knowledge-based expert system was developed to diagnose faults in a three phase induction motor system . A software tool called KAPPA PC 2.1 was used to develop the expert system, the system is modeled in MATLAB SIMULINK and the simulation results at normal and fault conditions was rewritten as a set of if-then rules by which the expert system can discriminate the fault.",2008,0,
1147,1148,System independent and distributed fault management system,"This paper will outline a distributed and dynamic fault management system and practice of it. This work shows that proposed platform-independent, distributed and reusable fault management system architecture can be an integral part of the next generation of network management systems. Another feature of the proposed fault management system is being an extensible fault management computing framework for researchers. By the proposed infrastructure , researcher can carry out their original work on this framework by the help of the event, correlator and alarm programming interfaces. Proposed architecture is applicable not only to network management system but also to intrusion detection systems, business systems. We present these architecture and the use of advanced technologies such as RMI and Java. Finally, we demonstrate how these technological solutions have been implemented in the distributed fault management system called JADFAME - Java distributed fault management engine.",2003,0,
1148,1149,Automated antenna detection and correction methodology in VLSI designs,"As more and more devices are packed on a single chip and as the complexities of VLSI designs are increasing, antenna detection and correction is becoming an increasingly challenging task. The paper presents a methodology, which employs a combination of prevention and correction of antennae at various stages of ASIC (Application specific Integrated Circuits) design flow such as cell library development, block design flow and chip design flow. The methodology advocates adding protection diodes only in a certain number of cells in the library. We have implemented this methodology in our ASIC design flow and are able to solve antenna issues in designs with negligible impact on die size (24% increase in die-size in less than 5% of the designs) and performance (0.3%-0.6% worst case impact to delay). By employing this methodology, we found that the number of antennae in the final layout reduced to very small number and even to zero in some cases, and we were able to save the time involved in correcting antennae.",2003,0,
1149,1150,A validation fault model for timing-induced functional errors,"The violation of timing constraints on signals within a complex system can create timing-induced functional errors which alter the value of output signals. These errors are not detected by traditional functional validation approaches because functional validation does not consider signal timing. Timing-induced functional errors are also not detected by traditional timing analysis approaches because the errors may affect output data values without affecting output signal timing. A timing fault model, the Mis-Timed Event (MTE) fault model, is proposed to model timing-induced functional errors. The MTE fault model formulates timing errors in terms of their effects on the lifespans of the signal values associated with the fault. We use several examples to evaluate the MTE fault model. MTE fault coverage results shows that it efficiently captures an important class of errors which are not targeted by other metrics",2001,0,
1150,1151,JULIET: a distributed fault tolerant load balancer for .NET Web services,"The execution time of computationally-intensive applications such as protein folding and fractal generation can be reduced by implementing these applications as Web services that run in parallel. Additionally, some of these Web services may save state periodically to resume execution later on. However, currently, there is no solution to load balance this class of Web services, and to replicate the saved state for the purposes of resumption. This paper describes the architecture of JULIET, a system that load balances .NET Web services across a Windows cluster in a distributed fashion. The system is also fault tolerant since it supports failovers and replication of data generated by the Web services at the application level. The system is designed to be minimally-visible to the Web service and the client that consumes it.",2004,0,
1151,1152,A binary spelling interface with random errors,"An algorithm for design of a spelling interface based on a modified Huffman's algorithm is presented. This algorithm builds a full binary tree that allows to maximize an average probability to reach a leaf where a required character is located when a choice at each node is made with possible errors. A means to correct errors (a delete-function) and an optimization method to build this delete-function into the binary tree are also discussed. Such a spelling interface could be successfully applied to any menu-orientated alternative communication system when a user (typically, a patient with devastating neuromuscular handicap) is not able to express an intended single binary response, either through motor responses or by using of brain-computer interfaces, with an absolute reliability",2000,0,
1152,1153,Integrating reliability into the design of fault-tolerant power electronics systems,"This paper presents a methodology for integrating reliability considerations into the performance analysis carried out during the design of fault-tolerant power converters. The methodology relies on using a state-space representation of the power converter, based on averaging, similar to the ones used when analyzing linear time-invariant systems, and assumes an unknown-but-bounded uncertainty model for the converter uncontrolled inputs, such as load or variations in input voltage. The converter must be designed such that, for any uncontrolled input, the state variables remain within a region of the state space defined by performance requirements, e.g., output voltage tolerance or switch ratings. In the presence of component faults, and depending on the uncontrolled inputs, the converter may or may not meet performance requirements. Given the uncertain nature of these uncontrolled inputs, and for each particular fault, we introduce an analytical method to compute the probability that the performance requirements are met, which will define the reliability of the converter for each particular fault. By including these probabilities in a Markov reliability model, it is possible to obtain the overall converter reliability. The application of the methodology is illustrated with a case study of a fault-tolerant interleaved buck converter.",2008,0,
1153,1154,Automated duplicate detection for bug tracking systems,"Bug tracking systems are important tools that guide the maintenance activities of software developers. The utility of these systems is hampered by an excessive number of duplicate bug reports-in some projects as many as a quarter of all reports are duplicates. Developers must manually identify duplicate bug reports, but this identification process is time-consuming and exacerbates the already high cost of software maintenance. We propose a system that automatically classifies duplicate bug reports as they arrive to save developer time. This system uses surface features, textual semantics, and graph clustering to predict duplicate status. Using a dataset of 29,000 bug reports from the Mozilla project, we perform experiments that include a simulation of a real-time bug reporting environment. Our system is able to reduce development cost by filtering out 8% of duplicate bug reports while allowing at least one report for each real defect to reach developers.",2008,0,
1154,1155,Designing equally fault-tolerant configurations for kinematically redundant manipulators,"In this article, the authors examine the problem of designing nominal manipulator Jacobians that are optimally fault-tolerant to multiple joint failures. In this work, optimality is defined in terms of the worst case relative manipulability index. Building on previous work, it is shown that for a robot manipulator working in three-dimensional workspace to be equally fault-tolerant to any two simultaneous joint failures, the manipulator must have precisely six degrees of freedom. A corresponding family of Jacobians with this property is identified. It is also shown that the two-dimensional workspace problem has no such solution.",2009,0,
1155,1156,A partition-based approach for identifying failing scan cells in scan-BIST with applications to system-on-chip fault diagnosis,"We present a new partition-based fault diagnosis technique for identifying failing scan cells in a scan-BIST environment. This approach relies on a two-step scan chain partitioning scheme. In the first step, an interval-based partitioning scheme is used to generate a small number of partitions, where each element of a partition consists of a set of scan cells. In the second step, additional partitions are created using an earlier-proposed random-selection partitioning method. Two-step partitioning leads to higher diagnostic resolution than a scheme that relies only on random-selection partitioning, with only a small amount of additional hardware. The proposed scheme is especially suitable for a system-on-chip (SOC) composed of multiple embedded cores, where test access is provided by means of a TestRail that is threaded through the internal scan chains of the embedded cores. We present experimental results for the six largest ISCAS-89 benchmark circuits and for two SOCs crafted from some of the ISCAS-89 circuits.",2003,0,
1156,1157,A simple fault detection of the open-switch damage in BLDC motor drive systems,This paper proposes a novel fault detection algorithm for a brushless DC (BLDC) motor drive system. This proposed method is configured without the additional sensor for fault detection and identification. The fault detection and identification are achieved by a simple algorithm using the operating characteristic of the BLDC motor. The drive system after the fault identification is reconfigured by four-switch topology connecting a faulty leg to the middle point of DC-link using bidirectional switches. This proposed method can also be embedded into existing BLDC motor drive systems as a subroutine without excessive computational effort. The feasibility of a novel fault detection algorithm is validated in simulation.,2007,0,
1157,1158,"Design, Simulation, and Fault Analysis of a 6.5-MV LTD for Flash X-Ray Radiography","The design of a 6.5-MV linear transformer driver (LTD) for flash-radiography experiments is presented. The design is based on a previously tested 1-MV LTD and is predicted to be capable of producing diode voltages of 6.5 MV for a 50-Omega radiographic-diode load. Several fault modes are identified, and circuit simulations are used to determine their effect on the output pulse and other components. For all the identified fault modes, the peak load voltage is reduced by less than 5%",2006,0,
1158,1159,Large-scale fault isolation,"Of the many distributed applications designed for the Internet, the successful ones are those that have paid careful attention to scale and robustness. These applications share several design principles. In this paper, we illustrate the application of these principles to common network monitoring tasks. Specifically, we describe and evaluate 1) a robust distributed topology discovery mechanism and 2) a mechanism for scalable fault isolation in multicast distribution trees. Our mechanisms reveal a different design methodology for network monitoring-one that carefully trades off monitoring fidelity (where necessary) for more graceful degradation in the presence of different kinds of network dynamics.",2000,0,
1159,1160,"A router for improved fault isolation, scalability and diagnosis in CAN","Controller Area Network (CAN) provides an inexpensive and robust network technology in many application domains. However, the use of CAN is constrained by limitations with respect to fault isolation, bandwidth, wire length, namespaces and diagnosis. This paper presents a solution to overcome these limitations by replacing the CAN bus with a star topology. We introduce a CAN router that detects and isolates node failures in the value and time domain. The CAN router ensures that minimum message interarrival times are satisfied and reserves CAN identifiers for individual CAN nodes. In addition, the CAN router exploits knowledge about communication relationships for a more efficient use of communication bandwidth through multicast messaging. An implementation of the CAN router based on a Multi-Processor System-on-a-Chip (MPSoC) shows the feasibility of the proposed solution.",2010,0,
1160,1161,Linux Highly Available (HA) Fault-Tolerant Servers,"High availability is becoming increasingly important as our business depend more and more on computers. Unfortunately, many off-the-shelf solutions for high availability (HA) are expensive and require expertise. This paper explains the design and implementation of an inexpensive high-availability solution for our business-critical needs without requiring the use of expensive additional hardware or software. Along with discussion on high availability, this paper also discusses the data integrity of files and database, of the services which are to be made highly available. Using HTTP as the service example and MySQL as the database to be replicated for data integrity, a two node cluster has been configured to implement the concept.",2007,0,
1161,1162,An Online Model Checking Tool for Safety and Liveness Bugs,"Modern software model checkers are usually used to find safety violations. However, checking liveness properties can offer a more natural and effective way to detect errors, particularly in complex concurrent and distributed e-business systems. Specifying global liveness properties which should always eventually be true proves to be more desirable, but it is hard for existing software model checkers to verify liveness in real codes because doing so requires finding an infinite execution. For solving such a challenge, this paper proposes an online checking tool to verify the safety and liveness properties of complex systems. We adopt the linear temporal logic to describe the semantics of the finite model checking, use binary instrumentation to obtain the distribute states and apply a checking engine to dynamically verify the finite trace linear temporal logic properties. At last, we demonstrate the method in a distributed system using distributed protocol Paxos and achieve good results by experiments.",2008,0,
1162,1163,Failure diagnosis of discrete event systems: the case of intermittent faults,"The diagnosis of ""intermittent"" faults in dynamic systems modeled as discrete event systems is considered. In many systems, faulty behavior often occurs intermittently, with fault events followed by corresponding ""reset"" events for these faults, followed by new occurrences of fault events, and so forth. Since these events are usually unobservable, it is necessary to develop diagnostic methodologies for intermittent faults. This paper addresses this issue by: (1) proposing a modeling methodology for discrete event systems with intermittent faults; (2) introducing new notions of diagnosability associated with fault and reset events; and (3) developing necessary and sufficient conditions, in terms of the system model and the set of observable events, for these notions of diagnosability. The associated necessary and sufficient conditions are based upon the technique of ""diagnosis"" introduced in earlier work, albeit the structure of the diagnosis needs to be enhanced to capture the dynamic nature of faults in the system model. The diagnosability conditions are verifiable in polynomial time in the number of states of the diagnosis.",2002,0,
1163,1164,Starting Synchrophasor measurements in Egypt: A pilot project using fault recorders,"The analysis of the large systems blackouts during the last years have pointed out the need for the function of real-time wide area monitoring, protection and control (RT WAM PAC). Phaser measurements (PM) has proved to be the vital source of data necessary for many applications in power system RT WAM PAC. The paper explains this new technology, the reasons behind its use and points out the role it plays in the overall view of RT WAM PAC. The recent applications of the synchrophasors projects allover the world is highlighted. Generic and specific technical requirements for synchrophasors measurements are explained. The paper also illustrates, using actual recorded case from the fault recording system in the Egyptian Power Network, how the synchronized voltage phasors at different locations within the system, could be presented to the system operator during different operating states, using a software program designed specially for this purpose. A pilot project for PM in Egypt using the existing disturbance recorders is presented showing the benefits which could be gained from such a project.",2008,0,
1164,1165,Maximizing the Fault Tolerance Capability of Fixed Priority Schedules,"Real-time systems typically have to satisfy complex requirements, mapped to the task attributes, eventually guaranteed by the underlying scheduler. These systems consist of a mix of hard and soft tasks with varying criticality, as well as associated fault tolerance requirements. Additionally, the relative criticality of tasks could undergo changes during the system evolution. Time redundancy techniques are often preferred in embedded applications and, hence, it is extremely important to devise appropriate methodologies for scheduling real-time tasks under failure assumptions.In this paper, we propose a methodology to provide a priori guarantees in fixed priority scheduling (FPS) such that the system will be able to tolerate one error per every critical task instance. We do so by using integer linear programming (ILP) to derive task attributes that guarantee re-execution of every critical task instance before its deadline, while keeping the associated costs minimized. We illustrate the effectiveness of our approach, in comparison with fault tolerant (FT) adaptations of the well-known rate monotonic (RM) scheduling, by simulations.",2008,0,
1165,1166,Application of Neuro-fuzzy Network for Fault Diagnosis in an Industrial Process,"The purpose of this paper is to present results that were obtained in fault diagnosis of an industrial process. The diagnosis algorithm combines an artificial neural network (ANN) based supplement of a fuzzy system in a block-oriented configuration. A methodology for designing the system is described. As a motivating example, a chemical plant with a recycle stream is considered. Faults in the supply of raw materials and in controllers are simulated. The performance of the system in handling simultaneous faults is also analysed. The running test results show that the strategy appears to be better suited to diagnose faults of such an industrial process.",2007,0,
1166,1167,Reliability analysis of AUV based on fuzzy fault tree,"Traditional fault tree analysis method need obtain exact value of the event occurrence's probability, un-completeness and fuzziness of the data is ignored. Fuzzy fault tree analysis method is proposed in study on the system reliability. The fuzzy fault tree of the AUV is established. Using operational rule calculated the fuzzy probability of the top event which is AUV can't work normally and analysis the results. The results showed that this method can resolve the problem of fuzzy data and have part events failure criterion in fault tree analysis. It provided valuable reference for the design of the AUV, fault diagnosis and maintenance.",2009,0,
1167,1168,Remote Synchronization of Onboard Crystal Oscillator for QZSS Using L1/L2/L5 Signals for Error Adjustment,"A new error adjustment method for remote synchronization of the onboard crystal oscillator for the quasi-zenith satellite system (QZSS) using three different frequency positioning signals (L1/L2/L5) is proposed. The error adjustment method that uses L1/L2 positioning signals was demonstrated in the past. In both methods, the frequency-dependent part and the frequency-independent part were considered separately, and the total time information delay was estimated. By adopting L1/L2/L5, synchronization was improved by approximately 15% compared with that using L1/L2 and approximately 10% compared with that using L1/L5.",2007,0,
1168,1169,An Efficient Fault Simulator for QDI Asynchronous Circuits,"Testing asynchronous circuits is a difficult task because of two main reasons; first, the absence of a global clock does not allow the use of traditional test generation techniques used for synchronous circuits. Second, correct (i.e., hazard- free) operations of asynchronous circuits are usually obtained by introducing redundancies, that is, sacrificing the testability. So test frameworks such as fault simulator for synchronous circuits are not applicable for asynchronous circuits. In this paper we present an efficient fault simulator for template-based asynchronous circuits which is based on checking sequence of signals in templates. Our proposed fault simulator provides higher fault coverage by taking into consideration the detection of a special class of faults called premature firing faults without introducing any hardware redundancy in the designed circuit. Experimental results on a set of circuits have shown the effectiveness of the fault simulator.",2008,0,
1169,1170,Spatio-temporal boundary matching algorithm for temporal error concealment,"In this paper, a novel temporal error concealment algorithm, called spatio-temporal boundary matching algorithm (STBMA), is proposed to recover the information lost in the video transmission. Different from the classical boundary matching algorithm (BMA), which just considers the spatial smoothness property, the proposed algorithm introduces a new distortion function to exploit both the spatial and temporal smoothness properties to recover the lost motion vector (MV) from candidates. The new distortion function involves two terms: spatial distortion term and temporal distortion term. Since both the spatial and temporal smoothness properties are involved, the proposed method can better minimize the distortion of the recovered block and recover more accurate MV. The proposed algorithm has been tested on H.264 reference software JM 9.0. The experimental results demonstrate the proposed algorithm can obtain better PSNR performance and visual quality, compared with BMA which is adopted in H.264",2006,0,
1170,1171,Formal development of software for tolerating transient faults,"Transient faults constitute a wide-spread class of faults typical in control systems. These are faults that appear for some time during system operation and might disappear and reappear later. However, even by appearing for a short time, they might cause dangerous system errors. Hence designing mechanisms for tolerating transient faults is an acute issue, especially in the development of safety-critical control systems. In this paper we propose a formal approach to specifying software-based mechanisms for tolerating transient faults in the B method. We focus on deriving a general specification and development pattern which can be applied in the development of various control systems. We illustrate an application of the proposed patterns by an example from avionics software product line.",2005,0,
1171,1172,Multi-level fault injection experiments based on VHDL descriptions: a case study,"The probability of transient faults increases with the evolution of technologies. There is a corresponding increased demand for an early analysis of erroneous behaviors. This paper reports on results obtained with SEU-like fault injections in VHDL descriptions of digital circuits. Several circuit description levels are considered, as well as several fault modeling levels. These results show that an analysis performed at a very early stage in the design process can actually give a helpful insight into the response of a circuit when a fault occurs.",2002,0,
1172,1173,An innovative fault injection method in embedded systems via background debug mode,"The embedded systems usage in different applications is prevalent in recent years. These systems include a wide range of equipments from cell phones to medical instruments, which consist of hardware and software. In many examples of embedded systems, fault occurrence can lead to serious dangers in system behavior (for example in satellites). Therefore, we try to increase the fault tolerance feature in these systems. Therefore, we need some mechanisms that increase the robustness and reliability of such systems. These objects cause the on-line test to be a great concern. It is not important that these mechanisms work in which level (Hardware level, Software level or Firmware). The major concern is that how well these systems can provide debugging, test and verification features for the user regardless of their implementation levels. Background Debug Module is a real time tool for these features. In this paper we apply an innovative way to use the BDM tool for fault injection in an embedded system.",2009,0,
1173,1174,Using a Fault Hierarchy to Improve the Efficiency of DNF Logic Mutation Testing,"Mutation testing is a technique for generating high quality test data. However, logic mutation testing is currently inefficient for three reasons. One, the same mutant is generated more than once. Two, mutants are generated that are guaranteed to be killed by a test that kills some other generated mutant. Three, mutants that when killed are guaranteed to kill many other mutants are not generated as valuable mutation operators are missing. This paper improves logic mutation testing by 1) extending a logic fault hierarchy to include existing logic mutation operators, 2) introducing new logic mutation operators based on existing faults in the hierarchy, 3) introducing new logic mutation operators having no corresponding faults in the hierarchy and extending the hierarchy to include them, and 4) addressing the precise effects of equivalent mutants on the fault hierarchy. An empirical study using minimal DNF predicates in avionics software showed that a new logic mutation testing approach generates fewer mutants, detects more faults, and outperforms an existing logic criterion.",2009,0,
1174,1175,FAME: a fault-pattern based memory failure analysis framework,"A memory failure analysis framework is developed-the Failure Analyzer for MEmories (FAME). The FAME integrates the Memory Error Catch and Analysis (MECA) system and the Memory Defect Diagnostics (MDD) system. The fault-type based diagnostics approach used by MECA can improve the efficiency of the test and diagnostic algorithms. The fault-pattern based diagnostics approach used by MDD further improves the defect identification capability. The FAME also comes with a powerful viewer for inspecting the failure patterns and fault patterns. It provides an easy way to narrow down the potential cause of failures and identify possible defects more accurately during the memory product development and yield ramp-up stage. An experiment has been done on an industrial case, demonstrating very accurate results in a much shorter time as compared with the conventional way.",2003,0,
1175,1176,Genome-Wide Search for Splicing Defects Associated with Amyotrophic Lateral Sclerosis (ALS),"Amyotrophic lateral sclerosis (ALS) is a progressive neurodegenerative disease caused by the degeneration of motor neurons. Although the cause of ALS is unknown, mutations in the gene that produces the SOD1 enzyme are associated with some cases of familial ALS. SOD1 is a powerful antioxidant that protects the body from damage caused by superoxide, a toxic free radical. It has been proposed that defects in splicing of some mRNAs, induced by oxidative stress, can play a role in ALS pathogenesis. Alterations of splicing patterns have also been observed in ALS patients and in ALS murine models, suggesting that alterations in the splicing events can contribute to ALS progression. Using Exon 1.0 ST GeneChips, which allow the definition of alternative splicing events (ASEs) , the SH-SY5Y neuroblastoma cell line has been profiled after treatment with paraquat, which by inducing oxidative stress alters the patterns of alternative splicing. Furthermore, the same cell line stably transfected with wt and ALS mutant SOD has also been profiled. The integration of the two ALS models efficiently moderates ASE false discovery rate, one of the most critical issues in high-throughput ASEs detection. This approach allowed the identification of a total of 14 splicing events affecting respectively both internal coding exons and 5' UTR of known gene isoforms.",2009,0,
1176,1177,QoS-aware connection resilience for network-aware grid computing fault tolerance,"Current grid computing fault tolerance leverages IP dynamic rerouting and schemes implemented in the application or in the middleware to overcome both software and hardware failures. Despite the flexibility of current grid computing fault tolerant schemes in recovering inter-service connectivity from an almost comprehensive set of failures, they might not be able to repristinate also connection QoS guarantees, such as minimum bandwidth and maximum delay. This phenomenon is exacerbated when, as in global grid computing, the grid computing sites are not connected by dedicated network resources but share the same network infrastructure with other Internet services. This paper aims at showing the advantages of integrating grid computing fault tolerance schemes with next generation networks (NGNs) resilient schemes. Indeed, by combining the utilization of generalized multi-protocol label switching (GMPLS) resilient schemes, such as path restoration, and application or middleware layer fault tolerant schemes, such as service migration or replication, it is possible to guarantee the necessary QoS to the connections between grid computing sites while limiting the required network and computational resources.",2005,0,
1177,1178,A Fast Fault Location Algorithm Based on Pre-computed for Optical Burst Switching Network,"Aiming at the disadvantages of existing location algorithms, this paper proposed and experienced an effective method of fault location algorithm based on pre-computed for optical burst switching network. In order to minimize the monitoring cost, we introduced monitoring-cycle by which the network is divided into a number of monitoring domain. Each monitoring domain has a monitor, when faults occurred, fault codes are generated. According to the fault codes, we can search the binary tree algorithm to achieve the pre-computed of faults in the OBS network. Examples prove that the algorithm can not only realize single fault location but also multi-fault location.",2009,0,
1178,1179,MV generator low-resistance grounding and stator ground fault damage,"Most of the in-plant medium-voltage generators are grounded through resistors ranging from 200 to 400 A. In some unusual situations, the resistors may even be as high as 1200 A. Low-resistance grounding is a preferred choice for a medium-voltage power distribution system. However, extensive generator stator ground fault damages had been reported due to the prolonging generator neutral ground current, which would continue to flow even after the main and field breaker opened. This ground fault current could continue to flow for as long as 5 s depending on the generator open-circuit time constant T'<sub>do</sub>. As a result, the higher the generator neutral current, would lead to the higher stator ground fault damages. An IEEE Working Group has recently completed its study and made recommendations for medium-voltage generator grounding in a multiple source industrial environment. Based on the considerations of transient overvoltage, ground fault damages, and ground fault protection, the working group suggests a few variations of grounding systems, which basically are low-resistance grounding systems during normal operating conditions, and the generator would be switched to a high-resistance grounded system from a normal low-resistance grounded system when a ground fault occurs in the generator stator. The purposes of this paper are: 1) to examine the generator transient over-voltage and currents under the low-resistance ground fault conditions, and also to evaluate their corresponding stator ground fault damages, and 2) to establish an acceptable maximum system ground fault level. For comparison purposes, three versions of low-resistance grounding systems have been considered and they are: a low-resistance grounding system with a neutral breaker at the generator (hereinafter called a ""neutral breaker system""); a low-resistance grounding system with the generator neutral low resistor being switched to a high-resistor after a stator ground fault (hereinafter called a ""hybrid system""); and a low-resistance grounding system similar to the current practice (hereinafter called a ""traditional system""). The simulation study is conducted with the aid of the Electro Magnetic Transient Program. An experimental analog generator model is also used to verify the - simulation results.",2004,0,
1179,1180,Fault-tolerant adaptive and minimal routing in mesh-connected multicomputers using extended safety levels,"The minimal routing problem in mesh-connected multicomputers with faulty blocks is studied. Two-dimensional meshes are used to illustrate the approach. A sufficient condition for minimal routing in 2D meshes with faulty blocks is proposed. Unlike many traditional models that assume all the nodes know global fault distribution, our approach is based on the concept of an extended safety level, which is a special form of limited fault information. The extended safety level information is captured by a vector associated with each node. When the safety level of a node reaches a certain level (or meets certain conditions), a minimal path exists from this node to any nonfaulty nodes in 2D meshes. Specifically, we study the existence of minimal paths at a given source node, limited distribution of fault information, and minimal routing itself. We propose three fault-tolerant minimal routing algorithms which are adaptive to allow all messages to use any minimal path. We also provide some general ideas to extend our approaches to other low-dimensional mesh-connected multicomputers such as 2D tori and 3D meshes. Our approach is the first attempt to address adaptive and minimal routing in 2D meshes with faulty blocks using limited fault information",2000,0,
1180,1181,Study and Realizing of Method of AC Locating Fault in Distribution System,"The idea of AC locating fault method in distribution system is that inject an AC signal into the fault phase after a single line to ground fault happened and then diagnose the fault along the transmission line with the handled AC signal detector utilizing dichotomy method until the fault is determined. The frequency of the injected AC signal used in this study is 60 Hz. Compared with the injected S signal technique, this method is called low frequency AC signal injection method. In this paper, the hardware of the signal source and the software design are introduced. The SCM managing pulse is used in the control section of the hardware, and the application of PWM control technique in this hardware is discussed in this reference; as the software design, the PWM signal is generated by coding based on the relation between the injected signal and PWM waveforms. The high frequency PWM signal excited a couple of breakers in the inversion source and then the output terminal will get high stable injected signals by filtering and generate AC signal with invariable frequency and adjustable voltage, based on which the signal detector could detect the required signal easily. The proposed signal source device reduces the difficulty of high impedance to ground detecting and improves the accuracy and reliability of locating fault. This technique, which allows the ground detecting and is convenient for engineers' operation, reduces the locating fault time, improves its efficiency and is proved by simulation and analysis on its validity.",2010,0,
1181,1182,Notice of Retraction<BR>A novel weighted voting algorithm based on neural networks for fault-tolerant systems,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Voting algorithms are used in a wide area of control systems from real-time and safety-critical control systems to pattern recognition, image processing and human organization systems in order to arbitrating among redundant results of processing in redundant hardware modules or software versions. From a point of view, voting algorithms can be categorized to agreement-based voters like plurality and majority or some voters which produce output regardless to agreement existence among the results of redundant variants. In some applications it is necessary to use second type voters including median and weighted average. Although both of median and weighted average voters are the choicest voters for highly available application, weighted average voting is often more trustable than median. Meanwhile median voter simply selects the mid-value of results; weighted average voter assigns weight to each input, based on their pre-determined priority or their differences, so that the share of more trustable inputs will increase rather than the inputs with low probable correctness. This paper introduces a novel weighted average voting algorithm based on neural networks that is capable of improving the rate of system reliability. Our experimental results showed that the neural weighted average voter has increases the reliability 116.63% in general and 309.82%, 130.27% and 9.37% respectively for large, medium and small errors in comparison with weighted average, and 73.87% in general and 160.44%, 83.59% and 7.52% respectively for- large, medium and small errors in comparison with median voter.",2010,0,
1182,1183,Fault Diagnosis of Variable Frequency Speed Regulation System Based on Information Fusion,"The multi-sensor information fusion theory has been widely used in the fault diagnosis domain. In order to improve the reliability of fault diagnosis of the variable frequency speed regulation system (VFSRS), this paper presents a new fault diagnosis method of the VFSRS based on the multi-sensor information fusion. A calculating method of the basic probability assignment (BPA) for the VFSRS was given. Taking the invisible electric fault of the VFSRS as an example, this paper presents the implementing process of this fault diagnosis method in detail. The diagnosis result indicates that the multi-sensor information fusion has the stability and the ability of fault tolerance, and can improve the accuracy and reliability of fault diagnosis of the VFSRS effectively.",2007,0,
1183,1184,Error probability analysis of TAS/MRC-based scheme for wireless networks [point-to-point link example],"We develop the framework to analyze the symbol-error probability (SEP) for the scheme integrating transmit antenna selection (TAS) with maximal-ratio combining (MRC) used in wireless networks. Applying this scheme, the transmitter always selects an optimal antenna out of all possible antennas based on channel state information (CSI) feedback. Over a flat-fading Rayleigh channel, we develop the closed-form SEP expressions for a set of commonly used constellations when assuming perfect and delayed CSI feedback, respectively. We also derive the Chernoff-bounds of the SEP's for both perfect and delayed feedback. Our analyses show that while the antenna diversity improves the system performance, the feedback delay can significantly impact the SEP of TAS/MRC schemes.",2005,0,
1184,1185,Design and control of an intelligent dual-arm manipulator for fault-recovery in a production scenario,"This paper describes the design and control methodology used for the development of a dual-arm manipulator as well as its deployment in a production scenario. Multi-modal and sensor-based manipulation strategies are used to guide the robot on its task to supervise and, when necessary, solve faulty situations in a production line. For that task the robot is equipped with two arms, aimed at providing the robot with total independence from the production line. In other words, no extra mechanical stoppers are mounted on the line to halt targeted objects, but the robot will employ both arms to (a) stop with one arm a carrier that holds an object to be inserted/replaced, and (b) use the second arm to handle such object. Besides, visual information from head and wrist-mounted cameras provide the robot with information such as the state of the production line, the unequivocal detection/recognition of the targeted objects, and the location of the target in order to guide the grasp.",2009,0,
1185,1186,Error analysis in Croatian morphosyntactic tagging,"In this paper, we provide detailed insight on properties of errors generated by a stochastic morphosyntactic tagger assigning multext-East morphosyntactic descriptions to Croatian texts. Tagging the Croatia Weekly newspaper corpus by the CroTag tagger in stochastic mode revealed that approximately 85 percent of all tagging errors occur on nouns, adjectives, pronouns and verbs. Moreover, approximately 50 percent of these are shown to be incorrect assignments of case values. We provide various other distributional properties of errors in assigning morphosyntactic descriptions for these and other parts of speech. On the basis of these properties, we propose rule-based and stochastic strategies which could be integrated in the tagging module, creating a hybrid procedure in order to raise overall tagging accuracy for Croatian.",2009,0,
1186,1187,A Survey of Mobile Agent-Based Fault-Tolerant Technology,This paper surveys the state of the art of agentbased fault tolerance techniques. Existing mobile agent-based fault-tolerant techniques are identified on prevent mobile agents from being blocked by a failure.,2005,0,
1187,1188,On the outphasing power amplifier nonlinearity analysis and correction using digital predistortion technique,"This paper proposes a comprehensive theoretical and experimental analysis of the source of nonlinearity exhibited by outphasing based power amplifiers The important load-pulling effect engendered by the outphasing decomposition and the Chireix combiner is first investigated. Its effect on the two power amplifiers behavior is then identified as the dominant source of nonlinearity in the LINC system. In addition, this paper suggests the application of baseband predistortion technique to mitigate this nonlinear behavior.",2008,0,
1188,1189,Temperature correction to chemoresistive sensors in an e-NOSE-ANN system,"The influence of the temperature coefficient of resistance in the chemoresistive response of inherently conductive polymer (ICP) sensors in the performance of an artificial neural network (ANN) e-natural olfactory sensor emulator (e-NOSE) system is evaluated. Temperature was found to strongly influence the response of the chemoresistors, even over modest ranges (ca. 2/spl deg/C). An e-NOSE array of eight ICP sensor elements, a relative humidity (RH/spl plusmn/0.1%) sensor, and a resistance temperature device (RTD/spl plusmn/0.1/spl deg/C) was tested at five different RH levels while the temperature was allowed to vary with the ambient. A temperature correction algorithm based on the temperature coefficient of resistance /spl beta/ for each material was independently and empirically determined then applied to the raw sensor data prior to input to the ANN. Conversely, uncorrected data was also passed to the ANN. The performance of the ANN was evaluated by determining the error found between the actual humidity versus the calculated humidity. The error obtained using raw input sensor data was found to be 10.5% and using temperature corrected data, 9.3%. This negligible difference demonstrates that the ANN was capable of adequately addressing the temperature dependence of the chemoresistive sensors once temperature was inclusively passed to the ANN.",2003,0,
1189,1190,Application of a matched filter approach for finite aperture transducers for the synthetic aperture imaging of defects,"The suitability of the synthetic aperture imaging of defects using a matched filter approach on finite aperture transducers was investigated. The first part of the study involved the use a finite-difference time-domain (FDTD) algorithm to simulate the phased array ultrasonic wave propagation in an aluminum block and its interaction with side-drilled hole-like defects. B-scans were generated using the FDTD method for three active aperture transducer configurations of the phased array (a) single element and (b) 16-element linear scan mode, and (c) 16-element steering mode. A matched filter algorithm (MFA) was developed using the delay laws and the spatial impulse response of a finite size rectangular phased array transducer. The conventional synthetic aperture focusing technique (SAFT) algorithm and the MFA were independently applied on the FDTD signals simulated with the probe operating at a center frequency of 5 MHz and the processed B-scans were compared. The second part of the study investigated the capability of the MFA approach to improve the SNR. Gaussian white noise was added to the FDTD generated defect signals. The noisy B-scans were then processed using the SAFT and the MFA and the improvements in the SNR were estimated. The third part of the study investigated the application of the MFA to image and size surface-crack-like defects in pipe specimens obtained using a 45 steered beam from a phased array probe. These studies confirm that MFA is an alternative to SAFT with little additional computational burden. It can also be applied blindly, like SAFT, to effect synthetic focusing with distinct advantages in treating finite transducer effects, and in handling steered beam inspections. Finally, limitations of the MFA in dealing with larger-sized transducers are discussed.",2010,0,
1190,1191,Lazy verification in fault-tolerant distributed storage systems,"Verification of write operations is a crucial component of Byzantine fault-tolerant consistency protocols for storage. Lazy verification shifts this work out of the critical path of client operations. This shift enables the system to amortize verification effort over multiple operations, to perform verification during otherwise idle time, and to have only a subset of storage-nodes perform verification. This paper introduces lazy verification and describes implementation techniques for exploiting its potential. Measurements of lazy verification in a Byzantine fault-tolerant distributed storage system show that the cost of verification can be hidden from both the client read and write operation in workloads with idle periods. Furthermore, in workloads without idle periods, lazy verification amortizes the cost of verification over many versions and so provides a factor of four higher write bandwidth when compared to performing verification during each write operation.",2005,0,
1191,1192,An autonomous FPGA-based emulation system for fast fault tolerant evaluation,"Platform FPGAs provide a high degree of reconfigurability and a high density of integration. These features make these devices very suitable for hardware emulation and in particular for fault tolerance evaluation. There are several FPGA-based approaches that enhance notably the fault tolerance evaluation process achieving an important speed up. However, such methods are limited by the communication between the FPGA and the host computer, which manages the emulation process. In order to minimize this communication and therefore accelerate the overall process, an autonomous emulation system is proposed in this paper. This solution profits from additional hardware resources available in current platform FPGAs such as embedded RAM. In the proposed system, a complete emulation campaign and its management is embedded in the FPGA, accelerating emulation process up to two orders of magnitude without losing flexibility with respect to other hardware solutions.",2005,0,
1192,1193,A fault tolerance mechanism for network intrusion detection system based on intelligent agents (NIDIA),"The intrusion detection system (IDS) has as objective to identify individuals that try to use a system in way not authorized or those that have authorization to use but they abuse of their privileges. The IDS to accomplish its function must, in some way, to guarantee reliability and availability to its own application, so that it gets to give continuity to the services even in case of faults, mainly faults caused by malicious agents. This paper proposes an adaptive fault tolerance mechanism for network intrusion detection system based on intelligent agents. We propose the creation of a society of agents that monitors a system to collect information related to agents and hosts. Using the information which is collected, it is possible: to detect which agents are still active; which agents should be replicated and which strategy should be used. The process of replication depends on each type of agent, and its importance to the system at different moments of processing. We use some agents as sentinels for monitoring and thus allowing us to accomplish some important tasks such load balancing, migration, and detection of malicious agents, to guarantee the security of the proper IDS",2006,0,
1193,1194,A novel method for transmission network fault location using genetic algorithms and sparse field recordings,"The paper presents an approach to locate a fault in a transmission network based on waveform matching. Matching during-fault recorded phasor with the during fault simulated phasor is used to determine the fault location. The search process to find the best waveform match is actually an optimization problem. The genetic algorithm (GA) is introduced to find the optimal solution. The proposed approach is suitable for the situations where only the data recorded sparsely is available. Under such circumstances, it can offer more accurate results than other known techniques.",2002,0,
1194,1195,Lowering Error Floor of LDPC Codes Using a Joint Row-Column Decoding Algorithm,"Low-density parity-check codes using the belief-propagation decoding algorithm tend to exhibit a high error floor in the bit error rate curves, when some problematic graphical structures, such as the so-called trapping sets, exist in the corresponding Tanner graph. This paper presents a joint row-column decoding algorithm to lower the error floor, in which the column processing is combined with the processing of each row. By gradually updating the pseudo-posterior probabilities of all bit nodes, the proposed algorithm minimizes the propagation of erroneous information from trapping sets into the whole graph. The simulation indicates that the proposed joint decoding algorithm improves the performance in the waterfall region and lowers the error floor. Implementation results into field programmable gate array (FPGA) devices indicate that the proposed joint decoder increases the decoding speed by a factor of eight, compared to the traditional decoder.",2007,0,
1195,1196,Error-robust Scalable Extension of H.264/AVC Ubiquitous Streaming Using the Adaptive Packet Interleaving Mechanism,"In this paper, we realize error-robust scalable extension of H.264/AVC ubiquitous video streaming using an adaptive packet interleaving mechanism among heterogeneous networks and devices. The state-of-the-art SVC is used to provide combined scalability to adjust spatiotemporal resolutions and SNR quality. In an error-prone channel, a method of bandwidth estimation and a three-state network transition chain are proposed to adjust the transmission bit-rate and the interleaving window size, respectively. Unlike past packet interleaving methods, the interleaving window size can be adjusted dynamically to compromise pre-processing delay, e.g., packet re-arrangement time for interleaving, and quality improvement. Additionally, with the understanding of the global SVC bitstream structure, a SVC coder is designed to increase the decoding efficiency and real-timely extract the proper SVC-quality bitstream based on the network condition. In our experiments, performances of the adaptive packet interleaving mechanism w.r.t. distinct interleaving window sizes and network conditions are demonstrated for distinct kinds of videos.",2009,0,
1196,1197,Test mode method and strategy for RF-based fault injection analysis for on-chip relaxation oscillators under EMC standard tests or RFI susceptibility characterization,"Nowadays some microcontroller clock circuits have been implemented using relaxation oscillators instead of quartz type approach to attend cost effective designs. The oscillator is compensated over temperature and power supply and trimming during device test phase adjusts the oscillation frequency on target to overcome process variations. In that way, the relaxation oscillator becomes competitive with regard to ceramic resonator options. However, robust applications as industrial, automotive and aero spatial, requires aggressive EMC tests reproducing the behavior in these environments. High levels of RF interference introduce frequency deviation, jitter or clock corruption causing severe faults on the application. This work discusses the impact of RF interference in relaxation oscillators proposing a strategy to implement test mode in microcontrollers and other complex SOCs, allowing yet characterization and fault debug. Theoretical analysis and experimental results with a silicon implementation are presented and discussed.",2010,0,
1197,1198,Intelligent agent-based system using dissolved gas analysis to detect incipient faults in power transformers,Condition monitoring and software-based diagnosis tools are central to the implementation of efficient maintenance management strategies for many engineering applications including power transformers.,2010,0,
1198,1199,Algorithm of fault diagnosis for satellite network,"Automated fault diagnosis becomes increasingly important to satellite work. In an early paper, we introduced system level diagnosis theory into satellite iietwork firstly and presented two-level-node graph, a novel modeling method,. Based on these work, a new test invalidation model under certain fault pattern is presented and a diagnosis algorithni is proposed in this paper. Diagnosis can be divided into two steps, local diagnosis and centralized diagnosis. The former is distributed, which can reduce the diagnosis delay by collecting test results in satellites parallelly. The latter is centralized, which can make use of the regularity of satellite network. The procedure of local diagnosis is described by activity cycle diagram. During diagnosing, little professional knowledge about satellite needs to be involved. An example illustrates the diagnosis algorithm and its effect.",2004,0,
1199,1200,Grid-connected PV system with power-factor correction capability,"In this paper a grid-connected PV system is presented. Its main feature is that, besides injecting energy into the grid, it behaves as an active filter. The reference signal used to modulate the output current is obtained through an adaptive filter. The main advantages of the technique are that the system becomes almost independent of the parameter variations, and that it lends itself to a very simple hardware implementation. A 1 kW prototype was built and tested. The results obtained show good compensation of the current drawn by nonlinear loads.",2002,0,
1200,1201,Analysis on Fault of Yun-Guang hybrid AC/DC Power Transmission System,"This paper addresses the transient response of instant ground fault in Yun-Guang hybrid UHVDC/AC power transmission system. A detailed bipolar UHVDC electromagnetic transient model is established by PSCAD/EMTDC software. The control strategies for UHVDC system are investigated and the main factors of commutation failure are analyzed. A lot of simulation results demonstrate that Yunnan rectifier AC system fault can not cause UHVDC commutation failure. Metallic ground fault of Guangdong inverter AC system will result in UHVDC commutation failure but enough transition resistance can reduce the chance of it. When one pole ground fault of UHVDC line occurs, DC current of the fault pole has an obvious overshoot and non-fault pole is influenced a little. UHVDC can come back to normal operation once the instant ground fault is cleared.",2008,0,
1201,1202,2D Frequency Selective Extrapolation for Spatial Error Concealment in H.264/AVC Video Coding,"The frequency selective extrapolation extends an image signal beyond a limited number of known samples. This problem arises in image and video communication in error prone environments where transmission errors may lead to data losses. In order to estimate the lost image areas, the missing pixels are extrapolated from the available correctly received surrounding area which is approximated by a weighted linear combination of basis functions. In this contribution, we integrate the frequency selective extrapolation into the H.264/AVC coder as spatial concealment method. The decoder reference software uses spatial concealment only for I frames. Therefore, we investigate the performance of our concealment scheme for I frames and its impact on following P frames caused by error propagation due to predictive coding. Further, we compare the performance for coded video sequences in TV quality against the non-normative concealment feature of the decoder reference software. The investigations are done for slice patterns causing chequerboard and raster scan losses enabled by flexible macroblock ordering (FMO).",2006,0,
1202,1203,TMS320 DSP based neural networks on fault diagnostic system of turbo-generator,"Artificial neural networks (ANN) are massive parallel interconnections of simple neurons that function as a collective system. More and more people are paying attention to ANN on fault diagnostic of turbo-generator for its association of thought, recollection and study function. However, the disadvantage of ANN lies in its huge data computation and the low speed of convergence. If we realize ANN with common CPU, it needs so much time on computing the huge data, so that real-time fault diagnosis become impossible. In fact, most of the computation in ANN is multiplication and addition. While digital signal processors (DSP) has altitude advantage on multiplication and addition computation, it can perform parallel multiplication and addition in a single cycle clock. Consequently we design a master/slave system to solve the problem. The slave system was mainly made up of DSP, which perform high speed ANN calculation. The master system was made up of PC, which performs data communication and real-time fault diagnosis. In this paper we bring forward a practical system design and present particular design method of hardware and software by using back-propagation (BP) network often used in fault diagnosis.",2003,0,
1203,1204,A Diagnostic Tree Approach for Fault Cause Identification in the Attitude Control Subsystem of Satellites,"Space and Earth observation programs demand stringent guarantees ensuring smooth and reliable operations of space vehicles and satellites. Due to unforeseen circumstances and naturally occurring faults, it is desired that a fault-diagnosis system be capable of detecting, isolating, identifying, or classifying faults in the system. Unfortunately, none of the existing fault-diagnosis methodologies alone can meet all the requirements of an ideal fault- diagnosis system due to the variety of fault types, their severity, and handling mechanisms. However, it is possible to overcome these shortcomings through the integration of different existing fault-diagnosis methodologies. In this paper, a novel learning-based, diagnostic-tree approach is proposed which complements and strengthens existing efficient fault detection mechanisms with an additional ability to classify different types of faults to effectively determine potential fault causes in a subsystem of a satellite. This extra capability serves as a semiautomatic diagnostic decision support aid to expert human operators at ground stations and enables them to determine fault causes and to take quick and efficient recovery/reconfiguration actions. The developed diagnosis/analysis procedure exploits a qualitative technique denoted as diagnostic tree (DX-tree) analysis as a diagnostic tool for fault cause analysis in the attitude control subsystem (ACS) of a satellite. DX-trees constructed by our proposed machine-learning-based automatic tree synthesis algorithm are demonstrated to be able to determine both known and unforeseen combinations of events leading to different fault scenarios generated through synthetic attitude control subsystem data of a satellite. Though the immediate application of our proposed approach would be at ground stations, the proposed technique has potential for being integrated with causal model-based diagnosis and recovery techniques for future autonomous space vehicle missions.",2009,0,
1204,1205,Assessing the Dependability of SOAP RPC-Based Web Services by Fault Injection,"This paper presents our research on devising a dependability assessment method for SOAP-based Web Services using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing SOAP RPC-based applications and derive a new method and fault model for testing web services. From this we have implemented an extendable fault injector framework and undertaken some proof of concept experiments with a system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy within our system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard web service to the stateful environment of an OGSA service.",2003,0,
1205,1206,Fault-Tolerant Encryption for Space Applications,"This paper is concerned with the use of commercial security algorithms like the Advanced Encryption Standard (AES) in Earth observation small satellites. The demand to protect the sensitive and valuable data transmitted from satellites to ground has increased and hence the need to use encryption on board. AES, which is a very popular choice in terrestrial communications, is slowly emerging as the preferred option in the aerospace industry including satellites. This paper first addresses the encryption of satellite imaging data using five AES modes - ECB, CBC, CFB, OFB and CTR. A detailed analysis of the effect of single even upsets (SEUs) on imaging data during on-board encryption using different modes of AES is carried out. The impact of faults in the data occurring during transmission to ground due to noisy channels is also discussed and compared for all the five modes of AES. In order to avoid data corruption due to SEUs, a novel fault-tolerant model of AES is presented, which is based on the Hamming error correction code. A field programmable gate array (FPGA) implementation of the proposed model is carried out and measurements of the power and throughput overhead are presented.",2009,0,
1206,1207,A Fault-Tolerant Routing Algorithm of P2P Network Based on Hierarchical Structure,"Fault-tolerant routing in P2P network has been a hot point. To raise the performance of fault-tolerant routing can highly enhance the stability and efficiency of P2P network. Through research, we find most routing errors is caused by highly dynamic characteristic of P2P network, such as peers frequently join, leave and fail, which is the main factor that induce routing errors. We proposed an improved algorithm to raise the performance of fault-tolerant routing of P2P network based on hierarchical structure theory, which is named as FTARH (Fault-tolerant Algorithm of Routing with Hiberarchy). The new algorithm makes full use of the superfluous capability of super peers to enhance the performance of fault-tolerant routing of entire P2P network.",2010,0,
1207,1208,Towards Interactive Fault Localization Using Test Information,"Finding the location of a fault is a central task of debugging. Typically, a developer employs an interactive process for fault localization. To accelerate this task, several approaches have been proposed to automate fault localization. In practice, testing-based fault localization (TBFL), which uses test information to locate faults, has become a research focus. However, experimental results reported in the literature showed that current automation of fault localization can only serve as a means to confirming the search space and prioritizing search sequences, not a substitute of the interactive fault localization process. In this paper, we propose an approach based on test information to support the entire interactive fault localization process. During this process, the information gathered from previous interaction steps can be used to provide the ranking of suspicious statements for the current interaction step. As a feasibility study of our approach, we performed an experiment on applying our approach together with some other TBFL approaches on the Siemens programs, which have been used in the literature. Our experimental results show the effectiveness of our approach.",2006,0,
1208,1209,Correction of bias field in MR images using singularity function analysis,"A new approach for correcting bias field in magnetic resonance (MR) images is proposed using the mathematical model of singularity function analysis (SFA), which represents a discrete signal or its spectrum as a weighted sum of singularity functions. Through this model, an MR image's low spatial frequency components corrupted by a smoothly varying bias field are first removed, and then reconstructed from its higher spatial frequency components not polluted by bias field. The thus reconstructed image is then used to estimate bias field for final image correction. The approach does not rely on the assumption that anatomical information in MR images occurs at higher spatial frequencies than bias field. The performance of this approach is evaluated using both simulated and real clinical MR images.",2005,0,
1209,1210,Categorizing and Analysis of Activated Faults in the FlexRay Communication Controller Registers,"FlexRay communication protocol is expected becoming the de-facto standard for distributed safety-critical systems. In this paper, transient single bit-flip faults were injected into the FlexRay communication controller to categorize and analyze the activated faults. In this protocol, an activated fault results in one or more error types which are boundary violation, conflict, content, freeze, synchronization, and syntax. To study the activated faults, a FlexRay bus network, composed of four nodes, was modeled by verilog HDL; and a total of 135,600 transient faults were injected in only one node, where 9,342 (6.9%) of the faults were activated. The results show that the synchronization error is the widespread error with the occurrence ratio of about 70.1%. The Boundary violation and the syntax errors have the occurrence ratios of 32.4% and 24.6%, respectively. The results also show that the Freeze error which more frequent resulted system failures has the occurrence ratio of about 17.3%.",2009,0,
1210,1211,Fault-tolerant scheduling using primary-backup approach for optical grid applications,"Fault-tolerant scheduling is an important issue for optical gird applications because of a wide range of grid resource failures. To improve the availability of the DAGs (directed acyclic graphs), a primary-backup approach is considered when making DAG scheduling decision. Experiments demonstrate the effectiveness and the practicability of the proposed scheme.",2009,0,
1211,1212,A new audio skew detection and correction algorithm,"The lack of synchronisation between a sender clock and a receiver audio clock in an audio application results in an undesirable effect known as ""audio skew"". This paper proposes and implements a new approach to detecting and correcting audio skew, focusing on the accuracy of measurements and on the algorithm's effect on the audio experience of the listener. The algorithms presented are shown to remove audio skew successfully, thus reducing delay and loss and hence improving audio quality.",2002,0,
1212,1213,Algebraic expression for extra degree added to hypercube interconnection network in case of multiple edge-faults,"Researchers have used more than one method for implementing algorithms on faulty hypercube interconnection networks. Some modified the structures of the hypercube whilst others didn't, implementing their operations on a faulty hypercube. Modifying the structure of a hypercube entails adding extra degrees to each node, called reconfiguration of hypercubes. I improved an algebraic expression for added extra degrees to a hypercube to make it non-faulty",2001,0,
1213,1214,A Fault-tolerance Framework for Distributed Component Systems,"The requirement for higher reliability and availability of systems is continuously increasing even in domains not traditionally strongly concerned by such issues. Required solutions are expected to be efficient, flexible, reusable on rapidly evolving hardware and of course at low cost. Combining both model and component seems to be a very promising cocktail for building solutions to this problem. Hence, we will present in this paper an approach using a model as its first structural citizen all along the development process. Our proposal will be illustrated with an application modeled with UML (extended with some of its dedicated profiles). Our approach includes an underlying execution infrastructure/middleware, providing fault-tolerance services. For the component aspect, our framework promotes firstly an infrastructure based on the Component/Container/Connectorparadigm to provide run-time facilities enabling transparent management of fault-tolerance (mainly fault-detection and redundancy mechanisms). For the model-driven point of view, our framework provides tool support for assisting the users to model their applications and to deploy and configure them on computing platforms. In this paper we focus on the run-time support offered by the component framework, specially the replication-aw are interaction mechanism enabling a transparent replication management mechanisms and some additional system components dedicated to fault-detection and replicas management.",2008,0,
1214,1215,Segment based X-Filling for low power and high defect coverage,"Many X-Filling strategies are proposed to reduce test power during scan based testing. Because their main motivation is to reduce the switching activities of test patterns in the test process, some of them are prone to reduce the test ability of test patterns, which may lead to low defect coverage. In this paper, we propose a segment based X-filling(SBF) technique to reduce test power using multiple scan chains, with minimal impact on defect coverage. Different from the previous filling methods, our X-filling technique is segment based and defect coverage aware. The method can be easily incorporated into traditional ATPG flow to keep capture power below a certain limit and keep the defect coverage at a high level.",2009,0,
1215,1216,"An automated industrial fish cutting machine: Control, fault diagnosis and remote monitoring","In this paper, an automated industrial fish cutting machine, which was developed and tested in the Industrial Automation Laboratory (IAL) of the University of British Columbia, is presented including its hardware structure, control sub-system, fault diagnosis sub-system and the remote monitoring sub-system. First, the hardware of the machine including the mechanical conveyer system, pneumatic system and the hydraulic system, and the associated sensors are introduced. Next, a fuzzy position control system is designed for the control of the cutting table moving along the horizontal (x) direction and its performance is compared with that with traditional proportional-integral-derivative (PID) control. A multi-sensor neuro-fuzzy fault diagnosis system is developed as well for the purpose of providing accurate and reliable diagnosis of the machine states in an automated factory environment. Finally, a web-based remote monitoring system is discussed, which allows engineers and researches to remotely monitor the health of the machine from any remote geographic location through the Internet.",2008,0,
1216,1217,A Fault Tolerant Wired/Wireless Sensor Network Architecture for Monitoring Pipeline Infrastructures,"This paper proposes a new fault-tolerant sensor network architecture for monitoring pipeline infrastructures. This architecture is an integrated wired and wireless network. The wired part of the network is considered the primary network while the wireless part is used as a backup among sensor nodes when there is any failure in the wired network. This architecture solves the current reliability issues of wired networks for pipelines monitoring and control. This includes the problem of disabling the network by disconnecting the network cables due to artificial or natural reasons. In addition, it solves the issues raised in recently proposed network architectures using wireless sensor networks for pipeline monitoring. These issues include the issues of power management and efficient routing for wireless sensor nodes to extend the life of the network. Detailed advantages of the proposed integrated network architecture are discussed under different application and fault scenarios.",2008,0,
1217,1218,Fault simulation and modelling of microelectromechanical systems,High-reliability and safety-critical markets for microelectromechanical systems are driving new proposals for the integration of efficient built-in test and monitoring functions. The realisation of this technology will require support tools and validation methodologies including fault simulation and testability analysis and full closed-loop simulation techniques to ensure cost and quality targets. This article proposes methods to extend the capabilities of mixed signal and analogue integrated circuit fault simulation techniques to MEMS by including failure mode and effect analysis data and using behavioural modelling techniques compatible with electrical simulators.,2000,0,
1218,1219,A power transformer protection with recurrent ANN saturation correction,"Current transformers (CTs) are present in electric power systems for protection and measurement purposes and they are susceptible to the saturation phenomenon. This paper presents an alternative approach to the correction of distorted waveforms caused by CT saturation. The method uses recurrent artificial neural networks (ANN) algorithms. As an example of an application, a complete protection system for a power transformer based on the deferential logic has been utilized. The EMTP-ATP software has been chosen as the computational tool to simulate the electrical system in order to generate data to train and test the ANNs. Many ANN architectures were trained and tested. Encouraging results related to the application of the new method are presented.",2005,0,
1219,1220,Fault tolerance for communication-based multirobot formation,"This paper investigates the ability of fault tolerance for multirobot formation, which is important for practical formation in complex environment. Our model enables mobile robots group to continue to complete given tasks by reorganizing their formation, when some members are in failure. First, to build such model, a multi-agent architecture is presented, which is implemented through communication. Second, we introduce the hierarchy graph of multirobot formation to be the theoretical foundation of the fault tolerance system. The graph analysis is suitable for general leader-follower formation format. And then, the failure detection mechanism for formation is discussed. Finally, integrated fault tolerance algorithm is investigated, including supplement for faulty robots and formation reconfiguration. The improved agent architecture adding the fault tolerance module is also presented. The experiments on real multiple mobile robots demonstrate our design is feasible.",2004,0,
1220,1221,A self-optimization of the fault management strategy for device software,"With the growth of network technologies, abundance of network resources, and increase of various services, mobile devices have gained much functionality and intelligence. At the same time, mobile devices are becoming complicated and many software related problems appear. The traditional remote repair method needs the software providers to supply fault information with corresponding repair strategy. It is inconvenient for users when the sold mobile devices have software faults. However, it is impossible for the manufacturers to supply all the fault information and repair-strategy before selling them. So far, no method has been given to collect repair-strategy from the sold mobile device and optimize the self-repair strategy. In this paper, we propose a self-optimization method to learn the software repair strategy from the sold mobile devices and to optimize self-repair strategy based on the Open Mobile Alliance (OMA) Device Management (DM) standard. The managed objects (MOs) are defined for collecting the strategy data and the self-optimization algorithm is proposed and implemented at the central server.",2009,0,
1221,1222,Guidelines for 2D/3D FE transient modeling of inductive saturable-core Fault Current Limiters,"Fault Current Limiters (FCLs) are expected to play an important role in protection of future power systems. FCLs can be classified in three groups: passive, solid-state and hybrid FCLs. Passive FCLs have merit to inherently react on a fault, requiring no fault detection and triggering circuit. Inductive FCLs based on core saturation belong to this group. Analytical models, used for design of inductive FCLs, are not accurate enough; BH curve cannot be expressed as an explicit function. Numerical models offer better approximations, but they often do not include effects such as leakage and fringing fluxes, which can have considerable influence on the result. Verification of such models is of utmost importance. Finite element modeling (FEM) tools offer possibility to model any inductive FCL topology, while all the effects, e.g. non-linear BH curve, fringing effects etc., are taken into account. However, the modeling of these devices in FEM softwares is difficult. This paper introduces the guidelines for development of 2D/3D transient FE models of inductive FCLs in Ansys. The guidelines are developed with respect to the single-core inductive FCL topology. The model can be applied to any inductive FCL and presents a valuable tool for design, verification and optimization of these devices. Signals' waveforms, obtained through the transient analysis, provide precise depiction of FCL operation during both normal and fault regimes. The model is validate by means of lab experiment. Simulation and experimental results show very good matching. In addition, modeling results are used to prove that single-core FCL topology operates properly during both nominal and fault regimes.",2009,0,
1222,1223,Model-based fault-tolerant control reconfiguration for general network topologies,This article describes a fault-tolerant approach to systems with arbitrary network topologies that uses a model-based diagnosis and control reconfiguration mechanism. The authors illustrate this technique using a wireless sensor network as an example,2001,0,
1223,1224,Harmonic distortion and measurement principles based on digital fault recorder (DFR) analysis,"Each type of device causing harmonics has a particular shape of harmonic current and voltage (amplitude and phase displacement). This work provides a methodology for analyzing the distortion from the data record of digital fault recorders in order to quantify the distortion in current and voltage. This can be done by decomposing the signal into its constituent components in the frequency domain, because of the fact that it is not practical to obtain and represent all the system detail for analysis. It can lead to inaccurate estimation of distortion in voltages and currents. A simple but realistic approach for resonance analysis is presented.",2009,0,
1224,1225,Predicting the severity of a reported bug,"The severity of a reported bug is a critical factor in deciding how soon it needs to be fixed. Unfortunately, while clear guidelines exist on how to assign the severity of a bug, it remains an inherent manual process left to the person reporting the bug. In this paper we investigate whether we can accurately predict the severity of a reported bug by analyzing its textual description using text mining algorithms. Based on three cases drawn from the open-source community (Mozilla, Eclipse and GNOME), we conclude that given a training set of sufficient size (approximately 500 reports per severity), it is possible to predict the severity with a reasonable accuracy (both precision and recall vary between 0.65-0.75 with Mozilla and Eclipse; 0.70-0.85 in the case of GNOME).",2010,0,
1225,1226,Software solution for fault record analysis in power transmission and distribution,"Digital protection relays provide the functionality of recording network disturbances during faults. Meanwhile the digital share in the installed relay number is a substantial one, so the utilities can gather valuable information with a large coverage of their grid and can start to enjoy the additional benefits of modem technology beyond the functions built into the devices. After a fault the operating personnel wants to obtain a most precise fault location to narrow the search for possible damage on the line. The fault locator precision of a single relay is limited by physics and by the grid conditions of mixed lines, load taps etc. But an easy-to-use software system for relay fault records can provide the desired precision to the utility personnel. The system is open to fault records of any relay, which is accomplished via the Comtrade data format. It also contains the parameters of segmented or untransposed lines. Furthermore it uses sophisticated self-adapting algorithms for analysis beyond those used at the protection relays.",2004,0,
1226,1227,REST-Based SOA Application in the Cloud: A Text Correction Service Case Study,"In this paper, we present a REST-based SOA system, Set It Right (SIR), where people can get feedback on and help with short texts. The rapid development of the SIR system, enabled by designing it as a set of services, and also leveraging commercially offered services, illustrates the strength of the SOA paradigm. Finally, we evaluate the Cloud Computing techniques and infrastructures used to deploy the system and how cloud technology can help shorten the time to market and lower the initial costs.",2010,0,
1227,1228,An approach to detecting duplicate bug reports using natural language and execution information,"An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as duplicate and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. In this paper we present a new approach that further involves execution information. In our approach, when a new bug report arrives, its natural language information and execution information are compared with those of the existing bug reports. Then, a small number of existing bug reports are suggested to the triager as the most similar bug reports to the new bug report. Finally, the triager examines the suggested bug reports to determine whether the new bug report duplicates an existing bug report. We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67%-93% of duplicate bug reports in the Firefox bug repository, compared to 43%-72% using natural language information alone.",2008,0,
1228,1229,Intelligent Agents for Fault Tolerance: From Multi-agent Simulation to Cluster-Based Implementation,"Recent research in multi-agent systems incorporate fault tolerance concepts, but does not explore the extension and implementation of such ideas for large scale parallel computing systems. The work reported in this paper investigates a swarm array computing approach, namely 'Intelligent Agents'. A task to be executed on a parallel computing system is decomposed to sub-tasks and mapped onto agents that traverse an abstracted hardware layer. The agents intercommunicate across processors to share information during the event of a predicted core/processor failure and for successfully completing the task. The feasibility of the approach is validated by simulations on an FPGA using a multi-agent simulator, and implementation of a parallel reduction algorithm on a computer cluster using the Message Passing Interface.",2010,0,
1229,1230,Research on RTOS-Integrated TMR for Fault Tolerant Systems,"Safety and availability are issues of major importance in many critical systems. A RTOS (realtime operating system)-integrated fault-tolerant system using TMR technology is presented in this paper. The system incorporates three homogeneous microcomputers and provides the fault-tolerant function through system-APIs to applications. As it is integrated with RTOS, the system is more general-purpose, and programmers need not pay too much attention to the fault tolerance technology. This system works in normal and degraded (duple or even single modular) modes, and can tolerate transient or permanent faults. The system also provides MultiTask-support fault-tolerant function, and reconfiguration after a fault occurs is transparent to applications. Meanwhile, a novel seamless software upgrade method through intelligent state-transition-control is brought forward.",2007,0,
1230,1231,A system architecture assisting user trial-and-error process in in-silico drug design,"Screening on computers, or in-silico screening, has the potential to dramatically reduce the total cost and time required for the whole drug design process. Recently, a variety of in-silico screening systems has been reported on literatures. Nonetheless, scientists still have much difficulty in benefiting from such systems. The reason can be explained from the fact that scientists' trial-and-error consideration, whose purpose is to precisely describe molecules on a computer, in prior to in-silico screening stage takes a long time. We present a flexible user-support environment that assists in scientists' trial-and-error consideration with a ""trial set"" concept. On this environment, scientists utilize the trial set to easily complete a sequence of tasks accompanied with parameter changes. The experiment in this paper shows that the first prototype system featured by this trial set effectively works. Furthermore, we describe the future plan of the system.",2004,0,
1231,1232,Flight technical error analysis of the SATS higher volume operations simulation and flight experiments,"This paper provides an analysis of flight technical error (FTE) from recent SATS experiments, called the higher volume operations (HVO) simulation and flight experiments, which NASA conducted to determine pilot acceptability of the HVO concept for normal operating conditions. Reported are FTE results from simulation and flight experiment data indicating the SATS HVO concept is viable and acceptable to low-time instrument rated pilots when compared with today's system (Baseline). Described is the comparative FTE analysis of lateral, vertical, and airspeed deviations from the Baseline and SATS HVO experimental flight procedures. Based on FTE analysis, all evaluation subjects, low-time instrument-rated pilots, flew the HVO procedures safely and proficiently in comparison to today's system (Baseline). In all cases, the results of the flight experiment validated the results of the simulation experiment and confirm the utility of the simulation platform for comparative human in the loop (HITL) studies of SATS HVO and Baseline operations.",2005,0,
1232,1233,Cancer-radiotherapy equipment as a cause of soft errors in electronic equipment,"The undesirable production of secondary neutrons by cancer-radiotherapy linear accelerators (linac) has been demonstrated to cause soft errors in nearby electronics through the <sup>10</sup>B(n,a)<sup>7</sup>Li reaction. <sup>10</sup>B is a component in the BPSG used as a dielectric material in some integrated-circuit (IC) fabrication processes.",2005,0,
1233,1234,Diagnostics using airborne survey and fault location systems as the means to increase OHTL reliability,"Airborne survey application for diagnostics of overhead transmission lines (OHTL) is quite relevant for power utilities of industrialized counties where power network counts thousands or millions km, of which considerable part has reached 30-50 year lifetime and older. Airborne survey based on aerial scanning as a method of OHTL condition monitoring, is efficient instrument of detection of the line elements deviation off regular condition, serves as a convenient facility of network utility inventory. Advantage of aerial scanning is a combination of high survey accuracy with high work productivity. Processing of digital survey data allows to get essential data required for OHTL reliability analysis: precise span lengths, sag and tension values, conductor clearance to ground, crossed and adjacent objects, clearance to vegetation, distance to nearby trees that may damage OHTL if fallen. For analysis of OHTL reliability, existing software packages allow to carry out modeling condition of separate elements and entire line under extreme ice and wind loads, check safety of conductor clearance to ground and crossed lines under condition of significant conductor overheating determined by necessity to ensure transmission under long-term or short-term (but considerable) load increase. Collection, storage, systemizing, practical use of survey data for development and implementing management decisions and rational usage of network resources is reasonable to accomplish with a specialized information system. Information system helps to provide integrate OHTL monitoring data, modules of record and analysis of technical condition of separate components and entire line, 2D and 3D representation of objects with high georeference accuracy. One of negative examples of insufficient OHTL reliability is fault current caused by lightning, conductor or insulator mechanical damage, etc. Duration of OHTL malfunction, timing and success of emergency elimination depends greatly on accuracy of fa- ult location (FL) on line. Advanced FL system allows to locate fault with accuracy of 5 to 150 m. Combined with aerial scanning data and visualizing line section detected by FL system essentially improves efficiency of service technology, emergency recovery of electric network by maintenance crew, and hence increases system reliability of power objects.",2005,0,
1234,1235,Range Non-linearities Correction in FMCW SAR,"The limiting factor to the use of Frequency Modulated Continuous Wave (FMCW) technology with Synthetic Aperture Radar (SAR) techniques to produce lightweight, cost effective, low power consuming imaging sensors with high resolution, is the well known presence of non-linearities in the transmitted signal. This results in contrast and range resolution degradation, especially when the system use is intended for long range applications, as it is the case for SAR. The paper presents a novel processing solution, which completely solves the non- linearity problem. It corrects the non-linearity effects for the whole range profile at once, differently from the algorithms described in literature so far, which work only for very short range intervals. The proposed method operates directly on the deramped data and it is very computationally efficient.",2006,0,
1235,1236,Remote sensing of power system arcing faults,Electromagnetic radiation in the form of atmospheric radiowaves (or sferics) originate from power system apparatus when transient fault currents are present. A system to monitor these events via the detection of the induced very high frequency (VHF) sferic radiation has been in operation since November 1998. This system is part of an ongoing research program to develop overhead line fault detection and location equipment. This paper details the implementation of the sferic monitoring system and the latest developments that aim to improve event detection and triggering efficiency. Example transient sferic radiations records taken from the extensive data archive are presented. Fourier time frequency domain analysis is employed to extract features from the sferic signal data. Finally the future application of such monitoring technologies to power distribution networks is discussed.,2000,0,
1236,1237,Extended transfer bound error analysis in the presence of channel random nuisance parameter,"In this paper, we present the extended transfer bound analysis for the error performance of a general trellis code in the channel with the overall correlated continuous valued nuisance parameters. We introduce proper parameter model and include it into a new extended form of the transfer function. In this way, both, the new additional parameter space and the original error space are incorporated into system error analysis. An example application with simple trellis code and Rayleigh fading channel is investigated in order to demonstrate the functionality of the principle. Computer simulation results are presented for two different codes and various fading scenarios, and comparisons are made among analytical and measured system error performances. It was shown that for fading amplitude our approach was able to predict correct error asymptotes",2005,0,
1237,1238,Sub-picture: ROI coding and unequal error protection,Region-of-interest coding and unequal error protection are two important tools in video communication systems to improve the received visual quality. One common property of the two techniques is that unequal coding or transmission is applied to improve the quality of the most important parts of images. The proposed sub-picture coding technique facilitates both region-of-interest coding and unequal error protection by partitioning images to regions of interest and separating the corresponding coded data units from each other. Simulation results show that the overall subjective quality is considerably improved compared to the conventional coding schemes.,2002,0,
1238,1239,Fault detection and isolation in the NT-OMS/RCS,"In this paper, we consider the problem of test design for real-time fault detection and diagnosis in the Space Shuttle's non-toxic orbital maneuvering system and reaction control system (NT-OMS/RCS). For demonstration purposes, we restrict our attention to the shaft section of the NT-OMS/RCS, which consists of 160 faults (each fault being either a leakage, blockage, igniter fault, or regulator fault) and 128 sensors. Using the proposed tests, we are able to uniquely isolate a large number of the faults of interest in the NT-OMS/RCS. Those that cannot be uniquely isolated can generally be resolved into small ambiguity groups and then uniquely isolated via manual/automated commands. Simulation of the NT-OMS/RCS under various fault conditions was conducted using the TRICK<sup></sup> modeling software.",2004,0,
1239,1240,A fault-driven lightweight process improvement approach,"Process improvement is of high importance and with crucial impact on business and prosperity for software developing companies. The requirements on software are that it needs to be produced faster, cheaper and with higher quality. A recent trend in software development is the use of agile methods. The general idea of more lightweight approaches can also be applied to process improvement. The authors describe a fault-driven lightweight process improvement approach to be used between projects. The objective is to decrease the number of faults and hence shorten the project lead-time. The fault-driven process improvement approach sets focus on business requirements and relevance for the company associated. We discuss the need for a lightweight approach and introduce a lightweight process improvement method. It also reports on some findings from an industrial study and presents some conclusions.",2003,0,
1240,1241,Online Defects Inspection Method for Velcro Based on Image Processing,"Quality control is a crucial issue in producing velcro, and defects existing in velcro can dramatically downgrade velcro level. Manual inspection can not meet the requirements of production efficiency, so an online feasible inspection method is referred to control the surface quality of the velcro. The original algorithm for the edge detection has been improved, and the flaws are extracted according to the first-order characteristic value. And these defects are classified according to the spectrum. Finally, the experiments have indicated that the various defects can be detected by proposed algorithm accurately, and the defects inspection method has been efficient and of great significance.",2010,0,
1241,1242,An Automated System for Analyzing Impact of Faults in IP Telephony Networks,"The widespread use of IP telephony (IPT) has introduced corresponding management issues related to the diagnosis and impact analysis of faults in the IPT network. There are complex logical relationships between the various elements present in the IPT network ranging from call processing engines, PSTN voice gateways, and conferencing and voice mail servers, to endpoints like IP-based handsets. Without an understanding of these complex relationships, traditional element-oriented network fault-management applications are inadequate to assess IPT service impact in the presence of a network or system fault. In this paper, we present a proposal for determining the root cause and assessing the impact of a fault in an IPT network on voice services and end users by automatically executing a series of diagnostic tests from various probe points in the network, and correlating the resulting information with the faults reported from the network. We use a CIM-based model of the IPT network to aid the diagnostic and impact assessment process",2006,0,
1242,1243,Holistic schedulability analysis of a fault-tolerant real-time distributed run-time support,"The feasibility test of a hard real time system must not only take into account the temporal behavior of the application tasks but also the behavior of the run-time support in charge of executing applications. The paper is devoted to the schedulability analysis of a run-time support for distributed dependable hard real time applications. In contrast to previous works that consider rather simple run-time supports (e.g. a real time kernel made of a simple tick scheduler and an unreliable communication protocol), our work deals with a complex run-time support with fault tolerance capabilities and made of multiple tasks that invoke each other",2000,0,
1243,1244,The Fault-Tolerant Design in Space Information Processing System Based on COTS,"In order to make Space Information Processing System (SIPS) based on Commercial-Off-The-Shelf (COTS) have a much stronger ability of radiation resistance in space, this paper presents multilevel fault-tolerant technique based on FPGA and Single Event Latch-up (SEL) resistance protection circuit. The multilevel fault-tolerant technique includes the dual fault-tolerant design on system level, the redundancy design of memory on module level and the fault-tolerant design of FPGA on chip level. By reliability analysis and experimentation, it can be concluded that the reliability of SIPS has been greatly increased by making use of fault-tolerant design. Moreover, this fault-tolerant design has been implemented successfully and run well.",2009,0,
1244,1245,Fault diagnosis in optical access network using Bayesian network,"Due to ever rising need for higher bandwidth dictated by the increasing amounts and quality of services (Asymmetric Digital Subscriber Line (ADSL), Internet Protocol television (IPTV) and Voice over Internet Protocol (VOIP)), and the global development of technology, Fiber To The Home (FTTH) is becoming an affordable answer for the service provider. Before fully launching the service for the mass market the most important prerequisite is being able to provide customers with a high quality experience. To be able to achieve that, one of the main factors that service providers have to ensure is accurate and precise diagnosis of customer reported faults. Low quality of diagnostics prolongs the period of service degradation and increases the likelihood of repeated faults which results in dissatisfaction amongst customers. That is why our objective is to improve the accuracy of diagnostics performed by network technicians to at least 80%. In this paper we are introducing a solution based on Bayesian network and presenting the results of the applied method.",2010,0,
1245,1246,Misalignment between emission scan and X-ray CT derived attenuation maps causes artificial defects in attenuation corrected myocardial perfusion SPECT,"Attenuation correction increases the specificity of myocardial perfusion SPECT. However, we noticed an unusually high number of scans with defects only visible in the attenuation-corrected images. We suspected these to be false positive readings. Visual inspection using the supplied software suggested mismatch in the ventrodorsal (Y-) direction between SPECT images and transmission maps as an explanation. As the fusion tool only allows for a coarse grading, we wrote software to quantify the mismatch. A phantom study was done to verify that the observed mismatch can cause the defect patterns visible in the attenuation-corrected images. 25 patients who showed the most pronounced artifact were chosen for re-alignment and re-evaluation. Overall, the defects in the attenuation-corrected images got less intense (15/25) or even vanished (6/25) after re-aligning emissions images and transmission maps. In response to our complaints, the vendor replaced the support rollers which should prevent the bed from deflecting with a re-engineered, more robust version. No more clinically relevant artefacts were observed after this modification. Evaluation of another 28 probably-normal patients showed the mean mismatch between emission and transmission scan to be significantly reduced. We conclude that great care has to be taken to ensure correct alignment of the scans even in a dual-modality imaging device. Bed deflection can be a major source for misalignment and artifacts.",2004,0,
1246,1247,BDD based analysis of parametric fault trees,"Several extensions of the fault tree (FT) formalism have been proposed in the literature. One of them is called parametric fault tree (PFT) and is oriented to the modeling of redundant systems, and provides a compact form to model the redundant parts of the system. Using PFTs instead of FTs to model systems with replicated parts, the model design is simplified since the analyst can fold subtrees with the same structure in a single parametric subtree, reducing the number of elements in the model. The method based on binary decision diagrams (BDD) for the quantitative analysis of FTs, is adapted in this paper to cope with the parametric form of PFTs: an extension of BDDs called parametric BDD (pBDD) is used to analyze PFTs. The solution process is simplified by using pBDDs: comparing the pBDD obtained from a PFT, with the ordinary BDD obtained from the unfolded FT, we can observe a reduction of the number of nodes inside the pBDD. Such reduction is proportional to the level of redundancy inside the PFT and leads to a consequent reduction of the number of steps necessary to perform the analysis. Concerning the qualitative analysis, we can observe that several minimal cut sets (MCS) obtained from the FT model of a redundant system, involve basic events relative to similar components. A parametric MCS (pMCS) allows to group such MCSs in an equivalence class, and consequently, to evidence only the failure pattern, regardless the identity of replicated components. A method to derive pMCSs from a PFT is provided in the paper",2006,0,
1247,1248,An efficient error concealment implementation for MPEG-4 video streams,"This paper presents an efficient error concealment implementation for damaged MPEG-4 video bitstreams. The chosen spatial and temporal concealment algorithms are designed to fit in real-time decoders and are advantageously combined in a hybrid spatial/temporal approach to provide visually more plausible pictures than basic concealment techniques. In addition, the encoder impact on the visual quality of the reconstruction in presence of channel errors is highlighted",2001,0,
1248,1249,Adaptive Checkpoint Replication for Supporting the Fault Tolerance of Applications in the Grid,"A major challenge in a dynamic Grid with thousands of machines connected to each other is fault tolerance. The more resources and components involved, themore complicated and error-prone becomes the system. Migol is an adaptive Grid middleware, which addresses the fault tolerance of Grid applications and services by providing the capability to recover applications from checkpoint files automatically. A critical aspect for an automatic recovery is the availability of checkpoint files: If a resource becomes unavailable, it is very likely that the associated storage is also unreachable, e. g. due to a network partition. A strategy to increase the availability of checkpoints isreplication.In this paper, we present the Checkpoint Replication Service. A key feature of this service is the ability to automatically replicate and monitor checkpoints in the Grid.",2008,0,
1249,1250,Analyses and correction of the dynamic properties of the VALYDYNE <sup>TM</sup> differential pressure sensor,"The paper presents the results of the modeling and measurement study of the differential pressure transducer MP45 series made by the Validyne<sup>TM</sup> Corp. This research is necessary during the construction of the device for identification of the human airducts model using the time method. The transducer parameters in the manufacturer's model are significantly different from the parameters calculated after the measurement study. This paper presents verification of models and frequency range of transducer. A new, more accurate model is proposed. The results obtained show that it is necessary to make a correction of the dynamic properties of this pressure sensor. The effects of this correction using first- and second-order correctors are presented and analyzed",2001,0,
1250,1251,Scheduling algorithms for ultra-reliable execution of tasks under both hardware and software faults,"Summary form only given, as follows. We study the development of integrated fault-tolerant scheduling algorithms. The proposed algorithms ensure ultra-reliable execution of tasks where both hardware and software failures are considered, and system performance improvement. Also, the proposed algorithms have the capability for on-line system-level fault diagnosis.",2003,0,
1251,1252,Real-Time Implementation of Fault Detection in Wireless Sensor Networks Using Neural Networks,"This paper presents the real-time implementation of a neural network-based fault detection for wireless sensor networks (WSNs). The method is implemented on TinyOS operating system. A collection tree network is formed and multi-hoping data is sent to the base station root. Nodes take environmental measurements every N seconds while neighboring nodes overhear the measurement as it is being forwarded to the base station and record it. After nodes complete M and receive/store M measurements from each neighboring node, recurrent neural networks (RNNs) are used to model the sensor node, the node's dynamics, and interconnections with neighboring nodes. The physical measurement is compared against the predicted value and a given threshold of error to determine sensor fault. By simply overhearing network traffic, this implementation uses no extra bandwidth or radio broadcast power. The only cost of the approach is battery power required to power the receiver to overhear packets and MCU processor time to train the RNN.",2008,0,
1252,1253,Design Diverse-Multiple Version Connector: A Fault Tolerant Component Based Architecture,"Component based software engineering (CBSE) is a new archetype to construct the systems by using reusable components ldquoas it isrdquo. To achieve high dependability in such systems, there must be appropriate fault tolerance mechanism in them at the architectural level. This paper presents a fault tolerant component based architecture that relies on the C2 architectural style and is based on design diverse and exception handling fault tolerance strategies. The proposed fault tolerant component architecture employs special-purpose connectors called design diverse-multiple version connectors (DD-MVC). These connectors allow design diverse n-versions of components to run in parallel. The proposed architecture has a fault tolerant connector (FTC), which detects and tolerates different kinds of errors. The proposed architecture adjusts the tradeoff between dependability and efficiency at run time and exhibits the ability to tolerate the anticipated and unanticipated faults effectively. The applicability of proposed architecture is demonstrated with a case study.",2008,0,
1253,1254,Adaptive unequal error protection for subband image coding,"An adaptive subband image coding system is proposed to investigate the performance offered by implementing unequal error protection among the subbands and within the subbands. The proposed system uses DPCM and PCM codecs for source encoding the individual subbands, and a family of variable rate channel codes for forward error correction. A low resolution family of trellis coded modulation codes and a high resolution family of punctured convolutional codes are considered. Under the constraints of a fixed information rate, and a fixed transmission bandwidth, for any given image, the proposed system adaptively selects the best combination of channel source coding rates according to the current channel condition. Simulations are performed on the AWGN channel, and comparisons are made with corresponding systems where the source coder is optimized for a noiseless transmission (classical optimization) and a single channel code is selected. Our proposed joint source-channel systems greatly outperform any of the nonadaptive conventional nonjoint systems that use only a single channel code at all channel SNRs, extending the useful channel SNR range by an amount that depends on the code family. A nonjoint adaptive equal error protection system is considered which uses the classically optimized source codec, but chooses the best single channel code for the whole transmission according to the channel SNR. Our systems outperform the corresponding adaptive equal error protection system by at most 2 dB in PSNR; and more importantly, show a greater robustness to channel mismatch. It is found that most of the performance gain of the proposed systems is obtained from implementation of unequal error protection among the subbands, with at most 0.7 dB in PSNR additional gain achieved by also applying unequal error protection within the subbands. We use and improve a known modeling technique which enables the system to configure itself optimally for the transmission of an arbitrary image, by only measuring the mean of lowest frequency subband and variances of all the subbands",2000,0,
1254,1255,Fault diagnosis based on granular matrix-SDG and its application,"The hierarchical fault diagnosis based on granular matrix and Signed Directed Graph (SDG) is presented in the paper. Granular Computing (GrC) theory can be introduced into SDG-based fault diagnosis to optimize the decision table. The rules of fault diagnosis are reasoned out through searching the associated path of the SDG model. The redundant nodes of the failure diagnosis rules are reduced by the attribute reduction algorithm based on granular matrix, which can simplify the solution of failure diagnosis, avoid the setting of the redundant sensor, and decrease the complexity of collocating sensor network. Compared with the traditional failure diagnosis based on SDG, the designed scheme and an experimental example of a hot nitric acid cooling failure diagnosis system show that the hierarchical fault diagnosis based on granular matrix and SDG in the paper is not only feasibly and effectively, but also valuable in practice.",2009,0,
1255,1256,Fault-Tolerant Bit-Parallel Multiplier for Polynomial Basis of GF(2m),"In this paper, we present novel fault-tolerant architecture for bit-parallel polynomial basis multiplier over GF(2m) which can correct the erroneous outputs using linear code. We have designed a parity prediction circuit based on the code generator polynomial that leads lower space overhead. For bit-parallel architectures, the space overhead is about 11%. Moreover, there is only marginal time overhead due to incorporation of error-correction capability that amounts to 3.5% in case of the bit-parallel multiplier. Unlike the existing concurrent error correction (CEC) multipliers or triple modular redundancy (TMR) techniques for single error correction, the proposed architectures have multiple error-correcting capabilities.",2009,0,
1256,1257,Practical Methods for Geometric and Photometric Correction of Tiled Projector,"We describe a novel, practical method to create largescale, immersive displays by tiling multiple projectors on curved screens. Calibration is performed automatically with imagery from a single uncalibrated camera, without requiring knowledge of the 3D screen shape. Composition of 2D-mesh-based coordinate mappings, from screen-tocamera and from camera-to-projectors, allows image distortions imposed by the screen curvature and camera and projector lenses to be geometrically corrected together in a single non-parametric framework. For screens that are developable surfaces, we show that the screen-to-camera mapping can be determined without some of the complication of prior methods, resulting in a display on which imagery is undistorted, as if physically attached like wallpaper. We also develop a method of photometric calibration that unifies the geometric blending, brightness scaling, and black level offset maps of prior approaches. The functional form of the geometric blending is novel in itself. The resulting method is more tolerant of geometric correction imprecision, so that visual artifacts are significantly reduced at projector edges and overlap regions. Our efficient GPUbased implementation enables a single PC to render multiple high-resolution video streams simultaneously at frame rate to arbitrary screen locations, leaving the CPU largely free to do video decompression and other processing.",2006,0,
1257,1258,3DPPS for early detection of arcing faults,"New approach to high impedance fault detection, which allows for detecting it basing on yet some random arcing at the beginning of the fault, is presented in this paper. The proposed solution was developed within novel protection methodology - 3D power protection scheme (3DPPS). The identification of the fault is based on monitoring of symmetry deviations of three phase voltage or current signals. Fundamental signal components carry the biggest amount of information on the actual state of the protected system and are processed in order to extract out the necessary information proving an occurrence of a high impedance fault that must be cleared for safety purposes.",2010,0,
1258,1259,An analysis of fault effects and propagations in ZPU: The world's smallest 32 bit CPU,"This paper presents an analysis of the effects and propagations of transient faults by simulation-based fault injection into the ZPU processor. This analysis is done by injecting 5800 transient faults into the main components of ZPU processor that is described in VHDL language. The sensitivity level of various points of ZPU processor such as PC, SP, IR, Controller, and ALU against fault manifestation is considered and evaluated. The behavior of ZPU processor against injected faults is reported. Besides, it is shown that about 50.25% of faults are recovered during simulation time; 46.47% of faults are effective and the remainders 3.28% of faults are latent. Moreover, a comparison of the behavior of ZPU processor in fault injection experiments against some common microprocessors is done. The results will be used in the future research for proposing a fault-tolerant mechanism for ZPU processor.",2010,0,
1259,1260,Design of the distributed fault recorder based on TCP/IP,"A new kind of fault recorder is presented to overcome the shortcomings of existing fault recorders in power system, such as unreasonable structure design, low communication speed, small data storage capacity, low reliability and so on. The recorder, with MC68332 as kernel, adopts high speed synchro-sampling and computer network communication techniques. The network communication based on TCP/IP between the master station and recording modules is adopted. The principles, hardware and software design are introduced in detail. A RTOS, named Vxworks and modular design are applied, which make the system functions easy to be expanded and maintained. The recorder has been put into practical operation in some substations. The operation results show that it has the advantages of easy to be extended, convenient to be installed, better anti-interference, high reliability, etc.",2008,0,
1260,1261,An enhancement of fault-tolerant routing protocol for Wireless Sensor Network,"As more and more real Wireless Sensor Network's (WSN) applications have been tested and deployed over the last decade, the research community of WSN realizes that several issues need to be revisited from practical angles, such as reliability and availability. Furthermore, fault-tolerance is one of the main issues in WSNs since it becomes critical in real deployed environments where network stability and reduced inaccessibility times are important. Basically, wireless sensor networks suffer from resource limitations, high failure rates and faults caused by the defective nature of wireless communication and the wireless sensor itself. This can lead to situations, where nodes are often interrupted during data transmission and blind spots occur in the network by isolating some of the devices. In this paper, we address the reliability issue by designing an enhanced fault-tolerant mechanism for Ad hoc On-Demand Distance Vector (AODV) routing protocol applied in WSN called the ENhanced FAult-Tolerant AODV (ENFAT-AODV) routing protocol. We apply a backup route technique by creating a backup path for every node on a main path of data transmission. When a node gets failure to deliver a data packet through the main path, it immediately utilizes its backup route to become a new main path for the next coming data packet delivery to reduce a number of data packets dropped and to maintain the continuity of data packet transmission in presence of some faults (node or link failures). Furthermore, with increased failure rate, this proposed routing protocol improves the throughput, reduces the average jitter, provides low control overhead and decreases the number of data packets dropped in the network. As a result, the reliability, availability and maintainability of the network are achieved. The simulation results show that our proposed routing protocol is better than the original AODV routing.",2010,0,
1261,1262,A Hybrid Approach to Fault Detection and Correction in SoCs,"The reliability of Systems-on-Chip (SoCs) is very important with respect to their use in different types of critical applications. Several fault tolerance techniques have been proposed to improve their fault detection and correction capabilities. These approaches can be classified in two basic categories: software-based and hardware-based techniques. In this paper, we propose a hybrid approach to provide fault detection and correction capabilities of transient faults for processor-based SoCs. This solution improves a previous one, aimed at fault detection only, and combines some modifications of the source code at high level with the introduction of an Infrastructure Intellectual Property (TIP). The main advantage of the proposed method lies in the fact that it does not require modifying the microprocessor core. Experimental results are provided to evaluate the effectiveness of the proposed method.",2007,0,
1262,1263,Overflow Detection and Correction in a Fixed-Point Multiplier,"The fixed-point binary representation, an integer format with an implied binary point, is an alternative to the IEEE floating-point binary layout. Systems that do not support the IEEE floating-point format, e.g., mobile devices, use the fixed-point format because it fits well into integer data paths whereas floating-point requires its own data path. Software developers who port to fixed-point systems often face issues when balancing range and precision. Those issues, overflow and large rounding errors, often arise from arithmetic operations, making debugging more difficult. The proposed solution limits hardware support to a set of fixed-point formats and adjusts the format of the output based on the user supplied format and overflow. The format of the result is readjusted on overflow in order to return a useful result but with the sacrifice of precision. In addition to the corrected result, the overflow flag is raised so the software and subsequent logic are aware of the readjustment in the result's format. This work has been implemented in a fixed-point multiplier because multiplication yields the largest overflow among the four basic arithmetic operations. In order to detect overflow early, the fixed-point multiplier adopts preliminary overflow detection. With the idea of taking the burden of fixed-point scaling off of the programmer, the fixed-point multiplier with overflow detection and correction provides a starting point towards mitigating fixed-point errors.",2007,0,
1263,1264,Notice of Violation of IEEE Publication Principles<BR>Using Current Signature Analysis Technology to Reliably Detect Cage Winding Defects in Squirrel-Cage Induction Motors,"Notice of Violation of IEEE Publication Principles<BR><BR>""Using Current Signature Analysis Technology to Reliably Detect Cage Winding Defects in Squirrel-Cage Induction Motors""<BR>by I.M. Culbert and W. Rhodes<BR>in the IEEE Transactions on Industry Applications, Vol. 43, No. 2, March/April 2007<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains portions of original text from the papers cited below. The original text was copied without attribution and without permission.<BR><BR>Figure 5:<BR>""Development of a Tool to Detect Faults in Induction Motors via Current Signature Analysis"",<BR>by M. Fenger, B. A. Lloyd, and W. T. Thomson<BR>in the Proceedings of the IEEE-IAS/PCA Cement Industry Conference, Dallas, TX, May 4-9, 2003, pp. 37-46.<BR><BR>Equation 3:<BR>""Case Histories of Rotor Winding fault diagnosis in induction motors"",<BR>by W. T. Thomson, and D. Rankin<BR>in the Proceedings of the International Conference on Condition Monitoring, University College of Swansea, March 31-April 3, 1987, pp. 798-819.<BR><BR>This paper will demonstrate, through industrial case histories, the application of current signature analysis (CSA) technology to reliably diagnose rotor winding problems in squirrel-cage motors. Many traditional CSA methods result in false alarms and/or misdiagnosis of healthy machines due to the presence of current components in the broken cage winding frequency domain, which are not the result of such defects. Such components can result from operating conditions, motor design, and drive components such as mechanical load fluctuations, speed-reducing gearboxes, etc. Due to theoretical advancements, it is now possible to predict many of these current components, thus making CSA testing less error prone and therefore a much more reliable technology. Reliable detection of the in- eption of broken cage winding problems, or broken rotor bars, prior to failure allows for remedial actions to be taken to avoid significant costs associated with consequential motor component damage and unplanned downtime associated with such in-service failures",2007,0,
1264,1265,Development of an ATP-based fault simulator for underground power cables,The paper reports the development of an ATP-based fault simulator for underground power cables. Emphasis is given to the design philosophy and a detailed description of the user interface which has been developed. Case studies are also included to illustrate the applications of the software,2000,0,
1265,1266,Enhance Fault Localization Using a 3D Surface Representation,"Debugging is a difficult and time-consuming task in software engineering. To locate faults in programs, a statistical fault localization technique makes use of program execution statistics and employs a suspiciousness function to assess the relation between program elements and faults. In this paper, we develop a novel localization technique by using a 3D surface to visualize previous suspiciousness functions and using fault patterns to enhance such a 3D surface. By clustering realistic faults, we determine various fault patterns and use 3D points to represent them. We employ spline method to construct a 3D surface from those 3D points and build our suspiciousness function. Empirical evaluation on a common data set, Siemens suite, shows that the result of our technique is more effective than four existing representative such techniques.",2010,0,
1266,1267,Error Sector Inventory on Optical Disc for Defect Avoidance during Recording / Playback Digest of Technical Papers,"This paper proposes a method to create on-the-fly inventory of error sectors, so that subsequent recordings / playback are free from the audio-video hiccups and retries and still compliant with DVD-Video / DVD+RW logical standard. This will in-turn aid in creating good user experience (no audio-video hiccups) and helps in using processor MIPS efficiently (no retries).",2008,0,
1267,1268,Research on Error Analysis and Calibration Method of Laser Scan Range Finder,"The technology of laser scan range finder is researched, a line laser is used to survey the object section outlines. The system is composed of line laser, CCD camera, Frame Grabber, computer and the software. The mathematical model of the system is defined and studied, the error is analysed, the calibration method is researched. the system is calibrated and tested, the result showed that the survey data moved towards with the stripe shape tallies and the system achieve the accuracy requirements.",2010,0,
1268,1269,Towards latent faults prevision in an automatic controlled plant,The paper presents an approach to develop a program for the latent fault prevision dedicated to an automatic controlled plant. The new approach is based on a soft-computing processing of a set of variables to work out a quality index for evaluating the efficiency of the plant operation. The trend of this index is the base for the prevision of latent faults. The aim of the approach is to realise an intelligent supervision program to insert in the existing automation system. A robotic soldering cell is considered.,2000,0,
1269,1270,A QoS-aware fault tolerant middleware for dependable service composition,"Based on the framework of service-oriented architecture (SOA), complex distributed systems can be dynamically and automatically composed by integrating distributed Web services provided by different organizations, making dependability of the distributed SOA systems a big challenge. In this paper, we propose a QoS-aware fault tolerant middleware to attack this critical problem. Our middleware includes a user-collaborated QoS model, various fault tolerance strategies, and a context-aware algorithm in determining optimal fault tolerance strategy for both stateless and stateful Web services. The benefits of the proposed middleware are demonstrated by experiments, and the performance of the optimal fault tolerance strategy selection algorithm is investigated extensively. As illustrated by the experimental results, fault tolerance for the distributed SOA systems can be efficient, effective and optimized by the proposed middleware.",2009,0,
1270,1271,Development of a novel humidity sensor with error-compensated measurement system,"This paper documents the creation of a complete PC-based humidity sensing system, implemented using LabVIEW from National Instruments. The humidity sensor, which has a measured sensitivity of 0.25%/RH%, is manufactured by thin film technology from a novel combination of SiO/In<sub>2</sub>O<sub>3</sub>. Both the humidity sensor and a standard temperature sensor are interfaced to a PC using a front-end signal conditioning circuit. The entire system has been analyzed mathematically and the necessary algorithms for error-compensation have been developed. The resulting measurement system is efficient, accurate and flexible.",2002,0,
1271,1272,An experiment family to investigate the defect detection effect of tool-support for requirements inspection,"The inspection of software products can help to find defects early in the development process and to gather valuable information on product quality. An inspection is rather resource intensive and involves several tedious tasks like navigating, sorting, or checking. Tool support is thus hoped to increase effectiveness and efficiency. However, little empirical work is available that directly compares paper-based (i.e., manual) and tool-based software inspections. Existing reports on tool support for inspection generally tend to focus on code inspections while little can be found on requirements or design inspection. We report on an experiment family: two experiments on paper-based inspection and a third experiment to empirically investigate the effect of tool support regarding defect detection effectiveness and inspection effort in an academic environment with 40 subjects. Main results of the experiment family are: (a) The effectiveness is similar for manual and tool-supported inspections; (b) the inspection effort and defect overlap decreased significantly with tool support, while (c) efficiency increased considerably with tool support.",2003,0,
1272,1273,An Experimental Study of Packet Loss and Forward Error Correction in Video Multicast over IEEE 802.11b Network,"Video multicast over wireless local area networks (WLANs) faces many challenges due to varying channel conditions and limited bandwidth. A promising solution to this problem is the use of packet level forward error correction (FEC) mechanisms. However, the adjustment of the FEC rate is not a trivial issue due to the dynamic wireless environment. This decision becomes more complicated if we consider the multi-rate capability of the existing wireless LAN technology that adjusts the transmission rates based on the channel conditions and the coverage range. In order to explore the above issues we conducted an experimental study of the packet loss behavior of the IEEE 802.11b protocol. In our experiments we considered different transmission rates under the broadcast mode in indoor and outdoor environments. We further explored the effectiveness of packet level FEC for video multicast over wireless networks with multi-rate capability. In order to evaluate the system quantitatively, we implemented a prototype using open source drivers and socket programming. Based on the experimental results, we provide guidelines on how to efficiently use FEC for wireless video multicast in order to improve the overall system performance. We show that the Packet Error Rate (PER) increases exponentially with distance and using a higher transmission rate together with stronger FEC is more efficient than using a lower transmission rate with weaker FEC for video multicast.",2009,0,
1273,1274,Mechanical fault detection in induction motors,"This paper presents a study to detect mechanical irregularities in low voltage random wound induction motors by means of stator current monitoring and spectrum analysis. An analysis of the MMF and permeance functions to classify frequency components of the stator field in both the stator and rotor reference frame is presented. In addition, a test rig designed to introduce different degrees of static eccentricity in the motor with new movable bearing housings is described in detail. Experimental tests prove the theoretical analysis discussed and significant results are presented.",2003,0,
1274,1275,Error Separation and Compensation of Inductosyn Angle Measuring System,"The long period error and short period error of the inductosyn have been studied. And the study result is that the long period error mainly includes the first order error and secondary error in the period of 360A and the short period error mainly includes first order, secondary, third, fifth harmonic error, and so on. A novel model of error separation and compensation is firstly presented according to the error characteristic of inductosyn. And the fourier transform are compared with least-squares in many aspects, The implement method of the error separation based on least-squares is also discussed in detail. One new measuring method is proposed, which can use less than test position to attain the long period error and the short period error. The experiments show that the error compensation method can improve the precision of the inductosyn.",2010,0,
1275,1276,"Influence of random, pile-up and scatter corrections in the quantification properties of small-animal PET scanners","The potential of PET imaging for pre-clinical studies will be fully realized only if repeatable, reliable and accurate quantitative analysis can be performed. The characteristic blurring of PET images due to positron range and non co-linearity, as well as random, pile-up and scatter contributions, that may be significant for fully 3D PET acquisitions of small animal, make it difficult their quantitative analysis. In this work specific activity versus specific counts in the image calibration curves for 3D-OSEM reconstructions from a commercially available small animal PET scanner are determined. Both linear and non-linear calibration curves are compared and the effect of corrections for random and scatter contributions are studied. To assess the improvement in the calibration procedure when scatter and random corrections are considered, actual data from a rat tumor pre- and post-cancer therapy are analyzed. The results show that correcting for random and scatter corrections can increase the sensitivity of PET images to changes in the biological response of tumors by more than 15%, compared to uncorrected reconstructions.",2007,0,
1276,1277,The discovery of the fault location in NIGS,"A new method is discovered for calculating the fault distance of the overhead line of the Neutral Indirect Grounded System (NIGS) in power distribution networks, in which the single phase to ground fault point or distance is difficult to detect, because the zero sequence current is in lower value. It is found that the information of the fault distance is kept in the zero sequence voltage vector which may be measured at the tail terminal of the questioned line by digging the data. Then an algorithm to calculate the fault location on the overhead lines is proposed by considering that the zero sequence voltage vector at the tail terminal. The value of the zero sequence voltage is determined by the fault location, and the phase angle also contains the distance traveled by the load current to the fault point. The system analysis for parameters is conducted for the NIGS by considering line is actual and by the two terminals' parameters.",2010,0,
1277,1278,"The $100,000 Keying Error","Losing $100K hurts, but other input mistakes can cost much more.",2008,0,
1278,1279,On the design of fault-tolerant logical topologies in wavelength-routed packet networks,"In this paper, we present a new methodology for the design of fault-tolerant logical topologies in wavelength-routed optical networks supporting Internet protocol (IP) datagram flows. Our design approach generalizes the ""design protection"" concepts, and relies on the dynamic capabilities of IP to reroute datagrams when faults occur, thus achieving protection and restoration, and leading to high-performance cost-effective fault-tolerant logical topologies. In this paper, for the first time we consider resilience properties during the logical topology optimization process, thus extending the optimization of the network resilience also to the space of logical topologies. Numerical results clearly show that our approach outperforms previous ones, being able to obtain very effective survivable logical topologies with limited computational complexity.",2004,0,
1279,1280,Exploiting Parametric Power Supply and/or Temperature Variations to Improve Fault Tolerance in Digital Circuits,"The implementation of complex functionality in low-power nano-CMOS technologies leads to enhance susceptibility to parametric disturbances (environmental, and operation-dependent). The purpose of this paper is to present recent improvements on a methodology to exploit power-supply voltage and temperature variations in order to produce fault-tolerant structural solutions. First, the proposed methodology is reviewed, highlighting its characteristics and limitations. The underlying principle is to introduce on-line additional tolerance, by dynamically controlling the time of the clock edge trigger driving specific memory cells. Second, it is shown that the proposed methodology is still useful in the presence of process variations. Third, discussion and preliminary results on the automatic selection (at gate level) of critical FF for which DDB insertion should take place are presented. Finally, it is shown that parametric delay tolerance insertion does not necessarily reduce delay fault detection, as multi-vdd or multi-frequency self-test can be used to recover detection capability.",2008,0,
1280,1281,Comparing Fault-based Testing Strategies of General Boolean Specifications,Testing Boolean specifications in general form (GF) by the IDNF-oriented approaches always results in superabundant cost and missing detection of some faults. This paper proposes GF-oriented approaches to improve them. The experimental results show that the GF-oriented strategies could enhance the fault detection capability and reduce the sizes of test sets.,2007,0,
1281,1282,A fast algorithm for SVPWM in three phase power factor correction application,"A novel algorithm for space vector pulse width modulation in three phase power factor correction applications is proposed. The durations for the active vectors that formed the sector containing the desired reference voltage vector are calculated directly by matrix pre-decomposing, without looking up sinusoidal and tangential tables, based on TMS320F240. Therefore running speed and control precise of the program can be improved greatly. As a result the switching frequency and power density of the rectifier is increased considerably. A new method for detecting sector is given as well. Simulated and experimental results are provided to verify the proposed algorithm in the end of the paper.",2004,0,
1282,1283,Faults Coverage Improvement Based on Fault Simulation and Partial Duplication,"A method how to improve the coverage of single faults in combinational circuits is proposed. The method is based on Concurrent Error Detection, but uses a fault simulation to find Critical points - the places, where faults are difficult to detect. The partial duplication of the design with regard to these critical points is able to increase the faults coverage with a low area overhead cost. Due to higher fault coverage we can increase the dependability parameters. The proposed modification is tested on the railway station safety devices designs implemented in the FPGA.",2010,0,
1283,1284,Using Redundant Threads for Fault Tolerance of OpenMP Programs,"As the wide application of multi-core processor architecture in the domain of high performance computing, fault tolerance for shared memory parallel programs becomes a hot spot of research. For years, checkpointing has been the dominant fault tolerance technology in this field, and recently, many research works have been engaged with it. However, to those programs which deal with large amount of data, checkpointing may induce massive I/O transfer, which will adversely affect scalability. To deal with such a problem, this paper proposes a fault tolerance approach, making use of redundancy, for shared memory parallel programs. Our scheme avoids saving and restoring computational state during the program's execution, hence does not involve I/O operations, so presents explicit advantage over checkpointing in scalability. In this paper, we introduce our approach and the related compiler tool in detail, and give the experimental evaluation result.",2010,0,
1284,1285,A mixed-integer formulation for fault detection and diagnosis: modelling and an illustrative example,"In this paper, we propose a mixed integer optimization for diagnosis of fault events which changes the structure of the system model. The proposed approach aims it selecting the right model of the systems among a bank of models when some faults occur. This approach suits to the case where both abrupt faults (intermittent or permanent such as saturation or sudden shutdown) and incipient faults (continuous) are considered. The optimization helps finding the best combination of faults that have occurred. So doing, we go further than analyzing incidence matrix of residual. Also, this approach allows to introduces fault occurrence logics such as the ones encountered when establishing fault trees.",2002,0,
1285,1286,A Systematic Approach of Improving a Fault Tolerant Fuel Cell Bus Powertrain Safety Level,"This paper focuses on the safety issue of fuel cell bus powertrain system. It demonstrates a systematic way of evaluation and refinement of the safety degree to advance the confidence of a kind of fuel cell bus powertrain system, and meanwhile a specific approach of safety level evaluation is used. Via functionally summing up the potential faults which may appear and cause economic losses or occupant injury, the deficiency of the powertrain is located. Based on the occurrence, the severity and the detecting possibility, the numerical expressions of the faults property are measured. Through combining these evaluating results, a general safety level can be achieved, which directly reflects the inappropriateness that should be adjusted or redesigned. Continuous improvement, including using the passive methods of hardware redundancy and the active methods of fault diagnosis, detection and fault tolerant control in the electric control system is carried out, leading to a much more reliable system and the updated safety level shows the enhanced confidence of the fuel cell powertrain system. In order to get a more practical control system, a hardware in the loop test bench is set up to test the performance of the controllers, the core of the powertrain system, under different working conditions, especially those when faults appear. The actual execution of the approach shows that it is an objective analysis method for the innovative powertrain system and can help to improve the system reliability of the fuel cell bus systematically.",2006,0,
1286,1287,Error Estimation for the Computation of Force Using the Virtual Work Method on Finite Element Models,"In most finite element analysis, the numerical error range of the computed force must be specified. In this paper, an error estimation method for force computation using the virtual work method is presented. The merits of the proposed method include its simplicity and its applicability in force computation when the objects being studied are touching other objects.",2009,0,
1287,1288,Analytically redundant controllers for fault tolerance: Implementation with separation of concerns,"Diversity or redundancy based software fault tolerance encompasses the development of application domain specific variants and error detection mechanisms. In this regard, this paper presents an analytical design strategy to develop the variants for a fault tolerant real-time control system. This work also presents a generalized error detection mechanism based on the stability performance of a designed controller using the Lyapunov Stability Criterion. The diverse redundant fault tolerance is implemented with an aspect oriented compiler to separate and thus reduce this additional complexity. A Mathematical Model of an Inverted Pendulum System has been used as a case study to demonstrate the proposed design framework.",2010,0,
1288,1289,Spatial error concealment based on directional decision and intra prediction,"The paper presents a novel spatial error concealment algorithm based on directional decision and intra prediction. Unlike previous approaches that simultaneously recover the pixels inside a missing macroblock (MB), we propose to recover them 44 block by 44 block. Each missing 1616 MB in an intra frame is first divided into 16 blocks, each with size 44, then recovered block by block using Intra_44 prediction. Previously-recovered blocks can be used in the recovery process afterwards. The principle advantage of this approach is the improved capability of recovering MB with edges and the lower computational complexity. The proposed algorithm has been tested on the H.264/AVC reference software JM7.2. Experimental results demonstrate the advantage of the proposed method.",2005,0,
1289,1290,Research and Assessment of the Reliability of a Fault Tolerant Model Using AADL,"In order to solve the problem of the assessment of the reliability of the fault tolerant system, the work in this paper is devoted to analyze a subsystem of ATC (air traffic control system), and use AADL (architecture analysis and design language) to build its model. After describing the various software and hardware error states and as well as error propagation from hardware to software, the work builds the AADL error model and convert it to GSPN (general stochastic Petri net). Using current Petri Net technology to assess the reliability of the fault tolerant system which is based on ATC as the background, this paper receives good result of the experiment.",2008,0,
1290,1291,Scheduling and voltage scaling for energy/reliability trade-offs in fault-tolerant time-triggered embedded systems,"In this paper we present an approach to the scheduling and voltage scaling of low-power fault-tolerant hard real-time applications mapped on distributed heterogeneous embedded systems. Processes and messages are statically scheduled, and we use process re-execution for recovering from multiple transient faults. Addressing simultaneously energy and reliability is especially challenging because lowering the voltage to reduce the energy consumption has been shown to increase the transient fault rates. In addition, time-redundancy based fault-tolerance techniques such as re-execution and dynamic voltage scaling-based low-power techniques are competing for the slack in the schedules. Our approach decides the voltage levels and start times of processes and the transmission times of messages, such that the transient faults are tolerated, the timing constraints of the application are satisfied and the energy is minimized. We present a constraint logic programming-based approach which is able to find reliable and schedulable implementations within limited energy and hardware resources.",2007,0,
1291,1292,Effective Post-BIST Fault Diagnosis for Multiple Faults,"With the increasing complexity of LSI, built-in self test (BIST) is one of the promising techniques in the production test. From our observation during the manufacturing test, multiple stuck-at faults often exist in the failed chips during the yield ramp-up. Therefore the authors propose a method for diagnosing multiple stuck-at faults based on the compressed responses from BIST. The fault diagnosis based on the compressed responses from BIST was called the post-BIST fault diagnosis (Takahashi et al., 2005, Takamatsu, 2005). The efficiency on the success ratio and the feasibility of diagnosing large circuits are discussed. From the experimental results for ISCAS and STARC03 (Sato et al., 2005) benchmark circuits, it is clear that high success ratios that are about 98% are obtained by the proposed diagnosis method. From the experimental result for the large circuits with 100K gates, the feasibility of diagnosing the large circuits within the practical CPU times can be confirmed. The feasibility of diagnosing multiple stuck-at faults on the post-BIST fault diagnosis was proven",2006,0,
1292,1293,An Approach of Fault Diagnosis for System Based on Fuzzy Fault Tree,"Due to the complicacy of the communication control system (CCS) structure and the variations in operating conditions, the occurrence of a fault inside CCS is uncertain and random. Aiming at limitations of the current fault diagnosis of CCS, the paper presents an approach to fault diagnosis of fuzzy fusion based on fuzzy fault tree. Elaborated method considers the characteristic of diagnostic object to establish fuzzy fault tree, convert the index of fault rate into fuzzy number of fault rate, perform the fuzzy analysis for the fault tree, determine the confidence interval of probability of top event, and achieve fuzzy reasoning diagnosis result. The details of fuzzy number design are described in the paper and an application example of the method is also provided. The results show that the proposed fuzzy fault tree analysis method is effective and available for fault diagnose of CCS.",2008,0,
1293,1294,Sequential Element Design With Built-In Soft Error Resilience,"This paper presents a built-in soft error resilience (BISER) technique for correcting radiation-induced soft errors in latches and flip-flops. The presented error-correcting latch and flip-flop designs are power efficient, introduce minimal speed penalty, and employ reuse of on-chip scan design-for-testability and design-for-debug resources to minimize area overheads. Circuit simulations using a sub-90-nm technology show that the presented designs achieve more than a 20-fold reduction in cell-level soft error rate (SER). Fault injection experiments conducted on a microprocessor model further demonstrate that chip-level SER improvement is tunable by selective placement of the presented error-correcting designs. When coupled with error correction code to protect in-pipeline memories, the BISER flip-flop design improves chip-level SER by 10 times over an unprotected pipeline with the flip-flops contributing an extra 7-10.5% in power. When only soft errors in flips-flops are considered, the BISER technique improves chip-level SER by 10 times with an increased power of 10.3%. The error correction mechanism is configurable (i.e., can be turned on or off) which enables the use of the presented techniques for designs that can target multiple applications with a wide range of reliability requirements",2006,0,
1294,1295,Data hiding for error concealment in H.264/AVC,"Recently, data hiding has been proposed to improve the performance of error concealment algorithms. In this paper, a new data hiding-based error concealment algorithm is proposed, that allows the increase of video quality in H.264/AVC wireless video transmission and real-time applications. Data hiding is used for carrying to the decoder the values of some inner pixels to be used to reconstruct lost macro blocks into intra frames through a bi-linear interpolation process.",2004,0,
1295,1296,Mitigating Soft Errors in System-on-Chip Design,"With the continuous downscaling of CMOS technologies, the reliability has become a major bottleneck in the evolution of the next generation scaling. Technology trends such as transistor downsizing, use of new materials and high performance computer architecture continue to increase the sensitivity of systems to soft errors. Today the technologies are moving into the period of nanotechnologies and system-on-chip (SoC) designs are widely used in most of the applications, the issues of soft errors and reliability in complex SoC designs are set to become and increasingly challenging. This paper gives a review to the soft error in SoC designs and then presents the fault tolerant solution.",2008,0,
1296,1297,Evolution and Search Based Metrics to Improve Defects Prediction,"Testing activity is the most widely adopted practice to ensure software quality. Testing effort should be focused on defect prone and critical resources i.e., on resources highly coupled with other entities of the software application.In this paper, we used search based techniques to define software metrics accounting for the role a class plays in the class diagram and for its evolution over time. We applied Chidamber and Kemerer and the newly defined metrics to Rhino, a Java ECMA script interpreter, to predict version 1.6R5 defect prone classes. Preliminary results show that the new metrics favorably compare with traditional object oriented metrics.",2009,0,
1297,1298,"Cache and memory error detection, correction, and reduction techniques for terrestrial servers and workstations","As the size of the SRAM cache and DRAM memory grows in servers and workstations, cosmic-ray errors are becoming a major concern for systems designers and end users. Several techniques exist to detect and mitigate the occurrence of cosmic-ray upset, such as error detection, error correction, cache scrubbing, and array interleaving. This paper covers the tradeoffs of these techniques in terms of area, power, and performance penalties versus increased reliability. In most system applications, a combination of several techniques is required to meet the necessary reliability and data-integrity targets.",2005,0,
1298,1299,DSP-Based Sensorless Electric Motor Fault Diagnosis Tools for Electric and Hybrid Electric Vehicle Powertrain Applications,"The integrity of electric motors in work and passenger vehicles can best be maintained by frequently monitoring its condition. In this paper, a signal processing-based motor fault diagnosis scheme is presented in detail. The practicability and reliability of the proposed algorithm are tested on rotor asymmetry detection at zero speed, i.e., at startup and idle modes in the case of a vehicle. Regular rotor asymmetry tests are done when the motor is running at a certain speed under load with stationary current signal assumption. It is quite challenging to obtain these regular test conditions for long-enough periods of time during daily vehicle operations. In addition, automobile vibrations cause nonuniform air-gap motor operation, which directly affects the inductances of electric motors and results in a noisy current spectrum. Therefore, it is challenging to apply conventional rotor fault-detection methods while examining the condition of electric motors as part of the hybrid electric vehicle (HEV) powertrain. The proposed method overcomes the aforementioned problems by simply testing the rotor asymmetry at zero speed. This test can be achieved at startup or repeated during idle modes where the speed of the vehicle is zero. The proposed method can be implemented at no cost using the readily available electric motor inverter sensors and microprocessing unit. Induction motor fault signatures are experimentally tested online by employing the drive-embedded master processor (TMS320F2812 DSP) to prove the effectiveness of the proposed method.",2009,0,
1299,1300,Inverse Fault Detection and Diagnosis Problem in Discrete Dynamic Systems,This paper investigates an inverse fault detection and diagnosis problem in discrete dynamic systems. The problem is how to adjust the system parameters according to observation value of inputs and outputs so that the system is concordant. First we formulate the problem as a least square problem with interval coefficients. Then two algorithms for this problem are presented. The first algorithm based on the expected value of observation value of inputs and outputs. We are only required to solve a classical least square problem in this algorithm and the algorithm is robust. The second algorithm by using linear programming approach can deal with large scale systems and suit for on line adjustment.,2007,0,
1300,1301,Acceleration of a model based scatter correction technique for Positron Emission Tomography using high performance computing technique,"Positron Emission Tomography (PET) is a widely used and powerful metabolic imaging technique for functional diagnosis of organs. Compton scattering is a physical effect that results in distortions in the reconstructed image. The model based, so-called, Scatter Simulation (SSS) algorithm is an appropriate solution for scatter correction. However, the SSS algorithm is extremely computation intensive. The application of the SSS algorithm in clinical environment requires the application of high performance computing (HPC) techniques. In this work we give a survey about different high performance computing techniques and introduce the selection process of optimal HPC platform for the implementation of the Single Scatter Simulation algorithm.",2010,0,
1301,1302,What Makes a Good Bug Report?,"In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.",2010,0,
1302,1303,The Measure of human error: Direct and indirect performance shaping factors,"The goal of performance shaping factors (PSFs) is to provide measures to account for human performance. PSFs fall into two categoriesdirect and indirect measures of human performance. While some PSFs such as time to complete a task are directly measurable, other PSFs, such as fitness for duty, can only be measured indirectly through other measures and PSFs, such as through fatigue measures. This paper explores the role of direct and indirect measures in human reliability analysis (HRA) and the implications that measurement theory has on analyses and applications using PSFs. The paper concludes with suggestions for maximizing the reliability and validity of PSFs.",2007,0,
1303,1304,Fault-tolerant design of the IBM pSeries 690 system using POWER4 processor technology,"The POWER4-based p690 systems offer the highest performance of the IBM eServer pSeriesTM line of computers. Within the general-purpose UNIX server market, they also offer the highest levels of concurrent error detection, fault isolation, recovery, and availability. High availability is achieved by minimizing component failure rates through improvements in the base technology, and through design techniques that permit hard- and soft-failure detection, recovery, and isolation, repair deferral, and component replacement concurrent with system operation. In this paper, we discuss the fault-tolerant design techniques that were used for array, logic, storage, and I/O subsystems for the p690. We also present the diagnostic strategy, fault-isolation, and recovery techniques. New features such as POWER4 synchronous machine-check interrupt, PCI bus error recovery, array dynamic redundancy, and minimum-element dynamic reconfiguration are described. The design process used to verify error detecti on, fault isolation, and recovery is also described.",2002,0,
1304,1305,Error Resilient Coding Based on Reversible Data Embedding Technique for H. 264/AVC Video,"In this paper, a prescription-based error concealment (PEC) method is proposed. PEC relies on pre-analyses of the concealment error image (CEI) for I-frames and the optimal error concealment scheme for P-frames at encoder side. CEI is used to enhance the image quality at decoder side after error concealment (by spatial interpolation or zero motion) of the corrupted intra-coded MBs. A set of pre-selected error concealment methods is evaluated for each corrupted inter-coded MB to determine the optimal one for the decoder. Both the CEI and the scheme indices are considered as the prescriptions for decoder and transmitted along with the video bit stream based on a reversible data embedding technique. Experiments show that the proposed method is capable of achieving PSNR improvement of up to 1.48 dB, at a considerable bit-rate, when the packet loss rate is 20%",2005,0,
1305,1306,Emulation of faults and remedial control strategies in a multiphase power converter drive used to analyse fault tolerant drive systems for Aerospace applications,"This paper describes how an experimental test rig, a multiphase power converter drive and its control have been used to emulate failures (of the converter and machine) and control strategies to study a way of achieving fault tolerant drive systems employed especially in Aerospace applications. Experimental results which validate simulation of the emulated faults are presented.",2009,0,
1306,1307,Data diverse fault tolerant architecture for component based systems,"Of late, component based software design has become a major focus in software engineering research and computing practice. These software components are used in a wide range of applications some of which may have mission critical requirements. In order to achieve required level of reliability, these component-based designs have to incorporate special measures to cope up with software faults. This paper presents a fault tolerant component based data driven architecture that is based on C2 architectural framework and implements data diverse fault tolerance strategies. The proposed design makes a trade-off between platform flexibility, reliability and efficiency at run time and exhibits its ability to tolerate faults in a cost effective manner. Application of proposed design is exhibited with a case study.",2009,0,
1307,1308,Rail defect diagnosis using wavelet packet decomposition,"One of the basic tasks in railway maintenance is inspection of the rail in order to detect defects. Rail defects have different properties and are divided into various categories with regard to the type and position of defects on the rail. This paper presents an approach for the detection of defects in rail based on wavelet transformation. Multiresolution signal decomposition based on wavelet transform or wavelet packet provides a set of decomposed signals at distinct frequency bands, which contains independent dynamic information due to the orthogonality of wavelet functions. Wavelet transform and wavelet packet in tandem with various signal processing methods, such as autoregressive spectrum, energy monitoring, fractal dimension, etc., can produce desirable results for condition monitoring and fault diagnosis. Defect detection is based on decomposition of the signal acquired by means of magnetic coil and Hall sensors from the railroad rail, and then applying wavelet coefficients to the extracted signals. Comparing these extracted coefficients provides an indication of the healthy rail from defective rail. Experimental results are presented for healthy rail and some of the more common defects. Deviation of wavelet coefficients in the healthy rail case from the case with defects shows that it is possible to classify healthy rails from defective ones.",2003,0,
1308,1309,Communication strategy and fault-tolerance abilities development in bio-inspired hardware systems with FPGA-based artificial cell network,"The paper deals through computer-aided modeling, numerical simulation and experimental research with the bio-inspired digital systems, in order to implement VLSI hardware which exhibits the abilities of living organisms, such as: evolution capabilities, self-healing and fault-tolerance. The theoretical backgrounds of the work are founded in cellular embryology's basic concepts. In the first stage of the researches a new model for an FPGA-based artificial cell is proposed and developed. Also a new communication strategy inside the cell networks is presented, in order to reproduce with high fidelity the complex phenomena and interaction rules in bio-inspired hardware systems. In the next steps the fault-tolerance and self-healing phenomena between these cells in a bi- dimensional structure is careful analyzed and simulated. The final purpose is to design a bio-inspired hardware system (embryonic machine) with programmable FPGA arrays, for study and experiment basic properties of living organisms.",2008,0,
1309,1310,Novel Convergence Model for Efficient Error Concealment using Information Hiding in Multimedia Streams,Error concealment using information hiding has been an efficient tool to combat channel impairments that degrade the transmitted data quality by introducing channel errors/packet losses. The proposed model takes a stream of multimedia content and the binarised stream is subjected to bit level enhanced mapping procedure (PRASAN - Enhanced NFD approach) accompanied with a set of convergence models that ensure a high degree of convergence for a given error norm. The mapping is performed between the current frames with respect to the previous frame in case of video data. This approach often referred to as the correlation generation is followed by convergence mathematical function generation. This function is derived based on trying out the various convergence methodologies in a weighted round robin environment and choosing the best matching function by computing the mean square error. This error is termed map-fault and is kept a minimum. The test data taken are subjected to noisy channel environments and the power signal to noise ratios obtained experimentally support firmly the advantage of the proposed methodology in comparison to existing approaches,2007,0,
1310,1311,A Pervasive Temporal Error Concealment Algorithm for H.264/AVC Decoder,"In real-time video transmission over error-prone network, the packet loss cannot be avoided which causes the video quality reduction in the destination. In this paper, we propose a pervasive temporal error concealment (PTEC) algorithm for H.264/AVC Inter frame decoding to eliminate the error effect for Human Visual System (HVS). To increase the error concealment (EC) accuracy, the 4x4 block size is used as the basic motion vector (MV) recovery unit, and the MV of the lost macroblock (MB) is recovered by employing the MV information of the neighboring intact MBs. The simulation results show that the proposed algorithm can achieve better EC performance compared with the existing TEC methods. Because of its simple composition, it is pervasive to be used in the real-time multimedia communication systems with the video coding standard H.264/AVC.",2010,0,
1311,1312,Bug-Inducing Language Constructs,"Reducing bugs in software is a key issue in software development. Many techniques and tools have been developed to automatically identify bugs. These techniques vary in their complexity, accuracy and cost. In this paper we empirically investigate the language constructs which frequently contribute to bugs. Revision histories of eight open source projects developed in multiple languages are processed to extract bug-inducing language constructs. Twenty six different language constructs and syntax elements are identified. We find that most frequent bug-inducing language constructs are function calls, assignments, conditions, pointers, use of NULL, variable declaration, function declaration and return statement. These language constructs account for more than 70 percent of bug-inducing hunks. Different projects are statistically correlated in terms of frequencies of bug-inducing language constructs. Developers within a project and between different projects also have similar frequencies of bug-inducing language constructs. Quality assurance developers can focus code reviews on these frequent bug-inducing language constructs before committing changes.",2009,0,
1312,1313,Transient stability prediction algorithm based on post-fault recovery voltage measurements,This paper presents a novel technique for predicting transient stability status of a power system following a large disturbance. The prediction is based on the synchronously measured samples of the fundamental frequency voltage magnitudes at each generation station. The voltage samples taken immediately after clearing the faults are input to a support vector machine classifier to identify the transient stability condition. The classifier is trained using examples of the post-fault recovery voltage measurements (inputs) and the corresponding stability status (output) determined using a power angle-based stability index. Studies with the New England 39-bus system indicate that the proposed algorithm can correctly recognize when the power system is approaching to the transient instability.,2009,0,
1313,1314,Fast vignetting correction and color matching for panoramic image stitching,"When images are stitched together to form a panorama there is often color mismatch between the source images due to vignetting and differences in exposure and white balance between images. In this paper a low complexity method is proposed to correct vignetting and differences in color between images, producing panoramas that look consistent across all source images. Unlike most previous methods which require complex non-linear optimization to solve for correction parameters, our method requires only linear regressions with a low number of parameters, resulting in a fast, computationally efficient method. Experimental results show the proposed method effectively removes vignetting effects and produces images that are highly visually consistent in color and brightness.",2009,0,
1314,1315,A generic method for fault injection in circuits,"Microcircuits dedicated to security in smartcards are targeted by more and more sophisticated attacks like fault attacks that combine physical disturbance and cryptanalysis. The use of simulation for circuit validation considering these attacks is limited by the time needed to compute the result of the chosen fault injections. Usually, this choice is made by the user according to his knowledge of the circuit functionality. The aim of this paper is to propose a generic and semi-automatic method to reduce the number of fault injections using types of data stored in registers (latch by latch)",2006,0,
1315,1316,Scheduling for energy efficiency and fault tolerance in hard real-time systems,"This paper studies the dilemma between fault tolerance and energy efficiency in frame-based real-time systems. Given a set of K tasks to be executed on a system that supports L voltage levels, the proposed heuristic-based scheduling technique minimizes the energy consumption of tasks execution when faults are absent, and preserves feasibility under the worst case of fault occurrences. The proposed technique first finds out the optimal solution in a comparable system that supports continuous voltage scaling, then converts the solution to the original system. The runtime complexity is only (LK<sup>2</sup>). Experimental results show that the proposed approach produces near-optimal results in polynomial time.",2010,0,
1316,1317,Principles of multi-level reflection for fault tolerant architectures,"We present the principles of multi-level reflection as an enabling technology for the design and implementation of adaptive fault tolerant systems. By exhibiting the structural and behavioral aspects of a software component, the reflection paradigm enables the design and implementation of appropriate non-functional mechanisms at a meta-level. The separation of concerns provided by reflective architectures makes reflection a perfect match for fault tolerance mechanisms. However, in order to provide the necessary and sufficient information for error detection and recovery, reflection must be applied to all system layers in an orthogonal manner. This is the main motivation behind the notion of multi-level reflection that is introduced. We describe the basic concepts of this new architectural paradigm, and illustrate them with concrete examples. We also discuss some practical work that has recently been carried out to start implementing the proposed framework.",2002,0,
1317,1318,Fault Behavior in Fire Control System Based on Extendable Petri Net,"Use the Petri net theory and studied out a synthetical expression method of static state and trends behavior in fire control system fault diagnose. The method may be convenient and succinct for fault phenomenon of multilayer rank order and many route. Amount of work is simplified in the fault diagnose system. The fault information characteristic, fault information molde and propagation behavior fashion are expounded. The algorithm for system fault information characteristic is put. The Petri net molde of fault diagnose administrative levels and mixed fault is set up. The fault information propagation route is gained in system analysis. The trends transition of fault behavior is analyzed, to find the solution of system the smallest cut set and path.",2010,0,
1318,1319,The impact of technology scaling on soft error rate performance and limits to the efficacy of error correction,The soft error rate (SER) of advanced CMOS devices is higher than all other reliability mechanisms combined. Memories can be protected with error correction circuitry but SER in logic may limit future product reliability. Memory and logic scaling trends are discussed along with a method for determining logic SER.,2002,0,
1319,1320,Using regression trees to classify fault-prone software modules,"Software faults are defects in software modules that might cause failures. Software developers tend to focus on faults, because they are closely related to the amount of rework necessary to prevent future operational software failures. The goal of this paper is to predict which modules are fault-prone and to do it early enough in the life cycle to be useful to developers. A regression tree is an algorithm represented by an abstract tree, where the response variable is a real quantity. Software modules are classified as fault-prone or not, by comparing the predicted value to a threshold. A classification rule is proposed that allows one to choose a preferred balance between the two types of misclassification rates. A case study of a very large telecommunications systems considered software modules to be fault-prone, if any faults were discovered by customers. Our research shows that classifying fault-prone modules with regression trees and the using the classification rule in this paper, resulted in predictions with satisfactory accuracy and robustness.",2002,0,
1320,1321,An RT-level fault model with high gate level correlation,"With the advent of new RT-level design and test flows, new tools are needed to migrate at the RT-level the activities of fault simulation testability analysis, and test pattern generation. This paper focuses on fault simulation at the RT-level, and aims at exploiting the capabilities of VHDL simulators to compute faulty responses. The simulator was implemented as a phototypical tool, and experimental results show that simulation of a faulty circuit is no more costly than simulation of the original circuit. The reliability of the fault coverage figures computed at the RT-level is increased thanks to an analysis of inherent VHDL redundancies, and by foreseeing classical synthesis optimizations. A set of rules is used to compute a fault list that exhibits good correlation with stuck-at faults",2000,0,
1321,1322,Spatial Optical Distortion Correction in an FPGA,"Due to the complexities of the image processing algorithms, correcting spatial distortion of optical images quickly and efficiently is a major challenge. This paper describes an efficient pipelined parallel architecture for optical distortion correction in imaging systems using a low cost FPGA device. The proposed architecture produces a fast, almost realtime solution for the correction of image distortion implemented using VHDL HDL with a single Xilinx FPGA XCS31000-4 device. The experimental results show that the barrel and pincushion distortion can be corrected with a very low residual error. The system architecture can be applied to other imaging processing algorithms in optical systems",2006,0,
1322,1323,A new algorithm for atmospheric correction of the multiangular and hyperspectral data acquired during the DAISEX campaign,"The main scientific objective of DAISEX (Digital Airborne Spectrometer Experiment) was to demonstrate the retrieval of geo/biophysical variables from imaging spectrometer data. Target variables included surface temperature, Leaf Area Index (LAI), canopy biomass, leaf water content, canopy height, canopy structure and soil properties. The imaging spectrometers used for DAISEX were the DAIS-7915, HyMap and POLDER. The campaign took place during the summers of 1998, 1999 and 2000 in Barrax (Spain) and Colmar (France). A new algorithm is under development for the atmospheric correction of the hyperspectral and multiangular data acquired during this campaign. This algorithm is intended to improve the current atmospheric correction by taking into account the coupling between atmosphere and surface (including a non-Lambertian treatment of the latter). Moreover, the hyperspectral data allow to characterise in detail the absorption and the multiangular characteristics of the data allow to describe accurately the aerosol scattering. The method consists in identifying some pixels on an image with an a priori information about its BRDF and assume that the atmosphere is the same over the whole image. Applying a radiative transfer code we can reproduce the reflectance measured by the sensor by modifying the parameters describing the surface and the aerosols through an iterative process. Once the atmosphere is known the system atmosphere-surface is uncoupled and the reflectance for the whole image can be obtained.",2003,0,
1323,1324,Design by extrapolation: an evaluation of fault-tolerant avionics,"Over the past 30 years, safety-critical avionics systems such as fly-by-wire (FBW) flight controls, full-authority digital engine controls, and other systems have been introduced on many commercial and military airplanes and spacecraft. Early FBW systems, such as on the F-16 and Airbus A320, were considered revolutionary and were introduced with extreme caution. These early systems and their successors all make use of redundant and fault-tolerant avionics to provide the required dependability and safety, but have used significantly different architectures. This paper examines the different levels of criticality and fault tolerance required by different types of avionics systems, establishes architectural categories of fault-tolerant architectures, and identifies the discriminating features of the different approaches. Examples of discriminators include the level of redundancy, methods of engaging backup systems, protection from software errors, and the use of dissimilar hardware and software. The strengths and weaknesses of the different approaches are identified. The paper concludes with some speculation on trends for future systems based on this evaluation of previous systems",2001,0,
1324,1325,Do stack traces help developers fix bugs?,"A widely shared belief in the software engineering community is that stack traces are much sought after by developers to support them in debugging. But limited empirical evidence is available to confirm the value of stack traces to developers. In this paper, we seek to provide such evidence by conducting an empirical study on the usage of stack traces by developers from the ECLIPSE project. Our results provide strong evidence to this effect and also throws light on some of the patterns in bug fixing using stack traces. We expect the findings of our study to further emphasize the importance of adding stack traces to bug reports and that in the future, software vendors will provide more support in their products to help general users make such information available when filing bug reports.",2010,0,
1325,1326,"Using composition to design secure, fault-tolerant systems","Complex systems must be analyzed in smaller pieces. Analysis must support both bottom-up (composition) and top-down (refinement) development, and it must support the consideration of several critical properties, e.g., functional correctness, fault tolerance and security, as appropriate. We describe a mathematical framework for performing composition and refinement analysis and discuss some lessons learned from its application. The framework is written and verified in PVS",2000,0,
1326,1327,Emission-based scatter correction in SPECT imaging,"Scatter correction in single photon emission computed tomography (SPECT) has been focused on either using multiple-window acquisition technique or the scatter modeling technique in iterative image re-construction. We propose a technique that uses only the emission data for scatter correction in SPECT. We assume that the scatter data can be approximated by convolving the primary data with a scatter kernel followed by the normalization using the scatter-to-primary ratio (SPR). Since the emission data is the superposition of the primary data and the scatter data, the convolution normalization process approximately results in the sum of the scatter data and a convolved version of scatter data with the kernel. By applying a proper scaling factor, we can make the estimation approximately equal to or less than the scatter data anywhere in the projection domain. Phantom and patient cardiac SPECT studies show that using the proposed emission-based scatter estimation can effectively reduce the scatter-introduced background in the reconstructed images. And additionally, the computational time for scatter correction is negligible as compared to no scatter correction in iterative image reconstruction.",2010,0,
1327,1328,Bio - Inspired & Traditional Approaches to Obtain Fault Tolerance,"Applying some observable phenomena from cells, focused on their organization, function, control and healing mechanisms, a simple fault tolerant implementation can be obtained. Traditionally, fault tolerance has been added explicitly to a system by including redundant hardware and/or software, which takes over when an error has been detected. These concepts and ideas have been applied before with the triple modular redundancy. Our approach is to design systems where redundancy was incorporated implicitly into the hardware and to mix bio-inspired and traditional approaches to deal with fault tolerance. These ideas are shown using a discrete cosine transform (application) as organ, its MAC (function) interconnected as cell and parity redundancy checker (error detector) as immune system to obtain a fault tolerance design",2006,0,
1328,1329,Comparison of worst case errors in linear and neural network approximation,Sets of multivariable functions are described for which worst case errors in linear approximation are larger than those in approximation by neural networks. A theoretical framework for such a description is developed in the context of nonlinear approximation by fixed versus variable basis functions. Comparisons of approximation rates are formulated in terms of certain norms tailored to sets of basis functions. The results are applied to perceptron networks,2002,0,
1329,1330,Application of multi-agent in control and fault diagnosis systems,"Multi-agent system with distributed structure is an important research field in intelligent control and fault diagnosis system. Based on the research of cooperation and coordination function of multi-agent, a systematic structure which integrates control, diagnosis and monitoring is established and those models, cooperation strategy, reasoning machine are also designed. This new decentralization system has been successfully used in actual product line, and provides a new way for industrial control problem.",2004,0,
1330,1331,Model-based information extraction method tolerant of OCR errors for document images,"A new method for information extraction from document images is proposed in this paper as the basis for a document reader which can extract required keywords and their logical relationship from various printed documents. Such documents obtained from OCR results may have not only unknown words and compound words, but also incorrect words due to OCR errors. To cope with OCR errors, the proposed method adopts robust keyword matching which searches for a string pattern from two dimensional OCR results consisting of a set of possible character candidates. This keyword matching uses a keyword dictionary that includes incorrect words with typical OCR errors and segments of words to deal with the above difficulties. After keyword matching, a global document matching is carried out between keyword matching results in an input document and document models which consist of keyword models and their logical relationship. This global matching determines the most suitable model for the input document and solves word segmentation problems accurately even if the document has unknown words, compound words, or incorrect words. Experimental results obtained for 100 documents show that the method is robust and effective for various document structures",2001,0,
1331,1332,Notice of Retraction<BR>An intelligent method for the control of magnitude of parabolic-like transmission error of a pair of gears,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Traditionally, the control of magnitude of parabolic-like transmission error of a pair of gears requires a lot of time-consuming trial-and-error manual procedures. To advance design efficiency, this paper has proposed an intelligent method to control efficiently and accurately the magnitude of parabolic-like transmission error. The intelligent method is devised based on the development of a system of governing equations under the conditions of tooth contact and constraints for magnitude of parabolic-like transmission error. Design parameters required to be determined are transformed into the roots of the system of equations. Just applying Newton's root finding method, parameters needed to be designed are obtained automatically and efficiently. To show how to apply the proposed method, a pair of external gears composed of an involute gear and a circular-arc gear is adopted to be an example. The gear pair is verified numerically the magnitude of parabolic-like transmission error is really controlled.",2010,0,
1332,1333,Detection and Classification of Wood Defects by ANN,"X-ray as a method of measurement was adopted to detect wood defects nondestructively. Due to the intensity of x-ray that crosses the object changes, defects in wood were detected by the difference of X-ray absorption parameter, and therefore it used computer to process and analyze the image. On the basis of image processing of nondestructive testing and characteristic construction, defects mathematic model were established by using characteristic parameters. According to signal characters of nondestructive testing, artificial neural networks were set up. Meanwhile, adopt BP networks model to recognize all characteristic parameters, which reflected characters of wood defects. BP networks used coefficient matrix of each unit, including input layer, intermediate layer (concealed layer) and output layer, to get the model of input vector and finish networks recognition through the networks learning. The test results show that the method is very successful for detection and classification of wood defects",2006,0,
1333,1334,Fault isolation in discrete event systems by observational abstraction,"We propose a method for fault isolation in discrete event systems such as object oriented control systems, where the observations are the logged error messages. The method is based on automatic abstraction that preserves only the behavior relevant to fault isolation. In this way we avoid the state space explosion, and a model checker can be used to reason about the temporal properties of the system. The result is a fault isolation table that maps possible error logs to isolated faults, and fault isolation thus reduces to table lookup. The fault isolation table can also be used as an analysis tool at the design level to find both faults that cannot be isolated as well as redundant error messages.",2003,0,
1334,1335,FIONA: a fault injector for dependability evaluation of Java-based network applications,"The use of network applications for high availability systems requires the validation of its fault tolerance mechanisms to avoid unexpected behavior during execution. FIONA is a fault injection tool to experimentally validate these mechanisms of Java distributed applications. The tool uses JVMTI, a new interface for the development of debugging and monitoring tools that enables the instrumentation of Java applications. This approach provides complete transparency between the application under test and the fault injection tool, as well as portability. FIONA injects communication faults, making it possible to conduct the dependability evaluation of UDP based network protocols developed in Java.",2004,0,
1335,1336,Neural vision sensors for surface defect detection,"Vision sensors are built from a camera and intelligent hardware and/or software. Steadily decreasing microelectronic costs have spawned a large number of vision sensory applications, such as surface defect detection. A constructive method for defect detection entails a mixture of mathematical and intelligent modules. Such a heterogeneous modular system can be realized in many ways. In this paper we discuss a packet-switched implementation on a macro-enriched field-programmable gate-array.",2004,0,
1336,1337,Analysis of Error Sources Towards Improved Form Processing,"Automatic form processing is an important application of document analysis subject. Such a system requires to be trained and tested on a standard database of forms collected from real-life. However, to the best of our knowledge, the only such available databases are NIST Special Databases. These databases consist of images of synthesized form documents. On the other hand, recently we developed a form database, samples of which had been taken from the real-life. ISIFormReader, a form processing system, also developed recently, has been tested using these real-life samples. An intensive study of the processing errors showed that writers' idiosyncracies are one of the major reasons of such errors as analyzed in U. Bhattacharya, et al., (2006). In the present paper, we investigated various other sources of errors which together cause a major concern. These include sample forms which are low in contrast, noisy, smudgy, skewed, scaled disturbing its aspect ratio and so on. An analysis of errors due to similar such sources is important towards development of an improved form processing system.",2006,0,
1337,1338,A genetic algorithm for automated horizon correlation across faults in seismic images,"Finding corresponding seismic horizons which have been separated by a fault is still performed manually in geological interpretation of seismic images. The difficulties of automating this task are due to the small amount of local information typical for those images, resulting in a high degree of interpretation uncertainty. Our approach is based on a model consisting of geological and geometrical knowledge in order to support the low-level image information. Finding the geologically most probable matches of several horizons across a fault is a combinatorial optimization problem, which cannot be solved exhaustively since the number of combinations increases exponentially with the number of horizons. A genetic algorithm (GA) has been chosen as the most appropriate strategy to solve the optimization problem. Our implementation of a GA is adapted to this particular problem by introducing geological knowledge into the solution process. The results verify the suitability of the method and the appropriateness of the parameters chosen for the horizon correlation problem.",2005,0,
1338,1339,Fast and Accurate Automatic Defect CLuster Extraction for Semiconductor Wafers,"Reduction in integrated circuit (IC) half technology, which will no longer be sustainable by traditional fault isolation and failure analysis techniques. There is an urgent need for diagnostic software tools with (which manifest as clusters) observed from manufacturing defects can be traced back to a specific process, equipment or technology, a novel data mining algorithm defects from test data logs. This algorithm and provides accurate detection of 99%.",2010,0,
1339,1340,SDG-based fault diagnosis and application based on reasoning method of granular computing,"Signed directed graph (SDG) as a qualitative model is used in fault diagnosis, because it can express causal relationships among variables of large-scale complex industry systems. However, many relevant results are included in diagnostic conclusions leading to low resolution in based-SDG fault diagnosis, so to solve this problem, granule is used to formally express the elements of SDG model in this paper, after that granular base containing knowledge which reflects the causal relation of faults and symptoms is constructed. And a searching and reasoning method based on granule is used in searching of fault source, consequently fault source is obtained by searching granular base and computing the most similarity. So the resolution could be improved. A 65t/h steam boiler system is taken as an example in the paper, and its answer show the method is feasible.",2010,0,
1340,1341,Skew Detection and Correction Method of Fabric Images Based on Hough Transform,"To solve the skew situation of scanned fabric image, a method based on Hough transform for skew detection and correction in fabric images was presented. By combining the characteristics of fabric images and the weft direction information extracted by Sobel operator, this method performed hierarchical Hough transform on the weft boundary to detect the skew angle of fabric image. Finally, a rotation algorithm based on the image linear storage structure was introduced, and the skew image was corrected rapidly. The skew detect algorithm has been experimented on various skew angles of fabric image and very promising results have been achieved given more than 99% accuracy. Experimental results show that the proposed method with high adaptability is more accurate and rapidly than traditional Hough transform.",2009,0,
1341,1342,Fiber Optical Gyro Fault Diagnosis based on Wavelet Transform and Neural Network,"Fault diagnosis plays an important role in detecting the reliability of integrated navigation. This paper proposed an intelligent method which combined wavelet transform with neural network to enhance efficiency. The combined method between wavelet transform and neural network was in series. Based on Daubechies, wavelet symmetry had been constructed. Through Daubechies eight-layer wavelet decomposing, detailed information of eight layers was achieved. Then the 8-dimensional eigenvector was used to train three-layer RBF neural network as fault sample. For RBF network is good at classifying, the network can detect a fault on-line after training. At the same time, it can classify faults and alarm. Gyro signals were chosen as the simulation inputs, the results indicated the method's applicability and effectiveness.",2008,0,
1342,1343,Development of customized distribution automation system (DAS) for secure fault isolation in low voltage distribution system,"This paper presents the development of customized distribution automation system (DAS) for secure fault isolation at the low voltage (LV) down stream, 415/240 V by using the Tenaga Nasional Berhad (TNB) distribution system. It is the first DAS research work done on customer side substation for operating and controlling between the consumer side system and the substation in an automated manner. Most of the work is focused on developing very secure fault isolation whereby the fault is detected, identified, isolated and remedied in few seconds. Supervisory Control and Data Acquisition (SCADA) techniques has been utilized to build Human Machine Interface (HMI) that provides a graphical operator interface functions to monitor and control the system. Microprocessor based Remote Monitoring Devices have been used for customized software to be downloaded to the hardware. Power Line Carrier (PLC) has been used as communication media between the consumer and the substation. As result, complete DAS fault isolation system has been developed for cost reduction, maintenance time saving and less human intervention during faults.",2008,0,
1343,1344,"Faults identification, location and characterization in electrical systems using an analytical model-based approach","The start of the electrical energy market has encouraged distributors to make new investments at distribution level so as to attain higher quality levels. The service continuity is one of the aspects of greater importance in the definition of the quality of the electrical energy. For this reason, the research in the field of fault diagnostic for distribution systems is spreading ever more. This paper presents a novel methodology to identify, locate and characterize the faulty events in the electrical distribution systems. The methodology can be extended to all types of faulty events and is applicable to reconfigurable systems. After having described the guidelines of the methodology, the Authors describe the architecture of the diagnostic control system implementing the analytic procedure. Finally the results of some relevant applications are reported.",2005,0,
1344,1345,A length compensation method to eliminate the varying length defect in one dimensional fisheye views,"Graphical fisheye view is an effective technique for visualizing and navigating large information structures. However, there are still technical difficulties that hinder its broader applications. One of the prominent problems is the varying length effect seen in most fisheye views. The varying length effect refers to a phenomenon that the length (or height) of a fisheye component is not fixed, but it varies with the location of the focal point. This effect may bring some disadvantages that reduce the usability of a fisheye component. To overcome this defect, sporadical solutions have been seen for specific implementations, but a systematic method has not been seen yet. This paper proposes a length compensation method to eliminate the varying length defect for one dimensional fisheye components. The method provides solutions for handling both discrete and continuous magnifications respectively. The mathematical foundation of the method is given, and the implemented prototype proves that it is effective.",2010,0,
1345,1346,Optimizing Cauchy Reed-Solomon Codes for Fault-Tolerant Network Storage Applications,"In the past few years, all manner of storage applications, ranging from disk array systems to distributed and wide-area systems, have started to grapple with the reality of tolerating multiple simultaneous failures of storage nodes. Unlike the single failure case, which is optimally handled with RAID level-5 parity, the multiple failure case is more difficult because optimal general purpose strategies are not yet known. Erasure coding is the field of research that deals with these strategies, and this field has blossomed in recent years. Despite this research, the decades-old Reed-Solomon erasure code remains the only space-optimal (MDS) code for all but the smallest storage systems. The best performing implementations of Reed-Solomon coding employ a variant called Cauchy Reed-Solomon coding, developed in the mid 1990's. In this paper, we present an improvement to Cauchy Reed-Solomon coding that is based on optimizing the Cauchy distribution matrix. We detail an algorithm for generating good matrices and then evaluate the performance of encoding using all implementations Reed-Solomon codes, plus the best MDS codes from the literature. The improvements over the original Cauchy Reed-Solomon codes are as much as 83% in realistic scenarios, and average roughly 10% over all cases that we tested",2006,0,
1346,1347,Software Fault Protection with ARINC 653,"With flight software becoming ever more complex, assuming that it behaves perfectly is no longer realistic. At the same time Verification and Validation (V&V) is consuming up to 50% of flight software development costs. The adaptation of fault protection concepts to flight software is attractive, particularly in the context of the fault containment and health management capabilities of ARINC 653. We propose a proactive, unified, model-based approach in which the behavior of the software is monitored against a model of its expected behavior. We describe how that may be incorporated into the ARINC 653 health management architecture. We describe software capabilities that facilitate software fault protection. These capabilities include enhancements to the ARINC 653 application executive, tools for software instrumentation, and a temporal logic runtime monitoring framework for high-level specification and monitoring. We analyze the aspects of the software that should be modeled and the types of failure responses. We show how these concepts may be applied to the Mission Data System (MDS) flight software framework.",2007,0,
1347,1348,Fractal-ANN Tool for Classification of Impulse Faults in Transformers,"Transformers are impulse tested in laboratory to assess their insulation strength against atmospheric lightning strikes. Inadequate insulation may cause transformer winding to fail during such tests. Detection of such faults is an important issue for repair and maintenance of such transformers. This paper describes the application of the concept of fractal geometry to obtain the features inherent in the impulse response of transformers subjected to impulse test. Fractal features such as fractal dimension (calculated by Higuchi, Kartz, Petrosian and Box counting methods), lacunarity and entropy has been used for collection of proper features from the current waveforms. Artificial Neural Network (ANN) has been used to classify the patterns inherent in the features extracted from Fractal analysis. The complex nature of transformer winding and its impulse response gives rise to a complex non-linear pattern of fractal features. In this regard, the application of ANN for pattern classification has greatly reduced the complexity and at the same time increased the accuracy in the fault localization and identification system. The proposed tool has been tested to identify the type and location of faults by analyzing experimental impulse responses of analog and digital transformer models.",2005,0,
1348,1349,Automated Diagnosis of Product-Line Configuration Errors in Feature Models,"Feature models are widely used to model software product-line (SPL) variability. SPL variants are configured by selecting feature sets that satisfy feature model constraints. Configuration of large feature models can involve multiple stages and participants, which makes it hard to avoid conflicts and errors. New techniques are therefore needed to debug invalid configurations and derive the minimal set of changes to fix flawed configurations. This paper provides three contributions to debugging feature model configurations: (1) we present a technique for transforming a flawed feature model configuration into a constraint satisfaction problem (CSP) and show how a constraint solver can derive the minimal set of feature selection changes to fix an invalid configuration, (2) we show how this diagnosis CSP can automatically resolve conflicts between configuration participant decisions, and (3) we present experiment results that evaluate our technique. These results show that our technique scales to models with over 5,000 features, which is well beyond the size used to validate other automated techniques.",2008,0,
1349,1350,A COTS wrapping toolkit for fault tolerant applications under Windows NT,"The paper presents a software toolkit that allows one to enhance the fault tolerant characteristics of a user application running under a Windows NT platform through sets of interchangeable and customizable fault tolerant interposition agents (FTI agents). Interposition agents are non-application software programs executed in an intermediate layer between the software application and the operating system in order to wrap the application software, intercepting and possibly modifying all the communications between the application and the surrounding hardware and software environment. The process is completely transparent to both the user application and the operating system and allows the achievement of a high degree of software based reliability in a wide variety of domains",2000,0,
1350,1351,Implementation of a quantum corrections in a 3D parallel drift-diffusion simulator,"We describe an implementation of density-gradient quantum corrections in a 3D drift-diffusion (D-D) semiconductor simulator based on finite element method. Mesh efficiency of the 3D semiconductor device simulator with quantum mechanical corrections is achieved by parallelisation of the code for a memory distributed multiprocessor environment. The Poisson equation, the current continuity equation, and the density gradient equation with an appropriate finite element discretisation have to be solved iteratively. Moreover, parallel algorithms are employed to speed up the self-consistent solution. In order to test our 3D semiconductor device simulator, we have carried out a careful calibration against experimental I-V characteristics of a 67 nm Si MOSFET achieving an excellent agreement. Then we demonstrate a relative impact of quantum mechanical corrections in this device.",2007,0,
1351,1352,Error resilient H.264/AVC video over satellite for low packet loss rates,"The performance of video over satellite is simulated. The error resilience tools of intra macroblock refresh and slicing are optimized for live broadcast video over satellite. The improved performance using feedback, using a cross- layer approach, over the satellite link is also simulated. The new Inmarsat BGAN system at 256 kbit/s is used as test case. This systems operates at low loss rates guaranteeing a packet loss rate of not more than 10~3. For high-end applications as 'reporter-in-the-field' live broadcast, it is crucial to obtain high quality without increasing delay.",2007,0,
1352,1353,An effective eye gaze correction operation for video conference using antirotation formulas,"The deviated eye gaze problem in video conferencing has been known and studied for many years. This paper suggests a simple and novel approach for face reorientation in a monocular setting, which is done by performing a combined rotation and antirotation operation on the image. Our approach on face reorientation does not require 3D modeling, registration or texture mapping. It is simple, efficient and robust. To make the eye gaze correction complete, an image warping technique is used for modifying eyelids and a simple transformation is used for correcting eye glares.",2003,0,
1353,1354,Automatic Identification of Bug-Introducing Changes,"Bug-fixes are widely used for predicting bugs or finding risky parts of software. However, a bug-fix does not contain information about the change that initially introduced a bug. Such bug-introducing changes can help identify important properties of software bugs such as correlated factors or causalities. For example, they reveal which developers or what kinds of source code changes introduce more bugs. In contrast to bug-fixes that are relatively easy to obtain, the extraction of bug-introducing changes is challenging. In this paper, we present algorithms to automatically and accurately identify bug-introducing changes. We remove false positives and false negatives by using annotation graphs, by ignoring non-semantic source code changes, and outlier fixes. Additionally, we validated that the fixes we used are true fixes by a manual inspection. Altogether, our algorithms can remove about 38%~51% of false positives and 14%~15% of false negatives compared to the previous algorithm. Finally, we show applications of bug-introducing changes that demonstrate their value for research",2006,0,
1354,1355,Taming coincidental correctness: Coverage refinement with context patterns to improve fault localization,"Recent techniques for fault localization leverage code coverage to address the high cost problem of debugging. These techniques exploit the correlations between program failures and the coverage of program entities as the clue in locating faults. Experimental evidence shows that the effectiveness of these techniques can be affected adversely by coincidental correctness, which occurs when a fault is executed but no failure is detected. In this paper, we propose an approach to address this problem. We refine code coverage of test runs using control- and data-flow patterns prescribed by different fault types. We conjecture that this extra information, which we call context patterns, can strengthen the correlations between program failures and the coverage of faulty program entities, making it easier for fault localization techniques to locate the faults. To evaluate the proposed approach, we have conducted a mutation analysis on three real world programs and cross-validated the results with real faults. The experimental results consistently show that coverage refinement is effective in easing the coincidental correctness problem in fault localization techniques.",2009,0,
1355,1356,Automated fault location system for primary distribution networks,"This paper presents the development, simulation results, and field tests of an automated fault location system for primary distribution networks. This fault location system is able to identify the most probable fault locations in a fast and accurate way. It is based on measurements provided by intelligent electronic devices (IEDs) with built-in oscillography function, installed only at the substation level, and on a database that stores information about the network topology and its electrical parameters. Simulations evaluate the accuracy of the proposed system and the experimental results come from a prototype installation.",2005,0,
1356,1357,Soft error considerations for computer web servers,"Soft errors are caused by cosmic rays striking sensitive regions in electronic devices. Termed as single event upset (SEU), in the past this phenomenon mostly affected the high altitude systems or avionics. The small geometries of today's nanodevices and their use in high-density and high-complexity designs make electronic systems sensitive even to the ground-level radiation. Therefore, large computer systems like workstations or computer web servers have become major victims of single event upsets. Given that the idea of cloud computing is an unavoidable trend for the next generation internet, which might involve almost every company in the IT industry, the urgency and criticality of the reliability rise higher then ever. This paper illustrates how soft errors are a reliability concern for computer servers. The soft error reduction techniques that are significant for the IT industry are summarized and a possible soft error rate (SER) reduction method that considers the cosmic ray striking angle to redesign the circuit board layout is proposed.",2010,0,
1357,1358,An Area-Efficient Approach to Improving Register File Reliability against Transient Errors,"This paper studies approaches to exploiting the space both within or across registers efficiently for improving the register file reliability against transient errors. The idea of our approach is based on the fact that a large number of register values are narrow (i.e., less than or equal to 16 bits for a 32-bit architecture); therefore, the upper 16 bits of the registers can be used to replicate the short operands for enhancing register integrity. This paper also adapts a prior register replication approach by selectively copying register values (i.e., long operands only) to the unused physical registers for enhancing reliability without incurring significant hardware cost. Our experiments indicate that on average, 993% register reads (regardless of short or long operands) can find their replicas available, implying significant improvement of register file integrity against transient errors.",2007,0,
1358,1359,Correction of the interpolation error of quadro-phase detection in interferometry,Laser interferometers using quadro-phase detectors for the fringe counting are frequently used for ultra-precise displacement measurements. The accuracy of interferometers in nanometrology is limited mainly by the interpolation error of the detector system. The new method based on applications of curve fitting using nonlinear last square method was developed for correcting the interpolation error of quadro-phase detectors. The results from the experimental data obtained in interferometrical comparator CMI IK-1 are presented.,2000,0,
1359,1360,GRIDTS: A New Approach for Fault-Tolerant Scheduling in Grid Computing,"This paper proposes GRIDTS, a grid infrastructure in which the resources select the tasks they execute, on the contrary to traditional infrastructures where schedulers find resources for the tasks. This solution allows scheduling decisions to be made with up-to-date information about the resources, which is difficult in the traditional infrastructures. Moreover, GRIDTS provides fault-tolerant scheduling by combining a set of fault tolerance techniques to cope with crash faults in components of the system. The solution is mainly based a tuple space, which supports the scheduling and also provides support for the fault tolerance mechanisms.",2007,0,
1360,1361,Combining dynamic fault trees and event trees for probabilistic risk assessment,"As system analysis methodologies, both event tree analysis (ETA) and fault tree analysis (FTA) are used in probabilistic risk assessment (PRA), especially in identifying system interrelationships due to shared events. Although there are differences between them, ETA and FTA, are so closely linked that fault trees (FT) are often used to quantify system events that are part of event tree (ET) sequences (J.D. Andrew et al., 2000). The logical processes employed to evaluate ET sequences and quantify the consequences are the same as those used in FTA. Although much work has been done to combine FT and ET, traditional methods only concentrate on combining static fault trees (SFT) and ET. Our main concern is considering how to combine dynamic fault trees (DFT) and ET. We proposed a reasonable approach in this paper, which is illustrated through a hypothetical example. Because of the complexity of dynamic systems, including the huge size and complicated dependencies, there may exist contradictions among different dynamic subsystems. The key benefit of our approach is that we avoid the generation of such contradictions in our model. Another benefit is that efficiency may be improved through modularization.",2004,0,
1361,1362,A New Diagnostic Model for Identifying Parametric Faults,"This paper presents a new approach to failure detection and isolation (FDI) for systems modeled as an interconnection of subsystems that are each subject to parametric faults. This paper develops the concept of a diagnostic model and the concept of a fault emulator which are used to model and parameterize subsystem faults. There are two stages to the FDI scheme. In the first stage there is a requirement to identify the diagnostic model. Once identified, the diagnostic model is used in the second stage to generate a residual. Artifacts within the measured residual are then used as a basis for identifying parametric faults. The scheme is distinct from others as it does not require an online recursive least squares type identifier.",2010,0,
1362,1363,A Posteriori Error Estimation and Adaptive Mesh Refinement Controlling in Finite Element Analysis of 3-D Steady State Eddy Current Fields,Several methods of <i>a posteriori</i> error estimation and adaptive refinement controlling in finite element analysis of 3-D steady-state eddy current field are described in this paper. An improved Z-Z method and a more efficient method of CIL are presented. The numerical models of TEAM Workshop Problem 7 and 21A are used to verify the validity of the presented method.,2010,0,
1363,1364,Label-based path switching and error-free forwarding in a prototype optical burst switching node using a fast 44 optical switch and shared wavelength conversion,We demonstrate for the first time optical burst switching using fast and scalable EO switch and a shared wavelength converter for contention resolution. 3.5 m label routing of variable-length bursts and error-free operation was achieved at 10 Gbps payload.,2006,0,
1364,1365,Design of fault diagnosis system for 3D Laser Scanning Machine based on internet,"According to the weakness of 3D Laser Scanning Machine in fault diagnosis, a new design approach for fault diagnosis has been developed. The new system consists of three layers, including the local fault diagnosis, fault service centre and multi-user cooperative diagnosis. The local fault diagnosis system, which is embedded in the existing CNC system, offers on-line, off-line diagnosis, fault compensation and remote communication services. With the support of the fault diagnosis database, the local fault diagnosis system chooses two agents that supply the user with a different view on problem as its intelligent analyzer to retrieve the optimal checkpoints. In addition, it builds the local fault diagnosis engine as a web service to facilitate users to share their diagnosis resource with others. The fault service center, which is supported by the customer service database, is designed to offer advanced suggestions to customers when their local fault diagnosis system fails to remove the malfunctions independently. It mainly provides four functions including fault diagnosis BBS, web-based retrieval, customer register and remote communication. It also adopts two agents to deal with web-based retrieval. The remote communication module supported by Audio/Video guide makes remote diagnosis effective. In order to acquire all of the users diagnosis resource, the customer register module registers all users web services in the registry; therefore the fault service center could bind and execute the customers web services when they are confused by a certain difficult problem. Finally, the key techniques for the system building are introduced, and the feasibility of the system is also testified.",2006,0,
1365,1366,Correction strategy for view maintenance anomaly after schema and data updating concurrently,"During maintaining the materialized view in the data warehouse, how to efficiently handle the concurrent updates is an important and intractable problem. The paper discusses typical situations that schema changes mix with data updates concurrently. And the reasons why concurrent updates result in view maintenance anomaly are analyzed. Based on the analysis, an enhanced commit agent is designed for dealing with non-order commit problem. Thus, the consistency between data warehouse and data source is guaranteed.",2005,0,
1366,1367,Assessing Fault Sensitivity in MPI Applications,"Today, clusters built from commodity PCs dominate high-performance computing, with systems containing thousands of processors now being deployed. As node counts for multi-teraflop systems grow to thousands and with proposed petaflop system likely to contain tens of thousands of nodes, the standard assumption that system hardware and software are fully reliable becomes much less credible. Concomitantly, understanding application sensitivity to system failures is critical to establishing confidence in the outputs of large-scale applications. Using software fault injection, we simulated single bit memory errors, register file upsets and MPI message payload corruption and measured the behavioral responses for a suite of MPI applications. These experiments showed that most applications are very sensitive to even single errors. Perhaps most worrisome, the errors were often undetected, yielding erroneous output with no user indicators. Encouragingly, even minimal internal application error checking and program assertions can detect some of the faults we injected.",2004,0,
1367,1368,Geometric correction of scanned topographic maps using capable input information,"For making digital maps by using the raster-vector conversion of printed binary topographic maps, one of the problems is how to correct geometric distortion that originates in the habit of individual scanner. To use innumerable resources of printed binary topographic maps effectively, we propose an interactive interface and geometric correction algorithms, which uses peculiar coordinates information in individual map. The examples prove that the interactive interface using a cross cursor is useful and efficient for getting accurate input coordinates, and the proposed correction algorithm demonstrates high accuracy of corrected coordinates.",2003,0,
1368,1369,GIS reliability analysis based trapezoid fuzzy fault tree,"GIS reliability refers to the ability to complete the requirements under the specified conditions and time. In this paper, there are five elements including object, conditions of use, use of time, functions and capabilities, to evaluate GIS reliability proposed by the combination in a engineering GIS. The paper imported trapezoid fuzzy fault tree into GIS reliability analysis for the fist time. The paper discussed how GIS reliability analysis uses trapezoid fuzzy fault tree method, mainly researches two problems that are GIS trapezoid fuzzy fault tree establishing and analysis step of GIS trapezoid fuzzy fault tree; uses example to adopt trapezoid fuzzy fault tree method computing GIS reliability analysis; because the technique considers not only GIS random uncertainty but also GIS fuzzy uncertainty, the result is precise, scientific and reasonable; finally summarizes up and points out the problems to need solving.",2010,0,
1369,1370,Fault detection methods for frequency converters fed induction machines,"The paper focuses on experimental investigation for stator faults detection and fault detection methods of electrical drive systems using voltage source inverter (VSI) fed cage rotor induction machines (CRIM). Two experimental investigations (one stator phase unbalance and one stator phase open) have been performed to study the behaviour of the electrical machine. A description of the measurement system including acquisition and processing of the data is presented and stator current signature, current Park's vector and instantaneous power as diagnostic techniques are considered.",2007,0,
1370,1371,Fault Analysis of the Stream Cipher Snow 3G,"Snow 3G is the backup encryption algorithm used in the mobile phone UMTS technology to ensure data confidentiality. Its design - a combiner with memory - is derived from the stream cipher Snow 2.0, with improvements against algebraic cryptanalysis and distinguishing attacks. No attack is known against Snow 3G today. In this paper, a fault attack against Snow 3G is proposed. Our attack recovers the secret key with only 22 fault injections.",2009,0,
1371,1372,Geometrical approach on masked gross errors for power systems state estimation,"In this paper, a geometrical based-index, called undetectability index (UI), that quantifies the inability of the traditional normalized residue test to detect single gross errors is proposed. It is shown that the error in measurements with high UI is not reflected in their residues. This masking effect is due to the ldquoproximityrdquo of a measurement to the range of the Jacobian matrix associated with the power system measurement set. A critical measurement is the limit case of measurement with high UI, that is, it belongs to the range of the Jacobian matrix, has an infinite UI index, its error is totally masked and cannot be detected in the normalized residue test at all. The set of measurements with high UI contains the critical measurements and, in general, the leverage points, however there exist measurements with high UI that are neither critical nor leverage points and whose errors are masked by the normalized residue test. In other words, the proposed index presents a more comprehensive picture of the problem of single gross error detection in power system state estimation than critical measurements and leverage points. The index calculation is very simple and is performed using routines already available in the existing state estimation software. Two small examples are presented to show the way the index works to assess the quality of measurement sets in terms of single gross error detection. The IEEE-14 bus system is used to show the efficiency of the proposed index to identify measurements whose errors are masked by the estimation processing.",2009,0,
1372,1373,A Study on the Non-Inductive Coils for Hybrid Fault Current Limiter Using Experiment and Numerical Analysis,"A Hybrid fault current limiter (FCL) proposed by our group previously is composed of a superconducting coil, a fast switch and a bypass reactor. The superconducting coil wound with two kinds of HTS wire has zero impedance when normal current flows in the coil. However, different quench characteristics of the HTS wire generate magnetic flux in the coil when fault current flows in the coil. As a result, the fast switch was opened by the repulsive force applied to the aluminum plate above the coil. In previous studies, our group verified operating characteristics and feasibility of the fast switch. In this paper, comparison of pancake type and solenoid type non-inductive coil wound with two kinds of the HTS wire was performed by using short-circuit test and finite element method. From these results, short-circuit characteristic of a coil can be acquired and magnitude of the repulsive force and magnetic field can be analysed.",2010,0,
1373,1374,Study of the impact of hardware fault on software reliability,"As software plays increasingly important roles in modern society, reliable software becomes desirable for all stakeholders. One of the root causes of software failure is the failure of the computer hardware platform on which the software resides. Traditionally, fault injection has been utilized to study the impact of these hardware failures. One issue raised with respect to the use of fault injection is the lack of prior knowledge on the faults injected, and the fact that, as a consequence, the failures observed may not represent actual operational failures. This paper proposes a simulation-based approach to explore the distribution of hardware failures caused by three primary failure mechanisms intrinsic to semiconductor devices. A dynamic failure probability for each hardware unit is calculated. This method is applied to an example Z80 system and two software segments. The results lead to the conclusion that the hardware failure profile is location related, time dependent, and software-specific",2005,0,
1374,1375,Automated fault analysis using an intelligent monitoring system,"Distribution feeders are complex systems comprised of numerous components, which are expected to function properly for decades. Electrical, mechanical and weather-related stresses combine to degrade components. Degradation accumulates over time, gradually impairing components' ability to perform properly and ultimately leading to failures, faults and outages. Work at Texas A&M has documented electrical parametric changes that occur as apparatus degrade. Taking advantage of these changes holds promise for helping utilities improve service quality and reliability, but intelligent algorithms and systems are required to acquire, analyze and otherwise manage the significant volume of data necessary to realize such benefits.",2009,0,
1375,1376,An Effective RM-Based Scheduling Algorithm for Fault-Tolerant Real-Time Systems,"Dependability is the representative property that predominantly distinguishes a hard real-time system from other computer systems besides timeliness. Primary/alternate version technique is a cost-effective means which trades the quality of computation results for promptness to tolerate the software faults. The kernel algorithm proposed in this paper employs the off-line backwards-RM scheme to pre-allocate time intervals to the alternate version and the on-line RM scheme to dispatch the primary version. The methodology is a dual-purpose strategy, which aims to (1) tolerate potential software faults by ensuring the accomplishment of the alternate version once its primary fails to execute or re-execute and (2) achieve better quality of service by maximizing the success rate of primary version.",2009,0,
1376,1377,A fast error correction technique for matrix multiplication algorithms,"Temporal redundancy techniques will no longer be able to cope with radiation induced soft errors in technologies beyond the 45 nm node, because transients will last longer than the cycle time of circuits. The use of spatial redundancy techniques will also be precluded, due to their intrinsic high power and area overheads. The use of algorithm level techniques to detect and correct errors with low cost has been proposed in previous works, using a matrix multiplication algorithm as the case study. In this paper, a new approach to deal with this problem is proposed, in which the time required to recompute the erroneous element when an error is detected is minimized.",2009,0,
1377,1378,Fault Tolerance & Testable Software Security: A Method of Quantifiable Non-Malleability with Respect to Time,"In this paper, we demonstrate there exists practical limits to the recoverability and integrity verification (non-malleability) of software with respect to time a property to the best of our knowledge not demonstrated previously; this in turn, implies practical limits to software security using current existing processing hardware. Non-malleability applied to software implies that it should be infeasible for an attacker to modify a piece of software, thus creating a software fault. Given the recoverability limitation, we demonstrate a quantifiable definition for secure software with respect to integrity/tamper resistance.",2007,0,
1378,1379,Error analysis of free-form surfaces for manufacturing applications,"Error analysis of free-form surfaces is a requirement to assure quality and to reduce manufacturing costs and rework. This paper proposes a new approach and algorithms for the error analysis of free-form surfaces. Given the measured surface as an input the approach first uses a statistical method to determine the number of test-points with suitable sample size for shape error analysis. Then, the system applies a robust mathematic model, Implicit polynomials (IP), to construct the model of the test-points. To perform detailed comparison of the shapes, the CAD model is geometrically adjusted with the input using model-based matching algorithm developed in this paper. Once the CAD model is adjusted, it is compared with input to reveal the errors between their shapes. To accomplish this task a new shape matching algorithm is developed. Experimental results on error analysis of a variety of the machined metal skin of aircraft are reported to show the validity of the proposed methodology.",2009,0,
1379,1380,Study on errors compensation of a vehicular MEMS accelerometer,"Based on micro electronic mechanical system technology, the micro inertial sensors are introduced, and the research and application conditions of MEMS micro accelerometers are also analyzed. Taking auto navigation and testing system as an example, simple mathematical models and compensational methods are developed to correct sensor errors and the validity is evaluated by experimental verification.",2005,0,
1380,1381,Control chart of mean with low alpha error probability,"A control chart of adjustment center imbalance of process with low alpha error probability and stability to unknown distribution parameter is designed. At the heart of algorithm is hypothesis check criterion. According to results of current controlled parameter measurements at every step is made a calculation of line inclination value and is tested hypothesis of its equality to null. If we accept this hypothesis, we consider the current process to be disordered.",2008,0,
1381,1382,Low-error carry-free fixed-width multipliers and their application to DCT/IDCT,"In this paper, we propose a low-error fixed-width redundant multiplier design. The design is based on the statistical analysis of the value of the truncated partial products in binary signed-digit representation with modified Booth encoding. The overall truncation error is significantly reduced with negligible hardware overhead. Simulation on DCT/IDCT of images with 256 gray levels shows our proposed multiplication design has higher PSNR/SNR.",2004,0,
1382,1383,Fault tolerant amplifier system using evolvable hardware,"This paper proposes the use evolvable hardware (EHW) for providing fault tolerance to an amplifier system in a signal-conditioning environment. The system has to maintain a given gain despite the presence of faults, without direct human intervention. The hardware setup includes a reconfigurable system on chip device and an external computer where a genetic algorithm is running. For detecting a gain fault, we propose a software-based built-in self-test strategy that establishes the actual values of gain achievable by the system. The performance evaluation of the fault tolerance strategy proposed is made by adopting two different types of fault-models. The fault simulation results show that the technique is robust and that the genetic algorithm finds the target gain with low error.",2010,0,
1383,1384,Fault Localization Based on Multi-level Similarity of Execution Traces,"Since automated fault localization can improve the efficiency of both the testing and debugging process, it is an important technique for the development of reliable software. This paper proposes a novel fault localization approach based on multi-level similarity of execution traces, which is suitable for object-oriented software. It selects useful test cases at class level and computes code suspiciousness at block level. We develop a tool that implements the approach, and conduct empirical studies to evaluate its effectiveness. The experimental results show that our approach has the potential to be effective in localizing faults for object-oriented software.",2009,0,
1384,1385,Real-Time Fisheye Lens Distortion Correction Using Automatically Generated Streaming Accelerators,"Fisheye lenses are often used in scientific or virtual reality applications to enlarge the field of view of a conventional camera. Fisheye lens distortion correction is an image processing application which transforms the distorted fisheye images back to the natural-looking perspective space. This application is characterized by non-linear streaming memory access patterns that make main memory bandwidth a key performance limiter. We have developed a fisheye lens distortion correction system on a custom board that includes a Xilinx Virtex-4 FPGA. We express the application in a high level streaming language, and we utilize Proteus, an architectural synthesis tool, to quickly explore the design space and generate the streaming accelerator best suited for our cost and performance constraints. This paper shows that appropriate ESL tools enable rapid prototyping and design of real-life, performance critical and cost sensitive systems with complex memory access patterns and hardware-software interaction mechanisms.",2009,0,
1385,1386,Voltage Sensor Fault Detection and Reconfiguration for a Doubly Fed Induction Generator,"Fault detection and reconfiguration of the control loops of a Doubly-Fed Induction Generator are described in this paper. The stator voltage is measured as well as observed. During fault free operation, the measured signal is used for the field oriented control. In case of a voltage sensor fault, the faulty measurement is identified and the control is reconfigured using the observer output. Operation without measuring the stator voltage is possible. Laboratory measurements prove this concept.",2007,0,
1386,1387,The Probabilistic Program Dependence Graph and Its Application to Fault Diagnosis,"This paper presents an innovative model of a program's internal behavior over a set of test inputs, called the probabilistic program dependence graph (PPDG), which facilitates probabilistic analysis and reasoning about uncertain program behavior, particularly that associated with faults. The PPDG construction augments the structural dependences represented by a program dependence graph with estimates of statistical dependences between node states, which are computed from the test set. The PPDG is based on the established framework of probabilistic graphical models, which are used widely in a variety of applications. This paper presents algorithms for constructing PPDGs and applying them to fault diagnosis. The paper also presents preliminary evidence indicating that a PPDG-based fault localization technique compares favorably with existing techniques. The paper also presents evidence indicating that PPDGs can be useful for fault comprehension.",2010,0,
1387,1388,A new probing scheme for fault detection and identification,"Probing technology has been used as a fault detection and identification method in computer networks and successful applications have been reported. One of the most appealing features of probing-based schemes is that it is an active approach. A set of probes can be sent on a periodic basis. If a network failure is detected, the outcomes of these probes are further analyzed to determine the root cause of the problem. However, the availability of a large set of such probes may in fact place a huge burden on management systems in terms of extra management traffic and storage space. Hence, the need of minimizing such a probing set has become highly desirable. In this work, we propose a preplanned probe selection scheme, in which a small set of probes are chosen such that it maintains the diagnostic power of the original set. The new approach is based on the constraint satisfaction problem paradigm and its powerful search techniques are exploited. The efficiency of the new algorithm has been demonstrated by the results reported.",2009,0,
1388,1389,An empirical study of software reuse vs. defect-density and stability,"The paper describes results of an empirical study, where some hypotheses about the impact of reuse on defect-density and stability, and about the impact of component size on defects and defect-density in the context of reuse are assessed, using historical data (data mining) on defects, modification rate, and software size of a large-scale telecom system developed by Ericsson. The analysis showed that reused components have lower defect-density than non-reused ones. Reused components have more defects with highest severity than the total distribution, but less defects after delivery, which shows that that these are given higher priority to fix. There are an increasing number of defects with component size for non-reused components, but not for reused components. Reused components were less modified (more stable) than non-reused ones between successive releases, even if reused components must incorporate evolving requirements from several application products. The study furthermore revealed inconsistencies and weaknesses in the existing defect reporting system, by analyzing data that was hardly treated systematically before.",2004,0,
1389,1390,"Animation can show only the presence of errors, never their absence","A formal specification animator executes and interprets traces on a specification. Similar to software testing, animation can only show the presence of errors, never their absence. However, animation is a powerful means of finding errors, and it is important that we adequately exercise a specification when we animate it. The paper outlines a systematic approach to the animation of formal specifications. We demonstrate the method on a small example, and then discuss its application to a non-trivial, system-level specification. Our aim is to provide a method for planned, documented and maintainable animation of specifications, so that we can achieve a high level of coverage, evaluate the adequacy of the animation, and repeat the process at a later time",2001,0,
1390,1391,Non-uniformity correction and calibration of a portable infrared scene projector,"A key attribute of any tester for FLIR systems is a calibrated uniform source. A uniform source ensures that any anomalies in performance are artifacts of the FLIR being tested and not the tester. Achieving a uniform source from a resistor array based portable infrared scene projector requires implementation of nonuniformity correction algorithms instead of controlling the bonding integrity of a source to a cooler, and the coating properties of the source typical of a conventional blackbody. The necessity to perform the non-uniformity correction on the scene projector is because the source is a two-dimensional array comprised of discrete resistive emitters. Ideally, each emitter of the array would have the same resistance and thus produce the same output for a given drive current. However, there are small variations from emitter to emitter over the thousands of emitters that comprise an array. Once a uniform output is achieved then the output must be calibrated for the system to be used as test equipment. Since the radiance emitted from the monolithic array is created by flowing current through micro resistors, a radiometric approach is used to calibrate the differential output of the scene projector over its dynamic range. The focus of this paper is to describe the approach and results of implementing non-uniformity correction and calibration on a portable infrared scene projector.",2002,0,
1391,1392,Effective data sharing system for fault tolerant Structural Health Monitoring system,"Structural Health Monitoring (SHM) system is a promising technology to determine the health condition of a structure and localize its damage. SHM system is widely used, especially in gigantic structures for its strong requirements of safety. Many researches about SHM system have been conducting. However, conventional SHM system did not take account of an accidental collapse caused by earthquake. As a result, there is a possibility that sensor nodes, network links, and a center server cause failures in processing or storing of data gathered in the building. These failures may lose important data that contains useful information for post-analyzing the collapse. The data includes when, where and why the structure is damaged and collapsed. Theses information have a great deal of potential for preventing similar damage or collapse and it will make great contribution to accelerate future structural researches. To solve the problem of data losses in a damage, data sharing system for SHM system is proposed. This system shares data within sensor nodes. The proposed system achieves the following three processes; node search, backup node selection and backup data transfer process. It is important to conduct these processes under the condition of limited resources due to small and not powerful sensor nodes. Backup node selection is an important process of this system. This paper mainly focused on this selection. Round-trip time, a size of free memory space of backup nodes and value of displacement caused by vibration are used for the selection. In this paper, the selection method based on displacement was checked. An experiment was conducted by using practical sensor nodes and acceleration data. From the result of this experiment, proposed system could share data by using selection method based on displacement.",2010,0,
1392,1393,Diagnosis of induction machine rotor defects from an approach of magnetically coupled multiple circuits,"The authors develop the squirrel cage induction machine models for the diagnosis of defects from an approach of magnetically coupled multiple circuits. The generalized models are established on the base of mathematical recurrences. The calculation of machine inductances (with and without rotor defects) is carried out by the tools of software MATLAB before the beginning of simulation under software SIMULINK. The experimental tests were carried out on four induction machines of power 4 kW, especially manufactured for the needs for the diagnosis and presenting defects in the rotor. The simulated results and the experimental ones are presented to confirm the validity of proposed models",2006,0,
1393,1394,Considering fault dependency and debugging time lag in reliability growth modeling during software testing,"Since the early 1970s tremendous growth has been seen in the research of software reliability growth modeling. In general, software reliability growth models (SRGMs) are applicable to the late stages of testing in software development and they can provide useful information about how to improve the reliability of software products. For most existing SRGMs, most researchers assume that faults are immediately detected and corrected. However, in practice, this assumption may not be realistic and satisfied. In this paper we first give a review of fault detection and correction processes in SRGMs. We show how several existing SRGMs based on NHPP models can be comprehensively derived by applying the time-dependent delay function. Furthermore, we show how to incorporate both failure dependency and time-dependent delay function into software reliability growth modeling. We present stochastic reliability models for software failure phenomenon based on NHPPs. Some numerical examples based on real software failure data sets are presented. The results show that the proposed framework to incorporate both failure dependency and time-dependent delay function into software reliability modeling has a useful interpretation in testing and correcting the software.",2004,0,
1394,1395,Application for fault location in electrical power distribution systems,"Fault location has been studied deeply for transmission lines due to its importance in power systems. Nowadays the problem of fault location on distribution systems is receiving special attention mainly because of the power quality regulations. In this context, this paper presents an application software developed in Matlabtrade that automatically calculates the location of a fault in a distribution power system, starting from voltages and currents measured at the line terminal and the model of the distribution power system data. The application is based on a N-ary tree structure, which is suitable to be used in this application due to the highly branched and the non- homogeneity nature of the distribution systems, and has been developed for single-phase, two-phase, two-phase-to-ground, and three-phase faults. The implemented application is tested by using fault data in a real electrical distribution power system.",2007,0,
1395,1396,Position and speed sensorless control for PMSM drive using direct position error estimation,"A new position and speed sensorless control approach is proposed for permanent magnet synchronous motor (PMSM) drives. The controller directly computes an error for the estimated rotor position and adjusts the speed according to this error. The derivation of the position error equation and an idea for eliminating the differential terms, are presented. The proposed approach is applied to a vector controlled PMSM AC drive and phase locked loop (PLL) control is employed for speed adjustment. Several simulations are carried out. The proposed control scheme is verified by experiments using a 3.7 kW salient pole PMSM",2001,0,
1396,1397,Control of a full-converter Permanent Magnet Synchronous Wind Generator with Neutral Point Clamped converters during a network fault,"This paper analyses the behaviour of a full-converter wind generation system with a back-to-back conversion structure using Neutral Point Clamped (NPC) converters during network faults. A Permanent Magnet Synchronous Generator (PMSG) is used as the generator. The main problems of this structure during voltage sags are firstly, the accumulation of power in the DC-link due to the reduction of power delivered to grid, and secondly, the control of the neutral point voltage. Three control strategies are proposed with the purpose of optimizing operation under network fault conditions. The characteristics of those strategies with regard to DC-link voltage control, torque variations required of PMSG and neutral point voltage control are also discussed.",2010,0,
1397,1398,Fault handling in embedded industrial measurement and control systems: issues and a case study,"With increasingly complex control systems used in a variety of commercial, aerospace, and military applications, system faults may occur during system operations. These faults inevitably result in abnormal operations and production shutdown or even disasters. Therefore, improving system reliability has become a major concern in safety-critical systems. This paper primarily addresses the issues in embedded fault-tolerant control system designs and presents a case study on vibration suppression in the aerospace industry. The design issues on embedded control systems such as component failures, sampling jitters and control delay, network-induced delay and packet loss in network transmission are discussed, all of which may have damaging effects on the closed-loop system performance. A case study on vibration control for a launch vehicle payload fairing using multiple embedded PZT actuators are also presented, where an adaptive actuator failure compensation scheme is successfully implemented. Fault-tolerant control turns out to be effective in creating more robust industrial measurement and control systems.",2003,0,
1398,1399,Concave and Convex Area on Planar Curve for Shape Defect Detection and Recognition,"A shape representation based on concave and convex area along a closed curve is presented. Curvature estimation is done to the input curve and searched for its critical points. Splitting the critical points into concave and convex critical points, the concave and convex area is computed. This technique is tested on shape defect detection of starfruit and also to shape recognition. In the first case, defect is measured with concave energy and obtained a stable measure, which is proportional with the defect. In shape recognition, starfruit's stem is identified to remove it from the starfruit shape, as it will contribute to false computation in defect measurement",2006,0,
1399,1400,Effect of channel estimation error onto the BER performance of PSAM-OFDM in Rayleigh fading,"In this paper, the current analysis focuses on the influence of BER performance in Rayleigh fading propagation environments, which results from the channel estimation error of pilot symbol assisted modulation (PSAM) in orthogonal frequency division multiplexing (OFDM) systems. This paper first characterizes the distribution of the amplitude and phase estimates using PSAM, and the formula for BER as a function of channel correlation and interpolation filter in time and frequency is given. Also interchannel interference due to Doppler effects is taken into account. Theoretical and simulation results show that channel estimation error leads to a 1-dB degradation in average signal-to-noise ratio for the parameters considered.",2003,0,
1400,1401,Fault-tolerant mobile agents in distributed objects systems,"A transactional agent is a mobile agent which manipulates objects in one or more than one object server so as to satisfy some constraints. There are some types of constraints depending on applications. ACID is one of the constraints, which shows traditional atomic transactions. There are other constraints like at-least-one constraint where a transaction can commit if at least one object server is successfully manipulated. An agent leaves a surrogate agent on an object server on leaving the object server A surrogate holds objects manipulated by the agent and recreates an agent if the agent is faulty. In addition, an agent is replicated by itself. Thus, transactional agents are fault-tolerant. We discuss how transactional agents with types of commitment constraints can commit. We discuss how to implement transactional agents.",2003,0,
1401,1402,"Eliminating exception handling errors with dependability cases: a comparative, empirical study","Programs fail mainly for two reasons: logic errors in the code and exception failures. Exception failures can account for up to two-thirds of system crashes, hence, are worthy of serious attention. Traditional approaches to reducing exception failures, such as code reviews, walkthroughs, and formal testing, while very useful, are limited in their ability to address a core problem: the programmer's inadequate coverage of exceptional conditions. The problem of coverage might be rooted in cognitive factors that impede the mental generation (or recollection) of exception cases that would pertain in a particular situation, resulting in insufficient software robustness. This paper describes controlled experiments for testing the hypothesis that robustness for exception failures can be improved through the use of various coverage-enhancing techniques: N-version programming, group collaboration, and dependability cases. N-version programming and collaboration are well known. Dependability cases, derived from safety cases, comprise a new methodology based on structured taxonomies and memory aids for helping software designers think about and improve exception handling coverage. All three methods showed improvements over control conditions in increasing robustness to exception failures but dependability cases proved most efficacious in terms of balancing cost and effectiveness",2000,0,
1402,1403,Decentralized Fault Detection and Management for Wireless Sensor Networks,"Wireless Sensor Networks are increasingly being deployed in long-lived, challenging application scenarios which demand a high level of availability and reliability. To achieve these characteristics in inherently unreliable and resource constrained sensor network environments, fault tolerance is required. This paper presents a generic and efficient fault tolerance algorithm for Wireless Sensor Networks. In contrast to existing approaches, the algorithm presented in this paper is entirely decentralized and can thus be used to support fully autonomic fault tolerance in sensor network environments.",2010,0,
1403,1404,Error resilience performance evaluation of H.264 I-frame and JPWL for wireless image transmission,"The visual quality obtained in wireless transmission strongly depends on the characteristics of the wireless channel and on the error resilience of the source coding. The wireless extensions of the JPEG 2000 standard (JPWL) and H.264 are the latest international standards for still image and video compression, respectively. However, few results have been reported to compare the rate-distortion (R-D) performance of JPEG 2000 and H.264. Conversely, comparative studies of error resilience between JPWL and H.264 for wireless still image transmission have not been thoroughly investigated. In this paper, we analyse the error resilience of image coding based on JPWL and H.264 I-frame coding in Rayleigh fading channels. Comprehensive objective and perceptual results are presented in relation to the error resilience performance of these two standards under various conditions. Our simulation results reveal that H.264 is more robust to transmission errors than JPWL for wireless still image transmission.",2010,0,
1404,1405,"Automotive signal fault diagnostics - part I: signal fault analysis, signal segmentation, feature extraction and quasi-optimal feature selection","The paper describes our research in vehicle signal fault diagnosis. A modern vehicle has embedded sensors, controllers and computer modules that collect a large number of different signals. These signals, ranging from simple binary modes to extremely complex spark timing signals, interact with each other either directly or indirectly. Modern vehicle fault diagnostics very much depend upon the input from vehicle signal diagnostics. Modeling vehicle engine diagnostics as a signal fault diagnostic problem requires a good understanding of signal behaviors relating to various vehicle faults. Two important tasks in vehicle signal diagnostics are to find what signal features are related to various vehicle faults, and how can these features be effectively extracted from signals. We present our research results in signal faulty behavior analysis, automatic signal segmentation, feature extraction and selection of important features. These research results have been incorporated in a novel vehicle fault diagnostic system, which is described in another paper (see Yi Lu Murphey et al., ibid., p.1076-98).",2003,0,
1405,1406,RACE: a software-based fault tolerance scheme for systematically transforming ordinary algorithms to robust algorithms,"We propose the robust algorithm-configured emulation (RACE) scheme for efficient parallel computation and communication in the presence of faults. A wide variety of algorithms originally designed for fault-free meshes, tori, and k-ary n-cubes can be transformed to corresponding robust algorithm through RACE. In particular optimal robust algorithms can be derived for total exchange (TE) and ascend/descend operations with a factor of 1+o (1) slowdown. Also, RACE can tolerate a large number of faulty elements, without relying on hardware redundancy or any assumption about the availability of a complete subarray",2001,0,
1406,1407,Efficient diagnosis of single/double bridging faults with Delta Iddq probabilistic signatures and Viterbi algorithm,"This paper presents an efficient method to diagnose single and double bridging faults. This method is based on Delta Iddq probabilistic signatures, as well as on the Viterbi algorithm, mainly used in telecommunications systems for error correction. The proposed method is a significant improvement over an existing one, based on maximum likelihood estimation. The (adapted) Viterbi algorithm takes into account useful information not considered previously. Simulation and experimental results are presented to validate the approach",2000,0,
1407,1408,More about arc-fault circuit interrupters,"Since the arc-fault circuit interrupter (AFCI) was commercially introduced in 1998, questions have arisen about how it detects arcs, whether it detects series and parallel arcs, and what types of AFCIs are available. Types other than the original branch/feeder AFCI are emerging. This paper is intended to provide an update regarding answers to those questions, following an earlier paper that introduced the basic functioning of the AFCI.",2004,0,
1408,1409,A hierarchical fault tolerant architecture for component-based service robots,"Due to the benefits of reusability and productivity, component-based approach has become the primary technology in service robot system development. However, because component developer cannot foresee the integration and operating condition of the components, they cannot provide appropriate fault tolerance function, which is crucial for commercial success of service robots. The recently proposed robot software frames such as MSRDS (Microsoft Robotics Developer Studio), RTC (Robot Technology Component), and OPRoS (Open Platform for Robotic Services) are very limited in fault tolerance support. In this paper, we present a hierarchically-structured fault tolerant architecture for component-based robot systems. The framework integrates widely-used, representative fault tolerance measures for fault detection, isolation, and recovery. The system integrators can construct fault tolerance applications from non-fault-aware components, by declaring fault handling rules in configuration descriptors or/and adding simple helper components, considering the constraints of components and the operating environment. To demonstrate the feasibility and benefits, a fault tolerant framework engine and test robot systems are implemented for OPRoS. The experiment results with various simulated fault scenarios validate the feasibility, effectiveness and real-time performance of the proposed approach.",2010,0,
1409,1410,"Optimized reasoning-based diagnosis for non-random, board-level, production defects","The ""back-end"" costs associated with debug of functional test failures can be one of the highest cost adders in the manufacturing process. As boards become more dense and more complex, debug of functional failures will become more and more difficult. Test strategies try to detect and diagnose failures early on in the test process (component and structural tests), but inevitably some defects are not detected until functional testing is done on the board. Finding these defects usually requires an ""expert"", with engineering level skills in both hardware and software. Depending on the complexity of the product, it could take several months (even years) to develop this level of expertise. During the initial product ramp, this expertise is usually most needed and often unavailable. Debug time is usually very long and scrap rates are generally high. This paper will provide an overview of reasoning-based diagnosis techniques and how they can significantly decrease debug time, especially during new product introduction. Because these engines are ""model-based"", there is no guarantee how they will perform in real life. In almost all cases, the reasoning engine will have to be modified based on instances where the reasoning engine could not correctly identify the failing component. Making these adjustments to the reasoning is a very complex and sometimes risky endeavor. While the new model may correctly identify the previously missed failure, the reasoning may have been altered to a point where several other diagnoses have now been unknowingly compromised. This paper will propose enhancements to the reasoning engine that will allow a simpler approach to adapting to diagnostic escapes without risking compromises to the original diagnostic engine",2005,0,
1410,1411,Online assessment of fault current considering substation topologies,"Changes to the power system, especially the installation of new generation sources and new operating conditions, result in higher fault currents. Circuit breakers, which are previously designed and coordinated for the power systems before the appearances of new generation sources and new operating conditions, may have inadequate fault current interruption capability. This paper discusses those operating conditions that result in interruption capability violations of circuit breakers, based on substation topologies/circuit breaker connections. An online application of fault current assessment is proposed to identify those unsafe operating conditions limited by circuit breaker interruption capabilities. Based on the findings of the paper, this application can give system operators suggestions of remedy actions that eliminate unsafe operating conditions by changing substation topologies. The design of such an online application is described and a MATLAB version is tested on IEEE 14-bus system.",2010,0,
1411,1412,Cascaded H-bridge Multilevel Inverter Drives Operating under Faulty Condition with AI-Based Fault Diagnosis and Reconfiguration,"The ability of cascaded H-bridge multilevel inverter drives (MLID) to operate under faulty condition including AI-based fault diagnosis and reconfiguration system is proposed in this paper. Output phase voltages of a MLID can be used as valuable information to diagnose faults and their locations. It is difficult to diagnose a MLID system using a mathematical model because MLID systems consist of many switching devices and their system complexity has a nonlinear factor. Therefore, a neural network (NN) classification is applied to the fault diagnosis of a MLID system. Multilayer perceptron (MLP) networks are used to identify the type and location of occurring faults. The principal component analysis (PCA) is utilized in the feature extraction process to reduce the NN input size. A lower dimensional input space will also usually reduce the time necessary to train a NN, and the reduced noise may improve the mapping performance. The genetic algorithm (GA) is also applied to select the valuable principal components to train the NN. A reconfiguration technique is also proposed. The proposed system is validated with simulation and experimental results. The proposed fault diagnostic system requires about 6 cycles (~100 ms at 60 Hz) to clear an open circuit and about 9 cycles (~150 ms at 60 Hz) to clear a short circuit fault. The experiment and simulation results are in good agreement with each other, and the results show that the proposed system performs satisfactorily to detect the fault type, fault location, and reconfiguration.",2007,0,
1412,1413,Capacity and error probability analysis for orthogonal space-time block codes over fading channels,"The capacity and error probability of orthogonal space-time block codes (STBCs) are considered for pulse-amplitude modulation/phase shift keying/quadrature-amplitude modulation (PAM/PSK/QAM) in fading channels. The approach is based on an equivalent scalar additive white Gaussian noise channel with a channel gain proportional to the Frobenius norm of the matrix channel for the STBC. Using this effective channel, capacity and probability of error expressions are derived for PSK/PAM/QAM modulation with space-time block coding. Rayleigh-, Ricean-, and Nakagami-fading channels are considered. As an application, these results are extended to obtain the capacity and probability of error for a multiuser direct sequence code-division multiple-access system employing space-time block coding.",2005,0,
1413,1414,A high efficient boost converter with power factor correction,"Boost converter is widely used as active power factor correction (PFC) pre-regulator. Its input voltage range is universal (90-265 V), and its output voltage is regulated at about 380 V. At low line (90 V) the switch's rms current is high, so the conduction loss of power switch MOSFET is large and the efficiency of whole converter is very low. This paper proposes a new control method that the output voltage varies with the input voltage change. Under this control the MOSFET's on-time is shortened, and the switch's RMS current decreases, which reduces the conduction loss and increases the boost converter efficiency. The distribution of power loss is analyzed by computing software (mathcad 2000) and the realization of this special control method is given. A 1200 W boost power factor corrector with average current control is built up. In order to improve the diode's turn-off loss the performance of a 600 V, 12 A silicon carbide (SiC) Schottky diode is also experimentally evaluated. Measurements of overall efficiency and reverse recovery behavior are compared between SiC diode and fast recovery diode.",2004,0,
1414,1415,Modeling Alpha and Neutron Induced Soft Errors in Static Random Access Memories,Experimental thermal neutron and alpha soft error test results of a 4 Mbit SRAM fabricated on a 0.25 mum process are evaluated using Vanderbilt University's RADSAFE toolkit. The capabilities of the radiation transport code are demonstrated by accurately reproducing experimental results and predicting operational soft error rates for the memory.,2007,0,
1415,1416,Byzantine Fault-Tolerant Web Services for n-Tier and Service Oriented Architectures,"Mission-critical services must be replicated to guarantee correctness and high availability in spite of arbitrary (Byzantine) faults. Traditional Byzantine fault tolerance protocols suffer from several major limitations. Some protocols do not support interoperability between replicated services. Other protocols provide poor fault isolation between services leading to cascading failures across organizational and application boundaries. Moreover, traditional protocols are unsuitable for applications with tiered architectures, long-running threads of computation, or asynchronous interaction between services. We present Perpetual, a protocol that supports Byzantine fault-tolerant execution of replicated services while enforcing strict fault isolation. Perpetual enables interaction between replicated services that may invoke and process remote requests asynchronously in long-running threads of computation. We present a modular implementation, an Axis2 Web Services extension, and experimental results that demonstrate only a moderate overhead due to replication.",2008,0,
1416,1417,Accurate algorithm for analysis of surface errors in reflector antennas and calculation of gain loss,"The distortion of antenna reflector can degrade the antenna performances obviously with the higher work frequency band. A new algorithm used for accurately analyzing the distorted reflectors and computing the axial, normal and radial deviations of distorted surface is developed. The analysis method with simulation values only one third of analysis results of ANSYS software is verified by calculating surface errors and gain losses of a 7.3-m parabolic antenna under different working conditions and comparing the results of accurate algorithm and ANSYS software method. Its application in both large space and ground antennas will greatly improve the design efficiency, reduce cost, and also provide the detailed precise distortion information for electrical designers.",2005,0,
1417,1418,Software Faults Diagnosis in Complex OTS Based Safety Critical Systems,"This work addresses the problem of software fault diagnosis in complex safety critical software systems. The transient manifestations of software faults represent a challenging issue since they hamper a complete knowledge of the system fault model at design/development time. By taking into account existing diagnosis techniques, the paper proposes a novel diagnosis approach, which combines the detection and location processes. More specifically, detection and location modules have been designed to deal with partial knowledge about the system fault model. To this aim, they are tuned during system execution in order to improve diagnosis during system lifetime. A diagnosis engine has been realized to diagnose software faults in a real world middleware platform for safety critical applications. Preliminary experimental campaigns have been conducted to evaluate the proposed approach.",2008,0,
1418,1419,Research on Multi-function Fault Management System Model Based on SNMP,"The improving network technology and application make it a challenge to the network manager. A feasible and efficient network management strategy will become an important method to insure the network running well. Therefore, itpsilas meaningful to be familiar with the network and network management technology. In accordance with practical network environment, we design and implement fault management system FMS based on SNMP. Using Client/Server architecture, our system established a distributed management model which is compose of Console/Manager/Agent, finished network status monitoring, event processing, fault alarm and log etc.",2008,0,
1419,1420,Combinational Logic Soft Error Correction,"We present two techniques for correcting radiation-induced soft errors in combinational logic - error correction using duplication, and error correction using time-shifted outputs. Simulation results show that both techniques reduce combinational logic soft error rate by more than an order of magnitude. Soft errors affecting sequential elements (latches and flip-flops) at combinational logic outputs are automatically corrected using these techniques",2006,0,
1420,1421,A new approach to fault-tolerant wormhole routing for mesh-connected parallel computers,"A new method for fault-tolerant wormhole routing in arbitrary dimensional meshes is introduced. The method was motivated by certain routing requirements of an initial design of the Blue Gene supercomputer at IBM Research. The machine is organized as a three-dimensional mesh containing many thousands of nodes and the routing method should tolerate a few percent of the nodes being faulty. There has been much work on routing methods for meshes that route messages around faults or regions of faults. The new method is to declare certain nonfaulty nodes to be ""lambs."" A lamb is used for routing but not processing, so a lamb is neither the source nor the destination of a message. The lambs are chosen so that every ""survivor node,"" a node that is neither faulty nor a lamb, can reach every survivor node by at most two rounds of dimension-ordered (such as e-cube) routing. An algorithm for finding a set of lambs is presented. The results of simulations on 2D and 3D meshes of various sizes with various numbers of random node faults are given. For example, on a 32  32  32 3D mesh with 3 percent random faults and using at most two rounds of e-cube routing for each message, the average number of lambs is less than 68, which is less than 7 percent of the number 983 of faults and less than 0.21 percent of the number 32,768 of nodes.",2004,0,
1421,1422,Design and application of Gray Field<sup>TM</sup> technology for defect inspection systems,"There has been increased interest in optical inspection tools that utilize UV illumination. This originates from the belief that diffraction limits will render tools employing longer wavelengths blind to many defects identified as being critical. In response to concerns over the applicability of UV illumination to rapid defect detection we performed a series of experiments to explore and develop new inspection techniques to provide the capability of detecting the dimensionally challenging defects associated with advanced technology nodes while maintaining high speeds needed for chipmakers' volume production lines. Initial results indicated that by radically redesigning the collection optics to a multiple perspective configuration that compiles information from six different scattering and reflecting directions, improved sensitivity, noise rejection and wafer throughput could be realized while using laser scanning illumination in the visible region of the spectrum. Also, defects that traditionally could only be observed in bright field tools were now detectable with ease at production worthy throughputs. Results are presented that show the optical experimental design data and simulations, and are corroborated by examples of defects from the resulting production defect inspection system, Compass<sup>TM</sup>. In addition, electron micrographs of a range of detected defects are presented that show the system versatility and the exact nature of the defects, thus allowing a clear understanding of the increase in sensitivity, speed and dimensional range these tools provide over traditional instrumentation to be made",2001,0,
1422,1423,A strategy to replace the damaged element for fault-tolerant induction motor drive,"In this paper, the best moment to replace to the damaged element in a fault-tolerant induction motor drive working with a open-loop and closed-loop control is presented, a previous stage of fault-diagnostic to detect a short-circuit or open-circuit failure in the power device is considered. The technique is based on the connection of bidirectional switches to electrically isolate the damaged element by mean of fuse blown corresponding and to replace only the damaged device by another healthful one at the most suitable moment, the main issue is to diminish the tracking error of the motor current during the fault transient. Experimental and simulation results are obtained in order to validate the technique proposed.",2008,0,
1423,1424,Boron emitters: Defects at the silicon - silicon dioxide interface,"An investigation of defects caused by boron diffusion into silicon is presented, using two techniques to directly compare the defects at an undiffused and lightly boron diffused Si-SiO<inf>2</inf> interface. The first technique uses field effect passivation induced by a MOS structure; the second uses Electron Paramagnetic Resonance measurements to determine the concentration of unpassivated P<inf>b</inf> centers on <111> oriented surfaces. It is found that additional defects introduced by the boron diffusion account for a relatively small proportion of total recombination at a well passivated <100> interface, while for more at <111> interfaces, as both the defect density and recombination increase by a factor of more than 2. The effect of the addition of LPCVD nitride on top of oxide layers is also explored. We show that exposure of samples to hot phosphoric acid (used to selectively remove silicon nitride) leads to significant changes to the Si-SiO2 interface, so that this treatment cannot be considered noninvasive.",2008,0,
1424,1425,Fault-oriented software robustness assessment for multicast protocols,"This paper reports a systematic approach for detecting software defects in multicast protocol implementations. We deploy a fault-oriented methodology and an integrated test system targeting software robustness vulnerabilities. The primary method is to assess protocol implementation by non-traditional interface fault injection that simulates network attacks. The test system includes a novel packet driving engine, a PDU generator based on Strengthened BNF notation and a few auxiliary tools. We apply it to two multicast protocols, IGMP and PIM-DM, and investigate their behaviors under active functional attacks. Our study proves its effectiveness for promoting production of more reliable multicast software.",2003,0,
1425,1426,A Distributed Replication Strategy Evaluation and Selection Framework for Fault Tolerant Web Services,"Redundancy-based fault tolerance strategies are proposed for building reliable Service-Oriented Architectures/Applications (SOA), which are usually developed on the unpredictable remote Web services. This paper proposes and implements a distributed replication strategy evaluation and selection framework for fault tolerant Web services. Based on this framework, we provide a systematic comparison of various replication strategies by theoretical formula and real-world experiments. Moreover, a user participated strategy selection algorithm is designed and verified. Experiments are conducted to illustrate the advantage of this framework. In these experiments, users from six different locations all over the world perform evaluation of Web services distributed in six countries. Over 1,000,000 test cases are executed in a collaborative manner and detailed results are also provided.",2008,0,
1426,1427,Fault-tolerance by regeneration: using development to achieve robust self-healing neural networks,"Opposed to the standard paradigm of 'fault-tolerance by redundancy', ontogeny offers the possibility to engineer artificial organisms which can re-grow faulty components. Similar to what happens in nature, organisms display self-healing: a homeostatic process which allows proper operation while suffering faults. In this paper we present a system which evolves developing spiking neural networks capable of controlling simulated Khepera robots in a wall avoidance task. Development is controlled by a decentralized process executed by each cell's identical growth program. To test the system's self-healing capability, networks are (1) subjected to random faults during development and (2) mutilated during operation. Results demonstrate how development can (i) rapidly produce proper neuro-controllers and (ii) re-grow neurons to recover normal operation. These results show that development, originally proposed to increase the evolvability of large phenotypes, also allow the production of artifacts with sustained fault-tolerance. These artifacts would be especially well-suited for tasks that require long periods of operation in absence of external maintenance.",2005,0,
1427,1428,AppWatch: detecting kernel bug for protecting consumer electronics applications,"Most consumer electronics products are equipped with diverse devices since they try to provide more services following the convergence trends. Device drivers for those devices are known to cause system failures. Most previous approaches to enhance reliability have been concerned with the kernel, not with applications. In consumer electronics, however, a main application plays a core role of the product. This paper proposes a new mechanism called AppWatch to keep a consumer electronics application reliable against misbehavior of device drivers. AppWatch exploits page management mechanism of the operating system to protect the address space of the application. Since AppWatch can be implemented at a low engineering cost, it is applicable to most systems only if they have the virtual memory system. AppWatch also provides selective protection of applications so that other unprotected applications are isolated from performance loss, if any. We have tested AppWatch in a consumer electronics environment. The result shows that AppWatch effectively protects application programs at a reasonable performance overhead in most workloads, whereas data-intensive workloads show high overhead. AppWatch also protects applications with little performance interference to other unprotected applications.",2010,0,
1428,1429,Fault-Tolerant Operation of a Battery-Energy-Storage System Based on a Multilevel Cascade PWM Converter With Star Configuration,"This paper focuses on fault-tolerant control for a battery-energy-storage system based on a multilevel cascade pulsewidth-modulation (PWM) converter with star configuration. During the occurrence of a single-converter-cell or single-battery-unit fault, the fault-tolerant control enables continuous operation and maintains state-of-charge balancing of the remaining healthy battery units. This enhances both system reliability and availability. A 200-V, 10-kW, 3.6-kWh laboratory system combining a three-phase cascade PWM converter with nine nickel-metal-hydride battery units is designed, constructed, and tested to verify the validity and effectiveness of the proposed fault-tolerant control.",2010,0,
1429,1430,"Combining Duplication, Partial Reconfiguration and Software for On-line Error Diagnosis and Recovery in SRAM-Based FPGAs","SRAM-based FPGAs are susceptible to Single-Event Upsets (SEUs) in radiation-exposed environments due to their configuration memory. We propose a new scheme for the diagnosis and recovery from upsets that combines i) duplication of the core to be protected, ii) partial reconfiguration to reconfigure the faulty part only, and iii) hardcore processor(s) for deciding when and which part will be reconfigured; executing the application in software instead of hardware during fault handling; and controlling the reconfiguration. A hardcore processor has smaller cross section and it is less susceptible than reconfigurable resources. Thus it can temporarily undertake the execution during upset conditions. Real experiments demonstrate that our approach is feasible and an area reduction of more than 40% over the dominant Triple Modular Redundancy (TMR) solution can be achieved at the cost of a reduction in the processing rate of the input.",2010,0,
1430,1431,A generic real-time computer Simulation model for Superconducting fault current limiters and its application in system protection studies,"A model for the SCFCL suitable for use in real time computer simulation is presented. The model accounts for the highly nonlinear quench behavior of BSCCO and includes the thermal aspects of the transient phenomena when the SCFCL is activated. Implemented in the RTDS real-time simulation tool the model has been validated against published BSCCO characteristics. As an example for an application in protection system studies, the effect of an SCFCL on a utility type impedance relay has been investigated using a real time hardware-in-the-loop (RT-HIL) experiment. The test setup is described and initial results are presented. They illustrate the effect of how the relay misinterprets the dynamically changing SCFCL impedance as an apparently more distant fault location. It is expected that the new real-time SCFCL model will provide a valuable tool not only for further protection system studies but for a wide range of RT-HIL experiments of power systems.",2005,0,
1431,1432,New fault tolerant robotic central controller for space robot system based on ARM processor,"A new fault tolerant robotic central controller with dual processing modules is introduced. Each processing module is composed of 32 bit ARM RISC processor and other commercial-off-the-shelf (COTS) devices. As well as, a set of fault handling mechanisms is implemented in the robotic central controller, which can tolerate a single fault. The robotic central controller software based on VxWorks is organized around a set of processes that communicate between each other through a routing process. Considering the demanding of the extremely tight constraints on mass, volume, power consumption and space environmental conditions, the new fault tolerant robotic central controller has been developed. Its excellent data processing capability is enough to meet the space robot missions.",2008,0,
1432,1433,Scheduling of Fault-Tolerant Embedded Systems with Soft and Hard Timing Constraints,"In this paper we present an approach to the synthesis of fault-tolerant schedules for embedded applications with soft and hard real-time constraints. We are interested to guarantee the deadlines for the hard processes even in the case of faults, while maximizing the overall utility. We use time/utility functions to capture the utility of soft processes. Process re-execution is employed to recover from multiple faults. A single static schedule computed off-line is not fault tolerant and is pessimistic in terms of utility, while a purely online approach, which computes a new schedule every time a process fails or completes, incurs an unacceptable overhead. Thus, we use a quasi-static scheduling strategy, where a set of schedules is synthesized off-line and, at run time, the scheduler will select the right schedule based on the occurrence of faults and the actual execution times of processes. The proposed schedule synthesis heuristics have been evaluated using extensive experiments.",2008,0,
1433,1434,Software implemented fault injection for safety-critical distributed systems by means of mobile agents,"The availability of inexpensive powerful microprocessors leads to increasing deployment of those electronic devices in ever new application areas. Currently, the automotive industry considers the replacement of mechanical or hydraulic implementations of safety-critical automotive systems (e.g., braking, steering) by electronic counterparts (so-called ""by-wire systems"") for safety, comfort, and cost reasons. In order to remain operational in the presence of faults, these kinds of systems are built as fault-tolerant distributed real-time systems consisting of interconnected control units. To assure the correct operation of the fault tolerance mechanisms, software implemented fault injection provides low cost and easy to control techniques to test the system under faulty conditions. In this paper we propose a distributed software implemented fault injection framework based on the mobile agent approach. Software agents are designed to utilize the real-time system's global time and messages to trigger the fault injection experiments. We introduce a lightweight agent implementation language to model the fault injection and the concerned system resources, agent migration and logging of the fault injection experiments.",2004,0,
1434,1435,Error rates of M-ary signals with. Multichannel reception in Nakagami-m fading channels,"In this letter, we present closed. form expressions for the exact average symbol-error rate (SER) of M-ary modulations with multichannel reception over Nakagami-m fading channels. The derived expressions extend already available results for the nondiversity case, to maximal-ratio combining-(MRC) and postdetection equal-gain combining (EGC) diversity systems. The average SERs are given in terms of Lauricella's multivariate hypergeometric function F<sub>D</sub> <sup>(n) </sup>. This function exhibits a finite integral representation that can be used for fast and accurate numerical computation of the derived expressions",2006,0,
1435,1436,&Rscr;&epsi;&Lscr;: a fault tolerance linguistic structure for distributed applications,"The embedding of fault tolerance provisions into the application layer of a programming language is a non-trivial task that has not found a satisfactory solution yet. Such a solution is very important, and the lack of a simple, coherent and effective structuring technique for fault tolerance has been termed by researchers in this field as the ""software bottleneck of system development"". The aim of this paper is to report on the current status of a novel fault tolerance linguistic structure for distributed applications characterized by soft real-time requirements. A compliant prototype architecture is also described. The key aspect of this structure is that it allows one to decompose the target fault-tolerant application into three distinct components respectively responsible for: (1) the functional service, (2) the management of the fault tolerance provisions, and (3) the adaptation to the current environmental conditions. The paper also briefly mentions a few case studies and preliminary results obtained exercising the prototype",2002,0,
1436,1437,Assessing Failure of Bridge Construction Using Fuzzy Fault Tree Analysis,Estimating exact probabilities of occurrence of bridge failure for the use in the conventional fault tree analysis (FTA) is difficult when fault events are imprecise such as human error. A fuzzy FTA model employing fuzzy sets and possibility theory to tackle this problem is proposed. An example of the collapse of cantilever gantry during construction demonstrates the capability of this approach that can assist safety engineer to better evaluate bridge performance.,2007,0,
1437,1438,Image mosaic method based on the image geometric correction for traffic accident scene,Image mosaic is one of important technologies for image processing. It is normally used to make up a seamless and high resolution image. There are some algorithms that deal with the image mosaic. But the most make simply two or more images seamlessly form a large image for a holographic scenic display. The post-processing on the photo from a traffic accident scene is required to reflect and allow inspectors to accurately determine the actual distance between objects in the scene. Therefore the images taken from the traffic accident scene need to be corrected before being spliced to each other. The image correction allows the information on the scene to be correctly displayed. The splicing on the corrected images ensures a thorough view and complete information gain that covers the whole scene.,2010,0,
1438,1439,Full contrast transfer function correction in 3D cryo-EM reconstruction,"Over the past years electron cryo-microscopy (cryo-EM) has established itself as an important tool in studying the three dimensional structure of biological molecules up to the resolution of 6-9 A. However, as we pursue even higher resolution (i.e., 3-4 A), the depth-of-field problem inherent in the contrast transfer function emerges as a limiting factor. This problem has been previously addressed in the research community (Jensen, G.J., 2000; DeRosier, D.J., 2000; Zhou, Z.H. and Chiu, W., 2003; Cohen, H.A. et al., 1984). We develop a full theoretical solution to this problem. We show that the projected image from the electron microscope corresponds to neither a slice, nor an Ewald sphere, in the Fourier space, but a pair of quadratic surfaces in that space. The general solutions to this problem for both single and double defocus exposures are developed. Simulations show the correctness of the theory.",2004,0,
1439,1440,2D Photonic Defect Layers in 3D Inverted Opals on Si Platforms,"Dielectric spheres synthesised for the fabrication of self-organized photonic crystals such as opals offer large opportunities for the design of novel nanophotonic devices. In this paper, we show a hexagonal superlattice monolayer of dielectric spheres inscribed on a 3D colloidal photonic crystal by e-beam lithography. The crystal is produced by a variation of the vertical drawing deposition method assisted by an acoustic field. The structures were chosen after simulations showed that a hexagonal super-lattice monolayer in air exhibits an even photonic band gap below the light cone if the refractive index of the spheres is higher than 1.93",2006,0,
1440,1441,A new motion compensation approach for error resilient video coding,"Multihypothesis motion-compensated prediction (MHMCP) can be used as an error resilience technique for video coding. Motivated by MHMCP, we propose a new error resilience approach named alternative motion-compensated prediction (AMCP), where two-hypothesis and one-hypothesis predictions are alternatively used with some mechanism. Both theory and simulation results show that in case of one frame loss, the expected converged error using AMCP is smaller than that using two-hypothesis MCP.",2005,0,
1441,1442,Generic Fault-Tolerance Mechanisms Using the Concept of Logical Execution Time,"Model-based development has become state of the art in software engineering. Unfortunately, the used code generators often focus on the pure application functionality. Features like automatic generation of fault-tolerance mechanisms are not covered. One main reason is the inadequacy of the used models. An adequate model must have amongst others explicit execution semantics and must be suited to support replica determinism and automatic state synchronization. These requirements are fulfilled when using the concept of logical execution time, a time-triggered approach. This approach hides the implementation details like the physical execution from the user, In contrast to other time-triggered paradigms. Within this paper, we present a solution to exploit this concept to realize major fault-tolerance mechanisms in a generic way.",2007,0,
1442,1443,TPT-RAID: a High Performance Box-Fault Tolerant Storage System,"TPT-RAID is a multi-box RAID wherein each ECC group comprises at most one block from any given storage box, and can thus tolerate a box failure. It extends the idea of an out-of band SAN controller into the RAID: data is sent directly between hosts and targets and among targets, and the RAID controller supervises ECC calculation by the targets. By preventing a communication bottleneck in the controller, excellent scalability is achieved while retaining the simplicity of centralized control. TPT-RAID, whose controller can be a software module within an out-of-band SAN controller, moreover conforms to a conventional switched network architecture, whereas an in-band RAID controller would either constitute a communication bottleneck or would have to also be a full-fledged router. The design is validated in an InfiniBand-based prototype using /SCSI and /SER, and required changes to relevant protocols are introduced.",2007,0,
1443,1444,Novel algorithms for earth fault indication based on monitoring of shunt resistance of MV feeder as a part of relay protection,Novel methods for very high-resistance earth fault identification and location in isolated or high impedance earthed distribution systems have been developed. The novel indication algorithms are able to detect and locate faults up to some hundreds of kilo-ohms. These algorithms were implemented in a microprocessor-based feeder terminal. The indication methods have proven to be very appropriate for the implementation and the preliminary results from the field installation and field experiments are promising,2001,0,
1444,1445,Genetic programming approach for fault modeling of electronic hardware,"This paper presents two variants of genetic programming (GP) approaches for intelligent online performance monitoring of electronic circuits and systems. Reliability modeling of electronic circuits can be best performed by the stressor - susceptibility interaction model. A circuit or a system is deemed to be failed once the stressor has exceeded the susceptibility limits. For on-line prediction, validated stressor vectors may be obtained by direct measurements or sensors, which after preprocessing and standardization are fed into the GP models. Empirical results are compared with artificial neural networks trained using backpropagation algorithm. The performance of the proposed method is evaluated by comparing the experiment results with the actual failure model values. The developed model reveals that GP could play an important role for future fault monitoring systems.",2005,0,
1445,1446,Petri sub-nets for minpath-based fault trees,"The choice of proper forms of fault tree (FT) and success tree (ST) representations, respectively inside of Petri nets (PNs) in the field of R&M modeling is not a trivial problem, because there is the danger of stray tokens inside of the sub-PNs of those trees which would disturb the system model's proper operation in the long run. The author has been advocating the use of disjunctive normal forms (DNFs=sum-of-products forms). However, typically in the field of graph connectivity problems the initially found FTs usually result from minpaths rather than from mincuts. The Boolean FT functions are therefore initially conjunctive normal forms (CNFs=product-of-sums forms). As the main result of this paper it is shown that for such FTs, sub-PNs can be designed systematically, even though they are not quite as simple as sub-PNs for FTs of DNFs. The main point is to allow for extra FT input places, and to gather all the tokens corresponding to the single variables of the diverse sums once the repairs of the corresponding components are finished. This way no stray tokens remain inside of the FT's sub-PN. As a consequence of the duality between FTs and STs, and since both trees are usually inserted in the overall system PN model, it suffices to find a DNF or a CNF of either tree's Boolean function. A CNF or a DNF of the other tree is then readily found via Shonnon's inversion theorem, i.e., it needs no complex Boolean algebra manipulations. The general results are formulated as PN design rules",2001,0,
1446,1447,A Mobile Agent-Based Architecture for Fault Tolerance in Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) are prone to failures as they are usually deployed in remote and unattended environments. To mitigate the effect of these failures, fault tolerance becomes imperative. Nonetheless, it remains to be a second tier activityit should not undermine the execution of the mission oriented tasks of WSNs through overly taxing their resources. We define architecture for fault tolerance in WSNs that is based on a federation of mobile agents that is used both for diagnostic intelligence and repair regimen, focusing on being lightweight in energy, communication and resources. Mobile agents are classified here as local, metropolitan, and global, providing fault tolerance at node, network and functional levels. Interactions between mobile agents are inspired by honey bee dance language that builds on semantics of errors classification and their demographic distribution. Our quantitative modeling substantiates that the proposed fault tolerance framework mandates minimalist communication through contextualized bee-inspired interactions, achieving adaptive sensitivity, and hysteresis-based stability",2010,0,
1447,1448,Fault-tolerance and noise modelling in nanoscale circuit design,"Fault-tolerance in integrated circuit design has become an alarming issue for circuit designers and semiconductor industries wishing to downscale transistor dimensions to their utmost. The motivation to conduct research on fault-tolerant design is backed by the observation that the noise which was ineffective in the large-dimension circuits is expected to cause a significant downgraded performance in low-scaled transistor operation of future CMOS technology models. This paper is destined to give an overview of all the major fault-tolerance techniques and noise models proposed so far. Summing and analysing all this work, we have divided the literature into three categories and discussed their applicability in terms of proposing circuit design modifications, finding output error probability or methods proposed to achieve highly accurate simulation results.",2010,0,
1448,1449,A low complexity and efficient slice grouping method for H.264/AVC in error prone environments,"In this paper, a new method is proposed for Macroblock (MB) importance classification of inter frames. Instead of selecting the most important MBs, the least important MBs are decided first. It makes use of the properties of skip mode in the H.264/AVC standard as the first step. Because the number of MBs chosen as skip mode in a frame varies, further classification is usually required. Four other different features therefore are considered to determine the Important Factor of the remaining MBs. It has been proved that the proposed method can provide good objective and subjective video quality performance, whilst also being simple and fast.",2009,0,
1449,1450,A fast method for fault section identification in series compensated transmission lines,"In this paper, a novel and fast method for fault section identification in compensated series transmission lines based on the high frequency traveling wave has been proposed. The method uses the relation of magnitude and polarity between wavefronts of high frequency travelling waves induced by fault. For accurately and fast extracting polarity and magnitude of travelling wave, wavelet transform and modulus maxima are used. Validation of this method is carried out by PSCAD/EMTP and MATLAB simulations for typical 400 kV power system faults. Simulation results reveal high performance of the method.",2010,0,
1450,1451,Operating characteristics of the Permanent-Magnet-Biased Saturation Based Fault Current Limiter,"This paper describes a topological configuration of a fault current limiter consisting of a permanent and saturable core. The operating characteristics of this permanent-magnet-biased saturation based fault current limiter (PMFCL) are simulated in details by means of finite element method (FEM) with ANSOFT. Firstly, the relationship between the total harmonic distortion of transient limiting current and its peak value is analyzed. Secondly, the paper presents the flux density variation with different source voltages as well as magnetic flux density distribution under different operation conditions. Finally, the fault current limiting characteristics influenced by the source voltages and the number of turns of the coil are investigated. The research results present analytical basis and calculation reference for the parameter optimization of this type of PMFCL.",2008,0,
1451,1452,Embedded-software-based approach to testing crosstalk-induced faults at on-chip buses,"Crosstalk effects on long interconnects are becoming significant for high-speed circuits. This paper addresses the problem of testing crosstalk-induced faults at on-chip buses in system-on-a-chip (SOC) designs. We propose a method to self-test on-chip buses at-speed, by executing an automatically synthesized program using on-chip processor cores. The test program, executed at system operational speed, can activate and capture the worst-case crosstalk effects on buses and achieve a complete coverage of crosstalk-induced logical and delay faults. This paper discusses the method and the framework for synthesizing such a test program. Based on the bus protocol, the instruction set architecture of an on-chip processor core, and the system specification, the method generates deterministic tests in the form of instruction sequences. The synthesized test program is highly modularized and compact. The experimental results show that, for testing interconnects between a processor core and any other on-chip core, a 3 K-byte program is sufficient to achieve the complete coverage for crosstalk-induced logical and delay faults",2001,0,
1452,1453,Compact Wireless Devices with Defected-Ground Structures,"In this paper are presented some investigations on micro- strip defected ground structures (DGS). It is shown that the presence of a slot in the ground plane can substantially enhance the electric coupling, or the electric part of a mixed coupling between resonators and its external feedings. The proposed technique can eliminate the very narrow coupling gaps needed for a tight coupling and thus can relax the fab rication tolerances.",2006,0,
1453,1454,Graph fitting test method for the interpolation error of moire fringe,"In order to realize the fast test for the interpolation error, the graph fitting test method for the interpolation error of Moire fringe is put forward in this paper. Firstly, the triangular wave Moire fringe photoelectric signal of the encoder whose phase difference is 90deg are sampled to get the Lissajous graph of the two signals. Secondly, the single wave represented by the founded subsection function is used to fit the practical Moire fringe Lissajous graph. Then, the fitting result is tested to verify whether it satisfies the accuracy requirement. Lastly, the founded subsection function instead of the practical wave function is used to calculate the interpolation error. Using the graph fitting method to sample the Moire fringe single of 15-bits photoelectric encoder to get the interpolation error curve, the tested maximum interpolation error is 70rdquo and the minimum error is - 69rdquo. Comparing with the interpolation error which is received from traditional test method, the change trend of the interpolation error curve is similar, and peak-peak value is almost equality. The results of experiment indicate that: the equipment is convenient and the examination method is efficient and feasible. The measure speed is fast and the manifestation result is intuitionistic. The system can be used in the working field. The method can avoid the speed influence and realize the dynamic interpolation error measure, which is significant for the research of encoder's dynamic accuracy characteristics.",2009,0,
1454,1455,Fault tolerance in systems design in VLSI using data compression under constraints of failure probabilities,"The design of space-efficient support hardware for built-in self-testing (BIST) is of critical importance in the design and manufacture of VLSI circuits. This paper reports new space compression techniques which facilitate designing such circuits using compact test sets, with the primary objective of minimizing the storage requirements for the circuit under test (CUT) while maintaining the fault coverage information. The compaction techniques utilize the concepts of Hamming distance, sequence weights, and derived sequences in conjunction with the probabilities of error occurrence in the selection of specific gates for merger of a pair of output bit streams from the CUT. The outputs of the space compactor may eventually be fed into a time compactor (viz. syndrome counter) to derive the CUT signatures. The proposed techniques guarantee simple design with a very high fault coverage for single stuck-line faults, with low CPU simulation time, and acceptable area overhead. Design algorithms are proposed in the paper, and the simplicity and ease of their implementations are demonstrated with numerous examples. Specifically, extensive simulation runs on ISCAS 85 combinational benchmark circuits with FSIM, ATALANTA, and COMPACTEST programs confirm the usefulness of the suggested approaches",2001,0,
1455,1456,Fault Current Limiter Based on Resonant Circuit Controlled by Power Semiconductor Devices,This work presents a resonant fault current limiter (FCL) controlled by power semiconductor devices. Initially the operation of two ideal resonant circuit topologies as fault current limiter are discussed. The analysis of these circuits is used to derive an alternative topology to the fault current limiter based on the connection of a series and a parallel resonant circuit. Digital models are implemented in the SimPowerSystem/Matlab simulation package to investigate the performance of the proposed FCL to protect transmission and distribution electric networks against short circuit currents. Transfer functions of the linear limiter models are used to identify the effect of each element of the FCL over its stability and its transient response. The developed analysis will be used to derive modifications in the FCL topology in such a way to improve their dynamic response.,2007,0,
1456,1457,How Much Fault Protection is Enough - A Deep Impact Perspective,"For the deep impact project, a myriad of fault protection (FP) monitors, symptoms, alarms and responses is engineered into the spacecraft FP software, common and yet custom to the flyby and impactor mother-daughter spacecraft. Device faults and functional faults are monitored, which are mapped 1-to-n into FP symptoms, per instance of the fault. Symptoms are then mapped n-to-1 to FP alarms, further down mapped n-to-1 to FP responses. Though the final statistics of 49 monitors, 921 symptoms, 667 alarms, and 39 responses appear to be staggering, it remains debatable whether the amount of on-board autonomous fault protection is sufficient and friendly to operate",2005,0,
1457,1458,Fault-tolerant Ethernet middleware for IP-based process control networks,"We present an efficient middleware-based fault-tolerant Ethernet (FTE) developed for process control networks. Our approach is unique and practical in the sense that it requires no change to commercial off-the-shelf hardware (switch, hub, Ethernet physical link, and network interface card) and software (commercial Ethernet NIC card driver and standard protocol such as TCP/IP) yet it is transparent to IP-based applications. The FTE performs failure detection and recovery for handling multiple points of network faults and supports communications with non-FTE-capable devices. Our experimentation shows that FTE performs efficiently, achieving less than 1-ms end-to-end swap time and less than 2-sec failover time, regardless of the concurrent application and system loads. In this paper, we describe the FTE architecture, the challenging technical issues addressed, our performance evaluation results, and the lessons learned in design and development of such an open-network-based fault-tolerant network",2000,0,
1458,1459,A method for inductor core loss estimation in power factor correction applications,"Conventional core loss estimation methods exhibit limitations in dealing with important aspects of switching power converter applications such as different duty cycles, discontinuous-conduction-mode, variable switching frequency, or variable duty cycle operation. These limitations are particularly evident when trying to estimate boost inductor core loss in power factor correction circuits. This paper first presents a core loss estimation method that addresses these limitations and then demonstrates an effective technique to estimate core losses in power factor correction circuits. Finally, the authors show examples of how this method can be conveniently incorporated into simulation software to automate the core loss estimation process. The inductor models that are developed to facilitate this automatic core loss estimation and the approaches to implement the calculation in simulation software, especially a program called SIMPLIS, are also provided",2002,0,
1459,1460,Research and Implementation of Fault-Tolerant Computer Interlocking System,"A new signal control system for railway stations, fault-tolerant all- electronic computer interlocking control system, is proposed,in which the computer-based interlocking system layer is constituted through the implementation of electronic security unit replacing the Relay, and the all-electronic fault-tolerant controlling for whole system is fulfilled through two of three fault-tolerant computer system. Furthermore, the overall structure, function and fault-tolerant security designing of the system are discussed in detail. The system can meet the requirements of high reliability, availability and real-time controlling, it also can monitor the external equipments and their own equipments real-timely. The system has been put into operation and run stably and reliabl.",2010,0,
1460,1461,A new method for fault section estimation in distribution network,"Determination of fault section is a necessary step for locating the fault in the distribution power system. In this paper a new practical based method is presented for fault section estimation in distribution system. In the proposed method, at first different zones is defined using impedance classifier. Then, the suitable locations for installing the cutout fuses are determined using expert of designer. After that, special settings for cutout fuse links are determined in such a way that they operate coordinately. Finally, current waveforms are used to distinguish which cutout fuse operated or in which section fault occurred.",2010,0,
1461,1462,Detecting processor hardware faults by means of automatically generated virtual duplex systems,"A virtual duplex system (VDS) can be used to increase safety without the use of structural redundancy on a single machine. If a deterministic program P is calculating a given function f, then a VDS contains two variants P<sub>a</sub> and P<sub>b</sub> of P which are calculating the diverse functions f<sub>a</sub> and f<sub>b</sub> in sequence. If no error occurs in the process of designing and executing P<sub>a</sub> and P<sub>b</sub>, then f= f<sub>a</sub>=f<sub>b</sub> holds. A fault in the underlying processor hardware is likely to be detected by the deviation of the results, i.e. f<sub>a</sub>(i)=f<sub>b</sub>(i) for input i. Normally, VDSs are generated by manually applying different diversity techniques. This paper, in contrast, presents a new method and a tool for the automated generation of VDSs with a high detection probability for hardware faults. Moreover, for the first time the diversity techniques are selected by an optimization algorithm rather than chosen intuitively. The generated VDSs are investigated extensively by means of software implemented processor fault injection.",2002,0,
1462,1463,Two Types of Action Error: Electrophysiological Evidence for Separable Inhibitory and Sustained Attention Neural Mechanisms Producing Error on Go/No-go Tasks,"Disentangling the component processes that contribute to human executive control is a key challenge for cognitive neuroscience. Here, we employ event-related potentials to provide electrophysiological evidence that action errors during a go/no-go task can result either from sustained attention failures or from failures of response inhibition, and that these two processes are temporally and physiologically dissociable, although the behavioral errora nonintended responseis the same. Thirteen right-handed participants performed a version of a go/no-go task in which stimuli were presented in a fixed and predictable order, thus encouraging attentional drift, and a second version in which an identical set of stimuli was presented in a random order, thus placing greater emphasis on response inhibition. Electrocortical markers associated with goal maintenance (late positivity, alpha synchronization) distinguished correct and incorrect performance in the fixed condition, whereas errors in the random condition were linked to a diminished N2P3 inhibitory complex. In addition, the amplitude of the error-related negativity did not differ between correct and incorrect responses in the fixed condition, consistent with the view that errors in this condition do not arise from a failure to resolve response competition. Our data provide an electrophysiological dissociation of sustained attention and response inhibition.",2009,0,
1463,1464,Proactive Cellular Network Faults Prediction Through Mobile Intelligent Agent Technology,"Cellular network faults prediction models using mobile intelligent agent are presented in this paper. Cellular networks are uncertain and dynamic in their behaviours and therefore we use different artificial intelligent techniques to develop platform independent, autonomous, reasoning and robust agents that can report on any unforeseen anomaly within the cellular network service provider. The specific design and implementation is done using Java agent development framework (JADE). The partial results obtained from the experiments conducted are presented and discussed in this paper.",2007,0,
1464,1465,Automated bug tracking: the promise and the pitfalls,"Bug tacking systems give developers a unique and clear view into user's everyday product experiences. Adding some statistical analysis and software teams can efficiently improve product quality. It's hard to tell precisely how well the error reporting system working, but this seems to be a bug weapon that has landed a permanent spot in microsoft's arsenal. Automated bug tracking, combined with statistical reporting, plays a key role for developers at the Mozilla Foundations, best known for its open source Web browser and email software. The sparse, random sampling approach produces enough data for the team to do what it call ""statistical debugging""-bug detection through statistical analysis.",2004,0,
1465,1466,"e-SAFE: An Extensible, Secure and Fault Tolerant Storage System","This paper describes e-SAFE , a scalable utility-driven distributed storage system that offers very high availability at an archival scale and reduces management overhead such as periodic repairs. e-SAFE is designed to provide a storage utility for environments such as large-scale data centers in enterprise networks where the servers experience temporary unavailability (possibly high load, temporary downtimes due to repair or software/hardware upgrades). e-SAFE is based on a simple principle: efficiently sprinkle data all over a distributed storage and robustly reconstruct even when many of them are unavailable. e-SAFE also provides strong guarantee on data-integrity. The use of Fountain codes for replicating file data blocks, an efficient algorithm for fast parallel encoding and decoding over multiple file segments, a utility module for service differentiation and auto-adjustments of design parameters, and a background replication mechanism hiding the cost of replication and dissemination from the user, provide a fast, durable and autonomous storage solution.",2007,0,
1466,1467,Fault-Tolerant Behavior-Based Motion Control for Offroad Navigation,"Many tasks examined for robotic application like rescue missions or humanitarian demining require a robotic vehicle to navigate in unstructured natural terrain. This paper introduces a motion control for a four-wheeled offroad vehicle trying to tackle the problems arising. These include rough ground, steep slopes, wheel slippage, skidding and others that are difficult to grasp with a physical model and often impossible to acquire with sensory equipment. Therefore, a more reactive approach is chosen using a behavior-based architecture. This way a certain generalization in unknown environment is expected. The resulting behavior network is described and experiments performed in a simulation environment as well as in real world are presented. Additionally the performance of the utilized vehicle in case of mechanical or electronic defects is examined in simulation.",2005,0,
1467,1468,Intermittent faults and effects on reliability of integrated circuits,"A significant amount of research has been aimed at analyzing the effects of high energy particles on semiconductor devices. However, less attention has been given to the intermittent faults. Field collected data and failure analysis results presented in this paper clearly show intermittent faults are a major source of errors in modern integrated circuits. The root cause for these faults ranges from manufacturing residuals to oxide breakdown. Burstiness and high error rates are specific manifestations of the intermittent faults. They may be activated and deactivated by voltage, frequency, and operating temperature variations. The aggressive scaling of semiconductor devices and the higher circuit complexity are expected to increase the likelihood of occurrence of the intermittent faults, despite the extensive use of fault avoidance techniques. Herein we discuss the effectiveness of several fault tolerant approaches, taking into consideration the specifics of the errors generated by intermittent faults. Several solutions, previously proposed for handling particle induced soft errors, are exclusively based on software and too slow for handling large bursts of errors. As a result, hardware implemented fault tolerant techniques, such as error detecting and correcting codes, self checking, and hardware implemented instruction retry, are necessary for mitigating the impact of the intermittent faults, both in the case of microprocessors, and other complex integrated circuits.",2008,0,
1468,1469,Research on the Application of Data Mining in Software Testing and Defects Analysis,"The high dependability software is not only one of software technique development commanding points, but also is the software industry development essential foundation, this paper summarizes the data mining to face the detect of the software credibility test, the appraisal and the technical aspect newest research, elaborated the data mining technology in the software flaw test application, including flaw test in commonly used data mining method, data mining system and software testing management system. Introduced specifically in view of the software flaw's different classification based on the connection rule's software flaw parsing technique's application, proposed based on the association rule's software detect evaluation method, the purpose of which is to decrease software defects and to achieve the rapid growth of software dependability.",2009,0,
1469,1470,Studying the fault-detection effectiveness of GUI test cases for rapidly evolving software,"Software is increasingly being developed/maintained by multiple, often geographically distributed developers working concurrently. Consequently, rapid-feedback-based quality assurance mechanisms such as daily builds and smoke regression tests, which help to detect and eliminate defects early during software development and maintenance, have become important. This paper addresses a major weakness of current smoke regression testing techniques, i.e., their inability to automatically (re)test graphical user interfaces (GUIs). Several contributions are made to the area of GUI smoke testing. First, the requirements for GUI smoke testing are identified and a GUI smoke test is formally defined as a specialized sequence of events. Second, a GUI smoke regression testing process called daily automated regression tester (DART) that automates GUI smoke testing is presented. Third, the interplay between several characteristics of GUI smoke test suites including their size, fault detection ability, and test oracles is empirically studied. The results show that: 1) the entire smoke testing process is feasible in terms of execution time, storage space, and manual effort, 2) smoke tests cannot cover certain parts of the application code, 3) having comprehensive test oracles may make up for not having long smoke test cases, and 4) using certain oracles can make up for not having large smoke test suites.",2005,0,
1470,1471,Performance enhancement defect tolerance in the cell matrix architecture,"This research concentrates on the area of fault tolerant circuit implementation in a field programmable type architecture, In particular, an architecture called the Cell Matrix, presented as a fault tolerant alternative to field programmable gate arrays using their Supercell approach, is studied. Architectural constraints to implement fault tolerant circuit design in this architecture are discussed. Some modifications of its basic Structure, such as the integration of circuitry for error correction and scan path, to enhance fault tolerant circuits design are introduced and are compared to the Supercell approach.",2004,0,
1471,1472,Fault management using passive testing for mobile IPv6 networks,"In this paper, we employ the communicating finite state machine (CFSM) model for networks to investigate fault management using passive testing. First, we introduce the concept of passive testing. Then, we introduce the CFSM model, the observer model and the fault model with necessary assumptions. We introduce the fault detection algorithm using passive testing. Then, we briefly present our new passive testing approach for fault location, fault identification, and fault coverage based on the CFSM model. We illustrate the effectiveness of our new technique through simulation of a practical protocol example, a 4-node mobile IPv6 network. Finally, conclusions and potential extensions are discussed",2001,0,
1472,1473,Electrical model for program disturb faults in non-volatile memories,Non-volatile memories (NVMs) are susceptible to special type of faults known as disturb faults. A class of these disturb faults are faults induced by high electric field stress known as program disturbs. In this paper we discuss the physical nature of the defects that are responsible for these faults in flash memories. We develop an electrical fault model for defects and simulate faulty cell behavior based on physical defect location (in gate oxide). We also evaluate the impact of these defects on cell performance. The modeling technique is flexible and applicable under different disturb conditions and defect characteristics.,2003,0,
1473,1474,Reliable JPEG 2000 wireless imaging by means of error-correcting MQ coder,"A new error resilience tool is proposed for robust JPEG 2000 imaging over noisy channels. In particular, a modified encoder, based on an MQ arithmetic coder with forbidden symbol, is introduced, along with a maximum likelihood error-correcting MQ decoder. The proposed technique features error detection, error concealment and error correction capability, thus adding new useful functionalities to JPEG 2000. Experimental results show that this technique largely outperforms the standard JPEG 2000 error resilience tools for error concealment and hard/soft channel decoding.",2004,0,
1474,1475,A Resource Management System for Fault Tolerance in Grid Computing,"In grid computing, resource management and fault tolerance services are important issues. The availability of the selected resources for job execution is a primary factor that determines the computing performance. The failure occurrence of resources in the grid computing is higher than in a tradition parallel computing. Since the failure of resources affects job execution fatally, fault tolerance service is essential in computational grids. And grid services are often expected to meet some minimum levels of quality of service (QoS) for desirable operation. However Globus toolkit does not provide fault tolerance service that supports fault detection service and management service and satisfies QoS requirement. Thus this paper proposes fault tolerance service to satisfy QoS requirement in computational grids. In order to provide fault tolerance service and satisfy QoS requirements, we expand the definition of failure, such as process failure, processor failure, and network failure. And we propose resource scheduling service, fault detection service and fault management service and show implement and experiment results.",2009,0,
1475,1476,Identification of a chemical process for fault detection application,"This paper presents the application results concerning the fault detection of a dynamic process using linear system identification and model-based residual generation techniques. The first step of the considered approach consists of identifying different families of linear models for the monitored system in order to describe the dynamic behaviour of the considered process. The second step of the scheme requires the design of output estimators (e.g., dynamic observers or Kalman filters) which are used as residual generators. The proposed fault detection and system identification schemes have been tested on a chemical process in the presence of sensor, actuator, component faults and disturbance. The results and concluding remarks have been finally reported.",2004,0,
1476,1477,A fault-tolerant system for Java/CORBA objects,"Frameworks like CORBA facilitate the development of distributed applications through the use of off-the-shelf components. Though the use of distributed components allows faster building of applications, it also reduces the application availability as failure of any component can make the application unavailable. In this paper we present the design and implementation of a fault-tolerant system for CORBA objects implemented in Java. The proposed fault tolerant system employs object replication. We use a three tier architecture in which the middle tier manages replication and acts as a proxy for replicated objects. The proxy ensures consistency and transparency. In the current implementation, the proxy uses the primary-site approach to ensure strong consistency. Saving and restoring of objects' state is done transparently and it does not require object implementation to have special functions implemented for this purpose.",2008,0,
1477,1478,A Byzantine Fault Tolerant Protocol for Composite Web Services,"This paper presents a Byzantine fault tolerant protocol for composite Web Services. We extend Castro and Liskov's well-known practical Byzantine fault tolerance method for the server client model and Tadashi Araragi's method for the agent system to a method for the composite Web Services. Different from other Byzantine tolerant methods for single web service, in composite Web Services we have to create replicas on both sides, while in the server-client model of Castro and Liskov's method, replicas are created only on the server side. We present a modular implementation, and experimental results demonstrate only a moderate overhead due to replication.",2010,0,
1478,1479,Crisp--A Fault Localization Tool for Java Programs,"Crisp is an Eclipse plug-in tool for constructing intermediate versions of a Java program that is being edited. After a long editing session, a programmer will run regression tests to make sure she has not invalidated previously tested functionality. If a test fails unexpectedly, Crisp allows the programmer to select parts of the edit that affected the failing test and to add them to the original program, creating an intermediate version guaranteed to compile. Then the programmer can re-execute the test in order to locate the exact reasons for the failure by concentrating on those affecting changes that were applied. Using Crisp, a programmer can it- eratively select, apply, and undo individual (or sets of) affecting changes and, thus effectively find a small set of failure-inducing changes. Crisp is an extension to our change impact analysis tool, Chianti, [6].",2007,0,
1479,1480,Defect tolerance of solid dielectric transmission class cable,"This paper addresses the issue of determining the level of defect that is likely to cause the failure of solid dielectric transmission class cables. It also proposes methods for predicting the level of defect that is likely to cause failure and to provide a simple analytic approximation for doing so in the case of conducting spheroids aligned with the electric field. A common assumption is that conducting particles > 100 m in length are likely to cause failure of extruded dielectric transmission cable. This analysis suggests that when the effects of operation at elevated temperature are included in the analysis, this is probably an appropriate criterion with a sound technical basis. For maximum background fields in the range of 15 kV/mm, as presently seen near the conductor shield of some transmission class cables, a worst-case particle length in the range of 0.1 mm is likely to be required to cause failure for the worst-case local polymer morphology in the range of the maximum operating temperature.",2005,0,
1480,1481,Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in SRAM-Based FPGAs,An approach combining the SRAM-based field-programmable gate array static cross-section with the results of fault injection campaigns allows predicting the error rate of any implemented application. Experimental results issued from heavy ion tests are compared with predictions to validate the proposed methodology.,2010,0,
1481,1482,A hybrid scatter correction for 3D PET based on an estimation of the distribution of unscattered coincidences: implementation on the ECAT EXACT HR+,"We implemented a hybrid scatter correction method for 3D PET that combines two scatter correction methods in a complementary way. The implemented scheme uses a method based on the discrimination of the energy of events (the estimation of trues method, ETM) and an auxiliary method (the single scatter simulation method, or the convolution-subtraction method), in an attempt to increase the accuracy of the correction over a wider range of acquisitions. The ETM takes into account the scatter from outside the field-of-view (FOV), which is not estimated with the auxiliary method. On the other hand, the auxiliary method accounts for events that have scattered with small angles, which have an energy that can not be discriminated from that of unscattered events using the ETM. The ETM uses the data acquired in an upper energy window above the photopeak (550-650 keV), to obtain a noisy estimate of the unscattered events in the standard window (350-650 keV). Our implementation uses the auxiliary method to correct the residual scatter in the upper window. After appropriate scaling, the upper window data is subtracted from the total coincidences acquired in the standard window, resulting in the final scatter estimate, after smoothing. We compare the hybrid method with the corrections used by default in the 2D and 3D modes of the ECAT EXACT HR+, using phantom measurements. Generally, the contrast was better with the hybrid method, although the relative errors of quantification were similar. We conclude that hybrid techniques such as the one implemented in this work can provide an accurate, general-purpose and practical way to correct the scatter in 3D PET, taking into account the scatter from outside the FOV",2000,0,
1482,1483,On the error performance of 8-VSB TCM decoder for ATSC terrestrial broadcasting of digital television,"The error performance of various 8-VSB TCM decoders for reception of terrestrial digital television is analyzed. In previous work, 8-state TCM decoders were proposed and implemented for terrestrial broadcasting of digital television. In this paper, the performance of a 16-state TCM decoder is analyzed and simulated. It is shown that not only a 16-state TCM decoder outperforms one with 8-states, but it also has much smaller error coefficients",2000,0,
1483,1484,Fault-Tolerant Operating Strategies Applied to Three-Phase Induction-Motor Drives,"This paper presents a comparative analysis involving several fault-tolerant operating strategies, applied to three-phase induction-motor drives, that intend to compensate for inverter faults. The results presented show the advantages and the inconveniences of several fault-tolerant drive structures, under different control techniques, such as the field-oriented control and the direct torque control. Experimental results concerning the performance of the three-phase induction motor, based on the analysis of some key parameters, like induction-motor efficiency, motor power factor, and harmonic distortion of both motor line currents and phase voltages, will be presented",2006,0,
1484,1485,Error detection and concealment for video conferencing service,"Compressed video bitstream is intended for real-time transmission over communication networks. However, it is very sensitive to transmission errors. Even a single bit error can lead to disastrous video quality degradation in both time and spatial domain. This quality deterioration is exacerbated when no error control mechanism is employed to protect coded video data against the error prone environments. In this paper, we present a new error detection and concealment algorithm to reduce the effects of transmission error in the video decoder.We verified that the proposed algorithm generates good performances in PSNR and objective visual quality through the computer simulation by H.324M mobile simulation set.",2009,0,
1485,1486,A persistent diagnostic technique for unstable defects,"We present a persistent diagnostic technique for unstable defects, such as open defects or delay defects. A new ""segment model"" diagnosis for the completely open defects is discussed. Here, we not only focus on the behavior of the principal offender, but also the behavior of the accomplices which cause the unstable behavior of the defect. In this paper, a technique using the layout information for an open fault diagnosis, and a testing method for the delay fault are discussed. Some experimental results of actual chips are shown.",2002,0,
1486,1487,Analytical redundancy based predictive fault tolerant control of a steer-by-wire system using nonlinear observer,"In this paper, a nonlinear observer based analytical redundancy methodology is presented for fault tolerant control of a steer by wire (SBW) system. A long-range predictor based on Diophantine identity has been utilized to improve the fault detection efficiency. The overall predictive fault tolerant control strategy was then implemented and validated on a steer by wire hardware in loop bench. The experimental results showed that the overall robustness of the SBW system was not sacrificed through the usage of analytical redundancy for sensors along with the designed FDIA algorithm. Moreover, the experimental results indicated that the faults could be detected faster using the developed analytical redundancy based algorithms for attenuating-type faults.",2010,0,
1487,1488,LIFTING: A Flexible Open-Source Fault Simulator,"This paper presents LIFTING (LIRMM fault simulator), an open-source simulator able to perform both logic and fault simulations for single/multiple stuck-at faults and single event upset (SEU) on digital circuits described in Verilog. Compared to existing tools, LIFTING provides several features for the analysis of the fault simulation results, meaningful for research purposes. Moreover, as an open-source tool, it can be customized to meet any user requirements. Experimental results show how LIFTING has been exploited on research fields. Eventually, execution time for large circuit simulations is comparable to the one of commercial tools.",2008,0,
1488,1489,Correction,"In the above title (ibid, vol. 44, issue 3, pp. 114-123, Jun 02), corrections were made to Equation 38. There were various typographical errors.",2007,0,
1489,1490,A classification approach for power distribution systems fault cause identification,"Power distribution systems play an important role in modern society. When distribution system outages occur, fast and proper restorations are crucial to improve the quality of services and customer satisfaction. Proper usages of outage root cause identification tools are often essential for effective outage restorations. This paper reports on the investigation and results of two popular classification methods: logistic regression (LR) and artificial neural network (ANN) applied on power distribution fault cause identification. LR is seldom used in power distribution fault diagnosis, while ANN has been extensively used in power system reliability researches. This paper discusses the practical application problems, including data insufficiency, imbalanced data constitution, and threshold setting that are often faced in power distribution fault cause identification problems. Two major distribution fault types, tree and animal contact, are used to illustrate the characteristics and effectiveness of the investigated techniques.",2006,0,
1490,1491,The Method of Error Controlling on the Vectorization of Dot Matrix Image,"A new vectorization method of dot matrix image based on error controlling is studied in this paper. The least-square algorithm is a main method of vectorization of dot matrix image. In the calculation of the fitting error by using the least-square algorithm, if the fitting error is in the range of threshold value, the result of vectorization will be considered to be right. So how to determine the threshold value is the key. In this paper, it has analyzed the vectorization errors of ideal bitmap by least-square algorithm and put forward the judgment formulas to evaluate recognition result. In order to complete vectorization automatically, it has presented a new method of the vectorization of dot matrix image-the least-square rolling recognition algorithm for double direction. It has solved the problem of selecting suitable dot group. The judgment formulas in this paper are scientific and accurate; the method of the vectorization is rapid and high efficient.",2008,0,
1491,1492,Sensor fault-tolerant vector control of induction motors,"The authors propose a multisensor switching strategy for fault-tolerant vector control of induction motors. The proposed strategy combines three current sensors and associated observers that estimate the rotor flux. The estimates provided by the observers are compared at each sampling time by a switching mechanism which selects the sensors-observer pair with the smallest error between the estimated flux magnitude and a desired flux reference. The estimates provided by the selected pair are used to implement a vector control law. The authors consider both field-oriented control and direct torque and flux control schemes. Pre-checkable conditions are derived that guarantee fault tolerance under an abrupt fault of a current sensor. These conditions are such that the observers that use measurements from the faulty sensor are automatically avoided by the switching mechanism, thus maintaining good performance levels even in the presence of a faulty sensor. Simulation results under realistic conditions illustrate the effectiveness of the scheme.",2010,0,
1492,1493,Error Simulation Analysis of Gripping Deviation of Plate Specimen Tensile Test Based on Finite Element Method,"The plate specimen tensile test is a common material mechanical property test in practical engineering, but the gripping deviation of plate specimen will bring tensile test result an error. In this paper, a finite element model of aluminum alloy notched specimen is built and a static finite element analysis is carried on, the influence of the plate specimen tensile test gripping position deviation rightness of experiment to the test result is studied. The result expresses that the gripping deviation of plate specimen will bring tensile test result an error bigger. It is necessary to raise the accuracy request of the gripping position of plate specimen tensile test.",2009,0,
1493,1494,On the efflciency of error concealment techniques in H.264/AVC coders,"In videoconferencing and video telephony applications operating in real time, a fluent transmission, even presenting visible errors, is often preferred over a correct but jerking transmission. This is the reason why error concealment techniques are adopted, within video codecs, to recover the transmission quality without affecting its fluency. In the framework of motion estimation based video codecs, like H.263 and H.264, error resilience facilities are made available in order to mitigate the effects of information loss during transmissions on packet networks. In this paper we focus on the adoption of error concealment techniques in H.264/AVC video coding, providing examples of both objective and subjective performance evaluation, when different algorithms are implemented at the decoder. Besides evaluating the hybrid concealment scheme already foreseen by the standard implementation, we also present a simple ""pure temporal"" replacement technique, which could be interesting for its good performance combined with a very low impact on the overall processing time",2005,0,
1494,1495,Semiparametric RMA Background-Correction for Oligonucleotide Arrays,Microarray technology has provided an opportunity to simultaneously monitor the expression levels of a large number of genes in response to intentional perturbations. A necessary step towards successful use of microarray technology is background correction which aims to remove noise. One of the most popular algorithms for background correction is the robust multichip average (RMA) procedure which relies on an unjustified parametric assumption. In this paper we first check the fitness of the RMA model using a graphical approach and then propose a new background correction method based on a semiparametric RMA model (semi-RMA). Evaluation of the proposed approach based on spike-in data and MAQC (microarray quality control project) data shows our semi-RMA model provides a better fit to microarray data than other approaches.,2007,0,
1495,1496,Architecture-Level Soft Error Analysis: Examining the Limits of Common Assumptions,"This paper concerns the validity of a widely used method for estimating the architecture-level mean time to failure (<i>MTTF</i>) due to soft errors. The method first calculates the failure rate for an architecture-level component as the product of its raw error rate and an architecture vulnerability factor (<i>AVF</i>). Next, the method calculates the system failure rate as the sum of the failure rates (<i>SOFR</i>) of all components, and the system <i>MTTF</i> as the reciprocal of this failure rate. Both steps make significant assumptions. We investigate the validity of the <i>AVF+SOFR</i> method across a large design space, using both mathematical and experimental techniques with real program traces from <i>SPEC</i> 2000 benchmarks and synthesized traces to simulate longer real-world workloads. We show that <i>AVF+SOFR</i> is valid for most of the realistic cases under current raw error rates. However, for some realistic combinations of large systems, long-running workloads with large phases, and/or large raw error rates, the <i>MTTF</i> calculated using <i>AVF+SOFR</i> shows significant-discrepancies from that using first principles. We also show that SoftArch, a previously proposed alternative method that does not make the <i>AVF+SOFR</i> assumptions, does not exhibit the above discrepancies.",2007,0,
1496,1497,Fault detection and diagnosis system for air-conditioning units using recurrent type neural network,"The air-conditioning systems of buildings have been diversified in recent years, and the complexity of the systems has increased. At the same time, stability in the system and low running cost are demanded. To solve these problems, various research projects have been done. The development of the energy load prediction systems and the fault detection and diagnosis systems have received great attention. The authors propose a real time fault diagnosis system for air conditioning units (the heating unit, the cooling unit, the air intake unit, and the air-recycling unit) using a recurrent type neural network",2000,0,
1497,1498,"REDFLAG a Run-timE, Distributed, Flexible, Lightweight, And Generic fault detection service for data-driven wireless sensor applications","Increased interest in Wireless Sensor Networks (WSNs) by scientists and engineers is forcing WSN research to focus on application requirements. Data is available as never before in many fields of study; practitioners are now burdened with the challenge of doing data-rich research rather than being data-starved. In-situ sensors can be prone to errors, links between nodes are often unreliable, and nodes may become unresponsive in harsh environments, leaving to researchers the onerous task of deciphering often anomalous data. Presented here is the REDFLAG fault detection service for WSN applications, a Run-timE, Distributed, Flexible, detector of faults, that is also Lightweight And Generic. REDFLAG addresses the two most worrisome issues in data-driven wireless sensor applications: abnormal data and missing data. REDFLAG exposes faults as they occur by using distributed algorithms in order to conserve energy. Simulation results show that REDFLAG is lightweight both in terms of footprint and required power resources while ensuring satisfactory detection and diagnosis accuracy. Because REDFLAG is unrestrictive, it is generically available to a myriad of applications and scenarios.",2009,0,
1498,1499,RFID and IPv6-enabled Ubiquitous Medication Error and Compliance Monitoring System,Became of the world's rapidly-aging population' costs for healthcare are getting higher and higher due partially to people who failed to comply with their medication regimens costing billion per year and affecting few million patients. This paper presents a combination of IPv6 network and RFID-based medication error monitoring system integrating with Wi-Fi and GSM wireless communication techniques that is able to collect user's medicine-taking records any time - anywhere as reference for the purpose of proper diagnosis and reduce the healthcare costs as a result.,2007,0,
1499,1500,A new approach for mitigating carrier phase multipath errors in multi-gnss real-time kinematic (RTK) receivers,"In this paper, we introduce a new approach for RTK positioning using triple-frequency combinations of GNSS measurements in presence of carrier phase multipath. The proposed method is based on a modification of the LAMBDA method, where the a-priori information on multipath errors is exploited as a constraint in the optimization and ambiguities search process to mitigate the effect of multipath. Triple-frequency combinations of measurements is used to formulate a new carrier phase multipath index, then incorporate it as additional constraint in the LAMBDA method cost function for multi-frequency ambiguity resolution. Simulations and real experiments shows the effectiveness of the developed scheme.",2010,0,
1500,1501,Microstrip Monopole Antenna With Enhanced Bandwidth Using Defected Ground Structure,"In this letter, a double U-shaped defected ground structure (DGS) is proposed to broaden impedance bandwidth of a microstrip-fed monopole antenna. The antenna structure consists of a simple trapezoid monopole with a DGS microstrip feedline for excitation and impedance bandwidth broadening. Measurement shows that the antenna has 10-dB return loss from 790 to 2060 MHz, yielding 112.4% impedance bandwidth improvement over that of traditional design.",2008,0,
1501,1502,Error whitening criterion for adaptive filtering: theory and algorithms,"Mean squared error (MSE) has been the dominant criterion in adaptive filter theory. A major drawback of the MSE criterion in linear filter adaptation is the parameter bias in the Wiener solution when the input data are contaminated with noise. We propose and analyze a new augmented MSE criterion called the Error Whitening Criterion (EWC). EWC is able to eliminate this bias when the noise is white. We will determine the analytical solution of the EWC, discuss some interesting properties, and develop stochastic gradient and other fast algorithms to calculate the EWC solution in an online fashion. The stochastic algorithms are locally computable and have structures and complexities similar to their MSE-based counterparts (LMS and NLMS). Convergence of the stochastic gradient algorithm is established with mild assumptions, and upper bounds on the step sizes are deduced for guaranteed convergence. We will briefly discuss an RLS-like Recursive Error Whitening (REW) algorithm and a minor components analysis (MCA) based EWC-total least squares (TLS) algorithm and further draw parallels between the REW algorithm and the Instrumental Variables (IV) method for system identification. Finally, we will demonstrate the noise-rejection capability of the EWC by comparing the performance with MSE criterion and TLS.",2005,0,
1502,1503,Multilevel full-chip gridless routing considering optical proximity correction,"To handle modern routing with nanometer effects, we need to consider designs of variable wire widths and spacings, for which gridless routers are desirable due to their great flexibility. The gridless routing is much more difficult than the grid-based one because the solution space of gridless routing is significantly larger than that of grid-based one. In this paper, we present the first multilevel, full-chip gridless detailed router. The router integrates global routing, detailed routing, and congestion estimation together at each level of the multilevel routing. It can handle non-uniform wire widths and consider routability and optical proximity correction (OPC). Experimental results show that our approach obtains significantly better routing solutions than previous works. For example, for a set of 11 commonly used benchmark circuits, our approach achieves 100% routing completion for all circuits while the famous state-of-the-art three-level routing and multilevel routing (multilevel global routing + flat detailed routing) cannot complete routing for any of the circuits. Besides, experimental results show that our multilevel gridless router can handle non-uniform wire widths efficiently and effectively (still maintain 100% routing completion for all circuits). In particular, our OPC-aware multilevel gridless router archives an average reduction of 11.3% pattern features and still maintains 100% routability for the 11 benchmark circuits.",2005,0,
1503,1504,A new adaptive hybrid neural network and fuzzy logic based fault classification approach for transmission lines protection,"In this paper, an adaptive hybrid neural networks and fuzzy logic based algorithm is proposed to classify fault types in transmission lines. The proposed method is able to identify all ten shunt faults in transmission lines with high level of robustness against variable conditions such as measured amplitudes and fault resistance. In this approach, a two-end unsynchronized measurement of the signals is used. For real-time estimation of unknown synchronization angle and three phase phasors a two-layer adaptive linear neural (ADALINE) network is used. The estimated parameters are fed to a fuzzy logic system to classify fault types. This method is feasible to be used in digital distance relays which are able to be programmed, to share and discourse data with all protective and monitoring devices. The proposed method is evaluated by a number of simulations conducted in PSCAD/EMTDC and MATLAB software.",2008,0,
1504,1505,Frame loss error concealment for spatial scalability using hallucination,"We present a new error concealment algorithm for spatially scalable video coding with frame loss in the enhancement layer, based on the technique of hallucination. For a lost enhancement layer frame, the error concealment is done as hallucinating its base layer frame, using the database trained from previously decoded frames nearby to the lost one. Simulation results show that the proposed method could out-perform the state-of-the-art error concealment algorithms of SVC significantly.",2009,0,
1505,1506,A Comparison of Mandani and Sugeno Inference Systems for a Space Fault Detection Application,"This research provides a comparison between the performances of TSK (Takagi, Sugeno, Kang)-type versus Mandani-type fuzzy inference systems. The main motivation behind this research was to assess which approach provides the best performance for a gyroscope fault-detection application, developed in 2002 for the European Space Agency (ESA) satellite ENVISAT. Due to the importance of performance in online systems we compare the application, developed with Mamdani model, with a TSK formulation using three types of tests: processing time for both systems, robustness in the presence of randomly generated noise; and sensitivity analysis of the systems' behaviors to changes in input data. The results show that the TSK model perform better in all three tests, hence we may conclude that replacing a Mamdani system with an equivalent TSK system could be a good option to improve the overall performance of a fuzzy inference system.",2006,0,
1506,1507,Magnetic Field Sensor Using a Physical Model to Pre-Calculate the Magnetic Field and to Remove Systematic Error due to Physical Parameters,"Speed and angle measurements of rotating shafts are very important in automotive applications. Typical sensing arrangements for angular measurements using the magnetic principle are analyzed in this paper. It is shown that such sensor arrangements are prone to phase errors. The phase error mainly depends on the distance between sensor element and rotating shaft. By employing finite element simulations, a variety of frequently used magnetic field sensor configurations are investigated. Measurements complement the simulations and confirm correct simulation results. Due to mounting tolerances and mechanical vibrations in automotive applications the sensor distance is varying and dynamic phase errors appear. The accuracy of measurement can only be improved if these errors get compensated. This can be done with the help of digital signal processing.",2007,0,
1507,1508,BLoG: Post-Silicon bug localization in processors using bug localization graphs,"Post-silicon bug localization - the process of identifying the location of a detected hardware bug and the cycle(s) during which the bug produces error(s) - is a major bottleneck for complex integrated circuits. Instruction Footprint Recording and Analysis (IFRA) is a promising post-silicon bug localization technique for complex processor cores. However, applying IFRA to new processor microarchitectures can be challenging due to the manual effort required to implement special microarchitecture-dependent analysis techniques for bug localization. This paper presents the Bug Localization Graph (BLoG) framework that enables application of IFRA to new processor microarchitectures with reduced manual effort. Results obtained from an industrial microarchitectural simulator modeling a state-of-the-art complex commercial microarchitecture (Intel Nehalem, the foundation for the Intel CoreTM i7 and CoreTM i5 processor families) demonstrate that BLoG-assisted IFRA enables effective and efficient post-silicon bug localization for complex processors with high bug localization accuracy at low cost.",2010,0,
1508,1509,FlowChecker: Detecting Bugs in MPI Libraries via Message Flow Checking,"Many MPI libraries have suffered from software bugs, which severely impact the productivity of a large number of users. This paper presents a new method called FlowChecker for detecting communication-related bugs inMPI libraries. The main idea is to extract program intentions of message passing (MPintentions), and to check whether theseMP-intentions are fulfilled correctly by the underlying MPI libraries, i.e., whether messages are delivered correctly from specified sources to specified destinations. If not, FlowChecker reports the bugs and provides diagnostic information. We have built a FlowChecker prototype on Linux and evaluated it with five real-world bug cases in three widely-used MPI libraries, including Open MPI, MPICH2, and MVAPICH2. Our experimental results show that FlowChecker effectively detects all five evaluated bug cases and provides useful diagnostic information. Additionally, our experiments with HPL and NPB show that FlowChecker incurs low runtime overhead (0.9-9.7% on three MPI libraries).",2010,0,
1509,1510,A H.263 compatible error resilient video coder,"We present an error resilient video coder compatible with the ITU-T H.263 standard. Resynchronization flag insertion, error detection, localization and concealment in the decoder, and dynamic programming mode selection based on error tracking are the three main adopted error-resilient strategies. An information feedback method, which utilizes the H.263 video bit stream but does not modify its syntax, is described. Simulation results for the binary symmetric channel (BSC) with random bit errors are given to show the robustness of the proposed video coder",2000,0,
1510,1511,Evaluating the Use of Requirement Error Abstraction and Classification Method for Preventing Errors during Artifact Creation: A Feasibility Study,"Defect prevention techniques can be used during the creation of software artifacts to help developers create high-quality artifacts. These artifacts should have fewer faults that must be removed during inspection and testing. The Requirement Error Taxonomy that we have developed helps focus developers' attention on common errors that can occur during requirements engineering. Our claim is that, by focusing on those errors, the developers will be less likely to commit them. This paper investigates the usefulness of the Requirement Error Taxonomy as a defect prevention technique. The goal was to determine if making requirements engineers' familiar with the Requirement Error Taxonomy would reduce the likelihood that they commit errors while developing a requirements document. We conducted an empirical study in which the participants were given the opportunity to learn how to use the Requirement Error Taxonomy by employing it during the inspection of a requirements document. Then, in teams of four, they developed their own requirements document. This requirements document was then evaluated by other students to identify any errors made. The hypothesis was that participants who find more errors during the inspection of a requirements document would make fewer errors when creating their own requirements document. The overall result supports this hypothesis.",2010,0,
1511,1512,Importance sampling for error event analysis of HMM frequency line trackers,"This paper considers the problem of designing efficient and systematic importance sampling (IS) schemes for the performance study of hidden Markov model (HMM) based trackers. Importance sampling (IS) is a powerful Monte Carlo (MC) variance reduction technique, which can require orders of magnitude fewer simulation trials than ordinary MC to obtain the same specified precision. We present an IS technique applicable to error event analysis of HMM based trackers. Specifically, we use conditional IS to extend our work in another of our paper to estimate average error event probabilities. In addition, we derive upper bounds on these error probabilities, which are then used to verify the simulations. The power and accuracy of the proposed method is illustrated by application to an HMM frequency tracker.",2002,0,
1512,1513,Towards Autonomic Fault Recovery in System-S,"System-S is a stream processing infrastructure which enables program fragments to be distributed and connected to form complex applications. There may be potentially tens of thousands of interdependent and heterogeneous program fragments running across thousands of nodes. While the scale and interconnection imply the need for automation to manage the program fragments, the need is intensified because the applications operate on live streaming data and thus need to be highly available. System-S has been designed with components that autonomically manage the program fragments, but the system components themselves are also susceptible to failures which can jeopardize the system and its applications. The work we present addresses the self healing nature of these management components in System-S. In particular, we show how one key component of System-S, the job management orchestrator, can be abruptly terminated and then recover without interrupting any of the running program fragments by reconciling with other autonomous system components. We also describe techniques that we have developed to validate that the system is able to autonomically respond to a wide variety of error conditions including the abrupt termination and recovery of key system components. Finally, we show the performance of the job management orchestrator recovery for a variety of workloads.",2007,0,
1513,1514,A novel gray clustering filtering algorithms for identifying the false alert in aircraft long-distance fault diagnosis,"The fault report is downloaded from the aircraft with ACARS for the line maintenance. This is widely attended currently. But the false alert often occurs in the fault report and drop the maintenance efficiency Aimed at the problem, the gray clustering filtering algorithms is set up based on gray cluster and filter theory .The algorithms can identify the false alert in the fault report effectively.",2007,0,
1514,1515,Research and Development of Thermal Error Compensation Embedded in CNC System,"The effective compensation upon machining error, which is given rise to by machine tool's thermal deformation, is an important fashion of improving machining efficiency in CNC system. The external thermal error compensation method is mostly adopted because of the closure in conventional CNC system. In order to solve embedded integral problem of real-time thermal error compensation function in CNC system, an concentrating approach is introduced by integral design. Of a concentrating manner, the thermal deformation error of X-axis screw is modeled and analyzed in THK6370 Horizontal processing centre. Not only is the inserted thermal error real-time compensation function realized in complete open CNC system, but also can the thermal error on-line real-time compensation be executed. Finally, the experiment validates the concentrating mode and the effective compensation of thermal deformation error can be implemented upon THK6370 horizontal machining centre X-axis screw.",2010,0,
1515,1516,The fault-tolerant technique in the Rotor Current Controller in Induction Wind Generator,"The thesis expounds the importance of the rotor current controller in induction wind generator in wind generator sets. The topic analyses the kind and the source of the faults of the rotor current controller. Also it design the method of the technology of tolerating faults in the rotor current controller. Some kinds of the technology of tolerating faults are used in hardware and software. So the system can identify the kinds of the fault and eliminate the fault, then making the system can control itself and achieve the best work state again. Using mix redundant methods in software part in order to protect the program running reliably and disposing dates correctly. The system which can be controlled remotely and showed communicates with the computer and can send the result of diagnosis to it in time.",2009,0,
1516,1517,An ACS robotic control algorithm with fault tolerant capabilities,"This paper demonstrates that an adaptive computing system (ACS) is good platform for implementing robotic control algorithms. We show that an ACS can be used to provide both good performance and high dependability. An example of an FPGA-implemented dependable control algorithm is presented. The flexibility of ACS is exploited by choosing the best precision for our application. This reduces the amount of required hardware and improves performance. Results obtained from a WILDFORCE emulation platform showed that even using 0.35 m technology, an FPGA-implemented control algorithm has comparable performance with the software-implemented control algorithm in a 0.25 m microprocessor. Different voting schemes are used in conjunction with multi-threading and combinational redundancy to add fault tolerance to the robotic controller. Error-injection experiments demonstrate that robotic control algorithms with fault tolerance techniques are orders of magnitude less vulnerable to faults compared to algorithms without any fault tolerant features",2000,0,
1517,1518,Error-free simplification of transparent Mamdani systems,"This paper shows that combinatorial complexity of fuzzy systems is at least in part caused by redundancy in these systems and presents the algorithm and its implementation for detection and removal of such redundancy for a special class of Mamdani systems. Performance of the simplification algorithm is demonstrated with uniformly impressive results on acknowledged benchmarks coming from different areas of engineering - truck backer-upper control, Mackey-Glass time series prediction and iris data classification.",2008,0,
1518,1519,Meshing Simulation and Experimental Analysis of Transmission Error for Modified Spiral Bevel Gear,"Based on the meshing theory of spiral bevel gears, the Transmission error (TE)curves of two different pairs of spiral bevel gears (one pair is modified spiral bevel gears, another one is general spiral bevel gears) are achieved. According to the characteristic of theoretical TE curve, the spiral bevel gear transmission error measurement system is designed, and the comparative experiment for the two pairs of gears were carried out on the system under different loads. The real TE of the two pair of gears has acquired. The experimental results show that modified spiral bevel gear pair is better than the general one in the dynamic behavior, which was verified the new method that modifies tooth face with modified tools is useful and effective to reduce gear vibration and noise.",2010,0,
1519,1520,Architectural-Based Validation of Fault-Tolerant Software,"Many architecture-centred approaches have been proposed for constructing dependable component-based systems. However, few of them provide an integrated solution for their development that combines fault prevention, fault removal, and fault tolerance techniques. This paper proposes a rigorous development approach based on an architectural abstraction, which combines formal methods and robustness testing. The architectural abstraction assumes a crash failure semantics, and when it is instantiated as an architectural element provides the basis for architecting fault tolerant systems. The architecture is formally specified using the B-method and CSP. Assurances that the software system is indeed dependable are obtained by combining formal specification for removing ambiguities from the architectural representation, and robustness testing for validating the source code against its software architecture. The feasibility of the proposed approach is illustrated in the context of a financial critical system.",2009,0,
1520,1521,Towards Optimal Resource Allocation in Partial-Fault Tolerant Applications,"We introduce Zen, a new resource allocation framework that assigns application components to node clusters to achieve high availability for partial-fault tolerant (PFT) applications. These applications have the characteristic that under partial failures, they can still produce useful output though the output quality may be reduced. Thus, the primary goal of resource allocation for PFT applications is to prevent, delay, or minimize the impact of failures on the application output quality. This paper is the first to approach this resource allocation problem from a theoretical perspective, and obtains a series of results regarding component assignments that provide the highest service availability under the constraints imposed by the application data flow graph and the hosting clusters. We show that (1) even simple versions of this resource allocation problem are NP-Hard, (2) a 2-approximate polynomial-time algorithm works for tree topologies, and (3) a simple greedy component placement performs well in practice for general application topologies. We implement a system prototype to study the application availability achieved by Zen compared to failure-oblivious placement, replication, and Zen+replication. Our experimental results show that three PFT applications achieve significant data output quality and availability benefits using Zen.",2008,0,
1521,1522,Motion correction of head movements in PET: realisation for routine usage,With the increase of scanner resolution head motion in PET brain studies becomes an increasingly serious limitation. Methods to correct for motion have been proposed. In this work the realisation of a motion tracking system in a PET environment and the motion correction of list mode data with the MAF method is presented. In a phantom study the method is validated and the loss in image quality is documented in a phantom with simulated movements. The relevance of motion correction for patient data above the level of system resolution is studied. In a real patient study we show the effect of motion and the applicability of the presented system.,2003,0,
1522,1523,Team-based fault content estimation in the software inspection process,"The main objective of software inspection is to detect faults within a software artifact. This helps to reduce the number of faults and to increase the quality of a software product. However, although inspections have been performed with great success, and although the quality of the product is increased, it is difficult to estimate the quality. During the inspection process, attempts with objective estimations as well as with subjective estimations have been made. These methods estimate the fault content after an inspection and give a hint of the quality of the product. This paper describes an experiment conducted throughout the inspection process, where the purpose is to compare the estimation methods at different points. The experiment evaluates team estimates from subjective and objective fault content estimation methods integrated with the software inspection process. The experiment was conducted at two different universities with 82 reviewers. The result shows that objective estimates outperform subjective when point and confidence intervals are used. This contradicts the previous studies in the area.",2004,0,
1523,1524,Jointly optimized error-feedback and realization for roundoff noise minimization in state-space digital filters,"Roundoff noise (RN) is known to exist in digital filters and systems under finite-precision operations and can become a critical factor for severe performance degradation in infinite impulse response (IIR) filters and systems. In the literature, two classes of methods are available for RN reduction or minimization-one uses state-space coordinate transformation, the other uses error feedback/feed-forward of state variables. In this paper, we propose a method for the joint optimization of error feedback/feed-forward and state-space realization. It is shown that the problem at hand can be solved in an unconstrained optimization setting. With a closed-form formula for gradient evaluation and an efficient quasi-Newton solver, the unconstrained minimization problem can be solved efficiently. With the infinite-precision solution as a reference point, we then move on to derive a semidefinite programming (SDP) relaxation method for an approximate solution of optimal error-feedback matrix with sum-of-power-of-two entries under a given state-space realization. Simulations are presented to illustrate the proposed algorithms and demonstrate the performance of optimized systems.",2005,0,
1524,1525,A highly resilient routing algorithm for fault-tolerant NoCs,"Current trends in technology scaling foreshadow worsening transistor reliability as well as greater numbers of transistors in each system. The combination of these factors will soon make long-term product reliability extremely difficult in complex modern systems such as systems on a chip (SoC) and chip multiprocessor (CMP) designs, where even a single device failure can cause fatal system errors. Resiliency to device failure will be a necessary condition at future technology nodes. In this work, we present a network-on-chip (NoC) routing algorithm to boost the robustness in interconnect networks, by reconfiguring them to avoid faulty components while maintaining connectivity and correct operation. This distributed algorithm can be implemented in hardware with less than 300 gates per network router. Experimental results over a broad range of 2D-mesh and 2D-torus networks demonstrate 99.99% reliability on average when 10% of the interconnect links have failed.",2009,0,
1525,1526,Using formal verification to eliminate software errors,"The use of software in safety critical railway applications is increasing. Techniques for developing and running such software are based on reducing the probability of an error in the software causing an unsafe system failure; e.g. that the system permits a train to proceed when it shouldn't. At the same time these techniques cause problems of the opposite nature; that the systems fail to allow trains to proceed even when it is safe to proceed. Such failures, although not directly dangerous, lead to stress and delayed traffic, which in turn can cause safety to be compromised. This paper shows how the use of formal verification can solve this problem. This technique can be used to find and eliminate all errors in the software, before the system is put into service. Formal verification is one of the corner stones of Prover iLock, an off-the-shelf commercial tool suite used for developing railway signalling applications.",2008,0,
1526,1527,Fault detection in a wastewater treatment plant,The wastewater treatment plants are very unstable and the waters to be treated are ill defined. The command of those processes needs to be done with advanced methods of control and supervision. Those methods have to take into account the bad knowledge of the processes. The most important situations that can imply problems for the plant are listed. The parameters that allow the comparison between the crisis situation and normal one are measured. The fuzzy logic method is used to distinguish the different situations in order to take a decision for the working of the plant. The method is tested in simulation and on a real plant.,2001,0,
1527,1528,Study of Superconducting Fault Current Limiters for System Integration of Wind Farms,"As electrical energy will be provided from renewable sources, the connection of a large number of wind farms to existing distribution networks may lead to the increasing fault levels beyond the capacity of existing switchgear, especially in urban areas. Fault current limiters (FCLs) are essentially expected to control the prospective short circuit currents. In this paper, investigations were carried out to assess the effectiveness of the resistive superconducting fault current limiters (SFCLs) for fault level management in wind power system. System studies confirmed that the superconducting fault current limiter (SFCL) could not only control the fault currents but also suppress the inrush currents, when wind farm has adopted in the case of the system interconnection. As a result, the highly efficient operation of the wind power system becomes more possible by introducing the superconducting fault current limiters.",2010,0,
1528,1529,An Improved Knowledge Connectivity Condition for Fault-Tolerant Consensus with Unknown Participants,"For self-organized networks that possess highly decentralized and self-organized natures, neither the identity nor the number of processes is known to all participants at the beginning of the computation because no central authority exists to initialize each participant with some context information. Hence, consensus, which is essential to solving the agreement problem, in such networks cannot be achieved in the ways for traditional fixed networks. To address this problem of Consensus with Unknown Participants (CUP), a variant of the traditional consensus problem was proposed in the literature, by relaxing the requirement for the original knowledge owned by every process about all participants in the computation. Correspondingly, the CUP problem considering process crashes was also introduced, called the Fault-Tolerant Consensus with Unknown Participants (FT-CUP) problem. In this paper, we propose a knowledge connectivity condition sufficient for solving the FT-CUP problem, which is improved from the one proposed in our previous work.",2010,0,
1529,1530,A .NET framework for an integrated fault diagnosis and failure prognosis architecture,"This paper presents a .NET framework as the integrating software platform linking all constituent modules of the fault diagnosis and failure prognosis architecture. The inherent characteristics of the .NET framework provide the proposed system with a generic architecture for fault diagnosis and failure prognosis for a variety of applications. Functioning as data processing, feature extraction, fault diagnosis and failure prognosis, the corresponding modules in the system are built as .NET components that are developed separately and independently in any of the .NET languages. With the use of Bayesian estimation theory, a generic particle-filtering-based framework is integrated in the system for fault diagnosis and failure prognosis. The system is tested in two different applications - bearing spalling fault diagnosis and failure prognosis and brushless DC motor turn-to-turn winding fault diagnosis. The results suggest that the system is capable of meeting performance requirements specified by both the developer and the user for a variety of engineering systems.",2010,0,
1530,1531,Prior Training of Data Mining System for Fault Detection,"Many approaches have been used to fault discovery in complex systems. Model based reasoning; data mining analysis; rule base methods are the few among those approaches. To be successfully applied, these approaches all have to have some knowledge about the system prior to faults detection during the system run. Fault Tree Analysis shows the possible causes of a system malfunction by enumerating the suspect components and their respective failure modes that may have induced the problem. The rule based inference build the model based on the expert knowledge. Those models and methods have one thing in common; they have presumed some prior-conditions. Complex systems often use fault trees to analyze the faults. Fault diagnosis, when an error occurs, is performed by engineers and analysts performing extensive examination of all data gathered during the mission. International space station (ISS) control center operates on the data feedback from the system and decisions are made based on threshold values by using fault trees. Since those decision-making tasks are safety critical and must be done promptly, the engineers who manually analyze the data are facing the challenge of time limit. To automate this process, this paper presents an approach that uses decision trees to discover faults from data in real-time and capture the contents of fault trees as prior knowledge and use them to set the initial state of the decision trees.",2007,0,
1531,1532,Cooperating search agents explore more than defecting search agents in the Internet information access,"In the Internet Information Access Problem, information-seeking agents (software or humans) are selfishly rational in obtaining the information sought. From a single agent's perspective, sending out as many queries as possible maximizes the chance of achieving the information sought. However, if every agent does the same, the information servers will be overloaded and most of the search agents won't be able to retrieve the information. Our previous results suggest that when behaviorally similar information-seeking agents cluster together, cooperation is promoted. In these experiments, the ranges of query (i.e., maximum logical distance from the information-seeking agents to potential information severs) is fixed for each search agent; agents only inquire the severs within the distance. The article evolves the range of the access distance. When similar agents (cooperators with cooperators and defectors with defectors), cluster together, cooperators tend to access diversified information sites while defectors tend to access only common information sites, resulting in high congestion. This phenomenon can be seen in human agents as well. When an agent sees too much competition or overuse of resource, it considers alternative choices. For example, when people see a congested highway, they tend to take other routes even if the routes may be longer. A similar phenomena is observed in our experiments. The results of the research can be used to help designing the Internet search agents that are efficient and less burdensome to information servers",2001,0,
1532,1533,Fault-tolerant strategy based on dynamic model matching for dual-redundant computer in the space robot,"Based on the requirements of space robot control system such as high reliability, low power consumption, small size and real-time, this paper presents a dual redundant fault-tolerant strategy for space robot controller. According to the peculiarity of the space robot control systems and its workflow, this strategy is based on the fault-tolerant hardware architecture and the linear running model of space robot software. The two computers communicate with each other by 485 bus and heartbeat line, and share field data through CAN bus in this architecture. The imported criterions include the heartbeat signal, the pattern matching, the dynamic synchronization data and the results returned from the other device. The fault-tolerant strategy which has redundancy criterion is formulated in this paper. This design can not only ensure the time performance of switching between two computers, but also can lower the communication data and improve the reliability and effectiveness of dual-redundant fault-tolerant system.",2008,0,
1533,1534,Similarity-Guided Streamline Placement with Error Evaluation,"Most streamline generation algorithms either provide a particular density of streamlines across the domain or explicitly detect features, such as critical points, and follow customized rules to emphasize those features. However, the former generally includes many redundant streamlines, and the latter requires Boolean decisions on which points are features (and may thus suffer from robustness problems for real-world data). We take a new approach to adaptive streamline placement for steady vector fields in 2D and 3D. We define a metric for local similarity among streamlines and use this metric to grow streamlines from a dense set of candidate seed points. The metric considers not only Euclidean distance, but also a simple statistical measure of shape and directional similarity. Without explicit feature detection, our method produces streamlines that naturally accentuate regions of geometric interest. In conjunction with this method, we also propose a quantitative error metric for evaluating a streamline representation based on how well it preserves the information from the original vector field. This error metric reconstructs a vector field from points on the streamline representation and computes a difference of the reconstruction from the original vector field.",2007,0,
1534,1535,Automatic Fault Localization for SystemC TLM Designs,"To meet today's time-to-market demands catching bugs as early as possible during the design of a system is absolutely essential. In Electronic System Level (ESL) design where SystemC has become the de-facto standard due to Transaction Level Modeling (TLM), many approaches for verification have been developed. They determine an error trace which demonstrates the difference between the required and the actual behavior of the system. However, the subsequent debugging process is very time-consuming, in particular due to TLM-related faults caused by complex process synchronization and concurrency. In this paper, we present an automatic fault localization approach for SystemC TLM designs. The approach determines components that can be changed such that the intended behavior of the design is obtained removing the contradiction given by the error trace. Techniques based on Bounded Model Checking (BMC) are used to find the components. We demonstrate the quality of our approach by experimental results.",2010,0,
1535,1536,No time for bugs,Motor-sport teams expect a fast turnaround when they tune the software that goes into the electronic controllers that manage the highly tuned engines used in their cars. Subtle changes to the way the embedded computer controls the engine can make the difference between winning and being stuck in the pits. And it means that the developers at one specialist company can have just two hours to make critical changes to their code.,2004,0,
1536,1537,Fault Classification for SRAM-Based FPGAs in the Space Environment for Fault Mitigation,"This letter proposes a classification algorithm to discriminate between recoverable and not recoverable faults occurring in static random access memory (SRAM)-based field-programmable gate arrays (FPGAs), with the final aim of devising a methodology to enable the exploitation of these devices also in space applications, typically characterized by long mission times, where permanent faults become an issue. By starting from a characterization of the radiation effects and aging mechanisms, we define a controller able to classify such faults and consequently to apply the appropriate mitigation strategy.",2010,0,
1537,1538,A Three-Phases Byzantine Fault Tolerance Mechanism for HLA-Based Simulation,"A large scale HLA-based simulation (federation) is composed of a large number of simulation components (federates), which may be developed by different participants and executed at different locations. Byzantine failures, caused by malicious attacks and software/hardware bugs, might happen to federates and propagate in the federation execution. In this paper, a three-phases (i.e., failure detection, failure location, and failure recovery) Byzantine Fault Tolerance (BFT) mechanism is proposed based on the decoupled federate architecture. By combining the replication, check pointing and message logging techniques, some redundant executions of federate replicas are avoided. The BFT mechanism is implemented using both Barrier and No-Barrier federate replication structures. Protocols are also developed to remove the epidemic effect caused by Byzantine failures. As the experiment results show, the BFT mechanism using No-Barrier replication outperforms that using Barrier replication significantly in the case that federate replicas have different runtime performance.",2010,0,
1538,1539,"Network Dependability, Fault-tolerance, Reliability, Security, Survivability: A Framework for Comparative Analysis","A number of qualitative and quantitative terms are used to describe the performance of what has come to be known as information systems, networks or infrastructures. However, some of these terms either have overlapping meanings or contain ambiguities in their definitions presenting problems to those who attempt a rigorous evaluation of the performance of such systems. The phenomenon arises because the wide range of disciplines covered by the term information technology have developed their own distinct terminologies. This paper presents a systematic approach for determining common and complementary characteristics of five widely-used concepts, dependability, fault-tolerance, reliability, security, and survivability. The approach consists of comparing definitions, attributes, and evaluation measures for each of the five concepts and developing corresponding relations. Removing redundancies and clarifying ambiguities will help the mapping of broad user-specified requirements into objective performance parameters for analyzing and designing information infrastructures",2006,0,
1539,1540,Software-implemented fault detection for high-performance space applications,"We describe and test a software approach to overcoming radiation-induced errors in spaceborne applications running on commercial off-the-shelf components. The approach uses checksum methods to validate results returned by a numerical subroutine operating subject to unpredictable errors in data. We can treat subroutines that return results satisfying a necessary condition having a linear form; the checksum tests compliance with this condition. We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent infinite-precision numerical calculations. We test both the general effectiveness of the linear fault tolerant schemes we propose, and the correct behavior of our parallel implementation of them",2000,0,
1540,1541,Probabilistic analysis of CAN with faults,"As CANs (controller area networks) are being increasingly used in safety-critical applications, there is a need for accurate predictions of failure probability. In this paper we provide a general probabilistic schedulability analysis technique which is applied specifically to CANs to determine the effect of random network faults on the response times of messages. The resultant probability distribution of response times can be used to provide probabilistic guarantees of real-time behaviour in the presence of faults. The analysis is designed to have as little pessimism as possible but never be optimistic. Through simulations, this is shown to be the case. It is easy to apply and can provide useful evidence for justification of an event-triggered bus in a critical system.",2002,0,
1541,1542,Implementation of an Overblowing Correction Controller and the proposal of a quantitative assessment of the sound's pitch for the anthropomorphic saxophonist robot WAS-2,"Since 2007, our research is related to the development of an anthropomorphic saxophonist robot, which it has been designed to imitate the saxophonist playing by mechanically reproducing the organs involved for playing a saxophone. Our research aims in understanding the motor control from an engineering point of view and enabling the communication. In a previous paper, the Waseda Saxophone Robot No. 2 (WAS-2) which is composed by 22-DOFs has been presented. Moreover, a feedback error learning with dead time compensation has been implemented to control the air pressure of the robot. However, such a controller couldn't deal with the overblowing effects (unsteady tones) that are found during a musical performance. Therefore; in this paper, the implementation of an Overblowing Correction Controller (OCC) has been proposed and implemented in order to assure the steady tone during the performance by using the pitch feedback signal to detect the overblowing condition and by defining a recovery position (off-line) to correct it. Moreover, a saxophone sound evaluation function (sustain phase) has been proposed to compare the sound produced by human players and the robot. A set of experiments were carried out to verify the improvements on the musical performance of the robot and its sound has been quantitatively compared with human saxophonists. From the experimental results, we could observe improvements on the pitch (correctness) and tone stability.",2010,0,
1542,1543,A middleware aided robust and fault tolerant dynamic reconfigurable architecture,Dynamic reconfiguration enhances embedded system with at run-time adaptive functionality and is an improvement in terms of resource utilization and system adaptability. SRAM-based FPGAs provides a dynamic reconfigurable platform with high logic density. The requirements for such an embedded high flexible system based on FPGAs are robustness and reliability to prevent operation interrupts or even system failures. The complexity of a dynamic reconfigurable system with adaptive processing module demands high effort for the user. Therefore a high level abstraction of the communication issues is required to support application development by an appropriate middleware. To achieve such a flexible embedded system we present our network-on-chip (NoC) approach system-on-chip wire (SoCWire) and outline its performance and suitability for robust dynamic reconfigurable systems. Furthermore we introduce a suitable embedded middleware concept to support the system reconfiguration and the software application development process.,2009,0,
1543,1544,An analysis of the fault correction process in a large-scale SDL production model,"Improvements in the software development process depend on our ability to collect and analyze data drawn from various phases of the development life cycle. Our design metrics research team was presented with a largescale SDL production model plus the accompanying problem reports that began in the requirements phase of development. The goal of this research was to identify and measure the occurrences of faults and the efficiency of their removal by development phase in order to target software development process improvement strategies. The number and severity of problem reports were tracked by development phase and fault class. The efficiency of the fault removal process using a variety of detection methods was measured Through our analysis of the system data, the study confirms that catching faults in the phase of origin is an important goal. The faults that migrated to future phases are on average eight times more costly to repair. The study also confirms that upstream faults are the most critical faults and more importantly it identifies detailed design as the major contributor of faults, including critical faults.",2003,0,
1544,1545,Design of turbo-coded modulation for the AWGN channel with Tikhonov phase error,"We design 1-b/symbol/Hz parallel concatenated turbo-coded modulation (PCTCM) for the additive white Gaussian noise (AWGN) channel with Tikhonov phase error. Constituent recursive convolutional codes are optimized so that the turbo codes have low error floors and low convergence thresholds. The pairwise error probability based on the maximum-likelihood decoding metric is used to select codes with low error floors. We also present a Gaussian approximation method that accurately predicts convergence thresholds for PCTCM codes on the AWGN/Tikhonov channel. Simulation results show that the selected codes perform within 0.6 dB of constellation constrained capacity, and have no detectable error floor down to bit-error rates of 10<sup>-6</sup>.",2005,0,
1545,1546,Numerical prediction of static form errors in the end milling of thin-walled workpiece,"Cutting deformation is the key factor influencing the precision and quality of the machined thin-walled workpiece, and to keep the maximum surface form errors under the permissible errors is the ultimate purpose of the form errors prediction. Cutting forces are analyzed and classified into six types according to combination of cutting depth, and cutting-force model for thin-walled workpiece machining is developed, then a finite-element model is presented to analyze the surface dimensional errors in peripheral milling of aerospace thin-walled workpieces. The efficient flexible iterative algorithm is proposed to calculate the deflections and the maximum surface form errors as contrasted with the rigid iterative algorithm used in the literatures. Meanwhile, some key techniques such as the finite-element modeling of the tool-workpiece system; the determinant algorithm to judge instantaneous immersion boundaries between a cutter element and the workpiece; iterative scheme for the calculations of tool-workpiece deflections considering the former convergence cutting position; and the method for calculating the position and magnitude of the maximum surface form errors are developed and presented in detail. The presented simulation model can control the surface errors in the permissible errors region without calculating the errors all over the workpiece, hence computing speed is greatly increased. The proposed approach is validated and proved to be efficient through comparing the obtained numerical results with the test results.",2006,0,
1546,1547,A rapid prototyping system for error-resilient multi-processor systems-on-chip,"Static and dynamic variations, which have negative impact on the reliability of microelectronic systems, increase with smaller CMOS technology. Thus, further downscaling is only profitable if the costs in terms of area, energy and delay for reliability keep within limits. Therefore, the traditional worst case design methodology will become infeasible. Future architectures have to be error resilient, i.e., the hardware architecture has to tolerate autonomously transient errors. In this paper, we present an FPGA based rapid prototyping system for multi-processor systems-on-chip composed of autonomous hardware units for error-resilient processing and interconnect. This platform allows the fast architectural exploration of various error protection techniques under different failure rates on the microarchitectural level while keeping track of the system behavior. We demonstrate its applicability on a concrete wireless communication system.",2010,0,
1547,1548,Fault tolerant insertion and verification: a case study,"The particular circuit structures that allow the building of a Fault Tolerant (FT) circuit have been extensively studied in the past, but currently there is a lack of CAD support in the design and evaluation of FT circuits. The aim of the AMATISTA European project (IST project 11762) is to develop a set of tools devoted to the design of FT digital circuits. The toolset is composed of: an automatic insertion tool and a simulation tool to validate the FT design. This paper is a case study describing how this set of FTI (Fault Tolerant Insertion) and FTV (Fault Tolerant Verification) tools have been used to increase the reliability in a typical automotive application.",2002,0,
1548,1549,A new approach for real-time multiple open-circuit fault diagnosis in voltage source inverters,"Practically all the diagnostic methods for open-circuit faults in voltage source inverters (VSI) developed during the last decades, are focused on the occurrence of single faults and do not have the capability to handle and identify multiple failures. This paper presents a new method for real-time diagnostics of multiple open-circuit faults in voltage source inverters feeding ac machines. In contrast with the majority of the methods found in the literature which are based on the motor phase currents average values, the average absolute values are used here as principal quantities in order to formulate the diagnostic variables. These prove to be more robust against the issue of false alarms, carrying also information about multiple open-circuit failures. Furthermore, by the combination of these variables with the machine phase currents average values, it is possible to obtain characteristic signatures, which allow for the detection and identification of single and multiple open-circuit faults.",2010,0,
1549,1550,Fault Detection and Recovery in a Transactional Agent Model,"Servers can be fault-tolerant through replication and checkpointing technologies in the client server model. However, application programs cannot be performed and servers might block in the two-phase commitment protocol due to the client fault. In this paper, we discuss the transactional agent model to make application programs fault-tolerant by taking advantage of mobile agent technologies where a program can move from a computer to another computer in networks. Here, an application program on a faulty computer can be performed on another operational computer by moving the program. A transactional agent moves to computers where objects are locally manipulated. Objects manipulated have to be held until a transactional agent terminates. Some sibling computers which the transactional gent has visited might be faulty before the transactional agent terminates. The transactional agent has to detect faulty sibling computers and makes a decision on whether it commits/aborts or continues the computation by skipping the faulty computers depending on the commitment condition. For example, a transactional agent has to abort in the atomic commitment if a sibling computer is faulty. A transactional agent can just drop a faulty sibling computer in the at-least-one commitment. We evaluate the transactional agent model in terms of how long it takes for the transactional agent to treat faulty sibling computers .",2007,0,
1550,1551,Analysis of Fault-Tolerant Performance of a Doubly Salient Permanent-Magnet Motor Drive Using Transient Cosimulation Method,"Doubly salient permanent-magnet (DSPM) motors offer the advantages of high power density and high efficiency. In this paper, it is examined that the DSPM motor is a new class of fault-tolerant machines, a potential candidate for many applications where reliability and power density are of importance. Fault analysis is performed in a DSPM motor drive, including internal and external faults. Due to the fact that the experimentation on a true motor drive for such a purpose is impractical because of its high cost and difficulty to make, a new cosimulation model of a DSPM motor drive is developed using coupled magnetic and electric circuit solvers. Last, to improve the performance of a DSPM motor drive with an open-circuited fault, a fault compensation strategy is proposed. Simulation and experimental results are presented, showing the effectiveness of the proposed cosimulation method and the high performance of the fault-tolerant characteristic of DSPM motor drives.",2008,0,
1551,1552,Defect-based reliability analysis for mission-critical software,"Most software reliability methods have been developed to predict the reliability of a program using only data gathered during the resting and validation of a specific program. Hence, the confidence that can be attained in the reliability estimate is limited since practical resource constraints can result in a statistically small sample set. One exception is the Orthogonal Defect Classification (ODC) method, which uses data gathered from several projects to track the reliability of a new program, Combining ODC with root-cause analysis can be useful in many applications where it is important to know the reliability of a program for a specific type of a fault. By focusing on specific classes of defects, it becomes possible to (a) construct a detailed model of the defect and (b) use data from a large number of programs. In this paper, we develop one such approach and demonstrate its application to modeling Y2K defects",2000,0,
1552,1553,Lung motion correction on respiratory gated 3-D PET/CT images,"Motion is a source of degradation in positron emission tomography (PET)/computed tomography (CT) images. As the PET images represent the sum of information over the whole respiratory cycle, attenuation correction with the help of CT images may lead to false staging or quantification of the radioactive uptake especially in the case of small tumors. We present an approach avoiding these difficulties by respiratory-gating the PET data and correcting it for motion with optical flow algorithms. The resulting dataset contains all the PET information and minimal motion and, thus, allows more accurate attenuation correction and quantification.",2006,0,
1553,1554,On The Generalization of Error-Correcting WOM Codes,"WOM (write once memory) codes are codes for efficiently storing and updating data in a memory whose state transition is irreversible. Storage media that can be classified as WOM includes flash memories, optical disks and punch cards. Error-correcting WOM codes can correct errors besides its regular data updating capability. They are increasingly important for electronic memories using MLCs (multi-level cells), where the stored data are prone to errors. In this paper, we study error-correcting WOM codes that generalize the classic models. In particular, we study codes for jointly storing and updating multiple variables - instead of one variable - in WOMs with multi-level cells. The error-correcting codes we study here are also a natural extension of the recently proposed floating codes. We analyze the performance of the generalized error- correcting WOM codes and present several bounds. The number of valid states for a code is an important measure of its complexity. We present three optimal codes for storing two binary variables in n q-ary cells, where n = 1,2,3, respectively. We prove that among all the codes with the minimum number of valid states, the three codes maximize the total number of times the variables can be updated.",2007,0,
1554,1555,Vertical Velocity Measurement - Processing of Sensor Data Using Altitude Corrections,"The system was designed for sensor measurement and data transfer. A new architecture of a multisensor system for temperature measurement using wireless communications was used in the paper. There are used sensors with digital or analog outputs. The control software of the system has been created. Different software were designed for wireless units. The integrated RF chip nRF9E5 was used as wireless units. Chip ensures wireless communication between control unit and sensors as well as wireless switch unit. The control unit controls system operation, i.e. communication, sensor data processing as well as work of actuator unit. Communication is ensured in the range of 300 m in the free space. The system was designed to operate with different type of sensors. The number of sensor can be variable. The system can used PC, PDA or mobile phone to communication with control unit.",2008,0,
1555,1556,An Integrated Framework for Checking Concurrency-Related Programming Errors,"Developing concurrent programs is intrinsically difficult. They are subject to programming errors that are not present in traditional sequential programs. Our current work is to design and implement a hybrid approach that integrates static and dynamic analyses to check concurrency-related programming errors more accurately and efficiently. The experiments show that the hybrid approach is able to detect concurrency errors in unexecuted parts of the code compared to dynamic analysis, and produce fewer false alarms compared to static analysis. Our future work includes but is not limited to optimizing performance, improving accuracy, as well as locating and confirming concurrency errors.",2009,0,
1556,1557,Robust Fault Detection in a Mixed <sub>2</sub>/<sub></sub> Setting: The Discrete - Time Case,"Robust fault detection problem for discrete-time LTI systems is considered. Allowing stochastic white noises and bounded unknown deterministic disturbances to model system uncertainties, it is shown that this problem can be cast as a mixed norm H<sub>2</sub>/H<sub>alpha</sub> residual generation problem. An example is presented to illustrate the application of the results.",2006,0,
1557,1558,Preventing human errors in power grid management systems through user-interface redesign,"Supervising an energy network is a critical and complex endeavor. Decisions are made under severe time constraints and errors can be costly, often resulting in a crippling impact on the network. Furthermore, the dispatchers who oversee these networks must navigate through multiple systems and tools to access the information they need to make good decisions quickly. Thousands of dynamic variables and hundreds of network configurations need to be considered. Energy network supervision is prone to human error because it requires high dispatcher attention and memory load. This paper shows that it is possible, without massive investments in dollars or in technology, to prevent human errors and to significantly reduce decision times by redesigning the user interfaces on the computer software used by the energy system's supervisors. It illustrates how the redesign of an energy transmission protection system user interface through a cognitive ergonomics approach, eliminates the cause of human errors induced by the existing user interface and reduces time to access information by 90 percent.",2007,0,
1558,1559,Defect tolerance in hybrid nano/CMOS architecture using tagging mechanism,In this paper we propose two efficient repair techniques for hybrid nano/CMOS architecture to provide high level of defect tolerance at a modest cost. We have applied the proposed techniques to a lookup table(LUT) based Boolean logic approach. The proposed repair techniques are efficient in utilization of spare units and viable for various Boolean logic implementations. We show that the proposed techniques are capable of handling upto 20% defect ratess in hybrid nano/CMOS architecture and upto 14% defect rates for large ISCAS'85 benchmark circuits synthesized into smaller sized LUTs.,2009,0,
1559,1560,Error concealment scheme implemented in H.264/AVC,"Video transmission over noisy channels, like wireless channels, leads to errors on video. Effect of information loss is worse in case of transmission of compressed video. With growing interest in compressed video transmission over such environments, error concealment is becoming more important. In this paper we describe error concealment scheme which uses weighted pixel averaging to obtain each pixel value of lost macroblock in intra coded pictures and also method used for error concealment in inter coded pictures. This method uses boundary matching approach. We focus on performance evaluation of the error concealment technique implemented in JM reference software whereby we used extended profile in encoder.",2008,0,
1560,1561,Design of Resource Space Model in Fault Diagnosis Knowledge of Rotating Machinery,"In this paper, we introduce the resource space model (RSM) which is a novelty semantic data model, to semantically store and manage information. Then we use the methods to classify some rotating mechanical fault diagnosis knowledge in semantic, construct RSM of part rotating mechanical fault diagnosis knowledge. In the end, we simply analyze the modelpsilas semantic characteristics in searching and management.",2008,0,
1561,1562,Rotor broken bars fault diagnosis for induction machines based on the wavelet ridge energy spectrum,"A new method for rotor broken bars fault diagnosis for induction machines based on the startup electromagnetic torque signal is presented. The fault characteristic torque frequency variation during the startup can be extracted using the wavelet ridge, which can be used to identify rotor broken bars fault. The wavelet coefficients modulus indicate the signal energy of the corresponding scale, so the wavelet coefficients modulus of the fault characteristic ridge will give us the magnitude variation law of the fault characteristic torque. According to these, the wavelet ridge energy spectrum is defined. Using the wavelet ridge energy spectrum of the fault characteristic torque as the fault severity index, the number of adjacent broken rotor bars can thus be given. Experimental results verify the feasibility of the proposed fault diagnosis method",2005,0,
1562,1563,An Immune Fault Detection System with Automatic Detector Generation by Genetic Algorithms,This work deals with fault detection of electronic analog circuits. A fault detection system for analog circuits based on cross-correlation and artificial immune systems is proposed. It is capable of detecting faulty components in analog circuits by analyzing its impulse response. The use of cross-correlation for preprocessing the impulse response drastically reduces the size of the detector used by the real-valued negative selection algorithm (RNSA). The proposed method makes use of genetic algorithms to automatically generate a small number of very efficient detectors. Results have demonstrated that the proposed system is able to detect faults in a Sallen-Key bandpass filter and in a universal filter.,2007,0,
1563,1564,Fault Injection Scheme for Embedded Systems at Machine Code Level and Verification,"In order to evaluate software from the third party whose source codes are not available, after a careful analysis of the statistic data sorted by orthogonal defect classification, and the corresponding relation between patterns of high level language programs and machine codes, we propose a fault injection scheme at machine code level suitable respectively to the IA32 ARM and MIPS architecture, which takes advantage of mutating machine code. To prove the feasibility and validity of this scheme, two sets of programs are chosen as our experimental target: Set I consists of two different versions of triangle testing algorithms, and Set II is a subset of the Mibench which is a collection of performance benchmark programs designed for embedded systems; we inject both high level faults into the source code written in C language and the corresponding machine code level faults directly into the executables, and monitor their running on Linux. The results from experiments show that at least 96% of total similarity degree is obtained. Therefore, we conclude that the effect of injecting corresponding faults on both the source code level and machine code level are mostly the same. Therefore, our scheme is rather useful in analyzing system behavior under faults.",2009,0,
1564,1565,Coordinated application of multiple description scalar quantization and error concealment for error-resilient MPEG video streaming,"Historically, multiple description coding (MDC) and postprocessing error concealment (ECN) algorithms have evolved separately. In this paper, we propose a coordinated application of multiple description scalar quantizers (MDSQ) and ECN, where the smoothness of the video signal helps to compensate for the loss of descriptions. In particular, we perform a reconstruction that is consistent with the data received at the decoder. When only a single description is available, the video is reconstructed in such a way that: 1) if we were to regenerate two descriptions (from the reconstructed video), one of them would be equivalent to the received description and 2) the reconstructed video is spatiotemporally smooth. Experimental results with several video sequences demonstrated a peak signal-to-noise ratio (PSNR) improvement of 0.9-2.8 dB for intracoded frames. The PSNR improvements for intercoded frames were negligible. However, for both cases, the visual improvements were much more striking than what the PSNR improvement suggested.",2005,0,
1565,1566,Issues insufficiently resolved in Century 20 in the fault-tolerant distributed computing field,"As the 21st Century has just opened up, it is a fitting time to reflect on the evolution of the fault-tolerant distributed computing technology that occurred in the last century. The author's view of that evolution is sketched in this paper, with emphasis on the major issues that were insufficiently resolved in the 20th Century. Such issues are naturally among what the author believes to be the prime subjects that need to be addressed in this decade by the research community. A substantial part of this paper deals with the issues that need to be resolved to advance the real-time fault-tolerant distributed computing branch into a mature practicing field",2000,0,
1566,1567,A multi-agent system-based intelligent identification system for power plant control and fault-diagnosis,"A large-scale power system is required to have a new control system to operate at a higher level of automation, flexibility, and robustness. In this paper, a multi-agent system based intelligent identification system (MAS-IIS) is presented for identification and fault-diagnosis methodologies that improve the performance of the plant in a wide-range of operation. With proposed architecture of a single agent and an organization of the multi-agent system, the MAS-IIS realizes on-line adaptive identifiers for control, and off-line identifiers for fault-diagnosis in real-time power plant operation. The proposed MAS-IIS is one of the functions in multi-agent system based intelligent control systems (MAS-ICSs) which has several functions that provide efficient ways to control locally and globally, and to accommodate and overcome the complexity of large-scale distributed systems",2006,0,
1567,1568,Seeded Fault Testing and In-situ Analysis of Critical Electronic Components in EMA Power Circuitry,"An investigation into the development of feasible detection strategies capturing and trending incipient signs of failure in electronic power and control circuitry of electromechanical actuator (EMA) systems was jointly funded and conducted by Lockheed Martin Aeronautics Company, Parker Aerospace, and Impact Technologies, LLC. The objective of this study was to experimentally evaluate feature-based and efficiency-based prognostic approaches for power drive and control electronics through application of component-level Highly Accelerated Life Testing (HALT) and circuit board-level seeded fault testing. The authors of this paper discuss collaborative work identifying system-critical components through an enhanced failure mode effect and criticality assessment (FMECA++) followed by accelerated aging of these components leading to insertion into the EMA system and analysis of test results. Component accelerated aging and EMA system testing was performed at Impact's facility with test system- specific knowledge provided by Lockheed and Parker.",2008,0,
1568,1569,An extreme value injection approach with reduced learning time to make MLNs multiple-weight-fault tolerant,"We propose an efficient method for making multilayered neural networks(MLN) fault-tolerant to all multiple weight faults in an interval by injecting intentionally two extreme values in the interval in a learning phase. The degree of fault-tolerance to a multiple weight fault is measured by the number of essential multiple links. First, we analytically discuss how to choose effectively the multiple links to be injected, and present a learning algorithm for making MLNs fault tolerant to all multiple (i.e., simultaneous) faults in the interval defined by two multi-dimensional extreme points. Then it is shown that after the learning algorithm successfully finishes, MLNs become fault tolerant to all multiple faults in the interval. The time in a weight modification cycle is almost linear for the fault multiplicity. The simulation results show that the computing time drastically reduces as the multiplicity increases.",2002,0,
1569,1570,Evaluation of Error Control Mechanisms Based on System Throughput and Video Playable Frame Rate on Wireless Channel,"Error control mechanisms are widely used in video communications over wireless channels. However for improving end-to-end video quality: they consume extra bandwidth and reduce effective system throughput. In this paper, considering the parameters of system throughput and playable frame rate as evaluating metrics, we investigate the efficiency of different error control mechanisms. We develop a throughput analytical model to present system effective throughput for different error control mechanisms under different conditions. For a given packet loss probability, both optimal retransmission times in adaptive ARQ and optimal number of redundant packets in adaptive FEC for each type of frames are derived by keeping the system throughput as a constant value. Also, end to end playable frame rates for the two schemes are computed. Then which error control scheme is the most suitable for which application condition is concluded. Finally empirical simulation experimental results with various data analysis are demonstrated.",2010,0,
1570,1571,Fusion of the MR image to SPECT with possible correction for partial volume effects,"Low spatial resolution and the related partial volume effects limit the diagnostic potential of brain single photon emission computed tomography (SPECT) imaging. As a possible remedy for this problem we propose a technique for the fusion of SPECT and MR images, which requires for a given patient the SPECT data and the T1-weighted MR image. Basically, after the reconstruction and coregistration steps, the high-frequency part of the MR, which would be unrecoverable by the set SPECT acquisition system + reconstruction algorithm, is extracted and added to the SPECT image. The tuning of the weight of the MR on the resulting fused image can be performed very quickly, any iterative reconstruction algorithm can be used and, in the case that the SPECT projections are not available, the proposed technique can also be applied directly to the SPECT image, provided that the performance of the scanner is known. The procedure has the potential of increasing the diagnostic value of a SPECT image. Even in the locations of SPECT-MR mismatch it does not significantly affect quantitation over regions of interest (ROIs) whose dimensions are decidedly larger than the SPECT resolution distance. On the other hand, appreciable corrections for partial volume effects are expected in the locations where the contrast in the structural MR matches the corresponding contrast in functional activity.",2006,0,
1571,1572,Eye gaze correction with stereovision for video-teleconferencing,"The lack of eye contact in desktop video teleconferencing substantially reduces the effectiveness of video contents. While expensive and bulky hardware is available on the market to correct eye gaze, researchers have been trying to provide a practical software-based solution to bring video-teleconferencing one step closer to the mass market. This paper presents a novel approach: based on stereo analysis combined with rich domain knowledge (a personalized face model), we synthesize, using graphics hardware, a virtual video that maintains eye contact. A 3D stereo head tracker with a personalized face model is used to compute initial correspondences across two views. More correspondences are then added through template and feature matching. Finally, all the correspondence information is fused together for view synthesis using view morphing techniques. The combined methods greatly enhance the accuracy and robustness of the synthesized views. Our current system is able to generate an eye-gaze corrected video stream at five frames per second on a commodity 1 GHz PC.",2004,0,
1572,1573,Detection and Diagnosis of Static Scan Cell Internal Defect,"In this paper, we study the impact, detection and diagnosis of the defect inside a scan cell, which is called scan cell internal defect. We first use SPICE simulation to understand how a scan cell internal defect impacts the operation of a single scan cell. To study the detectability and diagnosability of a scan cell internal defect in a production test environment, we inject scan cell internal defects into a scan-based industrial design and perform fault simulation by using production scan test patterns. Next, we evaluate how effective an existing scan chain diagnosis technique based on traditional fault models can diagnose scan cell internal defect. We finally propose a new diagnosis algorithm to improve scan cell internal defect diagnostic resolution using scan cell internal fault model. Experimental results show the effectiveness of the proposed scan cell internal fault diagnosis technique.",2008,0,
1573,1574,Reproducing non-deterministic bugs with lightweight recording in production environments,"Reproducing non-deterministic bugs is challenging. Recording program execution in production environments and reproducing bugs is an effective way to re-enable cyclic debugging. Unfortunately, most current record-replay approaches introduce large perturbations to either environments and/or execution flow, in addition to performance penalty and high storage overhead, which make them impracticable to be deployed in production environments. This paper presents Snitchaser - a fully user-space record-replay tool which can faithfully reproduce bugs by replaying system calls which are recorded with negligible perturbation and recording overhead. This is achieved by 1) a novel, lightweight system call interception mechanism without patching the binary instructions to reduce the perturbation to execution flow; 2) system call latch to save signal semantic; 3) periodic checkpointing to reduce the storage overhead. Snitchaser focuses on bugs caused by asynchronous events on heavily loaded, high throughput servers. Experimental results show that Snitchaser is capable of reproducing non-deterministic bugs efficiently at nearly no performance penalty. We also present two case studies on dealing with existing bugs in Lighttpd - a popular software used in many large scale systems.",2010,0,
1574,1575,Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows,"We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.",2010,0,
1575,1576,MATLAB Design and Research of Fault Diagnosis Based on ANN for the C3I System,Artificial neural networks (ANN) are an information-processing method of a simulation of the structure for biological neurons. C<sup>3</sup>I system as a modern combat unit can control and command the army action and can communicate to others. This paper makes a research on the approach of the artificial neural network for fault diagnosis of C<sup>3</sup>I system and constructs a fault diagnosis system of C<sup>3</sup>I system with ANN. And the system can analyze fault phenomena and detect C<sup>3</sup>I system fault. It will greatly improve the response to the C<sup>3</sup>I system fault diagnosis and maintenance efficiency.,2010,0,
1576,1577,Towards a control-theoretical approach to software fault-tolerance,"Existing schemes for software fault-tolerance are based on the ideas of redundancy and diversity. Although being experimentally tested valid, existing fault-tolerant schemes are mainly ad hoc and lack theoretically rigorous foundation. They substantially increase software complexity and incur high development costs. They also impose challenges for real-time concurrent software systems where timing requirements may be stringent and faults in concurrent processes can propagate one another. In This work we treat software fault-tolerance as a robust supervisory control (RSC) problem and propose a RSC approach to software fault-tolerance. In this approach the software component under consideration is treated as a controlled object that is modeled as a generalized Kripke structure or finite-state concurrent system, and an additional safety guarder or supervisor is synthesized and compounded to the software component to guarantee the correctness of the overall software system, which is aimed to satisfy a temporal logic (CTL*) formula, even if faults occur to the software component. The proposed RSC approach requires only a single version of software and is based on a theoretically rigorous foundation. It is essentially an approach of model construction and thus complementary to the approach of model checking. It is a contribution to the theory of supervisory control, software fault-tolerance as well as the emerging area of software cybernetics that explores the interplay between software and control.",2004,0,
1577,1578,Sensitivity analysis of modular dynamic fault trees,"Dynamic fault tree analysis, as currently supported by the Galileo software package, provides an effective means for assessing the reliability of embedded computer-based systems. Dynamic fault trees extend traditional fault trees by defining special gates to capture sequential and functional dependency characteristics. A modular approach to the solution of dynamic fault trees effectively applies Binary Decision Diagram (BOD) and Markov model solution techniques to different parts of the dynamic fault tree model. Reliability analysis of a computer-based system tells only part of the story, however. Follow-up questions such as Where are the weak links in the system?, How do the results change if my input parameters change? and What is the most cost effective way to improve reliability? require a sensitivity analysis of the reliability analysis. Sensitivity analysis (often called Importance Analysis) is not a new concept, but the calculation of sensitivity measures within the modular solution methodology for dynamic and static fault trees raises some interesting issues. In this paper we address several of these issues, and present a modular technique for evaluating sensitivity, a single traversal solution to sensitivity analysis for BOD, a simplified methodology for estimating sensitivity for Markov models, and a discussion of the use of sensitivity measures in system design. The sensitivity measures for both the Binary Decision Diagram and Markov approach presented in this paper is implemented in Galileo, a software package for reliability analysis of complex computer-based systems",2000,0,
1578,1579,The Application of Nerve Net Algorithm to Reduce Vehicle Weigh in Motion System Error,"In order to improve the Vehicle Weigh in Motion System with precision, the paper introduces Nerve Net Algorithm to error analysis. The article sets up a neural network model by determining the neural network input and output variables. Then, the function is defined by net training on MATLAB software. Finally through the experimental verification of Nerve Net Algorithm improving Weigh in Motion System accuracy is feasible.",2010,0,
1579,1580,Transient fault detection via simultaneous multithreading,"Smaller feature sizes, reduced voltage levels, higher transistor counts, and reduced noise margins make future generations of microprocessors increasingly prone to transient hardware faults. Most commercial fault-tolerant computers use fully replicated hardware components to detect microprocessor faults. The components are lockstepped (cycle-by-cycle synchronized) to ensure that, in each cycle, they perform the same operation on the same inputs, producing the same outputs in the absence of faults. Unfortunately, for a given hardware budget, full replication reduces performance by statically partitioning resources among redundant operations. We demonstrate that a Simultaneous and Redundantly Threaded (SRT) processor-derived from a a Simultaneous Multithreaded (SMT) processor-provides transient fault coverage with significantly higher performance. An SRT processor provides transient fault coverage by running identical copies for the same program simultaneously as independent threads. An SRT processor provides higher performance because it dynamically schedules its hardware resources among the redundant copies. However, dynamic scheduling makes is difficult to implement lockstepping, because corresponding instructions from redundant threads may not execute in the same cycle or in the same order. This paper makes four contributions to the design of SRT processors. First, we introduce the concept of the sphere of replication, which abstract both the physical redundancy of a lockstepped system and the logical redundancy of an SRT processor. This framework aids in identifying the scope of fault coverage and the input and output values requiring special handling. Second, we identify two viable spheres of replication in an SRT processor, and show that one of them provides fault detection while checking only committed stores and uncached loads. Third, we identify the need for consistent replication of load values, and propose and evaluate two new mechanisms for satisfying thi- - s requirement. Finally, we propose and evaluate two mechanisms-slack fetch and branch outcome queue-that enhance the performance of an SRT processor by allowing one thread to prefetch cache misses and branch results for the other thread. Our results with 11 SPEC95 benchmarks show that an SRT processor can outperform an equivalently sized, on-chip, hardware-replicated solution by 16% on average, with maximum benefit of up to 29%.",2000,0,
1580,1581,Video Error Concealment Using Spatio-Temporal Boundary Matching,"Transmission of videos in error prone environments may lead to video corruption or loss. Therefore error concealment at the decoder side has to be applied. Commonly error concealment techniques make use of the surrounding correctly received image data or motion information for concealment. In this paper, a novel spatio-temporal boundary matching algorithm (STBMA) by exploiting both spatial and temporal information to reconstruct the lost motion vectors (MV) is proposed, and also introduce a new side smoothness measurement. By using the motion vector that is found by the proposed algorithm, the lost macro block (MB) can be recovered. Compared with the well known boundary matching algorithm (BMA), the proposed algorithm is able to achieve higher PSNR as well as better visual quality.",2009,0,
1581,1582,Remote Fault Estimation and Thevenin Impedance Calculation from Relays Event Reports,"One of the typical features in modern relays is the generation of event reports during a disturbance. Event reports are records of regularly taken samples of the line currents and voltages as seen by the relay during a disturbance. This paper describes a software tool developed for the use of these event reports to perform the tasks of fault classification and estimation. The program estimates the 60 Hz values of currents and voltages by applying DFT (discrete Fourier transform) to the samples recorded by the relay every quarter of a cycle. Using these values the program performs the following calculations: 1) classification of fault, 2) estimation of the fault distance from the relay location and 3) Thevenin's impedance of the system at the fault point as well as a Thevenin's equivalent impedance of the system in front and behind the relay. The program was initially tested with data generated from ATP simulations and later with data from relays installed in an operating electrical network. The results show that the software is highly reliable, providing accurate estimation of faults and Thevenin's equivalent impedances",2006,0,
1582,1583,Novel Dual-Band Filter Incorporating Defected SIR and Microstrip SIR,"This letter presents a novel approach to design dual-band bandpass filter by using defected stepped impedance resonator (DSIR) and microstrip stepped impedance resonator (MSIR). A pair of MSIRs on the upper plane forms a cross coupled filtering passage, and a pair of DSIRs at the lower plane constructs a linear phase filtering passage. Both of them are fed by a common T-shaped microstrip feed line with source-load coupling. Then they are directly combined to construct a compact dual-band filter with two passbands centering at 2.35 GHz and 3.15 GHz, respectively. The measurement results agree well with the full-wave electromagnetic designed responses.",2008,0,
1583,1584,A Fast Analytical Approach to Multi-cycle Soft Error Rate Estimation of Sequential Circuits,"In this paper, we propose a very fast analytical approach to measure the overall circuit Soft Error Rate (SER) and to identify the most vulnerable gates and flip-flops. In the proposed approach, we first compute the error propagation probability from an error site to primary outputs as well as system bistables. Then, we perform a multi-cycle error propagation analysis in the sequential circuit. The results show that the proposed approach is four to five orders of magnitude faster than the Monte Carlo (MC) simulation-based fault injection approach with 92% accuracy. This makes the proposed approach applicable to industrial-scale circuits.",2010,0,
1584,1585,An optimal point in scheduling real-time tasks process based on fault tolerant imprecise computation model,"Fault tolerance is an important issue due to the critical nature of the supported tasks of real-time computer systems, since timing constraints must not be violated. The imprecise computation technique has been proposed as a way to handle transient overload and to enhance fault tolerant of real-time systems. This paper introduces an exact theoretical analysis for the imprecise computation model based on three principles of maximize reward-based test, minimize response-time test, and minimize errors test, then finds optimal-point in scheduling process to satisfy three scheduling conditions. Further this is also demonstrated by the simulation results.",2002,0,
1585,1586,Image quality assessment: from error visibility to structural similarity,"Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu/lcv/ssim/.",2004,0,
1586,1587,Real-Time Tasks Scheduling with Value Control to Predict Timing Faults During Overload,"Modern real-time applications are very dynamic and cannot cope with the use of worst case execution time to avoid overload situations. Therefore scheduling algorithms that are able to prevent timing faults during overload are required. In this context, the value parameter has become useful to add generality and flexibility to such systems. In this paper, we present the scheduling algorithm called DMB (dynamic misses based), which is capable of dynamically changing tasks value in order to adjust their importance according to its timing faults rate. The main goal of DMB is to allow the prediction of timing faults during overloads and thereby support a dynamic tuning of tasks fault rate. It is used to enhance the features of the previously defined TAFT (time-aware fault-tolerant) scheduler. Obtained results show that DMB in conjunction with TAFT reached the most promising results during overloads, allowing to control tasks degradation in a graceful and determined way",2007,0,
1587,1588,Error Vector Magnitude Analysis for OFDM Systems,"Error vector magnitude (EVM) is a popular figure-of-merit adopted by various communication standards for evaluating in-band distortions introduced in a communication system. In this paper, we regard EVM as a random variable and investigate its statistical distributions as the result of the following distortion mechanisms: phase noise, amplitude clipping, power amplifier nonlinearities, and gain/phase imbalances in orthogonal frequency division multiplexing (OFDM) systems. We relate key parameters characterizing the various distortion mechanisms to the statistical behavior of EVM; such statistical behavior can be used to verify compliance of the transmit signals to the requirements of the standard.",2006,0,
1588,1589,Semantic Impact and Faults in Source Code Changes: An Empirical Study,"Changes to source code have become a critical factor in fault predictions. Text or syntactic approaches have been widely used. Textual analysis focuses on changed text fragments while syntactic analysis focuses on changed syntactic entities. Although both of them have demonstrated their advantages in experimental results, they only study code fragments modified during changes. Because of semantic dependencies within programs, we believe that code fragments impacted by changes are also helpful. Given a source code change, we identify its impact by program slicing along the variable def-use chains. To evaluate the effectiveness of change impacts in fault detection and prediction, we compare impacted code with changed code according to size and fault density. Our experiment on the change history of a successful industrial project shows that: fault density in changed and impacted fragments are higher than other areas; for large changes, their impacts have higher fault density than changes themselves; interferences within change impact contribute to the high fault density in large changes. Our study suggests that, like change itself, change impact is also a high priority indicator in fault prediction, especially for changes of large scales.",2009,0,
1589,1590,Efficient multiway graph partitioning method for fault section estimation in large-scale power networks,"Fault section estimation (FSE) of large-scale power networks can be implemented effectively by the distributed artificial intelligence (AI) technique. In this paper, an efficient multiway graph partitioning method is proposed to partition the large-scale power networks into the desired number of connected subnetworks with balanced working burdens in performing FSE. The number of elements at the frontier of each subnetwork is also minimised in the method. The suggested method consists of three basic steps: forming the weighted depth-first-search tree of the studied power network; partitioning the network into connected, balanced subnetworks and minimising the number of the frontier nodes of the subnetworks through iterations so as to reduce the interaction of FSE in adjacent subnetworks. The relevant mathematical model and partitioning procedure are presented. The method has been implemented with the sparse storage technique and tested in the IEEE 14-bus, 30-bus and 118-bus systems, respectively. Computer simulation results show that the proposed multiway graph partitioning method is effective for the large-scale power system FSE using the distributed AI technique",2002,0,
1590,1591,Error tolerant DNA self-assembly by link-fracturing,"This paper proposes and evaluates link fracturing as an approach for error tolerance in self-assembly by utilizing a DNA chain as a link between two blocks of molecules. Through the use of restriction enzymes, link fracturing breaks the connecting DNA chain between two blocks if an incorrect assembly has occurred due to the erroneous growth of tiles. Two error tolerant techniques are proposed by fracturing of the DNA chain links, namely 1-link and 2-link. Using the tool Xgrow, simulations under the Kinetic Tile Assembly Model (KTAM) are performed. Results show that 2-link fracturing achieves an improvement in error rate as compared to a normal assembly; moreover this is accomplished with little overhead in assembly size and execution complexity. The 1-link method shows 100% error free growth with moderate overhead as compared to normal growth and other existing error tolerant methods.",2009,0,
1591,1592,Research on reliability modeling of complex system based on dynamic fault tree,"The traditional static fault trees with AND, OR, and Voting gates cannot capture the dynamic behavior of complex computer system failure mechanisms such as sequence dependent events, spares and dynamic redundancy management, and priorities of failure events. In this paper, dynamic fault tree modeling method is applied in complex computer system. This paper starts from dynamic fault tree analysis method and its logic gates. A complex computer system simulation example is given. We establish respectively each model, and then gain system-level dynamic fault tree and modularize this model and analyze other questions. At last, we can draw a conclusion that this model method of this paper analyzes well dynamic behaviors in computer system that combines hardware, software and human reason.",2009,0,
1592,1593,Estimating the number of faults remaining in software code documents inspected with iterative code reviews,"Code review is considered an efficient method for detecting faults in a software code document. The number of faults not detected by the review should be small. Current methods for estimating this number assume reviews with several inspectors, but there are many cases where it is practical to employ only two inspectors. Sufficiently accurate estimates may be obtained by two inspectors employing an iterative code review (ICR) process. This paper introduces a new estimator for the number of undetected faults in an ICR process, so the process may be stopped when a satisfactory result is estimated. This technique employs the Kantorowitz estimator for N-fold inspections, where the N teams are replaced by N reviews. The estimator was tested for three years in an industrial project, where it produced satisfactory results. More experiments are needed in order to fully evaluate the approach.",2005,0,
1593,1594,Automatically Finding and Patching Bad Error Handling,Bad error handling is the cause of many service outages. We address this problem by a novel approach to detect and patch bad error handling automatically. Our approach uses error injection to detect bad error handling and static analysis of binary code to determine which type of patch can be instantiated. We describe several measurements regarding the effectiveness of our approach to detect and patch bad error handling in several open source programs,2006,0,
1594,1595,Adaptive Replication Based Security Aware and Fault Tolerant Job Scheduling for Grids,"Most of the existing job scheduling algorithms for grids have ignored the security problem with a handful of exceptions. Moreover, existing algorithms using fixed- number job replications will consume excessive resources when grid security level changes dynamically. In this paper, a security aware and fault tolerant scheduling (SAFTS) algorithm based on adaptive replication is proposed which schedules the jobs by matching the user security demand and resource trust level and the number of the job replications changes adaptively with the dynamic of grid security. In experiments on RSBSME (remote sensing based soil moisture extraction) workload in a real grid environment, the average job scheduling success rate is 97%, and average grid utilization is 74%. Experiment results show that performance of SAFTS is better than non-security-aware and fixed-number job replication scheduling algorithms and SAFTS is fault-tolerant and scalable.",2007,0,
1595,1596,Fault-Tolerant Middleware for Grid Computing,"The major challenge in Grid environment is fault tolerance. Faults ranging from machine crashes, media failures, operator errors and random data corruption results in loss of data, both temporarily and permanently. The paper proposes a solution for handling faults in grid environment. Fault-Tolerance using Adaptive Replication in Grid Computing (FTARG) is an adaptive replication middleware which addresses the fault tolerance of Grid based applications by providing data replication at different sites. FTARG is an Aneka based Grid middleware especially designed for high-performance Grid based applications. FTARG enables data synchronization between multiple heterogeneous databases located in the Grid by supporting a variety of synchronization modes. Experimental analysis proves that proposed FTARG handles faults in the Grid by improving the performance of data management for large scale complex grid based applications.",2010,0,
1596,1597,Auto Regressive Model and Weighted Least Squares Based Packet Video Error Concealment,"In this paper, auto regressive (AR) model is applied to error concealment for block-based packet video encoding. Each pixel within the corrupted block is restored as the weighted summation of corresponding pixels within the previous frame in a linear regression manner. Two novel algorithms using weighted least squares method are proposed to derive the AR coefficients. First, we present a coefficient derivation algorithm under the spatial continuity constraint, in which the summation of the weighted square errors within the available neighboring blocks is minimized. The confident weight of each sample is inversely proportional to the distance between the sample and the corrupted block. Second, we provide a coefficient derivation algorithm under the temporal continuity constraint, where the summation of the weighted square errors around the target pixel within the previous frame is minimized. The confident weight of each sample is proportional to the similarity of geometric proximity as well as the intensity gray level. The regression results generated by the two algorithms are then merged to form the ultimate restorations. Various experimental results demonstrate that the proposed error concealment strategy is able to increase the peak signal-to-noise ratio (PSNR) compared to other methods.",2010,0,
1597,1598,A Digital Image Scrambling Method Based on AES and Error-Correcting Code,"A new scrambling method of true color images based on AES algorithm is proposed. It takes the structure of true color images into full consideration, and decreases the complexity. The analytical method of cryptography is first used in this paper to analyze the security of disordered images; at the same time, an error-correcting code is designed to prevent passive attacks based on bit modification. Experimental results show that the method is safe, efficient and has great ability of error correcting.",2008,0,
1598,1599,Use of fault tree analysis to improve residential gateway testing,"A residential gateway, heart of the strategy of most Telcos, is a centralized intelligent device between the operator's access network and the home's network. It terminates all external access networks and enables residential services to be delivered to the consumer. Besides a plethora of useful services, the growth in market depends upon the reputation of its resilience (availability, reliability and security). This emphasizes a near zero fault design and efficient testing should be taken care before its launch into the market. This paper deals with the analysis of failures, both from test and field data, aiming to increase the efficiency of laboratory testing. Using fault tree analysis, we study the faults that have passed through the testing phase and created failures in the customer premises. With the help of defined specifications, we have identified the zones in which testing in the laboratory needs to be improved.",2007,0,
1599,1600,Induction motor mechanical fault online diagnosis with the application of artificial neural network,"An online fault diagnostic algorithm for induction motor mechanical faults is presented based on the application of artificial neural networks. Two mechanical faults, the rotor bar breakage and air gap eccentricity, are considered. New feature coefficients obtained by wavelet packet decomposition of the stator current are used together with the slip speed as the input of a multi-layer neural network. The proposed algorithm is proved to be able to distinguish healthy and faulty conditions with high accuracy",2001,0,
1600,1601,Design and implementation of Weapons Fault Diagnosis Expert System Platform,"By analyzing the distinguishing features of the current popular weapons fault diagnosis expert system, as well as using component-based software design and complex knowledge representation methods, this paper proposes a solution of overall design and a description of user interfaces about the Weapons Fault Diagnosis Expert System Platform. It studies and discusses the complex knowledge representation about the reusable expert system. The implementation essentials in a variety of reasoning mechanisms are also discussed. It is proved that, a special weapons fault diagnosis expert system can be generated using this expert system platform and the special knowledge of the weapons fault.",2010,0,
1601,1602,Clock Domain Crossing Fault Model and Coverage Metric for Validation of SoC Design,"Multiple asynchronous clock domains have been increasingly employed in system-on-chip (SoC) designs for different I/O interfaces. Functional validation is one of the most expensive tasks in the SoC design process. Simulation on register transfer level (RTL) is still the most widely used method. It is important to quantitatively measure the validation confidence and progress for clock domain crossing (CDC) designs. In this paper, we propose an efficient method for definition of CDC coverage, which can be used in RTL simulation for a multi-clock domain SoC design. First, we develop a CDC fault model to present the actual effect of metastability. Second, we use a temporal dataflow graph (TDFG) to propagate the CDC faults to observable variables. Finally, CDC coverage is defined based on the CDC faults and their observability. Our experiments on a commercial IP demonstrate that this method is useful to find CDC errors early in the design cycles",2007,0,
1602,1603,Analysis of transient performance for DFIG wind turbines under the open switch faults,"The fast development of grid-integrated wind power introduces new requirements for the operation and control of power networks. In order to maintain the reliability of a host power grid, it is preferred that the grid-connected wind turbine should restore its normal operation with minimized power losses in events of grid fault. This paper presents the results for the transient performance of a 2MW doubly-fed induction generator (DFIG), a type of variable-speed wind turbine. The paper concentrates on transient performance of the said generator technology under open-switch grid faults. The simulation was performed using MATLAB - Simulink software. The results obtained have shown that the control schemes employed for the DFIG wind turbines played an effective role in the restoration of the normal operation for the wind turbine in response to grid faults. The results for both during and after the grid fault will be discussed in this paper.",2010,0,
1603,1604,Characterization of defects in photovoltaics using thermoreflectance and electroluminescence imaging,Thermal and electroluminescence (EL) imaging techniques are widely accepted as powerful tools for analyzing solar cells. We have identified and characterized various defects in photovoltaic devices with sub-micron spatial resolution using a novel thermoreflectance imaging technique that can simultaneously obtain thermal and EL images with a mega-pixel silicon-based CCD. Linear and non-linear shunt defects are investigated as well as electroluminescent breakdown regions at reverse biases as low as -5V. Pre-breakdown sites with electroluminescence are observed. The wavelength flexibility of thermoreflectance imaging is explored and thermal images of sub-micrometer defects are obtained through glass that would typically be opaque for infrared light. Image sequences show a 10s thermal transient response of a 15m defect in a polysilicon solar cell. Nanosecond reverse bias voltage pulses are used to detect breakdown regions in thin-film a-Si solar cells with EL.,2010,0,
1604,1605,Optimized Spacecraft Fault Protection for the WISE Mission,"The WISE project is a NASA-funded medium- class Explorer mission to map the entire sky in four infrared bands during the course of a 6-month survey. Because of the mission's limited financial resources, a traditional robustness strategy of full block-redundancy was not feasible. By leveraging aspects of the mission design that tend to reduce the risk associated with certain failures, the project has been able to adopt a robustness strategy of mitigating high-risk failures, while accepting the risk of low-impact faults or unlikely faults in heritage equipment with proven reliability. The resulting WISE flight system design is primarily single-string with some select functional- and block-redundancy and includes fault tolerance measures targeted at achieving the most cost- effective risk reduction possible for the system design. The fault protection team has been challenged with balancing the risk of faults, cost, and down-time with Ground Segment capabilities, heritage, and effectively designed fault mitigations. Faults were identified via a collection of analyses, and a criticality rating was applied to each fault to assess its impact to the mission. Taking into consideration each fault's impact and time criticality, mitigations to each possible fault were considered in areas such as on-board autonomy, the addition or use of functional and block redundancy, and ground system detection. Through this exercise, the project has realized a robust and reliable system design in line with the project's risk posture and cost constraints.",2008,0,
1605,1606,Recent Developments in Single-Phase Power Factor Correction,"The development of single-phase power factor correction (PFC) technologies was traditionally driven by the need for computers, telecommunication, lighting, and other electronic devices and systems to meet harmonic current limits defined by IEC 61000-3-2 and other regulatory standards. Recently, several new applications have emerged as additional drivers for the development of the technologies. One such application is commercial transport airplanes where single-phase PFC converters capable of meeting stringent airborne power quality requirements are required for in-flight entertainment (IFE), avionics, communication, and other single-phase loads. The proliferation of variable-speed motor drives in home appliances has also generated a new need for high-power (up to a few kilowatts), high-efficiency, and low-cost single-phase PFC converters. New PFC circuits, control methods, as well as EMI modeling and design techniques are being developed in response to these new requirements, which are reviewed in this paper. Specific subjects to be covered include airborne and home appliance applications, as well as EMI modeling and EMI filter design optimization.",2007,0,
1606,1607,Concurrent Error Detection in Digit-Serial Normal Basis Multiplication over GF(2m),"Parity prediction schemes have been widely studied in the past. Recently, it has been demonstrated that this prediction scheme can achieve fault-secureness in arithmetic circuits for stuck-at and stuck-open faults. For most cryptographic applications, encryption/decryption algorithms rely on computations in very large finite fields. The hardware implementation may require millions of logic gates and this may lead to the generation of erroneous outputs by the multiplier. In this paper, a concurrent error detection (CED) technique is used in the digit-serial basis multiplier over finite fields of characteristic two. It is shown that all types of normal basis multipliers possess the same parity prediction function.",2008,0,
1607,1608,An induction motor drive system with improved fault tolerance,"This paper investigates the utilization of a simplified topology that permits the fault-tolerant operation of a three-phase induction motor drive system. When one of the inverter legs is lost, the machine can operate with only two stator windings by connecting the machine neutral to a fourth converter leg. The structure and the operation principle of the system are presented. The machine model corresponding to the asymmetric two-windings machine is developed and a suitable controller is proposed. Experimental results are presented",2001,0,
1608,1609,Using Probabilistic Characterization to Reduce Runtime Faults in HPC Systems,"The current trend in high performance computing is to aggregate ever larger numbers of processing and interconnection elements in order to achieve desired levels of computational power, This, however, also comes with a decrease in the Mean Time To Interrupt because the elements comprising these systems are not becoming significantly more robust. There is substantial evidence that the Mean Time To Interrupt vs. number of processor elements involved is quite similar over a large number of platforms. In this paper we present a system that uses hardware level monitoring coupled with statistical analysis and modeling to select processing system elements based on where they lie in the statistical distribution of similar elements. These characterizations can be used by the scheduler/resource manager to deliver a close to optimal set of processing elements given the available pool and the reliability requirements of the application.",2008,0,
1609,1610,D-Q modeling and control of a single-phase three-level boost rectifier with power factor correction and neutral-point voltage balancing,"This paper deals with the analysis, design and operation of a control system for a single-phase three-level rectifier with a neutral-point-clamped (NPC) topology. Usually the desired operating conditions for this type of converter are: unity displacement factor, output DC voltage regulation and neutral point voltage balancing. A d-q reference frame has been used in this work to model the rectifier behaviour in order to exploit the results obtained in the field of three-phase converters. In this way, a space vector modulation PWM method has been used, with the possibility of using redundant switching states to achieve charge balancing of the capacitors. The time assignment of each redundant switching state is accomplished by utilizing a closed-loop control system. Validity of the modeling and control strategies are confirmed by the transient and steady state simulation and experimental results.",2002,0,
1610,1611,Reversible Data Hiding for Audio Based on Prediction Error Expansion,"This paper proposes a reversible data hiding method for digital audio using prediction error expansion technique. Firstly, the prediction error of the original audio is obtained by applying an integer coefficient predictor. Secondly, a location map is set up to record the expandability of all audio samples, and then it is compressed by lossless compression coding and taken as a part of secret information. Finally, the reconstructed secret information is embedded into the audio using prediction error expansion technique. After extracting the embedded information, the original audio can be perfectly restored. Experimental results show that the proposed algorithm can achieve high embedding capacity while keeping good quality of the stego-audio.",2008,0,
1611,1612,Error probability and SINR analysis of optimum combining in rician fading,"This paper considers the analysis of optimum combining systems in the presence of both co-channel interference and thermal noise. We address the cases where either the desired-user or the interferers undergo Rician fading. Exact expressions are derived for the moment generating function of the SINR which apply for arbitrary numbers of antennas and interferers. Based on these, we obtain expressions for the symbol error probability with M-PSK. For the case where the desired-user undergoes Rician fading, we also derive exact closed-form expressions for the moments of the SINR. We show that these moments are directly related to the corresponding moments of a Rayleigh system via a simple scaling parameter, which is investigated in detail. Numerical results are presented to validate the analysis and to examine the impact of Rician fading on performance.",2009,0,
1612,1613,A verification of fault tree for safety integrity level evaluation,"This study focuses on a novel approach which automatically proves the correctness and completeness of fault trees based on a formal model by model checking. This study represents that the model checking technique is useful when validating the correctness of informal safety analysis such as FTA. The benefits of this study are that it provides the probability of formally validating FTA by proving correctness and completeness of the fault trees. In addition to this benefit, it is possible that the CTL technique proves the FTA based SIL.",2009,0,
1613,1614,A Millimeter Wave Direct QPSK modulator MMIC Using PIN Technology And A Novel Approach to Self Error-Correction,"In this paper we present two topologies of QPSK (Quadrature Phase Shift Keying) Modulator for direct carrier modulation at 29.5 GHz for satellite communications using PIN technology. Previously published designs use PHEMT technology. PIN diodes allow operation at high power level (several watts at Ka band) and yield better switching performance compared to PHEMT switches. Our designs exhibit wide bandwidth (3GHz) and reasonable loss (5.5 db). In addition, we present a novel method for a self error correction QPSK modulator, which can yield high quality modulator at millimeter waves.",2002,0,
1614,1615,Satellite Ozone Retrieval Under Broken Cloud Conditions: An Error Analysis Based on Monte Carlo Simulations,"This paper investigates the influence of horizontally inhomogeneous clouds on the accuracy of total ozone column retrievals from space. The focus here is on retrievals based on backscattered ultraviolet light measurements in Huggins bands in the range of 315-340 nm. It is found that simplifying the description of cloud properties in the ozone-retrieval algorithm studied can produce errors of up to 6%, depending on the error in the assumed cloud parameters. Yet another finding is the fact that independent pixel approximation suffices for ozone-retrieval algorithms. This was found using three-dimensional Monte Carlo radiative transfer calculations in the Huggins bands",2007,0,
1615,1616,Strategy to Detect Bug in Pre-silicon Phase,"Bugs still escape to post-silicon despite huge effort has been put into validating the design in pre-silicon phase. This could cost an immediate stepping while some other bugs may have a software work around. Running more tests may still miss the bugs. Therefore it is necessary to have an effective strategy during pre-silicon phase. This paper will present a strategy to derive the test points from the validation objective, and set the domain to test based on the micro-architecture, before entering simulation environment. This strategy utilized coverage based validation (CVB), with test points and domain coded as coverage points, while the test generator directed the transactions into the domain to test. This provides a comprehensive validation coverage to the design under test.",2009,0,
1616,1617,Analysis of single-event effects in embedded processors for non-uniform fault tolerant design,"Advances in silicon technology and shrinking the feature size to nanometer scale make unreliability of nano devices the most important concern of fault-tolerant designs. Design of reliable and fault-tolerant embedded processors is mostly based on developing techniques that compensate adding hardware or software redundancy. The recently-proposed redundancy techniques are generally applied uniformly to a system and lead to inefficiencies in terms of performance, power, and area. Non-uniform redundancy requires a quantitative analysis of the system behavior encountering transient faults. In this paper, we introduce a custom fault injection framework that helps to locate the most vulnerable nodes and components of embedded processors. Our framework is based on an exhaustive transient fault injection to candidate nodes which are selected from a user-defined list. Furthermore, the list of nodes containing the microarchitectural state is also defined by user to validate execution of instructions. Based on the reported results, the most vulnerable nodes, components, and instructions are found and could be used for an effective non-uniform fault-tolerant redundancy technique.",2009,0,
1617,1618,Dual-Processor Design of Energy Efficient Fault-Tolerant System,"A popular approach to guarantee fault tolerance in safety-critical applications is to run the application on two processors. A checkpoint is inserted at the completion of the primary copy. If there is no fault, the secondary processor terminates its execution. Otherwise, should the fault occur, the second processor continues and completes the application before its deadline. In this paper, we study the energy efficiency of such dual-processor system. Specifically, we first derive an optimal static voltage scaling policy for single periodic task. We then extend it to multiple periodic tasks based on worst case execution time (WCET) analysis. Finally, we discuss how to further reduce system's energy consumption at run time by taking advantage of the actual execution time which is less than the WCET. Simulation on real-life benchmark applications shows that our technique can save up to 80% energy while still providing fault tolerance",2006,0,
1618,1619,DVHMM: variable length text recognition error model,"This paper proposes a text recognition error model called the dual variable length output hidden Markov model (DVHMM) and gives a parameter estimation algorithm based on the EM algorithm. Although existing probabilistic error models are limited to substitution (1, 1), insertion (1, 0), and deletion (0, 1) errors, the DVHMM can handle error patterns of any pair (i, j) of lengths including substitution, insertion, and deletion.",2002,0,
1619,1620,A decision tree based method for fault classification in transmission lines,"In this paper a novel and accurate method is proposed for fault classification in transmission lines. The method is based on decision tree and gets the 50 up to 950 Hz phasors of voltages and currents from one end of the line, as the inputs. The method is applied to a 400 kV transmission line, and the results showed the highest possible accuracy within less than quarter of a cycle after fault inception.",2008,0,
1620,1621,An Approach to the Development of Inference Engine for Distributed System for Fault Diagnosis,"The reliable and fault tolerant computers are key to the success to aerospace, and communication industries. Designing a reliable digital system, and detecting and repairing the faults are challenging tasks in order for the digital system to operate without failures for a given period of time. The paper presents a new and systematic software engineering approach of performing fault diagnosis parallel and distributed computing. The purpose of the paper would be to demonstrate a method to build a fault diagnosis for a parallel and distributed computing. The paper chooses a model posed a tremendous challenge to the user for fault analysis. The model is the classic PMC model that happens to be a parallel and distributed computing. The paper would also show a method for building an optimal inference engine by obtaining sub graphs that also preserve the necessary and sufficient conditions of the model. Coin words: Parallel and Distributed Computing, Artificial Intelligence.",2009,0,
1621,1622,Synthesis of Fault-Tolerant Embedded Systems,"This work addresses the issue of design optimization for fault- tolerant hard real-time systems. In particular, our focus is on the handling of transient faults using both checkpointing with rollback recovery and active replication. Fault tolerant schedules are generated based on a conditional process graph representation. The formulated system synthesis approaches decide the assignment of fault-tolerance policies to processes, the optimal placement of checkpoints and the mapping of processes to processors, such that multiple transient faults are tolerated, transparency requirements are considered, and the timing constraints of the application are satisfied.",2008,0,
1622,1623,A Fault Diagnosis Method in Satellite Networks,"Satellite network is a time-varying network, its network topology changed periodically. The common network fault diagnosis models and methods are not suitable to it. This paper is based three levels management architecture. The central management station manages the sub-stations and satellites, the sub-stations manage the satellite agents. When satellite network ruuning, if the satellite could not respond to the network management demands, the intra-domain cooperation or inter-domain cooperation would be activated. The suspectable fault satellite could be diagnosed through cooperation among the fault diagnosis agents in the satellite and the diagnosis task are disassembled. The simulation results shows that in the circumstance of the low faulty frequency, the new method put forward in this paper could be effectively used in satellite network with short cooperative time and low throughput.",2010,0,
1623,1624,Assessing inter-modular error propagation in distributed software,"With the functionality of most embedded systems based on software (SW), interactions amongst SW modules arise, resulting in error propagation across them. During SW development, it would be helpful to have a framework that clearly demonstrates the error propagation and containment capabilities of the different SW components. In this paper, we assess the impact of inter-modular error propagation. Adopting a white-box SW approach, we make the following contributions: (a) we study and characterize the error propagation process and derive a set of metrics that quantitatively represents the inter-modular SW interactions, (b) we use a real embedded target system used in an aircraft arrestment system to perform fault-injection experiments to obtain experimental values for the metrics proposed, (c) we show how the set of metrics can be used to obtain the required analytical framework for error propagation analysis. We find that the derived analytical framework establishes a very close correlation between the analytical and experimental values obtained. The intent is to use this framework to be able to systematically develop SW such that inter-modular error propagation is reduced by design",2001,0,
1624,1625,Using run-time reconfiguration for fault injection in hardware prototypes,"In this paper, a new methodology for the injection of single event upsets (SEU) in memory elements is introduced. SEUs in memory elements can occur due to many reasons (e.g. particle hits, radiation) and at any time. It becomes therefore important to examine the behaviour of circuits when an SEU occurs in them. Reconfigurable hardware (especially FPGAs) was shown to be suitable to emulate the behaviour of a logic design and to realise fault injection. The proposed methodology for SEU injection exploits FPGAs and, contrarily to the most common fault injection techniques, realises the injection directly in the reconfigurable hardware, taking advantage of run-time reconfiguration capabilities of the device. In this case, no modification of the initial design description is needed to inject a fault, that results in avoiding hardware overheads and specific synthesis, place and route phases.",2002,0,
1625,1626,Distributed Induction Generators: 3 - Phase Bolted Short - Circuit Fault Currents,"This paper presents methods for estimation of the value of the 3-phase bolted short circuit current of induction distributed generators. It justifies new, quick, and fairly simple way for approximation of the value of peak currents during the first several cycles of the fault. During the study, numerical simulations of the fault have been performed and compared to the results from the estimation. The analytical and numerical results have been validated by a scaled-down experimental setup with a 2 kW induction generator.",2007,0,
1626,1627,Error Pattern Analysis of Augmented Array Codes Using a Visual Debugging Tool,"Error analysis is very important in determining the error control capabilities of any code. Traditionally these are measured by analyzing BER performance. We have designed and implemented a visual debugging tool (VDT) for tracing trellis decoding process of a class of augmented array codes. We have implemented a user-friendly interface in utilizing the VDT for introducing errors at the specifically selected positions of the array. Also certain error patterns can be added to all possible array positions in sequence. The observation of the effect of these purposely inserted errors is facilitated by VDT interface. The VDT can be effectively employed to measure error detection and/or correction power of the Viterbi decoder. The tool allows users to trace decoding process step-by-step, so it can be utilized for teaching and research activities of code design and their practical applications.",2006,0,
1627,1628,Experimentally evaluating an automatic approach for generating safety-critical software with respect to transient errors,"This paper deals with a software modification strategy allowing on-line detection of transient errors. Being based on a set of rules for introducing redundancy in the high-level code, the method can be completely automated, and is therefore particularly suited for low-cost safety-critical microprocessor-based applications. Experimental results are presented and discussed, demonstrating the effectiveness of the approach in terms of fault detection capabilities",2000,0,
1628,1629,Human Factors in Large-Scale Biometric Systems: A Study of the Human Factors Related to Errors in Semiautomatic Fingerprint Biometrics,"The purpose of this paper is to demonstrate the importance of considering human factors in large-scale, complex biometric systems. A team of 19 board-certified latent print examiners conducted 1620 latent fingerprint image formatting tasks, 1797 encoding tasks, and 146 388 side-by-side comparison tasks of latent prints with potential matching tenprint candidates. Examiner feedback from ten encoding mistakes and 13 comparison mistakes provided significant data to demonstrate the deserved consideration of relating the Department of Defense (DoD) Human Factors Classification System (HFACS) to semiautomatic fingerprint biometrics. The increase in match rate of 10% and 7%, respectively, for encoding and comparison when verifications are conducted on these tasks provides striking evidence of the risks involved if large-scale biometric system integrators or owners design and operate biometric systems based solely on single human examiner conclusions without ample consideration of the error recovery mechanism of second-examiner involvement for low-quality data.",2010,0,
1629,1630,Tunable Infrared Semiconductor Lasers Based on Electromagnetically Induced Optical Defects,"We propose tunable midinfrared laser systems based on dynamic formation of electromagnetically induced optical defect sites. Such defects occur in a waveguide structure having a uniformly corrugated quantum well structure in the absence of any structural defect or phase slip. In the absence of a control laser field, such a corrugated structure causes a uniform perturbation of refractive index along the waveguide. However, when a relatively small region of such a waveguide structure is illuminated from the side by the control laser field, electromagnetically induced transparency occurs in this region while its refractive index corrugation is removed. We also show that such a coherently induced defect can be dynamically moved along the waveguide structure by just steering the control laser beam to illuminate different locations. We utilize the fact that the phase associated with this defect site can be adjusted via changing the length of the illuminated region to present a tunable distributed feedback laser where its lasing wavelength can be continuously varied within the stop-band. We study the case where two coherently induced defect sites happen along the waveguide structure and discuss the impacts of illumination of the whole waveguide structure with the control laser field. We show that the latter can either make the waveguide coherently transparent by destroying its refractive index perturbation and stop-band, or generate a gain-without-inversion grating. Formation of such a grating allows the waveguide to act as a tunable partly gain-coupled distributed feedback laser.",2007,0,
1630,1631,The Unbounded-Error Communication Complexity of Symmetric Functions,"We prove an essentially tight lower bound on the unbounded-error communication complexity of every symmetric function, i.e.,f(x,y)=D(|x Lambda y|), where D:{0,1,...,n}-rarr{0,1} is a given predicate and x,y range over {0,1}<sup>n</sup>. Specifically, we show that the communication complexity of f is between Theta(k/log<sup>5</sup> n) and Theta(k log n), where k is the number of value changes of D in {0,1,...,n}. The unbounded-error model is the most powerful of the basic models of communication (both classical and quantum), and proving lower bounds in it is a considerable challenge. The only previous nontrivial lower bounds for explicit functions in this model appear in the ground breaking work of Forster (2001) and its extensions. Our proof is built around two novel ideas. First, we show that a given predicate D gives rise to a rapidly mixing random walk on Z<sub>2</sub> <sup>n</sup>, which allows us to reduce the problem to communication lower bounds for typical predicates. Second, we use Paturi's approximation lower bounds (1992), suitably generalized here to clusters of real nodes in [0,n] and interpreted in their dualform, to prove that a typical predicate behaves analogous to PARITY with respect to a smooth distribution on the inputs.",2008,0,
1631,1632,Cloud-Rough Model Reduction with Application to Fault Diagnosis System,"During the system fault period, usually the explosive growth signals including fuzziness and randomness are too redundant to make right decision for the dispatcher. So intelligent methods must be developed to aid users in maintaining and using this abundance of information effectively. An important issue in fault diagnosis system (FDS) is to allow the discovered knowledge to be as close as possible to natural languages to satisfy user needs with tractability, and to offer FDS robustness. At this junction, the cloud theory is introduced. The mathematical description of cloud has effectively integrated the fuzziness and randomness of linguistic terms in a unified way. A cloud-rough model is put forward. Based on it, a method of knowledge representation in FDS is developed which bridges the gap between quantitative knowledge and qualitative knowledge. In relation to classical rough set, the cloud-rough model can deal with the uncertainty of the attribute and make a soft discretization for continuous ones. A novel approach, including discretization, attribute reduction, value reduction and data complement, is presented. The data redundancy is greatly reduced based on an integrated use of cloud theory and rough set theory. Illustrated with a power distribution FDS shows the effectiveness and practicality of the proposed approach.",2006,0,
1632,1633,High Performance Computational Grids Fault Tolerance at System Level,"Many complex scientific, mathematical applications require large time for completion. To deal with this issue, parallelization is popularly used. Distributing an application onto several machines is one of the key aspects of grid-computing. This paper focuses on a check point/restart mechanism used to overcome the problem of job suspension at a failed node in a computational Grid. The ability to checkpoint a running application and restart it later can provide many useful benefits including fault recovery by rolling back an application to a previous checkpoint, advanced resources sharing, better application response time by restarting applications from checkpoints instead of from scratch, and improved system utilization, efficient high performance computing and improved service availability.",2008,0,
1633,1634,Torsional oscillations of the turbine-generator due to network faults,"A model of the electromechanical system, suitable for the analysis of torsional oscillations due to the power system's faults, is established. Results of an example of computer simulation of transient torsional torques in the shaft-line due to a three-phase fault and the subsequent fault clearing, as obtained by the model, are presented. Effect of chosen fault clearing time is discussed.",2010,0,
1634,1635,Zoltar: A Toolset for Automatic Fault Localization,"Locating software components which are responsible for observed failures is the most expensive, error-prone phase in the software development life cycle. Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important process for the development of dependable software. In this paper we present a toolset for automatic fault localization, dubbed Zoltar, which hosts a range of spectrum-based fault localization techniques featuring BARINEL, our latest algorithm. The toolset provides the infrastructure to automatically instrument the source code of software programs to produce runtime data, which is subsequently analyzed to return a ranked list of diagnosis candidates. Aimed at total automation (e.g., for runtime fault diagnosis), Zoltar has the capability of instrumenting the program under analysis with fault screeners as a run-time replacement for design-time test oracles.",2009,0,
1635,1636,Fault-tree based evaluation of tiered autonomic systems,The success of self configuration in autonomic distributed systems depends on the availability of the management components and their interconnections. This paper describes a fault tree model to evaluate the availability of a tiered autonomic system that considers the failures of management components. The model predictions will guide the selection and placement of the management components to meet the service-level availability requirements at substantially lower cost and time.,2008,0,
1636,1637,GMPLS fault management and impact on service resilience differentiation,"Generalized Multi-Protocol Label Switching (GMPLS) is currently under standardization. It basically reuses the MPLS control plane (IP routing and signaling) for various technologies such as fiber switching, DWDM, SONET, and packet MPLS. Since GMPLS runs in core networks, fault management is of major concern. However, fast fault recovery and backup capacity assignments are very expensive and not all customers need this or are willing to pay for it. Therefore, we propose in this paper to use several protection and bandwidth-sharing schemes on the same network in order to provide differentiated services in the resilience space. This means an operator can offer and provide several customized services. The service management system implementing, the schemes is built on top of a GMPLS network management system developed in our lab.",2003,0,
1637,1638,Research and application of the fault information integration and intelligent analysis in distributed grid,"In recent years, to build smart grid becomes a new focus, and how to fully collect and make effective use of the information of the intelligent secondary device get abroad attention. This paper presents a grid fault information intelligent analysis and processing system. It is a platform of fault information intelligent processing, used for an incorporate control center system of province and district level. It works in two parts, substation and control center. In the substation, a slave system device gather the information of relays and deliver it to the master system in the control center through Ethernet, and a recorder proxy device gather the information of recorders and deliver it to the master system through another Ethernet channel. The software in the control center has a layered and distributed structure, and import the information of relays and recorders with two isolated server in the data layer. The application layer of the software system can gather the information to a integrated report of a grid disturbance or fault with special process, include of using the fault index,fault time, etc. This method is effective of reduce the requirement of the hardware resource of the slave system, and can decrease the chance of Ethernet block. It can support the decision of the dispatcher and fleetly deal the gird fault. This system has been used in the control center of Taizhou district in Zhejiang province, and remarkably improves the level of management. The system is programmed to integrate with information of other systems, such as SCADA and EMS, and will get whole view of the grid information to give stronger support of building a smart grid.",2010,0,
1638,1639,The fault coverage estimation for protocol conformance testing,"In this paper, we study the fault coverage of a transition tour of an extended finite state machine (Extended FSM). We consider single transition, output and assignment faults. The fault coverage is calculated based on the equivalent classical FSM.",2004,0,
1639,1640,Impedance-based fault location formulation for unbalanced primary distribution systems with distributed generation,"State-of-the-art impedance-based fault location formulations for power distribution systems suppose that the system is radial. However, the introduction of new generation technologies in distribution systems, such as distributed generation, changes the direction of the system load flow from unidirectional to multi-directional. Therefore, it is necessary to extended current impedance-based fault location formulations to take into account the presence of generation units in the distribution system. Moreover, the distribution feeders are inherently unbalanced. This characteristic decreases accuracy of current fault location estimates that are based on sequence or modal phasor quantities. In this paper it is presented an extended impedance-based fault location formulation using phase coordinates. Computational simulations test on a typical unbalanced deregulated distribution system are presented and compared with state-of-the art techniques. The extended formulation is implemented numerically and a case study is presented to demonstrate the methods accuracy.",2010,0,
1640,1641,Open phase faults detection in PMSM drives based on current signature analysis,"This paper deals with monitoring condition of electrical failures in variable speed of permanent magnet synchronous motor (PMSM) drives by stator current signature analysis. The objective of this study is to develop a detection method for the open phase fault in PMSM drives. The main idea consists in minimizing the number of sensors allowing the open stator phase fault of the system to study. The harmonics produced by current spectral analysis fault-related components are studied. The current waveform patterns for various modes of open phase winding are investigated. Simulation and experimental results are presented using a 1.1 kW, 6 poles three-phase PMSM. Comparison of simulation and experimental results show that the method is able to detect the open-phase fault in PMSM drive.",2010,0,
1641,1642,Fault Location in Power Transmission Lines Using a Second Generation Wavelet Analysis,"A novel but effective signal processing tool-second generation wavelet (SGW) analysis-is proposed to extract the features of fault-generated transient waves: the maxima of amplitude fluctuation and polarities. The transient current signals observed at each end of the power transmission line are firstly transformed into an aerial mode current signal when fault occurs. The original data of transient current signal is split into the approximation signal and the detail signal. The approximation signal is updated using the information contained in the detail signal. Meanwhile, The detail signal is predicted using information contained in the updated approximation signal. The updated approximation signal represents the steady component of signal and the updated detail signal contains the high-frequency component only and is exploited to extract the transient features. The maxima of the detail signal are recognized to identify the time-tags and the polarities of these time-tags are easy to determine. Finally, with reference to the maxima and their polarities derived from the high-frequency component of the aerial modal signals, the distance to the fault can be obtained. The simulation results have demonstrated that the proposed SGW analysis scheme is capable of extracting the transient features more effectively than the recently developed multi-resolution morphology gradient method",2005,0,
1642,1643,A temporal error concealment method for H.264/AVC using motion vector recovery,"Recently H.264/AVC is rapidly deployed in the mobile multimedia market, such as terrestrial and satellite DMB (digital multimedia broadcasting), and Internet multimedia streaming systems. To provide the better quality under the unreliable communication environments, we propose a motion vector (MV) recovery method for the temporal error concealment. The H.264/AVC adopts the various block sizes for the motion estimation and compensation, ranging from 16times16 to 4times4 block sizes. To increase the accuracy in the temporal error concealment, the 4times4 block size is used as the MV recovery unit. Flexible MB ordering (FMO) option, by which the neighboring MBs can be transmitted in the different packets, is used. The MVs of the lost MBs are recovered based on the MV tendency which is derived from the neighboring MVs. The simulation results show that the proposed method improves video quality up to 2.95dB and 2.45dB, compared with the H.264/AVC test model and Lagrange interpolation method, respectively.",2008,0,
1643,1644,The Rule Extraction of Fault Classification Based on Formal Concept Analysis,"An approach to extract the fault classification rules based on formal concept analysis is presented. According to the features of the real time database of the surveillance system, designed a method to transfer the records of the real database to formal context as well as to construct concept lattice, then extracted the association rules and fault classification rules. The association rule reflects the relationship among fault phenomena, and the fault classification rule indicates the fault type. A good result is gotten when the approach is used to the intelligent surveillance system of broadcasting transmitters.",2009,0,
1644,1645,Fault-Secure Interface Between Fault-Tolerant RAM and Transmission Channel Using Systematic Cyclic Codes,"The problem of designing a fault-secure interface between a fault-tolerant RAM memory system and a transmission channel, both protected against errors using cyclic linear error detecting and/or correcting codes is considered. The main idea relies on using the RAM check bits to control the correct operation of the parallel cyclic code encoder, so that the whole interface has no single point of failure.",2007,0,
1645,1646,Performance of watermarking as an error detection mechanism for corrupted H.264/AVC video sequences,"In this work we investigate the performance of watermarking as an error detection method for H.264/AVC encoded videos. The efficiency of a previously proposed forced even watermarking is evaluated in a more realistic error-prone transmission scenario. A less invasive watermarking scheme, the force odd watermarking, is proposed as alternative. In order to handle possible decoding desynchronization at the receiver, we implement a syntax check error detection mechanism together with watermarking and evaluate its performance.",2009,0,
1646,1647,Design and implementation of fault-tolerant transactional agents for manipulating distributed objects,"A transactional agent is a mobile agent which manipulates objects distributed in computers. A transactional agent is composed of routing and manipulation subagent. A way to move to computers is decided in the routing agent. Objects in each computer are manipulated in a manipulation agent. After visiting computers, a routing agent makes a decision on commitment by using the commitment condition. In addition, objects obtained from a computer in the manipulation agent have to be delivered to other computers where the transactional agent is performed. A schedule to visit computers is made from the input-output relation of manipulation agents. We discuss a model of transactional agent and implementation of a transactional agent on database servers and evaluate the transactional agents. We evaluate the transactional agent model in terms of accessing time compared with the traditional client-server model.",2005,0,
1647,1648,Design and Implementation of Fault Tolerance in the BACnet/IP Protocol,"Digital communication networks have become a core technology in advanced building automation systems. The building automation and control network (BACnet) is a standard data communication protocol designed specifically for building automation and control systems. BACnet provides the BACnet/IP (B/IP) protocol for data communication through the Internet. Every B/IP device uses a B/IP broadcast management device (BBMD) to deliver remote or global BACnet broadcast messages. In this paper, we propose a fault-tolerant BBMD for the B/IP protocol. The fault-tolerant BBMD improves the connectivity of B/IP networks because a backup BBMD automatically inherits the role of a defective primary BBMD. The fault-tolerant BBMD was designed to provide backward compatibility with existing B/IP devices. In this paper, we implemented the fault-tolerant BBMD and examined its validity using an experimental model.",2010,0,
1648,1649,Fault Detection for Fuzzy Systems With Intermittent Measurements,"This paper investigates the problem of fault detection for Takagi-Sugeno (T-S) fuzzy systems with intermittent measurements. The communication links between the plant and the fault detection filter are assumed to be imperfect (i.e., data packet dropouts occur intermittently, which appear typically in a network environment), and a stochastic variable satisfying the Bernoulli random binary distribution is utilized to model the unreliable communication links. The aim is to design a fuzzy fault detection filter such that, for all data missing conditions, the residual system is stochastically stable and preserves a guaranteed performance. The problem is solved through a basis-dependent Lyapunov function method, which is less conservative than the quadratic approach. The results are also extended to T--S fuzzy systems with time-varying parameter uncertainties. All the results are formulated in the form of linear matrix inequalities, which can be readily solved via standard numerical software. Two examples are provided to illustrate the usefulness and applicability of the developed theoretical results.",2009,0,
1649,1650,An experimental study of adaptive forward error correction for wireless collaborative computing,"This paper describes an experimental study of a proxy service to support collaboration among mobile users. Specifically, the paper addresses the problem of reliably multicasting Web resources across wireless local area networks, whose loss characteristics can be highly variable. The software architecture of the proxy service is described, followed by results of a performance study conducted on a mobile computing testbed. The main contribution of the paper is to show that an adaptive forward error correction mechanism, which adjusts the level of redundancy in response to packet loss behavior, can quickly accommodate worsening channel characteristics in order to reduce delay and increase throughput for reliable multicast channels",2001,0,
1650,1651,Considering Both Failure Detection and Fault Correction Activities in Software Reliability Modeling,"Software reliability is widely recognized as one of the most significant aspects of software quality and is often determined by the number of software uncorrected faults in the system. In practice, it is essential for fault correction prediction, because this correction process consumes a heavy amount of time and resources to predict whether reliability goals have been achieved. Therefore, in this paper we discuss a general framework of the modeling of the failure detection and fault correction process. Under this general framework, we not only verify the existing non-homogeneous poisson process (NHPP) models but also derive several new NHPP models. In addition, we show that these approaches cover a number of well-known models under different conditions. Finally, numerical examples are shown to illustrate the results of the integration of the detection and correction processes",2006,0,
1651,1652,A digital power factor correction (PFC) control strategy optimized for DSP,"A predictive algorithm for digital control power factor correction (PFC) is presented in this paper. Based on this algorithm, all of the duty cycles required to achieve unity power factor in one half line period are calculated in advance by digital signal processors (DSP). A boost converter controlled by these precalculated duty cycles can achieve sinusoidal current waveform. One main advantage is that the digital control PFC implementation based on this control strategy can operate at a high switching frequency which is not directly dependent on the processing speed of DSP. Input voltage feed-forward compensation makes the output voltage insensitive to the input voltage variation and guarantees sinusoidal input current even if the input voltage is distorted. A prototype of boost PFC controlled by a DSP evaluation board was set up to implement the proposed predictive control strategy. Both the simulation and experimental results show that the proposed predictive strategy for PFC achieves near unity power factor.",2004,0,
1652,1653,X-ray micro radiography using beam hardening correction,"The system presented provides high quality micro radiographs including very low contrast and low absorption objects. An important source of image distortion arises from beam hardening effects. When left uncorrected, distortion blurs low contrast image elements. Using a cooled digital X-ray semiconductor detector together with the proposed beam hardening correction procedure brings high dynamic range and very low noise of the acquired radiographs over the entire X-ray source spectrum. Both soft and hard parts of the object appear with high contrast and spatial resolution in the resulting radiographs. The beam hardening correction procedure is fully automated using a set of the calibrators and appropriate software modules",2005,0,
1653,1654,Test-Pattern Selection for Screening Small-Delay Defects in Very-Deep Submicrometer Integrated Circuits,"Timing-related defects are major contributors to test escapes and in-field reliability problems for very-deep submicrometer integrated circuits. Small delay variations induced by crosstalk, process variations, power-supply noise, as well as resistive opens and shorts can potentially cause timing failures in a design, thereby leading to quality and reliability concerns. We present a test-grading technique that uses the method of output deviations for screening small-delay defects (SDDs). A new gate-delay defect probability measure is defined to model delay variations for nanometer technologies. The proposed technique intelligently selects the best set of patterns for SDD detection from an <i>n</i>-detect pattern set generated using timing-unaware automatic test-pattern generation (ATPG). It offers significantly lower computational complexity and excites a larger number of long paths compared to a current generation commercial timing-aware ATPG tool. Our results also show that, for the same pattern count, the selected patterns provide more effective coverage ramp-up than timing-aware ATPG and a recent pattern-selection method for random SDDs potentially caused by resistive shorts, resistive opens, and process variations.",2010,0,
1654,1655,Actuator faults estimation using sliding mode state estimator,The modified method of the actuator faults estimator design for the continuous-time linear MIMO systems is treated in this paper. Based on the sliding mode state estimator the problem addressed is indicated as the approach giving necessary and sufficient condition for design. Lyapunov inequality implying two linear matrix inequalities from are outlined to posses a stabile solution for the optimal parameters in the prescribed estimator structure. A system model based example is presented to illustrate the properties of the proposed design method.,2010,0,
1655,1656,Fault tolerant scheduling of precedence task graphs on heterogeneous platforms,"Fault tolerance and latency are important requirements in several applications which are time critical in nature: such applications require guaranties in terms of latency, even when processors are subject to failures. In this paper, we propose a fault tolerant scheduling heuristic for mapping precedence task graphs on heterogeneous systems. Our approach is based on an active replication scheme, capable of supporting epsiv arbitrary fail-silent (fail-stop) processor failures, hence valid results will be provided even if epsiv processors fail. We focus on a bi-criteria approach, where we aim at minimizing the latency given a fixed number of failures supported in the system, or the other way round. Major achievements include a low complexity, and a drastic reduction of the number of additional communications induced by the replication mechanism. Experimental results demonstrate that our heuristics, despite their lower complexity, outperform their direct competitor, the FTBAR scheduling algorithm [3].",2008,0,
1656,1657,Fault-Tolerant and Fail-Safe Control Systems - Using Remote Redundancy,"This paper presents a novel redundancy concept for safety-critical control systems. By using signature-protected communication, it allows connecting each redundant peripheral just to the most proximate control computer while forwarding information to or from any other units (sensors, actuators, further control computers) over a bus system. We will show that wiring harness can thus be reduced drastically with regard to both weight and complexity without compromising fault tolerance characteristics. Moreover, since function and location are decoupled, remote redundancy can be shared between different subsystems if more than one control loop (e. g. brakes and steering) exists in the overall system. Finally, our approach is highly flexible and not at all restricted to a certain degree of fault tolerance, as example systems for both a fault-tolerant and a fail-safe application (steer-by-wire/flap control) will demonstrate.",2009,0,
1657,1658,Fault tolerant tracking control for nonlinear systems based on derivative estimation,"This paper presents a fault tolerant control (FTC)-architecture for trajectory tracking, where generalized actuator faults are online diagnosed and compensated for by the control system. Employing least-squares derivative estimators for identifying the faults inserts time-delays into the control loop. When a reference trajectory is to be tracked, the closed FTC loop represents a nonlinear time-varying time-delay system. Linearizing its dynamics around the reference trajectory makes it possible to determine tolerable delay times, which allows to deduce admissible intervals for the values of the FTC parameters. The FTC scheme is illustrated by simulations of an underactuated satellite with faulty actuators.",2010,0,
1658,1659,Automatic EEG Artifact Removal: A Weighted Support Vector Machine Approach With Error Correction,"An automatic electroencephalogram (EEG) artifact removal method is presented in this paper. Compared to past methods, it has two unique features: 1) a weighted version of support vector machine formulation that handles the inherent unbalanced nature of component classification and 2) the ability to accommodate structural information typically found in component classification. The advantages of the proposed method are demonstrated on real-life EEG recordings with comparisons made to several benchmark methods. Results show that the proposed method is preferable to the other methods in the context of artifact removal by achieving a better tradeoff between removing artifacts and preserving inherent brain activities. Qualitative evaluation of the reconstructed EEG epochs also demonstrates that after artifact removal inherent brain activities are largely preserved.",2009,0,
1659,1660,RFID-based information system for preventing medical errors,"A report by the Institute of Medicine of the National Academy of Sciences estimates that as many as 98,000 people die in U.S. hospitals each year because of medical errors. In this project, we propose an innovative IT-based approach to prevent errors in various medical processes by utilizing advances in radio frequency identification (RFID) and wireless communications. The goal of the study is to perform an in-depth study of existing RFID technologies for patient care in medical facilities. In the paper, a new system architecture that integrates various wireless technologies such as RFID and Wi-Fi was proposed. In the pilot study, we primarily focused on the limitations and shortcomings of passive RFID technologies in medical settings. Our experimental results show the reliability challenge in the current passive EPC Gen 2 RFID systems for use in a dynamic medical environment.",2009,0,
1660,1661,System RAS implications of DRAM soft errors,"While attention in the realm of computer design has shifted away from the classic DRAM soft-error rate (SER) and focused instead on SRAM and microprocessor latch sensitivities as sources of potential errors, DRAM SER nonetheless remains a challenging problem. This is true even though both cosmic ray-induced and alpha-particle-induced DRAM soft errors have been well modeled and, to a certain degree, well understood. However, the often-overlooked alignment of a DRAM hard error and a random soft error can have major reliability, availability, and serviceability (RAS) implications for systems that require an extremely long mean time between failures. The net of this effect is that what appears to be a well-behaved, single-bit soft error ends up overwhelming a seemingly state-of-the-art mitigation technique. This paper describes some of the history of DRAM soft-error discovery and the subsequent development of mitigation strategies. It then examines some architectural considerations that can exacerbate the effect of DRAM soft errors and may have system-level implications for today's standard fault-tolerance schemes.",2008,0,
1661,1662,Two fault tolerant control strategies for shunt active power filter systems,"This paper shows how to integrate fault detection, fault identification and fault compensation into two different types of fault tolerant active power filter systems. These proposed strategies can compensate the open-circuit and short-circuit failures occurring in the power converter devices used on active power filter systems. The fault compensation is achieved by reconfiguring the power converter topology by using isolating and connecting devices. These devices are used to redefine the post-fault converter topology. This allows for continuous free operation of the system after isolation of the faulty power switches of the converter. Experimental results demonstrate the validity of the proposed fault tolerant control strategies.",2002,0,
1662,1663,ADEV Calculated from Phase Noise Measurements and Its Possible Errors Due to FFT Sampling,"In this paper, we show that fast Fourier transform (FFT) sampling plays an important role in the calculation of Allan deviation (ADEV) while using the numerical integration as a tool for the time and frequency (T&F) conversion. In order to avoid generation of unreasonable ADEV values, FFT sampling data are re-generated with logarithmic frequency space using an interpolation skill. Therefore, results from both the numerical integration and the power-law processes could match each other quite well. Besides, spurs in spectral density have non-neglectful influences upon ADEV results. For example, when the data of our lab's phase noise measurement system are processed, the ADEV generated from the spectral density with spurs may reach to three times the one while spurs are removed",2006,0,
1663,1664,Fault models and test generation for hardware-software covalidation,Mixed hardware-software systems constitute a strong paradigm shift for system validation. The main barriers to overcome are finding the right fault models and optimizing the validation flow. This article presents a research summary of these issues.,2003,0,
1664,1665,Research and simulation of amplitude error and quadrature error for inductosyn,"Two-phase amplitude and quadrature error of inductosyn have a great impact on the detection accuracy, through researching amplitude and quadrature error of inductosyn,eliminate its effects, and improve accuracy of inductosyn. First, analysis the characteristics of these two errors and their impact on the measurement accuracy. Then, research a new method, in the two position of 0A and 90A, respectively detecte their sine and cosine winding voltage , achieve amplitude and quadrature error detection, and amend their by using the software . Finally,experiments show that this new detection and correction method of error could improve the accuracy of Inductosyn.",2010,0,
1665,1666,Fault-tolerant design in a networked vibration control system,"Rocket fairing should be able to sustain a certain level of vibration during launch process and its time in orbit. In active vibration control, actuator failures may lead to control performance deterioration or even catastrophic accidents. For this purpose, a networked vibration control system is used to ensure the reliability of rocket fairing system in the presence of actuator failures. However, in the distributed control systems, the sensors, controllers, and actuators are normally dislocated, and the control signal exchanges among them are realized via network communications. Inevitably, network-induced delay often degrades control system performance. Therefore, it is highly necessary to minimize its detrimental effects on control system performance so as to achieve more robust control authorities. This paper deals with the fault-tolerant design issues for a rocket fairing vibration control system including both actuator failure compensation and network-induced delay compensation. A Luenberger canonical form based actuator failure compensation scheme is proposed to accommodate some typical actuator failures, whose values, pattern and time instants are uncertain. A time-delay compensation scheme is then implemented to reduce damaging effects caused by the sensor-to-controller delay.",2003,0,
1666,1667,Low-Complexity Mobile Video Error Concealment Using OBMA,A low-complexity error concealment technique called the outer boundary matching algorithm (OBMA) for mobile video applications is studied extensively in this work. The OBMA technique is shown to provide an excellent tradeoff between the complexity and the quality of concealed video for a wide range of test video sequences.,2008,0,
1667,1668,A Low Complexity Error Concealment Method for H.264 Video Coding Facilitating Hardware Realization,"This paper presents a new error concealment algorithm, which is suitable for the H.264/AVC coding standard and the hardware implementation. This algorithm can be dividing into two major categories. In the spatial domain, we use the reliable neighboring pixel values with edge detection method to conceal all the lost pixels in the block. In the temporal domain, we propose a variable block size error concealment method. It consists of a block size determination step to determine the type of the lost macro-block and a motion vector recovery step to find the optimal motion vector from the current frame. In the block size determination step, we propose a criterion to determine the size type of the lost block from the neighbor's macro-blocks. In the motion vector recovery step, the optimal motion vector for the lost block chosen from the neighbor's block in the current frame. And it is the minimum value of the side match distortion of the lost block. This proposed algorithm can not only determine the most correct mode for the lost block in a easy way, but also save the much more computation time for error concealment, which is more suitable for hardware implementation.",2009,0,
1668,1669,Protective Relay Synchrophasor Measurements During Fault Conditions,"This paper describes details of the signal processing techniques that a protective relay uses to provide both synchronized phasor measurements and line distance protection. The paper also presents a comprehensive system model of normal and faulted power system operating conditions. Finally, the paper provides power system model test results that demonstrate the ability of the described protective relay to provide synchrophasor measurements during both normal and faulted conditions",2006,0,
1669,1670,Reliable Compare&Swap for fault-tolerant synchronization,"This paper presents two Compare&Swap protocols that, with respect to omission failures, are (1) fault-tolerant and (2) gracefully degrading, respectively. It shows that fault-tolerance and graceful degradation are close but distinct concepts, and that graceful degradation is inherently more costly than fault-tolerance. These Compare&Swap protocols are derived from consensus protocols proposed by Jayanti et al. (1999).",2003,0,
1670,1671,Isolation of software defects: extracting knowledge with confidence,"Software maintenance is one of the most time and effort-consuming phases of software development life cycle. Maintenance managers and personnel look for methods and tools supporting scheduling and performing different software maintenance tasks. To make such tools relevant, they should provide the maintainer/manager with some quantitative input useful for purposes of interpretation and understanding what factors influence maintenance efforts and activities. In this paper, a comprehensive multi-model prediction system is proposed. It dwells on evidence theory and a number of rule-based models independently developed using different methods. Application of evidence theory leads to determination of confidence levels of the generated rules as well as obtained predictions. The study is concerned with the effort needed for the removal of defects in existing software maintenance data. A multi-model prediction system is developed and prediction results are analyzed.",2005,0,
1671,1672,Enhancing error localization of DFT codes by weighted l<inf>1</inf>-norm minimization,"We consider the problem of decoding of real BCH discrete Fourier transform codes (RDFT) which are considered for joint source channel codes to provide robustness against errors in communication channels. In this paper, we propose to combine the subspace based algorithm like MUSIC algorithm with l<sub>1</sub>-norm minimization algorithm, which is promoted as a sparsity solution functional, to enhance the error localization of RDFT codes. Simulation results show that the combined algorithm performs better over the performances of these individual algorithms.",2008,0,
1672,1673,On modeling crosstalk faults,"Traditionally, digital testing of integrated semiconductor circuits has focused on manufacturing defects. There is another class of failures that happens due to circuit marginalities. Circuit-marginality failures are on the rise due to shrinking process geometries, diminishing supply voltage, sharper signal-transition rates, and aggressive styles in circuit design. There are many different marginality issues that may render a circuit nonoperational. Capacitive cross coupling between interconnects is known to be a leading cause for marginality-related failures. In this paper, we present novel techniques to model and prioritize capacitive crosstalk faults. Experimental results are provided to show effectiveness of the proposed modeling technique on large industrial designs.",2005,0,
1673,1674,Variable block size error concealment scheme based on H.264/AVC non-normative decoder,"As the newest video coding standard, H.264/AVC can achieve high compression efficiency. At the same time, due to the high-efficiently predictive coding and the variable length entropy coding, it is more sensitive to transmission errors. So error concealment (EC) in H.264 is very important when compressed video sequences are transmitted over error-prone networks and erroneously received. To achieve higher EC performance, this paper proposes variable block size error concealment scheme (VBSEC) by utilizing the new concept of variable block size motion estimation (VBSME) in H.264 standard. This scheme provides four EC modes and four sub-block partitions. The whole corrupted macro-block (MB) will be divided into variable block size adaptively according to the actual motion. More precise motion vectors (MV) will be predicted for each sub-block. We also produce a more accurate distortion function based on spatio-temporal boundary matching algorithm (STBMA). By utilizing VBSEC scheme based on our STBMA distortion function, we can reconstruct the corrupted MB in the inter frame more accurately. The experimental results show that our proposed scheme can obtain maximum PSNR gain up to 1.72 dB and 0.48 dB, respectively compared with the boundary matching algorithm (BMA) adopted in the JM11.0 reference software and STBMA.",2007,0,
1674,1675,An adaptive algorithm for tolerating value faults and crash failures,"The AQuA architecture provides adaptive fault tolerance to CORBA applications by replicating objects and providing a high-level method that an application can use to specify its desired level of dependability. This paper presents the algorithms that AQUA uses, when an application's dependability requirements can change at runtime, to tolerate both value faults in applications and crash failures simultaneously. In particular, we provide an active replication communication scheme that maintains data consistency among replicas, detects crash failures, collates the messages generated by replicated objects, and delivers the result of each vote. We also present an adaptive majority voting algorithm that enables the correct ongoing vote while both the number of replicas and the majority size dynamically change. Together, these two algorithms form the basis of the mechanism for tolerating and recovering from value faults and crash failures in AQuA",2001,0,
1675,1676,A corrective control for angle and voltage stability enhancement on the transient time-scale,"This paper deals with the development of a nonlinear programming methodology for evaluating load shedding (LS) as a corrective action to improve the dynamic security of power systems when angle or voltage instability is detected. A centralized corrective control is developed, on the basis of online DSA computations, in order to ensure corrective actions when a threatening contingency actually occurs. The algorithm is implemented and tested on the actual Italian grid managed by ENEL S.p.A",2000,0,
1676,1677,A study of adaptive forward error correction for wireless collaborative computing,"This paper addresses the problem of reliably multicasting Web resources across wireless local area networks (WLANs) in support of collaborative computing applications. An adaptive forward error correction (FEC) protocol is described, which adjusts the level of redundancy in the data stream in response to packet loss conditions. The proposed protocol is intended for use on a proxy server that supports mobile users on a WLAN. The software architecture of the proxy service and the operation of the adaptive FEC protocol are described. The performance of the protocol is evaluated using both experimentation on a mobile computing testbed as well as simulation. The results of the performance study show that the protocol can quickly accommodate worsening channel characteristics in order to reduce delay and increase throughput for reliable multicast channels.",2002,0,
1677,1678,Design of fault-tolerant logical topologies in wavelength-routed optical IP networks,"In this paper we illustrate a new methodology for the design of fault-tolerant logical topologies in wavelength-routed optical networks exploiting wavelength division multiplexing, and supporting both unicast and multicast IP datagram flows. Our approach to protection and restoration generalizes the ""design protection"" concepts, and relies on the dynamic capabilities of IP routing to re-route IP datagrams when faults occur, thus leading to high-performance cost-effective fault-tolerant logical topologies. Our design methodology for the first time considers the resilience properties or the topology during the logical topology optimization process, thus extending the optimization of the network resilience performance also on the space of the logical topologies. Numerical results clearly show that our approach is able to obtain very good logical topologies with limited complexity",2001,0,
1678,1679,The NanoBox project: exploring fabrics of self-correcting logic blocks for high defect rate molecular device technologies,"Trends indicate that emerging process technologies, including molecular computing, experiences an increase in the number of noise induced errors and device defects. In this paper, we introduce the NanoBox, a logic lookup table bit string. In this way, we contain and self-correct errors within the lookup table, thereby presenting a robust logic block to higher levels of logic design. We explore five different NanoBox coding techniques. We also examine the cost of implementing two different circuit blocks using a homogeneous fabric of NanoBox logic elements: 1) a floating point control unit from the IBM Power4 microprocessor and 2) a four-instruction ALU. In this initial investigation, our results are not meant to draw definitive conclusions about any specific NanoBox implementation, but rather to spur discussion and explore the feasibility of fine-grained error correction techniques in molecular computing systems.",2004,0,
1679,1680,A simulation model of focus and radial servos in compact disc players with disc surface defects,Compact disc players have been on the market in more than two decades. As a consequence most of the control servo problems have been solved. One large remaining problem to solve is the handling of compact discs with severe surface defects like scratches and fingerprints. This paper introduces a method for making the design of controllers handling surface defects easier. A simulation model of compact disc players playing discs with surface defects is presented. The main novel element in the model is a model of the surface defects. That model is based on data from discs with surface defects. This model is used to compare a high bandwidth with a low bandwidth controller's performance of handling surface defects.,2004,0,
1680,1681,Improved quality of experience of reconstructed H.264/AVC encoded video sequences through robust pixel domain error detection,"The transmission of H.264/AVC encoded sequences over noisy wireless channels generally adopt the error detection capabilities of the transport protocol to identify and discard corrupted slices. All the macroblocks (MBs) within each corrupted slice are then concealed. This paper presents an algorithm that does not discard the corrupted slices but tries to detect those MBs which provide major visual artefacts and then conceal only these MBs. Results show that the proposed solution, based on a set of image-level features and two support vector machines (SVMs), manages to detect 94.6% of those artefacts. Gains in peak signal-to-noise ratios (PSNR) of up to 5.74 dB have been obtained when compared to the standard H.264/AVC decoder.",2008,0,
1681,1682,Assessing reliability risk using fault correction profiles,"Building on the concept of the fault correction profile - a set of functions that predict fault correction events as a function of failure detection events - introduced in previous research, we define and apply reliability risk metrics that are derived from the fault correction profile. These metrics assess the threat to reliability of an unstable fault correction process. The fault correction profile identifies the need for process improvements and provides information for developing fault correction strategies. Applying these metrics to the NASA Goddard Space Flight Center fault correction process and its data, we demonstrate that reliability risk can be measured and used to identify the need for process improvement.",2004,0,
1682,1683,An Analysis of Fault Effects and Propagations in AVR Microcontroller ATmega103(L),"This paper presents an analysis of the effects and propagations of transient faults by simulation-based fault injection into the AVR microcontroller. This analysis is done by injecting 20000 transient faults into main components of the AVR microcontroller that is described in VHDL language. The sensitivity level of various points of the AVR microcontroller such as ALU, Instruction-Register, Program-Counter, Register-file and Flag Registers against fault manifestation is considered and evaluated. The behavior of AVR microcontroller against injected faults is reported and shown that about 41.46% of faults are recovered in simulation time, 53.84% of faults are effective faults and reminding 4.70% of faults are latent faults; moreover a comparison of the behavior of AVR microcontroller in fault injection experiments against some common microprocessors is done. Results of fault analyzing will be used in the future research to propose the fault-tolerant AVR microcontroller.",2009,0,
1683,1684,Online error detection and correction of erratic bits in register files,"Aggressive voltage scaling needed for low power in each new process generation causes large deviations in the threshold voltage of minimally sized devices of the 6T SRAM cell. Gate oxide scaling can cause large transient gate leakage (a trap in the gate oxide), which is known as the erratic bits phenomena. Register file protection is necessary to prevent errors from quickly spreading to different parts of the system, which may cause applications to crash or silent data corruption. This paper proposes a simple and cost-effective mechanism that increases the resiliency of the register files to erratic bits. Our mechanism detects those registers that have erratic bits, recovers from the error and quarantines the faulty register. After the quarantine period, it is able to detect whether they are fully operational with low overhead.",2009,0,
1684,1685,Intermittent Fault Detection and Isolation System,"Aging aircraft electronic boxes often pose a maintenance challenge in that often after malfunctioning during flight in the aircraft, they test good, or ldquoNo Fault Foundrdquo (NFF) during ground test. The reason many of these boxes behave in this manner is that they have intermittent faults, which are momentary opens in one or more circuits due to a cracked solder joint, corroded contact, sprung connector receptacle, or any number of other reasons. These NFF boxes often account for a substantial number of boxes processed through a maintenance facility, where no repair can be performed, because no problem can be detected. Conventional test equipment is designed to test the electronic box for nominal operation, and usually ldquoaverages out,rdquo and hence hides, any short term anomalous event. This paper describes a tester that was specifically designed to detect and isolate the intermittent circuits in an electronic box chassis. This new and innovative tester has been designated the intermittent fault detection and isolation system (IFDIS). The IFDIS very effectively compliments conventional testers. The IFDIS includes an environmental chamber and shake table to subject the box to simulated operational conditions, which greatly enhances the probability the intermittent circuit will manifest itself. The IFDIS also includes an intermittent fault detector which continuously and simultaneously monitors every electrical path in the chassis under test, while the box is exposed to a simulated operational environment. To determine the effectiveness of this new tester in detecting and isolating intermittent circuits, several dozen electronic boxes, identified by serial number, that had been to the repair facility and tested NFF multiple times were selected for IFDIS testing. One or more intermittent faults were detected, isolated and repaired in nearly every box. These boxes were then tested on the conventional tester and returned to service. We are currently monitoring th- - eir performance to determine their increased service life and reduced number of NFF incidents.",2008,0,
1685,1686,Fuzzy mapping of human heuristics for defect classification in gas pipelines using ultrasonic NDE,"This paper presents a methodology for classifying the common defects in steel pipelines for transporting petroleum and gas. Usually, the nondestructive evaluation (NDE) experts in the industry judges the defect type by mere observation which, based on the experience, may or may not be correct. The proposed methodology has attempted to map this heuristic understanding from the shape of the defect waveforms (A-scans) using ultrasonic sensors with the help of fuzzy logic and fuzzy set associations. As such, a subset of features was selected for a set of commonly occurring defects and a fuzzy inference system is then generated using heuristic rules to classify the defect. The initial tests have shown over 90% success rate which is promising for further investigation.",2007,0,
1686,1687,Identification and fault diagnosis of a simulated model of an industrial gas turbine,"In this study, a model-based procedure exploiting analytical redundancy for the detection and isolation of faults of a gas turbine system is presented. The diagnosis scheme is based on the generation of so-called ""residuals"" that are errors between estimated and measured variables of the process. The work is completed under both noise-free and noisy conditions. Residual analysis and statistical tests are used for fault detection and isolation, respectively. The final section shows how the actual size of each fault can be estimated using a multilayer perceptron neural network used as a nonlinear function approximator. The proposed fault detection and isolation tool has been tested on a single-shaft industrial gas turbine model.",2005,0,
1687,1688,Tolerance to multiple transient faults for aperiodic tasks in hard real-time systems,"Real-time systems are being increasingly used in several applications which are time-critical in nature. Fault tolerance is an essential requirement of such systems, due to the catastrophic consequences of not tolerating faults. In this paper, we study a scheme that guarantees the timely recovery from multiple faults within hard real-time constraints in uniprocessor systems. Assuming earliest-deadline-first scheduling (EDF) for aperiodic preemptive tasks, we develop a necessary and sufficient feasibility-check algorithm for fault-tolerant scheduling with complexity O(n<sup>2</sup>-), where n is the number of tasks to be scheduled and  is the maximum number of faults to be tolerated",2000,0,
1688,1689,A precise fault locator algorithm with a novel realization for MV distribution feeders,"In this paper, a novel and practical fault locator for radial distribution feeders is presented. The existing numerical relays of the MV panels are universally arranged in order to realize the proposed fault locator. This arrangement has been validated using an experimental set-up. The fault locator algorithm is carefully developed, modified, and intensively tested using the electromagnetic transient program (EMTP) for both phase and earth faults. Error correction of the computed fault distance via either rational or intelligent techniques is outlined. The results reveal a distinguished precision of the proposed fault locator",2006,0,
1689,1690,Exact computation of maximally dominating faults and its application to n-detection tests,"n-detection test sets for stuck-at faults have been shown to be useful in detecting unmodeled defects. It was also shown that a set of faults, called maximally dominating faults, can play an important role in controlling the increase in the size of an n-detection test set as n is increased. In an earlier work, a superset of the maximally dominating fault set was used. In this work, we propose a method to determine exact sets of maximally dominating faults. We also define a new type of n-detection test sets based on the exact set of maximally dominating faults. We present experimental results to demonstrate the usefulness of this exact set in producing high-quality n-detection test sets.",2002,0,
1690,1691,A Combined Approach for Information Flow Analysis in Fault Tolerant Hardware,"Fault tolerance in information security devices is difficult to establish due to the large number of possible interactions in the device (e. g. embedded code, boolean logic, electromagnetic interference, etc.) In previous work we examined information flow as a graph problem by composing orthogonal views of the device under analysis. In other work we used fault-tree analysis to reason about information flow as a systemic failure arising from certain configurations (or faults) in either the control logic or data flow 'backbone'. In this paper we combine these approaches by taking advantage of an alternative representation of fault trees as reliability block diagrams.",2007,0,
1691,1692,Turkish Word Error Detection Using Syllable Bigram Statistics,"In this study, we have designed and implemented a system, which uses n-gram statistical language model in order to facilitate optical character recognition, speech synthesis and recognition systems. First, the syllables bigram frequencies are extracted from Turkish corpora. Then, the test database including the words, which are written correctly and wrongly, is created. The probability of the words appears the given text is calculated and the wrongly and, correctly written words are determined. The system finds the wrongly written words about 86.13% with the proposed approach and the correctly written words are found about 88.32%",2006,0,
1692,1693,Frisch scheme identification for Errors-in-Variables systems,"This paper considers the problem of identification of dynamic Errors-in-Variables (EIV) systems. Some fatal errors of the well-known Frisch Scheme for EIV identification have been presented, and on the basis of it an improved recursive algorithm is proposed. The new algorithm can estimate both the system parameters and the noise variance with higher accuracy and computational efficiency. Simulations illustrate the theoretical results.",2010,0,
1693,1694,Assessment of the Effect of Memory Page Retirement on System RAS Against Hardware Faults,"The Solaris 10 operating system includes a number of new features for predictive self-healing. One such feature is the ability of the fault management software to diagnose memory errors and drive automatic memory page retirement (MPR), intended to reduce the negative impact of permanent memory faults that generate either correctable or uncorrectable errors on system reliability, availability, and serviceability (RAS). The MPR technique allows memory pages suffering from correctable errors and relocatable clean pages suffering from uncorrectable errors to be removed from use in the virtual memory system without interrupting user applications. It also allows relocatable dirty pages associated with uncorrectable errors to be isolated with limited impact on affected user processes, avoiding an outage for the entire system. This study applies analytical models, with parameters calibrated by field experience, to quantify the reduction that can be made by this operating system self-healing technique on the system interruptions, yearly downtime, and number of services introduced by hardware permanent faults, for typical low-end and mid-range server systems. The results show that significant improvements can be made on these three system RAS metrics by deploying the MPR capability",2006,0,
1694,1695,In-line wafer inspection data warehouse for automated defect limited yield analysis,"A data warehouse approach for the automation of process zone-by-zone defect limited yield analysis is presented in this paper. The system employs pre-calculation of adder defects extraction and clustered defects recognition, a newly developed wafer-wise defect record structure, and a graphical user interface purpose-designed for data selection navigation. Analysis time can be reduced to less than 1% of that of benchmarked conventional procedures",2000,0,
1695,1696,Transformer Power Fault Diagnosis System Design Based On The HMM Method,"Once failures for large-scale transformer power occur, which will result in catastrophic economic losses and social impact. Therefore, it is necessary to design and apply a state monitoring and fault diagnosis system for large-scale transformer in order to improve the reliability and accuracy for power transformer during its running, which will benefit increasing the power enterprise economic performance, promoting economic and social development. This paper aims at large-scale power transformer, introduces Hidden Markov Models theory into power transformer fault diagnosis field, and a fault diagnosis method using the HMM is put forward. Some issues and their settling methods about the HMM applying into power transformer brings are further analysed. The fault diagnosis principle bases on the HMM is discussed in detail. The power transformer faults are classified and each of their characteristic variables is determined, accordingly, the fault diagnosis model library for power transformer is researched. Finally, the fault diagnosis system for large-scale power transformer is designed.",2007,0,
1696,1697,A hybrid system approach towards redundant fault-tolerant control systems,This paper discusses the verification problem of redundancy management systems (RMS) in fault-tolerant control by using a hybrid system approach $the discrete-event-system (DES) abstracting strategy. The qualitative fault-tolerant criteria can be formally verified if a DES model is abstracted from the continuous/discrete-time dynamical system in a consistent way. The acquisition of the DES model and verification of fault-tolerant criteria are illustrated based on a concrete RMS of a redundant flight control system,2000,0,
1697,1698,Fault detection of power transformers using genetic programming method,"This paper proposes a novel method for insulation fault detection of power transformer using the genetic programming (GP) method. Fault detection can be seen as a problem of multi-class classification. GP is a way of automatically constructing computer programs using a process analogous to biological evolution. GP methods of problem solving have a great advantage in their power to represent solutions to complex classification problems. The flexibility of representation gives GP the capacity to represent classification problems with means unavailable to other techniques such as neural networks. A binary tree (Bi-tree) structure is presented to transfer an N-class problem into N-1 two-class problems. The proposed method has been tested on the actual records and compared with the conventional methods, fuzzy system method and artificial neural network method. The result shows that GP has advantages over the existing diagnosis methods and provides a new way to solve the problem of fault detection.",2004,0,
1698,1699,The Fault Diagnosis System of Electric Locomotive Based on MAS,"The efficiency and the effect of fault diagnosis are increasingly important for electric locomotive. However, due to the complexity of the locomotive system configuration and the locomotive inferior running condition, it is much difficult to realize the fault diagnosis system on electric locomotive. In this paper, a multi-objective fault diagnosis algorithm based on multi-agent is proposed. It considers the correlation of different fault diagnosis and takes advantage of the capabilities of the multiple agents in communication and cooperation to estimate and recognize particular faults. Furthermore, a fault diagnosis system architecture is designed based on multi-agent system to support diagnosing fault online for train drivers. The practical application results show the efficiency of the proposed system.",2009,0,
1699,1700,Research on high-speed fuzzy reasoning with CPLD for fault diagnosis expert system,"As an effective method for diagnosis reasoning, fuzzy reasoning is hard to meet the real-time challenge for its complex process and time-consuming. According to the principle of conventional fuzzy reasoning with software, a new method to design expert system fuzzy reasoning with CPLD for fault diagnosis is presented. In the new method, fuzzy operating is realized by function transform with ROM, and CPLD provides logic control and process coordination for fuzzy reasoning. After all, the whole fuzzy reasoning is finished with hardware, not software. It is validated by many experiments that the speed of fuzzy reasoning with this method is faster than traditional modes, and it can be applicable to many on-line diagnosis systems based on single-chip controller or DSP (digital signal processor).",2009,0,
1700,1701,A New Data Format and a New Error Control Scheme for Optical-Storage Systems,"For the requirement of the high-density disc, the coding efficiency and the performance of correcting errors become more and more important. We present a new data format and a new error control scheme of NVD (Next-generation Versatile Disc), which is one of the key technologies of optical-storage system. The new data format of NVD which significantly reduces data redundancy increases the encoding efficiency up to about 4% more than that of DVD. Meanwhile, using 2 error correction codes decoders and a more powerful interleaving process prior to DVD, the new error control scheme of NVD has largely improves the burst error correction capability, though NVD has less error correction codes.",2007,0,
1701,1702,Intra-distance Derived Weighted distortion for error resilience,"Intra coding is one of the most effective ways of reducing the impact of error propagation caused by predictive coding. However, intra coding requires a higher bitrate when compared to inter coding. In order to use Inter coding and reduce error propagation it is important that inter macroblocks predict from AsafeA areas that have a decreased chance of spreading errors. To this end we propose a low complexity method of biasing the prediction mechanism towards recently intra updated macroblocks. We devise a method of adjusting the distortion used in rate distortion optimization to take into account the temporal distance of the last Intra macroblock. Our simulations show that our intra-distance derived weighting (IDW) method improves video coding performance in a lossy environment by up to 1.4 dB for a modest increase in bitrate.",2009,0,
1702,1703,A Fault Detection Mechanism for SOA-Based Applications Based on Gauss Distribution,"Service-oriented architecture (SOA) is an ideal solution to build application system with low cost and high efficiency, but fault detection is not supported in most SOA-based applications. Based on Gauss distribution, a fault detection mechanism for SOA-based applications is proposed. The fault in SOA can be detected through comparing the calculated confidence interval with the predefined parameters at runtime according to the descriptor. Based on the fault detection algorithm, the reference service model is improved to support the proposed algorithm through adding some suitable components.",2009,0,
1703,1704,Error Analysis of L1-Regularized Support Vector Machine for Beta-Mixing Sequence,"Abstract-In this paper, the extension work on the performance of l<sub>1</sub>-regularized support vector machine(l<sub>1</sub>-svm) from the classical independent and identically distributed input sequence to the stationary -mixing input sequence is considered. We establish the bound of generalization error for the l<sub>1</sub>-mixing stationary sequence. It is interesting that our result is available even the size of the dictionary considered is infinite, which is different from most previous results of l<sub>1</sub>-regularized methods. From the established bound of the generalization error of l<sub>1</sub>-svm, we develop a sparsity oracle inequality of l<sub>1</sub>-svm for -mixing input sequence. Following the sparsity oracle inequality, the sufficient condition for the consistency of l<sub>1</sub>-svm with stationary -mixing input sequence can be obtained.",2010,0,
1704,1705,Compiling a benchmark of documented multi-threaded bugs,"Summary form only given. Testing multithreaded, concurrent, or distributed programs is acknowledged to be a very difficult task. We decided to create a benchmark of programs containing documented multithreaded bugs that can be used in the development of testing tool for the domain. In order to augment the benchmark with a sizable number of programs, we assigned students in a software testing class to write buggy multithreaded Java programs and document the bugs. This paper documents this experiment. We explain the task that was given to the students, go over the bugs that they put into the programs both intentionally and unintentionally, and show our findings. We believe this part of the benchmark shows typical programming practices, including bugs, of novice programmers. In grading the assignments, we used our technologies to look for undocumented bugs. In addition to finding many undocumented bugs, which was not surprising given that writing correct multithreaded code is difficult, we also found a number of bugs in our tools. We think this is a good indication of the expected utility of the benchmark for multithreaded testing tool creators.",2004,0,
1705,1706,Performance Analysis of Selected Error Control Protocols in Wireless Multimedia Sensor Networks,"Error control is an important mechanism for providing robust multimedia communication in wireless sensor networks. Although there have been several research works in analysis of error control mechanisms in wireless multimedia networks and wireless sensor networks, but none of them are directly applicable to the wireless multimedia sensor networks (WMSNs) which has resource and performance constraints of WSNs as well as QoS requirements of multimedia communications. In this paper, we comprehensively evaluate the performance of several error control mechanisms in WMSNs. The results of our analysis provide an extensive comparison between automatic repeat request (ARQ), forward error correction (FEC), and hybrid FEC/ARQ error control mechanisms in terms of frame loss rate, frame peak signal-to-noise ratio (PSNR), and energy efficiency.",2010,0,
1706,1707,Fault-Tolerant Optimal Neurocontrol for a Static Synchronous Series Compensator Connected to a Power Network,"This paper proposes a novel fault-tolerant optimal neurocontrol scheme (FTONC) for a static synchronous series compensator (SSSC) connected to a multimachine benchmark power system. The dual heuristic programming technique and radial basis function neural networks are used to design a nonlinear optimal neurocontroller (NONC) for the external control of the SSSC. Compared to the conventional external linear controller, the NONC improves the damping performance of the SSSC. The internal control of the SSSC is achieved by a conventional linear controller. A sensor evaluation and (missing sensor) restoration scheme (SERS) is designed by using the autoassociative neural networks and particle swarm optimization. This SERS provides a set of fault-tolerant measurements to the SSSC controllers, and therefore, guarantees a fault-tolerant control for the SSSC. The proposed FTONC is verified by simulation studies in the PSCAD/EMTDC environment.",2008,0,
1707,1708,Fault tolerant DTC for six-phase symmetrical induction machine,"In this paper, a new fault tolerant direct torque control (DTC) algorithm for six phase induction machines (6PIM) is introduced. The machine presents two sets of three phase windings spatially shifted by 60 electrical degrees. The aim of the proposed approach consists in computing an average stator voltage vector in order to control the mean values of the stator flux and the electromagnetic torque over a sampling period under open phase. The main advantages are fixed switching frequency, low torque ripples and reduced line current ripples in comparison with classical DTC. Simulation and experimental results show satisfying performances and validate the proposed method.",2009,0,
1708,1709,Fault Diagnosis With Convolutional Compactors,"This paper presents new nonadaptive fault-diagnosis techniques for scan-based designs. They guarantee accurate and time-efficient identification of failing scan cells based on results of convolutional compaction of test responses. The essence of the method is to use a branch-and-bound algorithm to narrow the set of scan cells down to certain sites that are most likely to capture faulty signals. This search is guided by a number of heuristics and self-learned information used to accelerate the diagnosis process for the subsequent test patterns. A variety of experimental results for benchmark circuits, industrial designs, and real fail logs confirm the feasibility of the proposed approach even in the presence of unknown states. The scheme remains consistent with a single test session scenario and allows high-volume in-production diagnosis.",2007,0,
1709,1710,Towards energy-aware software-based fault tolerance in real-time systems,"Many real-time systems employed in defense, space, and consumer applications have power constraints and high reliability requirements. In this paper, we focus on the relationship between fault tolerance techniques and energy consumption. In particular, we establish the energy efficiency of Application Level Fault Tolerance (ALFT) over other software-based fault tolerance methods. We then develop sensible energy-aware heuristics for ALFT schemes. The heuristics yield up to 40% energy savings.",2002,0,
1710,1711,Validation of hardware error recovery mechanisms for the SPARC64 V microprocessor,"The SPARC64 V microprocessor is designed for use in high-reliability, large-scale unix servers. In addition to implementing ECC for large SRAM arrays, the SPARC64 V microprocessor incorporates error detection and recovery mechanisms for processor logic circuits and smaller SRAM arrays. The effectiveness of these error recovery mechanisms was validated via accelerated neutron testing of Fujitsupsilas commercial unix server, the PRIMEPOWER 650. Soft errors generated in SRAM arrays were completely recovered by the implemented hardware mechanisms, and only 6.4% of the estimated neutron-induced logic circuit faults manifested as errors, 76% of which were recovered by hardware. From these tests, the soft error failure rate of the SPARC64 V microprocessor due to atmospheric neutron hits was confirmed to be well below 10 FIT.",2008,0,
1711,1712,A Reconfigurable Motor for Experimental Emulation of Stator Winding Interturn and Broken Bar Faults in Polyphase Induction Machines,"The benefits and drawbacks of a 5-hp reconfigurable induction motor, which was designed for experimental emulation of stator winding interturn and broken rotor bar faults, are presented in this paper. It was perceived that this motor had the potential of quick and easy reconfiguration to produce the desired stator and rotor faults in a variety of different fault combinations. Hence, this motor was anticipated to make a useful test bed for evaluation of the efficacy of existing and new motor fault diagnostics techniques and not the study of insulation failure mechanisms. Accordingly, it was anticipated that this reconfigurable motor would eliminate the need to permanently destroy machine components such as stator windings or rotor bars when acquiring data from a faulty machine for fault diagnostic purposes. Experimental results under healthy and various faulty conditions are presented in this paper, including issues associated with rotor bar-end ring contact resistances that showed the drawbacks of this motor in so far as emulation of rotor bar breakages. However, emulation of stator-turn fault scenarios was successfully accomplished.",2008,0,
1712,1713,Design and implementation of informatized assessment system on electrical fault maintenance-- taken electrical fault detection of refrigeration equipment as an example,"This paper narrates the design on constructing informatized training and assessment system of refrigerate equipment. Through the analysis of functional module such as RS485, intelligent embedded controllers, simulation training platform, the characteristics of higher efficiency, low-loss, lowcost, guarantee the fairness and just of the examination are mainly discussed.",2008,0,
1713,1714,Iterative correction of frequency response mismatches in time-interleaved ADCs: A novel framework and case study in OFDM systems,"In this paper, we study a versatile iterative framework for the correction of frequency response mismatch in time-interleaved ADCs. Based on a general time varying linear system model, we establish a flexible iterative framework, which enables the development of various efficient iterative correction algorithms. In particular, we study the Gauss-Seidel iteration in detail to illustrate how the correction problem can be solved iteratively, and show that the iterative structure can be efficiently implemented using Farrow-based variable digital filters with few general-purpose multipliers. Simulation results show that the proposed iterative structure performs better than conventional compensation structures. Moreover, a preliminary study on the BER performance of OFDM systems due to TI ADC mismatch is conducted.",2010,0,
1714,1715,A Method of Building the Fault Propogation Model of Distributed Application Systems Based on Bayesian Network,"Fault diagnosis is a key research part in the field of network fault management. In order to make effective fault diagnosis to the increasingly complicated distributed application systems(DAS) which are based on the computer network, Building an accurate and practicable fault propagation model(FPM) is generally the necessary prerequisite of the subsequent tasks such as probabilistic reasoning, fault recovery and failure prediction. In this paper, a method of constructing the FPM which combined sample datas and the expert knowledge was put forward based on Bayesian network. Firstly, an initial tree(T) including all the service nodes on the specific DAS was generated by the maximum weight spanning tree(MWST) algorithm with sample datas. Secondly, the initial tree(T) was revised according to expert experiences. Finally, the FPM of the DAS was learned using greedy search structure-learning algorithm with the revised structure(T') as its initial input model. In the end, the learned FPM using the proposed method was evaluated by calculating its BIC-score and comparing to the actual one. And the results show that the proposed method can give an accurate FPM of the distributed application system.",2009,0,
1715,1716,Fault diagnosis modeling of power systems using Petri Nets,"In this paper, Petri Nets (PN) is used for accurately fault diagnosis in power systems when some incomplete and uncertain alarm information of protective relays and circuit breakers is detected. After reviewing the Petri nets theory, then, models of fault diagnosis based on PN are built, and their corresponding logical testifications are carried out. Finally, the validity and feasibility of this method is illustrated by simulation results. It is shown from several cases that the faulted system elements can be diagnosed correctly by use of these models. By proposed method, it is possible to reduce diagnosis time and increase correctness of results compared to traditional methods. It is also suitable to online applications. The proposed method can easily be adapted to different power system network configurations.",2010,0,
1716,1717,Enhanced FPGA reliability through efficient run-time fault reconfiguration,"The expanded use of field programmable gate arrays (FPGA) in remote, long life, and system-critical applications requires the development and implementation of effective, efficient FPGA fault-tolerance techniques. FPGA have inherent redundancy and in-the-field reconfiguration capabilities, thus providing alternatives to standard integrated circuit redundancy-based fault-recovery techniques. Runtime reliability can be enhanced by using such unique features. Recovery from permanent logic and interconnect faults without runtime computer-aided design (CAD) support can be efficiently performed with the use of fine-grained and physical design partitioning. Faults are localized to small partitioned blocks that have fixed interfaces to the surrounding portions of the design, and the affected blocks are reconfigured with previously generated, functionally equivalent block instances that do not use the faulty resources. This technique minimizes the post-fault-detection system downtime, while requiring little area overhead. Only the finely located faulty portions of the FPGA are removed from use. In addition, the end user need not have access to CAD tools, making the algorithm completely transparent to system users. This approach has been efficiently implemented on a diverse set of FPGA architectures. The algorithm's flexibility is also apparent from the variable emphases that can be placed on system reliability, area overhead, timing overhead, design effort, and system memory. Given user-defined emphases, the algorithm can be modified to specific application requirements. Experiments using random s-independent and s-correlated fault models reveal that the approach enhances system reliability, while minimizing area and timing overhead",2000,0,
1717,1718,Evaluating Performance and Fault Tolerance in a Virtual Large-Scale Disk,"Recently, the exchange of data has increased with progress in information technology. The capacity of for storing data is also increasing. However, even if the capacity of global storage becomes extremely large, the capacity of local storage is always limited. Moreover, storage of files larger then the available local storage is impossible. This paper discusses use of a network to construct a cheap, high trust and PB class, decentralized storage system using many hundreds of PCs in an educational environment. The performance of this 'large-scale virtual disk' is investigated both during normal operation and in instances of failure.",2008,0,
1718,1719,Analysis of spreadsheet errors made by computer literacy students,"Spreadsheets have become a routine application in most organizations and universities. As a consequence, students are required to learn spreadsheet applications such as Microsoft Excel. The learning of spreadsheets is often accompanied by problems related to spreadsheet application and their mathematical content. The EXITS (Excel intelligent tutoring system) research project aims to develop a Microsoft Excel tutor that help students or learners to overcome their learning difficulties. In this paper, we analyse and classify spreadsheet errors made by students in order to determine the function that our system should perform and to generate an error library for student modelling purposes.",2004,0,
1719,1720,Crash fault detection in celerating environments,"Failure detectors are a service that provides (approximate) information about process crashes in a distributed system. The well-known ldquoeventually perfectrdquo failure detector, diamP, has been implemented in partially synchronous systems with unknown upper bounds on message delay and relative process speeds. However, previous implementations have overlooked an important subtlety with respect to measuring the passage of time in ldquoceleratingrdquo environments, in which absolute process speeds can continually increase or decrease while maintaining bounds on relative process speeds. Existing implementations either use action clocks, which fail in accelerating environments, or use real-time clocks, which fail in decelerating environments. We propose the use of bichronal clocks, which are a composition of action clocks and real-time clocks. Our solution can be readily adopted to make existing implementations of diamP robust to process celeration, which can result from hardware upgrades, server overloads, denial-of-service attacks, and other system volatilities.",2009,0,
1720,1721,A Job Pause Service under LAM/MPI+BLCR for Transparent Fault Tolerance,"Checkpoint/restart (C/R) has become a requirement for long-running jobs in large-scale clusters due to a meantime-to-failure (MTTF) in the order of hours. After a failure, C/R mechanisms generally require a complete restart of an MPI job from the last checkpoint. A complete restart, however, is unnecessary since all but one node is typically still alive. Furthermore, a restart may result in lengthy job requeuing even though the original job had not exceeded its time quantum. In this paper, we overcome these shortcomings. Instead of job restart, we have developed a transparent mechanism for job pause within LAM/MPI+BLCR. This mechanism allows live nodes to remain active and roll back to the last checkpoint while failed nodes are dynamically replaced by spares before resuming from the last checkpoint. Our methodology includes LAM/MPI enhancements in support of scalable group communication with fluctuating number of nodes, reuse of network connections, transparent coordinated checkpoint scheduling and a BLCR enhancement for job pause. Experiments in a cluster with the NAS parallel benchmark suite show that our overhead for job pause is comparable to that of a complete job restart. A minimal overhead of 5.6% is only incurred in case migration takes place while the regular checkpoint overhead remains unchanged. Yet, our approach alleviates the need to reboot the LAM run-time environment, which accounts for considerable overhead resulting in net savings of our scheme in the experiments. Our solution further provides full transparency and automation with the additional benefit of reusing existing resources. Executing continues after failures within the scheduled job, i.e., the application staging overhead is not incurred again in contrast to a restart. Our scheme offers additional potential for savings through incremental checkpointing and proactive diskless live migration, which we are currently working on.",2007,0,
1721,1722,An effective fault-tolerant routing methodology for direct networks,"Current massively parallel computing systems are being built with thousands of nodes, which significantly affect the probability of failure. M. E. Gomex proposed a methodology to design fault-tolerant routing algorithms for direct interconnection networks. The methodology uses a simple mechanism: for some source-destination pairs, packets are first forwarded to an intermediate node, and later, from this node to the destination node. Minimal adaptive routing is used along both subpaths. For those cases where the methodology cannot find a suitable intermediate node, it combines the use of intermediate nodes with two additional mechanisms: disabling adaptive routing and using misrouting on a per-packet basis. While the combination of these three mechanisms tolerates a large number of faults, each one requires adding some hardware support in the network and also introduces some overhead. In this paper, we perform an in-depth detailed analysis of the impact of these mechanisms on network behaviour. We analyze the impact of the three mechanisms separately and combined. The ultimate goal of this paper is to obtain a suitable combination of mechanisms that is able to meet the trade-off between fault-tolerance degree, routing complexity, and performance.",2004,0,
1722,1723,Decoupled-DFIG Fault Ride-Through Strategy for Enhanced Stability Performance During Grid Faults,"This paper proposes a decoupled fault ride-through strategy for a doubly fed induction generator (DFIG) to enhance network stability during grid disturbances. The decoupled operation proposes that a DFIG operates as an induction generator (IG) with the converter unit acting as a reactive power source during a fault condition. The transition power characteristics of the DFIG have been analyzed to derive the capability of the proposed strategy under various system conditions. The optimal crowbar resistance is obtained to exploit the maximum power capability from the DFIG during decoupled operation. The methods have been established to ensure proper coordination between the IG mode and reactive power compensation from the grid-side converter during decoupled operation. The viability and benefits of the proposed strategy are demonstrated using different test network structures and different wind penetration levels. Control performance has been benchmarked against existing grid code standards and commercial wind generator systems, based on the optimal network support required (i.e., voltage or frequency) by the system operator from a wind farm installed at a particular location.",2010,0,
1723,1724,A novel method for closed-loop error correction microwave and millimeter wave QPSK modulator,"QPSK modulators at microwave and millimeter waves can be very useful for direct modulation communication links. The main problem with such modulators is the error in phase and amplitude balance, which is quite large at MM-waves and degrades the performance of the modulator (carrier rejection, deviation from 90 degrees between states, etc.). In this paper we introduce a novel approach featuring an error correction scheme, which is simple to implement both in hybrid and MMIC forms. A very important feature of the new method is the self-generated reference signal, which enables a simple (low cost) and self contained implementation in a MMIC form of a high quality QPSK MM-wave modulator.",2001,0,
1724,1725,A Research on Identification System for Image Defects Based on Grating Technology,The structure and mechanism of identification system for image defects based on grating technology is introduced and some crucial technologies in the system are discussed. This paper then proposes some new threads on how to satisfy the real time requirement of the system. The detection experiment manifests that these methods are useful and can satisfy the demands of system design.,2009,0,
1725,1726,Notice of Retraction<BR>Diagnosis method for connection-related faults in motion system based on SVM,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In order to arising the safety and reliability, and monitoring the working states of numerical control system, aiming at the multi-kinds of potential connection-related faults, the construction and the principle of the system were analyzed, and the systemic diagnosis framework was developed. Using the position signal and the torque monitoring one, the parameters of support vector machine were trained where the Gaussian function was employed as nonlinear kernel. The mentioned faults were diagnosed benefiting from the decision function where the parameters were from the trained results. Above method was applied to an X-Y motion platform where data acquisitions, training of support vector machine and fault diagnosis were carried out. The results validate the feasibility of the SVM method.",2009,0,
1726,1727,Optimization of Hybridized Error Concealment for H.264,"Transmission of highly compressed video bitstreams can result in packet erasures when channel status is unfavorable, the consequence being not only the corruption of a single frame, but propagation to its successors. In order to avoid error-catalyzed artifacts from producing visible corruption of affected video frames, the use of error concealment at the video decoder becomes essential. The purpose of this paper proposes an efficient and integrated novel EC method for the latest video compression standard H.264/AVC, using not only spatially and temporally correlated information but also the tandem utilization of two new coding tools: directional spatial prediction for intracoding and variable block size motion compensation of H.264/AVC. Experiments performed using the proposed hybridization method of combining the above spatial and temporal estimation elements fulfilled the expectations of control-whole-scheme. The experimental results show that the proposed method offers excellent gains of up to 10.62 dB compared to that of the Joint Model (JM) decoder for a wide range of benchmark sequences without any considerable increase in time demand.",2008,0,
1727,1728,Incorporating fault debugging activities into software reliability models: a simulation approach,"A large number of software reliability growth models have been proposed to analyse the reliability of a software application based on the failure data collected during the testing phase of the application. To ensure analytical tractability, most of these models are based on simplifying assumptions of instantaneous & perfect debugging. As a result, the estimates of the residual number of faults, failure rate, reliability, and optimal software release time obtained from these models tend to be optimistic. To obtain realistic estimates, it is desirable that the assumptions of instantaneous & perfect debugging be amended. In this paper we discuss the various policies according to which debugging may be conducted. We then describe a rate-based simulation framework to incorporate explicit debugging activities, which may be conducted according to the different debugging policies, into software reliability growth models. The simulation framework can also consider the possibility of imperfect debugging in conjunction with any of the debugging policies. Further, we also present a technique to compute the failure rate, and the reliability of the software, taking into consideration explicit debugging. An economic cost model to determine the optimal software release time in the presence of debugging activities is also described. We illustrate the potential of the simulation framework using two case studies.",2006,0,
1728,1729,Research of Secure Multicast Key Management Protocol Based on Fault-Tolerant Mechanism,"As multicasting is increasingly used as an efficient communication mechanism for group-oriented applications in the Internet, the research of the multicast key management is becoming a hot issue. Firstly, we analyze the <i>n</i>-party GDH.2 multicast key management protocol and point out that it has the following flaws: lack of certification, vulnerability to man-in-the-middle attacks, and a single-point failure. In order to settle the issues mentioned above, a fault-tolerant and secure multicast key management protocol (FTS, for short) with using the fault-tolerant algorithm and the password authentication mechanism is proposed in this paper. In our protocol, legal members are able to agree on a key despite failures of other members. The protocol can also prevent man-in-the-middle attacks. Finally, we evaluate the security of FTS, and compare our protocol with the FTKM through performance analysis. The analytic results show that the protocol not only avoids the single-point failure but also improves the comprehensive performance.",2009,0,
1729,1730,Extending Lifetime of Wireless Sensor Networks using Forward Error Correction,"Communication between nodes in wireless sensor networks (WSN) is susceptible to transmission errors caused by low signal strength or interference. These errors manifest themselves as lost or corrupt packets. This often leads to retransmission, which in turn results in increased power consumption reducing node and network lifetime. In this paper, a convolution code FEC with Viterbi decoding on Mica2 nodes was implemented and evaluated to explore the possibility of extending the lifetime of a degrading WSN. Results were presented which suggest that our approach could be used in a WSN when increasing distance and channel noise degrade the network",2006,0,
1730,1731,Worst case reliability prediction based on a prior estimate of residual defects,"In this paper we extend an earlier worst case bound reliability theory to derive a worst case reliability function R(t), which gives the worst case probability of surviving a further time t given an estimate of residual defects in the software N and a prior test time T. The earlier theory and its extension are presented and the paper also considers the case where there is a low probability of any defect existing in the program. For the ""fractional defect"" case, there can be a high probability of surviving any subsequent time t. The implications of the theory are discussed and compared with alternative reliability models.",2002,0,
1731,1732,"Position mapping, energy calibration, and flood correction improve the performances of small gamma camera using PSPMT","The purpose of this study is to improve the performances of a small gamma camera using position sensitive PMT (PSPMT) by applying position mapping, energy calibration, and flood correction. The small gamma camera consists of a 5"" PSPMT coupled with either CsI(Tl) array or NaI(Tl) plate crystals. Flood images were obtained intrinsically in CsI(Tl) array system and with lead hole mask in NaI(Tl) plate system. The position mapping was performed by locating crystal arrays and hole positions in the two systems. The energy calibration was performed using energy discrimination table for each pixel array or for each hole position. The flood correction was performed using a uniformity correction table containing the relative efficiency of each image element. The resolution of the CsI(Tl) array system remained similar before and after corrections. On the other hand, the resolution of the NaI(Tl) plate system was improved about 16% after correction. The uniformity and linearity improved 33.9% to 11.6% and 0.5 mm to 0 mm, respectively, in the CsI(Tl) array system. The corrections more effectively improve the uniformity and linearity in the NaI(Tl) plate system, 21.3% to 9.2% and 0.5 mm to 0 mm, respectively. Furthermore, the resolution deterioration observed in the NaI(Tl) plate system at the outer part of FOV, was considerably diminished after the corrections. The results of this study indicate that the correction algorithms considerably improve the performances of a small gamma camera and the performance gain is more prominent in the system employing a plate type crystal.",2003,0,
1732,1733,FOTG: fault-oriented stress testing of IP multicast,"Network simulators provide a useful tool, for protocol evaluation. However, the results depend heavily on the simulated scenarios, especially for complex protocols such as multicast. There has been little work on scenario generation. In this work we present a fault-oriented test generation (FOTG) algorithm for automated stress testing of multicast protocols. FOTG processes an extended FSM model and uses a mix of forward and backward search techniques. Unlike traditional verification approaches, instead of starting from initial states, FOTG starts from a fault and uses cause-effect relations for automatic topology synthesis then uses backward implication to generate tests. Using FOTG we test various mechanisms commonly employed by multicast routing and validate our results through simulation.",2005,0,
1733,1734,Performability/Energy Tradeoff in Error-Control Schemes for On-Chip Networks,"High reliability against noise, high performance, and low energy consumption are key objectives in the design of on-chip networks. Recently some researchers have considered the impact of various error-control schemes on these objectives and on the tradeoff between them. In all these works performance and reliability are measured separately. However, we will argue in this paper that the use of error-control schemes in on-chip networks results in <i>degradable systems</i>, hence, performance and reliability must be measured jointly using a unified measure, i.e., <i>performability</i>. Based on the traditional concept of performability, we provide a definition for the ??Interconnect Performability??. Analytical models are developed for interconnect performability and expected energy consumption. A detailed comparative analysis of the error-control schemes using the performability analytical models and SPICE simulations is provided taking into consideration voltage swing variations (used to reduce interconnect energy consumption) and variations in wire length. Furthermore, the impact of noise power and time constraint on the effectiveness of error-control schemes are analyzed.",2010,0,
1734,1735,Temporal envelope correction for attack restoration in low bit-rate audio coding,"At reduced bit rates, the audio compression affects transient parts of signals, which results in pre-echo and loss of attack character. We propose in this paper an attacks restoration method based on the correction of the temporal envelope of the decoded signal, using a small set of coefficients transmitted through an auxiliary channel. The proposed approach is evaluated for single and multiple coding-decoding, using objective perceptual measures. The experimental results for MP3 and AAC coding exhibits an efficient restoration of the attacks and a significant improvement of the audio quality.",2009,0,
1735,1736,Implications of shorted turn faults in bar wound PM machines,"The paper discusses the effect of single turn short-circuits in the windings of fault tolerant permanent magnet machines. It is shown that previously recognised methods for dealing with shorted turns do not work for larger bar-wound machines, and a new method for protecting the windings against single turn faults is therefore proposed. The new method depends on rapid detection of the fault and injection of a current (of appropriate magnitude and phase) into the faulted winding. The paper gives a theoretical analysis supported by finite element simulations of the fault conditions and illustrates these with a case study taken from the aerospace industry.",2004,0,
1736,1737,Geographic routing in the presence of location errors,"In this paper, we propose a new geographic routing algorithm that alleviates the effect of location errors on routing in wireless ad hoc networks. In most previous work, geographic routing has been studied assuming perfect location information. However, in practice there could be significant errors in obtaining location estimates, even when nodes use GPS. Hence, existing geographic routing schemes will need to be appropriately modified. We investigate how such location errors affect the performance of geographic routing strategies. We incorporate location errors into our objective function by considering both transmission failures and backward progress. Each node then forwards packets to the node that maximizes this objective function. We call this strategy maximum expectation within transmission range (MER). Simulation results with MER show that accounting for location errors significantly improves the performance of geographic routing. We also show that MER is robust to the location error model and model parameters. Further, via simulations, we show that in a mobile environment MER performs better than existing approaches.",2005,0,
1737,1738,A low-complexity unequal error protection of H.264/AVC video using adaptive hierarchical QAM,"In this paper, a low-complexity unequal error protection (UEP) of H.264/AVC coded video using adaptive hierarchical quadrature amplitude modulation (HQAM), which takes into consideration the non-uniformly distributed importance of intracoded frame (I-frame) and predictive coded frame (P-frame) is proposed. Simulation results show that in terms of average peak signal-to-noise ratio (average PSNR), our proposed (EEP) scheme outperforms the equal error protection (EEP) by up to 5 dB",2006,0,
1738,1739,Kernel Classification via Integrated Squared Error,"Nonparametric kernel methods are widely used and proven to be successful in many statistical learning problems. Wellknown examples include the kernel density estimate (KDE) for density estimation and the support vector machine (SVM) for classification. We propose a kernel classifier that optimizes an integrated squared error (ISE) criterion based on a ""difference of densities"" formulation. Our classifier is sparse, like SVMs, and performs comparably to state-of-the-art kernel methods. Furthermore, and unlike SVMs, the ISE criterion does not require the user to set any unknown regularization parameters. As a consequence, classifier training is faster than for support vector methods.",2007,0,
1739,1740,Quantization errors in committee machine for gas sensor applications,"In a digital implementation of a gas identification system, the mapping of continuous real parameter values into a finite set of discrete values introduces an error into the system. This paper presents the results of an investigation into the effects of parameter quantization on different classifiers (KNN, MLP and GMM). We propose a committee machine to decrease the classification performance degradation due to the quantization errors. The simulation results show that the committee machine always outperforms a single classifier and the gain in classification performance is greater for a reduced number of bits.",2005,0,
1740,1741,Notice of Retraction<BR>The coordinate transformation method of GPS RTK and their relationship with the position error,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>We must calculate first the coordinate transformation parameters between WGS-84 coordinates system and the national or local coordinates system when we measure with GPS RTK. In order to have better results, a new idea will be put forward about the four-parameter transformation of the small regional area, and its validity is validated in this paper. At the same time, through comparing with the current method, we find that the parameters precision of the new method is slightly better than that of the current method in the same conditions. Finally, according to the new method, we discuss the effect of the common point error on the accuracy of coordinate transformation parameters when we use GPS RTK, and find to have a simple calculation process, a better result, a change at regular on between the common point error and the parameters precision. All this have an instructive role of operating conveniently GPS RTK.",2010,0,
1741,1742,Fault diagnostics in power electronics based brake-by-wire system,"A DC motor based brake-by-wire system is studied for the purpose of fault diagnostics of its power electronic switches. The voltages and currents generated in the switching circuit under normal and faulted conditions are fed to a fuzzy algorithm to detect the particular solid state power switch which is faulty, and the moment of the occurrence of the fault. The algorithm can be easily implemented using an inexpensive processor for on-board diagnostics applications.",2005,0,
1742,1743,Point defects in quartz crystals and their radiation response - a review [quartz resonator applications],A short review of Al-related point deflects and their radiation effects is presented. These defects exhibit spectroscopic signals which are monitored by a variety of experimental techniques. This discussion is useful to prospective researchers in the area of precision quartz resonators for frequency control in aerospace applications. Irradiation of quartz crystals at 77 K before and after irradiation at 300 K coupled with sweeping can be used for estimating the role of various point defects for their contribution in estimating the frequency offsets in quartz crystals in a radiation environment.,2004,0,
1743,1744,A Robust Fault Detection and Isolation Method in Load Frequency Control Loops,"In this paper, a robust sensor and actuator fault detection and isolation (FDI) method based on unknown input observers (UIO) is adopted and applied to the load frequency control loops of interconnected power systems. Changes in load demand are regarded as the unknown disturbances in the system and the designed UIOs and thus the proposed method are robust to these disturbances. Using selected different sets of measured variables in the UIO design, simulations are performed for the dynamical model of a power control system composed of two areas. As we distinguish the cases for successful and unsuccessful sensor and controller FDI, it is shown that the proposed scheme is able to detect and isolate sensor and controller faults for proper selections of measured variables. By using residuals generated by the UIOs, the designed ldquofault detection and isolation logicrdquo system shows the operator which sensor or controller is faulty. Hence the faulty sensor or controller can be replaced by a healthy one for a more reliable operation.",2008,0,
1744,1745,A pipelined architecture for real-time correction of barrel distortion in wide-angle camera images,"An efficient pipelined architecture for the real-time correction of barrel distortion in wide-angle camera images is presented in this paper. The distortion correction model is based on least-squares estimation to correct the nonlinear distortion in images. The model parameters include the expanded/corrected image size, the back-mapping coefficients, distortion center, and corrected center. The coordinate rotation digital computer (CORDIC) based hardware design is suitable for an input image size of 10281028 pixels and is pipelined to operate at a clock frequency of 40 MHz. The VLSI system will facilitate the use of a dedicated hardware that could be mounted along with the camera unit.",2005,0,
1745,1746,Fault Tolerance and Scaling in e-Science Cloud Applications: Observations from the Continuing Development of MODISAzure,"It can be natural to believe that many of the traditional issues of scale have been eliminated or at least greatly reduced via cloud computing. That is, if one can create a seemingly well functioning cloud application that operates correctly on small or moderate-sized problems, then the very nature of cloud programming abstractions means that the same application will run as well on potentially significantly larger problems. In this paper, we present our experiences taking MODISAzure, our satellite data processing system built on the Windows Azure cloud computing platform, from the proof-of-concept stage to a point of being able to run on significantly larger problem sizes (e.g., from national-scale data sizes to global-scale data sizes). To our knowledge, this is the longest-running eScience application on the nascent Windows Azure platform. We found that while many infrastructure-level issues were thankfully masked from us by the cloud infrastructure, it was valuable to design additional redundancy and fault-tolerance capabilities such as transparent idempotent task retry and logging to support debugging of user code encountering unanticipated data issues. Further, we found that using a commercial cloud means anticipating inconsistent performance and black-box behavior of virtualized compute instances, as well as leveraging changing platform capabilities over time. We believe that the experiences presented in this paper can help future eScience cloud application developers on Windows Azure and other commercial cloud providers.",2010,0,
1746,1747,FE-based equivalent Circuits for simulating transformer internal Faults,"A simple and efficient model is proposed to detect transformer internal faults. The winding's structure is preciously simulated for FEM. It is easy to be used for a transformer with internal turn-ground and turn-turn fault. Energy perturbation method is employed to calculate the equivalent circuit parameters of the transformer. With these parameters, state equations can be established to study the relationship among the terminal currents, flux and the fault locations. Through test of a transformer, it reveals that the method is reasonable and effective",2006,0,
1747,1748,Built-In Soft Error Resilience for Robust System Design,"Built-in soft error resilience (BISER) is an architecture-aware circuit design technique for correcting soft errors in latches, flip-flops and combinational logic. BISER enables more than an order of magnitude reduction in chip-level soft error rate with minimal area impact, 6-10% chip-level power impact, and 1-5% performance impact (depending on whether combinational logic error correction is implemented or not). In comparison, several traditional error-detection techniques introduce 40-100% power, performance and area penalties, and require significant efforts for designing and validating corresponding recovery mechanisms. In addition, BISER enables system design with configurable soft error protection features. Such features are extremely important for future designs targeting applications with a wide range of power, performance and reliability constraints. Design trade-offs associated with BISER and other existing soft error protection techniques are also analyzed.",2007,0,
1748,1749,Fault diagnosis of frequency convert system based on networked virtual instrument,"To obtain sufficient fault information and extract important fault characteristics are the key to fault diagnosis. For the core of control unit, some important fault characteristics must be obtained from users' on-site environment. However, at present, there are some problems in the design of virtual instrument which is on-site application-oriented. For example, the on-site resources often can not meet the requirements of fault diagnosis; sufficient knowledge in the application field and design ability of software and hardware must be obtained by the software designers. Based on the fault diagnosis of frequency convert system, this paper put forward a design method of networked virtual instrument which is on-site application-oriented. Remote resource is called by Internet technology, and networked virtual instrument is constructed in the case of insufficient on-site resource. Requirement design, resource calling and data management are relatively independent of flow and flow assembling, resource and resource matching, data dictionary and data image. Design process of networked virtual instrument is simplified.",2009,0,
1749,1750,A Simple Method to Measure the Image Complexity on a Fault Tolerant Cluster Computing,"The cluster computing is widely used for image processing in entertainment applications. Measuring the required number of nodes to be used in a cluster computing helps saving nodes processing time, even if this occurs on a fault tolerant scenario. The paper analyzes the images rendering process by ray tracing on the cluster computing using the freeware software fault tolerant message-passing interface with Povray software in a Linux-based operating system. This paper uses a simple method of image complexity measure; it is shown that depending on the images complexity level, a minimum number of cluster nodes is chose to render the image instead of using all the cluster nodes. It is useful to save machine processing time and meanwhile another image can be rendered in a parallel process with a previous one and where the solution is applied in a Fault Tolerant scenario, so independent of the network fault, the cluster continues working.",2010,0,
1750,1751,Fault-tolerant distributed mass storage for LHC computing,"In this paper we present the concept and first prototyping results of a modular fault-tolerant distributed mass storage architecture for large Linux PC clusters as they are deployed by the upcoming particle physics experiments. The device masquerading technique using an Enhanced Network Block Device (ENBD) enables local RAID over remote disks as the key concept of the ClusterRAID system. The block level interface to remote files, partitions or disks provided by the ENBD makes it possible to use the standard Linux software RAID to add fault-tolerance to the system. Preliminary performance measurements indicate that the latency is comparable to a local hard drive. With four disks throughput rates of up to 55MB/s were achieved with first prototypes for a RAIDO setup, and about 40M/s for a RAID5 setup.",2003,0,
1751,1752,Cleanroom: Edit-Time Error Detection with the Uniqueness Heuristic,"Many dynamic programming language features, such as implicit declaration, reflection, and code generation, make it difficult to verify the existence of identifiers through standard program analysis. We present an alternative verification, which, rather than analyzing the semantics of code, highlights any name or pair of names that appear only once across a program's source files. This uniqueness heuristic is implemented for HTML, CSS, and JavaScript, in an interactive editor called Cleanroom, which highlights lone identifiers after each keystroke. Through an online experiment, we show that Cleanroom detects real errors, that it helps developers find these errors more quickly than developers can find them on their own, and that this helps developers avoid costly debugging effort by reducing how many times a program is executed with potential errors. The simplicity and power of Cleanroom's heuristic may generalize well to other dynamic languages with little support for edit-time name verification.",2010,0,
1752,1753,Error control schemes for on-chip communication links: the energy-reliability tradeoff,"On-chip interconnection networks for future systems on chip (SoC) will have to deal with the increasing sensitivity of global wires to noise sources such as crosstalk or power supply noise. Hence, transient delay and logic faults are likely to reduce the reliability of across-chip communication. Given the reduced power budgets for SoCs, in this paper, we develop solutions for combined energy minimization and communication reliability control. Redundant bus coding is proved to be an effective technique for trading off energy against reliability, so that the most efficient scheme can be selected to meet predefined reliability requirements in a low signal-to-noise ratio regime. We model on-chip interconnects as noisy channels and evaluate the impact of two error recovery schemes on energy efficiency: correction at the receiver stage versus retransmission of corrupted data. The analysis is performed in a realistic SoC setting, and holds both for shared communication resources and for peer-to-peer links in a network of interconnects. We provide SoC designers with guidelines for the selection of energy efficient error-control schemes for communication architectures.",2005,0,
1753,1754,An approach for controller fault detection,"Monitoring and maintaining of control software becomes more and more important and difficult with the increase of control software in size and complexity. In this paper, an approach for control software fault detection is proposed, which is based on the monitoring of the discrepancies between the control outputs of the actual controller and the benchmark controller, a Linear Quadratic Gaussian (LQG) controller. The discrepancies are assumed to be Gaussian distribution with a stable mean under the normal situation. Faults in the actual controller are characterized by sudden jumps in the mean of the discrepancies. The fault detection is transferred into a jump point identification problem. A detector based on Generalized Likelihood Ratio (GLR) test is employed for the jump point identification. The proposed approach is applicable to general control software even it is only illustrated through a water heater case study with a simple PID controller.",2004,0,
1754,1755,"Symbol Error Probability of Rectangular QAM in MRC Systems With Correlated <formula formulatype=""inline""> <img src=""/images/tex/606.gif"" alt=""\eta""> </formula><formula formulatype=""inline""> <img src=""/images/tex/241.gif"" alt=""\mu""> </formula> Fading Channels","In the context of -  generalized fading scenarios, this paper provides a general closed-form expression for the average symbol error probability (SEP) of arbitrary <i>M</i>-ary quadrature amplitude modulation (QAM) constellations in maximal-ratio combining (MRC) schemes over non-identical correlated channels. For such a scenario, the moment-generating function (MGF) of the signal-to-noise ratio (SNR) at the combiner output is obtained by rearranging the Gaussian components used to model the correlation between the diversity branches. The average SEP is then derived in terms of Lauricella multivariate hypergeometric functions, which can be implemented in most popular numerical softwares, such as Mathematica. The proposed analytical expressions are validated through Monte Carlo simulations, and insightful discussions are provided from the numerical results.",2010,0,
1755,1756,Fault Ride Through operation of a DFIG wind farm connected through VSC HVDC,The electromechanical transients during a deloading of a DFIG turbine and the Fault Ride Through (FRT) capability of a DFIG wind farm connected through HVDC transmission lines are discussed. The electromechanical oscillations during a deloading operation of a DFIG wind turbine generator are simulated using BLADED software. Then power reduction control during a fault was achieved by reducing the power from the wind farm as a whole and by deloading the individual wind generator. A new power blocking technique applied at the offshore converter station was used to reduce the wind farm power output. Simultaneous control of the wind farm and wind turbine power outputs enabled a smooth power reduction during the fault.,2010,0,
1756,1757,XML Schema Based Faultset Definition to Improve Faults Injection Tools Interoperability,"This paper describes an XML schema formalization approach for the definition of basic fault sets which specify memory and/or register value corruption in microprocessor-based systems. SWIFI (software implemented fault injection) tools use fault injectors to carry out the fault injection campaign defined in a GUI-based application. However, the communication between the fault injector and the application is defined in an ad-hoc manner. Through this proposed XML schema definition different injectors could be used to carry out the same fault set injection. To validate this approach floating point register and memory corruptions with temporal triggers and routine interception mechanisms to carry out argument and return value, corruption has been considered. Moreover, an experimental tool called Exhaustifreg, consisting of a GUI Java application for defining the fault sets and injection policies and two injectors for SPARC and i386 architectures under RTEMS, has been developed. The XML-based approach improves the interoperability between SWIFI tools by uncoupling the injectors from the experiment manager in charge of the fault campaign.",2008,0,
1757,1758,Crosstalk Fault Detection for Interconnection Lines Based on Path Delay Inertia Principle,"The crosstalk fault becomes more and more important in the deep submicron SoC and its detection involves sophisticated timing measurement. In this paper, a new test scheme to detect the crosstalk fault, based on the path delay inertia, for interconnection lines in SoC is proposed. The scheme, without using timing measurement, applies a transition on the aggressor line and a critical width pulse, CWP, to the victim line and detects the propagation of the CWP at the output of the victim line. The scheme is simple and simulation analysis and experiments show that it is effective in detecting crosstalk faults",2005,0,
1758,1759,A new electromagnetic transient simulation method for faults in complex power system,"The study of the digital simulation of electromagnetic transients has been an everlasting issue in power systems. Especially, after improvement by Dommel, the time-domain Bergeron model has been successfully applied in EMTP. But the pre-processing before simulating transients caused by various faults and operations are quite troublesome. This paper presents a new electromagnetic transients simulation method which can be generally used to calculate most of the electric faults in power systems. Unlike EMTP, the presented method doesn't need to calculate the initial values of the system again when the structure or parameter of the system changes. Meanwhile, this method can simulate faults on random locations in single- and double-circuit lines including series compensated lines, bus faults, operation of breakers and open-circuit faults. In addition, the fault-start-angle can be set in the method. Because the new method can conveniently simulate not only inrush currents, overvoltages and harmonics components at different fault locations, but also developing faults, it is significant in line protection simulation verification and overvoltage computation. Lots of tests show that the method is both accurate and fast.",2002,0,
1759,1760,IP datacasting and channel error handling with DVB-H,"DVB-H is a terrestrial digital TV standard which consumes less power and allow user to move freely when receiving signals. Its deployment also signifies the convergence between broadcast network and data networks as both video signals and other data programs are transmitted in a shared media. In this paper we discuss the channel error problems under different scenarios of this convergence. Partial solutions, such as enhanced forward error correction were provided by the standard but other aspects still need further exploration. We present some early results and discuss possible directions in the future.",2005,0,
1760,1761,Comparison of Voltage and Flux Modulation Schemes of StatComs Regarding Transformer Saturation During Fault Recovery,"A transformer might be driven into saturation by faults in the connected system. In a transmission system with a static synchronous compensator and transformers connected at the point of common coupling, different modulation schemes utilized by the voltage source converter influence the transformer saturation in different ways. This paper compares the saturation effect during fault recovery when two alternative modulation schemes are utilized: voltage modulation and flux modulation. The comparison shows that utilization of flux modulation scheme tends to soften the saturation problem during fault recovery. However, this is achieved at a higher converter transient peak current.",2008,0,
1761,1762,Distributed fault diagnostics for tactical networks,"We present a design and an evaluation of a distributed fault diagnostic system (FDS) that copes with changing wireless network topology, complexity and size of fault propagation patterns, constrained bandwidth, and limited computing power of the mobile devices. The presented FDS consists of several components: run-time synthesis algorithm to generate network-wide fault dependency model (FPM), scalable Bayesian inference algorithms, and novel techniques for optimally distributing inference to ensure the scalability of our approach. We describe three algorithms for distributing inference, each of them using different technique for maximizing the fault-symptom locality: Fault-based Adaptive algorithm, Topology-based Adaptive algorithm, and Topology-based Probabilistic algorithm. We have evaluated the performance of the proposed approach in a simulated environment using abstract models of a real-life tactical network, and compared it to a centralized approach. We found that our techniques allows for a significant gain in the processing time (30 times improvement for the best performing technique), and exhibit only a minimal reduction (3% percentage points) in the accuracy of the fault diagnostics.",2010,0,
1762,1763,The GPS Contribution to the Error Budget of Surface Elevations Derived From Airborne LIDAR,"When using airborne LIDAR to produce digital elevation models, the global positioning system (GPS) positioning of the LIDAR instrument is often the limiting factor, with accuracies typically quoted as being 10-30 cm. However, a comprehensive analysis of the accuracy and precision of GPS positioning of aircraft over large temporal and spatial scales is lacking from the literature. Here, an assessment is made of the likely GPS contribution to the airborne LIDAR measurement error budget by analyzing more than 500 days of continuous GPS data over a range of baseline lengths (3-960 km) and elevation differences (400-2000 m). Height errors corresponding to the 95th percentile are <0.15 m when using algorithms commonly applied in commercial software over 3-km baselines. These errors increase to 0.25 m at 45 km and <0.5 m at 250 km. At aircraft altitudes, relative heights are shown to be potentially biased by additional errors approaching 0.2 m, partly due to unmodeled tropospheric zenith total delay (ZTD). The application of advanced algorithms, including parameterization of the residual ZTD, gives error budgets that are largely constant despite baseline length and elevation differences. In this case, height errors corresponding to the 95th percentile are <0.22 m out to 960 km, and similar levels are shown for one randomly chosen day over a 2300-km baseline.",2009,0,
1763,1764,Automatic detection of vowel pronunciation errors using multiple information sources,"Frequent pronunciation errors made by L2 learners of Dutch often concern vowel substitutions. To detect such pronunciation errors, ASR-based confidence measures (CMs) are generally used. In the current paper we compare and combine confidence measures with MFCCs and phonetic features. The results show that the best results are obtained by using MFCCs, then CMs, and finally phonetic features, and that substantial improvements can be obtained by combining different features.",2009,0,
1764,1765,"Background Calibration of Gain Errors in <formula formulatype=""inline""> <img src=""/images/tex/19188.gif"" alt=""\Pi \Delta \Sigma ""> </formula> A/D Converters","This paper describes a method of calibrating gain errors in each channel of Hadamard modulated parallel delta-sigma () converters. The  converters extend traditionally bandwidth-limited  modulators to applications such as wireless communications that require high-resolution wide-bandwidth converters. The focus of this paper is on the development and convergence of an adaptive calibration algorithm. A hardware implementation that verifies this paper can be found in an earlier work. The calibration is done in real time and is accomplished by adding an additional channel that is linearly dependent on the  channels that are used. By introducing this redundancy, an adaptive recursive least squares (RLS) algorithm is used to correct for gain errors within the channels. To demonstrate the calibration scheme, an eight-channel  converter with second-order  modulators and an oversampling ratio of 4 was simulated in MATLAB. Simulation results show a 40-dB reduction in unwanted distortion tones caused by 1% gain mismatches.",2010,0,
1765,1766,Design and Analysis of Synchronizable Error-Resilient Arithmetic Codes,"An error-resilient variable-length arithmetic code is presented whose codewords are represented by binary digits. The input sequence is partitioned in subsequences, each of which is individually encoded using an arithmetic coding scheme with an integrated bit-stuffing technique that restricts the number of consecutive ones in the output sequence. An all-ones sequence of fixed length is appended to serve as a sync marker when the codewords are concatenated. The bit-stuffing technique ensures that the sync markers do not occur anywhere except at the boundaries between the codewords. Expressions for the optimal choice of the marker length and the block length are derived. The performance of the proposed code is determined in terms of redundancy and error resilience. An upper bound on the average error rate is derived and its tightness is confirmed with computer simulations. The proposed code shows to significantly suppress the error rate at the expense of a minimum increase in redundancy.",2009,0,
1766,1767,Error detection by selective procedure call duplication for low energy consumption,"As commercial off-the-shelf (COTS) components are used in system-on-chip (SoC) design technique that is widely used from cellular phones to personal computers, it is difficult to modify hardware design to implement hardware fault-tolerant techniques and improve system reliability. Two major concerns of this paper are to: (a) improve system reliability by detecting transient errors in hardware, and (b) reduce energy consumption by minimizing error-detection overhead. The objective of this new technique, selective procedure call duplication (SPCD) is to keep the system fault-secured (preserve data integrity) in the presence of transient errors, with minimum additional energy consumption. The basic approach is to duplicate computations and then to compare their results to detect errors. There are 3 choices for duplicate computation: (1) duplicating every statement in the program and comparing results, (2) re-executing procedures through duplicated procedure calls, and comparing results, and (3) re-executing the whole program, and comparing the final results. SPDC combines choices (1) and(2). For a given program, SPCD analyzes procedure-call behavior of the program, and then determines which procedures can have duplicated statements [choice(1)] and which procedure calls can be duplicated [choice (2)] to minimize energy consumption with reasonable error-detection latency. Then, SPCD transforms the original program into a new program that can detect errors with minimum additional energy consumption by re-executing the statements or procedures. SPCD was simulated with benchmark programs; it requires less than 25% additional energy for error detection than previous techniques that do not consider energy consumption.",2002,0,
1767,1768,Electrical approach to defect depth estimation by stepped infrared thermography,"An electrical modelling and analysis is presented for thermal nondestructive evaluation of materials by stepped infrared (IR) thermography. A one-dimensional (1D) electrical analysis based on the Laplace transform technique of network-analysis and time delay in an RC ladder network is given for defect depth estimation. Defect depth is evaluated based on the time instants at which surface temperature evolution over the defect and nondefect regions of the material deviates from its constant initial slope, corresponding to the response of a semi-infinite material under similar conditions of step heating. The method is applicable even for three-dimensional geometries of materials having unknown thermal properties and variable surface emissivity. Experimental and simulated results validate the proposed method and give good estimation of defect depth, even under noisy conditions for a thermally anisotropic material and a nonflat bottom type of defect.",2004,0,
1768,1769,Patching Processor Design Errors with Programmable Hardware,"Equipping processors with programmable hardware to patch design errors lets manufacturers release regular hardware patches, avoiding costly chip recalls and potentially speeding time to market. For each error detected, the manufacturer creates a fingerprint, which the customer uses to program the hardware. The hardware watches for error conditions; when they arise, it takes action to avoid the error. Overall, our scheme enables an exciting new environment where hardware design errors can be handled as easily as system software bugs, by applying a patch to the hardware",2007,0,
1769,1770,An Interaction-Pattern-Based Approach to Prevent Performance Degradation of Fault Detection in Service Robot Software,"In component-based robot software, it is crucial to monitor software faults and deal with them on time before they lead to critical failures. The main causes of software failures include limited resources, component-interoperation mismatches, and internal errors of components. Message-sniffing is one of the popular methods to monitor black-box components and handle these types of faults during runtime. However, this method normally causes some performance problems of the target software system because the fault monitoring and detection process consumes a significant amount of resources of the target system. There are three types of overheads that cause the performance degradation problems: frequent monitoring, transmission of a large amount of monitoring-data, and the processing time for fault analysis. In this paper, we propose an interaction-pattern-based approach to reduce the performance degradation caused by fault monitoring and detection in component-based service robot software. The core idea of this approach is to minimize the number of messages to monitor and analyze in detecting faults. Message exchanges are formalized as interaction patterns which are commonly observed in robot software. In addition, important messages that need to be monitored are identified in each of the interaction patterns. An automatic interaction pattern-identification method is also developed. To prove the effectiveness of our approach, we have conducted a performance simulation. We are also currently applying our approach to silver-care robot systems.",2010,0,
1770,1771,Integrating Fault Recovery and Quality of Security in Real-Time Systems,"In the past five years, mandatory security requirements and fault tolerance have become critical criteria for most real-time systems. Although many conventional fault-tolerant or security approaches were investigated and applied to real-time systems, most existing schemes only addressed either security demands ignoring the fault-tolerant requirements or vice versa. To bridge this technology gap in real-time systems, in this paper we propose a way of integrating fault recovery and confidentiality services. The novel integration of security and fault recovery makes it possible to implement next-generation real-time systems with high reliability and quality of security. Experimental results from real-world applications show that our approach can significantly improve security over the conventional approaches by up to 661.56% while providing an efficient means of fault tolerance.",2007,0,
1771,1772,A hardware approach to concurrent error detection capability enhancement in COTS processors,"To enhance the error detection capability in COTS (commercial off-the-shelf)-based design of safety-critical systems, a new hardware-based control flow checking (CFC) technique is presented. This technique, control flow checking by execution tracing (CFCET), employs the internal execution tracing features available in COTS processors and an external watchdog processor (WDP) to monitor the addresses of taken branches in a program. This is done without any modification of application programs, therefore, the program overhead is zero. The external hardware overhead is about 3.5% using an Altera Flex 10K30 FPGA. For different workload programs, the execution time overhead and the error detection coverage of the technique vary between 33.3 and 140.8% and between 79.7 and 84.6% respectively. The errors are detected with about zero latency.",2005,0,
1772,1773,Multi resolution analysis for bearing fault diagnosis,A wavelet-based vibration analysis was done for Defense Applications. Vibration measurements were carried out in MSL Engine test bed in order to verify that it meets the specifications at different load conditions. Such measurements are often carried out in connection with troubleshooting in order to determine whether vibration levels are within acceptable limits at given engine speeds and maneuvers. A State-of-the-art portable Vibration Data Recorder is used for data acquisition of real-world signals. This paper is intended to take the reader through the various stages in a signal processing of the vibration data using modern digital technology. Vibration signals are post-analyzed using Wavelet Transform for data mining of the vibration signal observed from accelerometers. Wavelet Transform (WT) techniques are applied to decipher vibration characteristics due to Engine excitation to diagnose the faulty bearing. The Time-Scale analysis by WT achieves a comparable accuracy than Fast Fourier Transform (FFT) while having a lower computational cost with fast predictive capability. The result from wavelet analysis is validated using the LabVIEW software.,2010,0,
1773,1774,On error analysis and distributed phase steering for wireless network coding over fading channels,"Network coding notions promise significant gains in wireless networks' throughput and quality of service. Future systems employing such paradigms are known to be also highly scalable and resilient to node failure and churn rates. We propose a simple framework where a single relay listens to two nodes transmitting simultaneously over the same band in the presence of Nakagami-m fading. For this multiple-access channel (MAC), we derive in closed-form the exact bit error rate of antipodal signaling with maximum-likelihood detection. As the MAC is the bottleneck in error of the overall system, this provides a good performance measure of the aggregate architecture. Using the new error expressions derived, we then propose a simple closed-loop cooperation strategy where via a ternary feedback from the relay node, significant gains in signal to noise ratio at the relay can be achieved. Our novel error analysis method is applicable to a number of other systems such as the vertical Bell labs spacetime (V-BLAST) scheme and synchronous multi-user systems.",2009,0,
1774,1775,Error analysis and experimental tests of CATRASYS (Cassino Tracking System),"CATRASYS (Cassino Tracking System) is a low-cost, easyily operated system for monitoring large displacements together with rotation angles of a suitable end-effector, which can be easily attached to any mechanical system. In this paper we present basic performance of CATRASYS by using an analysis for error evaluation and showing experimental tests that have been carried out at the Laboratory of Robotics and Mechatronics in Cassino with available robots",2000,0,
1775,1776,Evaluation of fault protection methods using ATP and MathCAD,"This paper discusses combining the Alternative Transients Program (ATP) and MathCAD to teach protective relaying and to develop relay algorithms. A power system model is created in ATP with appropriate current and voltage measurements. The simulation output is converted to a COMTRADE format and imported into a detailed relay model implemented in MathCAD. The MathCAD model performs digital filtering calculations, symmetrical components calculations and models relay algorithms based on relay manufacturers published information. Focus here is on differential and ground fault protection for the common two bus, parallel line case. Emphasis is placed on fault detection and localization methods for ungrounded or high impedance grounded systems.",2008,0,
1776,1777,A fast algorithm to reduce 2-dimensional assignment problems to 1-dimensional assignment problems for FPGA-based fault simulation,"In this paper a polynomial time heuristic algorithm developed for the assignment optimization problem is introduced, which leads to an improved usage of field programmable gate array (FPGA) resources for hardware-based fault injection using an FPGA-based logic emulator. Logic emulation represents a new method of design validation utilizing a reprogrammable prototype of a digital circuit. In the past years various approaches to hardware-based fault injection using a hardware logic emulator have been presented. Some approaches insert additional functions at the fault location, while others utilize the reconfigurability of FPGAs. A common feature of each of these methods is the execution of hardware-based fault simulation using the stuck-at fault model at gate level.",2003,0,
1777,1778,Built-in sequential fault self-testing of array multipliers,"Microprocessor datapath architectures operate on signed numbers usually represented in two's-complement or sign-magnitude formats. The multiplication operation is performed by optimized array multipliers of various architectures which are often produced by automatic module generators. Array multipliers have either a standard, nonrecoded signed (or unsigned) architecture or a recoded (modified Booth's algorithm) architecture. High-quality testing of array multipliers based on a comprehensive sequential fault model and not affecting their well-optimized structure has not been proposed in the past. In this paper, we present a built-in self-testing (BIST) architecture for signed and unsigned array multipliers with respect to a comprehensive sequential fault model. The BIST architecture does not alter the well-optimized multiplier structure. The proposed test sets can be applied externally but their regular nature makes them very suitable for embedded, self-test application by simple specialized hardware which imposes small overheads. Two different implementations of the BIST architecture are proposed. The first implementation focuses on the test invalidation problem and targets robust sequential fault testing, while the second one focuses on test cost reduction (test time and hardware overhead).",2005,0,
1778,1779,Multiagent technology for fault tolerance and flexible control,"One of the main characteristics of multiagent systems (MAS) is fault tolerance. When an agent is unavailable for some reason, another agent with similar capabilities can theoretically compensate for this loss. The system can be designed not only for this kind of fault tolerance but also for others. Many key aspects of fault tolerance in MAS are described in this correspondence including social knowledge and physical distribution. We present a MAS framework that has been designed for control applications, in which fault tolerance and flexibility are key parts of the system",2006,0,
1779,1780,The Effects of Traffic Patterns on Power Consumption of Torus-Connected NoCs with Faults,"High performance, reliability, transient and permanent fault-tolerance, and low energy consumption are major objectives of Networks-on-Chip (NoCs). Since,different applications impose various communication requirements in NoCs, a number of research studies have revealed that the performance advantages of routing schemes are more noticeable on power consumption under different traffic patterns. However,the power consumption issues of NoCs have not been thoroughly investigated in the presence of faulty regions. To the best of our knowledge, this research is the first attempt to examine the effects of most popular traffic patterns (i.e., Uniform, Local, and Hot-Spot) on power consumption of NoCs in the presence of permanent faults.",2009,0,
1780,1781,The use of steganography to enhance error detection and correction in MPEG-2 video,"The transmission of data is always subject to corruption due to errors; however, video transmission, because of its real time nature, must often deal with these errors without retransmission of the corrupted data. Our MPEG-2 compliant codec uses data hiding principles to transmit parity checking information for the DCT coefficients and uses side information (as provided in the MPEG-2 standard) to enhance the recovery of lost differentially encoded values. This information allows for improved resynchronization by correctly resynchronizing at least 7 times as often, reducing the number of macroblocks in error by a factor of 2 and improving the PSNR of error regions by at least 8 dB (in I frames) while uncorrupted picture quality decreases by less than 0.5 dB PSNR. Our work also demonstrates the ability to recover lost differential motion vectors by transmitting final values.",2002,0,
1781,1782,Considering the Dependency of Fault Detection and Correction in Software Reliability Modeling,"Most existing software reliability growth models (SRGMs) focused on the fault detection process, while the fault correction process was ignored by assuming that the detected faults can be removed immediately and perfectly. However, these assumptions are not realistic. The fault correction process is a critical part in software testing. In this paper, we studied the dependency of the fault detection and correction processes in view of the number of faults. The ratio of corrected fault number to detected fault number is used to describe the dependency of the two processes, which appears S-shaped. Therefore, we adopt the logistical function to represent the ratio function. Based on this function, both fault correction and detection processes are modeled. The proposed models are evaluated by a data set of software testing. The experimental results show that the new models fit the data set of fault detection and correction processes very well.",2008,0,
1782,1783,Design Fault Directed Test Generation for Microprocessor Validation,"Functional validation of modern microprocessors is an important and complex problem. One of the problems in functional validation is the generation of test cases that has higher potential to find faults in the design. We propose a model based test generation framework that generates tests for design fault classes inspired from software validation. There are two main contributions in this paper. Firstly, we propose a microprocessor modeling and test generation framework that generates test suites to satisfy modified condition decision coverage (MCDC), a structural coverage metric that detects most of the classified design faults as well as the remaining faults not covered by MCDC. Secondly, we show that there exists good correlation between types of design faults proposed by software validation and the errors/bugs reported in case studies on microprocessor validation. We demonstrate the framework by modeling and generating tests for the microarchitecture of VESPA, a 32-bit microprocessor. In the results section, we show that the tests generated using our framework's coverage directed approach detects the fault classes with 100% coverage, when compared to model-random test generation",2007,0,
1783,1784,The Sliced Gaussian Mixture Filter with adaptive state decomposition depending on linearization error,"In this paper, a novel nonlinear/nonlinear model decomposition for the Sliced Gaussian Mixture Filter is presented. Based on the level of nonlin-earity of the model, the overall estimation problem is decomposed into a ""severely"" nonlinear and a ""slightly"" nonlinear part, which are processed by different estimation techniques. To further improve the efficiency of the estimator, an adaptive state decomposition algorithm is introduced that allows decomposition according to the linearization error for nonlinear system and measurement models. Simulations show that this approach has orders of magnitude less complexity compared to other state of the art estimators, while maintaining comparable estimation errors.",2010,0,
1784,1785,Optimistic Replication Approach for Transactional Mobile Agent Fault Tolerance,"The mobile agent is a computer program that can move between different hosts in heterogeneous networks. This paradigm is advantageous for distributed systems implementation, especially in mobile computing application characterized by low bandwidth, high latency and unreliable networks connections. Mobile agent is also attractive for distributed transactions applications. Although mobile agent has been studied for twenty years for some good reasons, it is not largely used in developing distributed systems for simple reasons: important issues like security and fault tolerance are not solved in effective way. In this paper we address the issue of fault tolerance in mobile agent systems and transactional support. We present the agent system design and describe the protocol of our approach in which we treat infrastructure failures to prevent a partial or complete loss of mobile agent and deal with semantic failures to ensure atomic execution and transactional support for mobile agent.",2010,0,
1785,1786,Suitable graphical user interface selection based on human errors using analytic hierarchy process,"In this paper, we propose a new model for the design method of graphical user interfaces for audio visual remote controllers based on analytic hierarchy processes. The goal of this model is to reduce the human error in order to modify the graphical user interface of a wireless remote controller to obtain the most suitable interface for every user. This paper proposes a new model with the seven evaluation criteria; inherent ability, lack of skill, lack of knowledge, slip, lapse, mistake and violation. As alternatives, we decided on four design strategies for the user interface; vision assistance, cognition assistance, operation assistance, and memorizing. The proposed method is evaluated by a prototype assuming a real-time OS on an embedded microprocessor. Furthermore, we confirmed this proposal as effective.",2010,0,
1786,1787,Experimental validation of fault detection and fault tolerance mechanisms,The paper deals with the problem of validating the effectiveness of hardware and software mechanisms decreasing system susceptibility to hardware faults. The validation process is based on the use of software implemented fault injector (FITS). The performed analysis concentrates on tuning the profile of faults and experiment set-ups. The presented simulation results are explained in context of the considered applications.,2002,0,
1787,1788,Fault table generation using Graphics Processing Units,"In this paper, we explore the implementation of fault table generation on a Graphics Processing Unit (GPU). A fault table is essential for fault diagnosis and fault detection in VLSI testing and debug. Generating a fault table requires extensive fault simulation, with no fault dropping, and is extremely expensive from a computational standpoint. Fault simulation is inherently parallelizable, and the large number of threads that a GPU can operate on in parallel can be employed to accelerate fault simulation, and thereby accelerate fault table generation. Our approach, called GFTABLE, employs a pattern parallel approach which utilizes both bit-parallelism and thread-level parallelism. Our implementation is a significantly modified version of FSIM, which is pattern parallel fault simulation approach for single core processors. Like FSIM, GFTABLE utilizes critical path tracing and the dominator concept to reduce runtime. Further modifications to FSIM allow us to maximally harness the GPU's huge memory bandwidth and high computational power. Our approach does not store the circuit (or any part of the circuit) on the GPU. Efficient parallel reduction operations are implemented in our implementation of GFTABLE. We compare our performance to FSIM*, which is FSIM modified to generate a fault table on a single core processor. Our experiments indicate that GFTABLE, implemented on a single NVIDIA GeForce GTX 280 GPU card, can generate a fault table for 0.5 million test patterns on average 7.85x faster when compared with FSIM*. With the NVIDIA Tesla server, our approach would be potentially 34.82x faster.",2009,0,
1788,1789,An architecture for physical injection of complex fault scenarios in CAN networks,"It has been reported that some particular fault scenarios may cause malfunction of the controller area network protocol. Although such scenarios are very unlikely, they become relevant when attempting to use the CAN protocol for critical applications. The fault injector described in this paper induces these fault scenarios at the physical layer of the CAN protocol by means of a software tool and a set of specifically designed circuits. Therefore, and in contrast to previous solutions, this fault injector is suitable to evaluate most of the dependability mechanisms that have been proposed for CAN networks.",2003,0,
1789,1790,Scalable Fault-Tolerant Distributed Shared Memory,"This paper shows how a state-of-the-art software distributed shared-memory (DSM) protocol can be efficiently extended to tolerate single-node failures. In particular, we extend a home-based lazy release consistency (HLRC) DSM system with independent check- pointing and logging to volatile memory, targeting shared-memory computing on very large LAN-based clusters. In these environments, where global coordination may be expensive, independent checkpointing becomes critical to scalability. However, independent checkpointing is only practical if we can control the size of the log and checkpoints in the absence of global coordination. In this paper we describe the design of our fault-tolerant DSM system and present our solutions to the problems of checkpoint and log management. We also present experimental results showing that our fault tolerance support is light-weight, adding only low messaging, logging and checkpointing overheads, and that our management algorithms can be expected to effectively bound the size of the checkpoints and logs or real applications.",2000,0,
1790,1791,MTF measurement and a phantom study for scatter correction in CBCT using primary modulation,"Recently, we proposed a scatter correction methods for X-ray imaging using primary modulation. A primary modulator with spatially variant attenuating materials is inserted between the X-ray source and the object to make the scatter and part of the primary distributions strongly separated in the Fourier domain. Linear filtering and demodulation techniques suffice to extract and correct the scatter for this modified system. The method has been verified by computer simulations and preliminary experimental results. In this work, we look into a hybrid method using both primary modulation and an anti-scatter grid. The reconstructed image resolution using the proposed approach is evaluated by MTF measurements, and the scatter correction performance is also investigated by experiments on a human chest phantom. The results using the proposed hybrid method are compared with those using an antiscatter grid only. Experiments with scatter inherently suppressed using a narrowly opened collimator and an anti-scatter grid (a slot-scan geometry) were also carried out. The comparison shows that the filtering in the proposed algorithm does not impair the image resolution, and the primary modulation method can effectively suppress the scatter artifacts. In the central region of interest, the reconstruction error relative to the image obtained using a slot-scan geometry is reduced from 18.10% to 0.48% for the human chest phantom, if the primary modulation method is used.",2006,0,
1791,1792,Fault Tolerant Planning for Critical Robots,"Autonomous robots offer alluring perspectives in numerous application domains: space rovers, satellites, medical assistants, tour guides, etc. However, a severe lack of trust in their dependability greatly reduces their possible usage. In particular, autonomous systems make extensive use of decisional mechanisms that are able to take complex and adaptative decisions, but are very hard to validate. This paper proposes a fault tolerance approach for decisional planning components, which are almost mandatory in complex autonomous systems. The proposed mechanisms focus on development faults in planning models and heuristics, through the use of diversification. The paper presents an implementation of these mechanisms on an existing autonomous robot architecture, and evaluates their impact on performance and reliability through the use of fault injection.",2007,0,
1792,1793,Content adaptive intra error concealment method,"In order to restore the information loss in video communication, a content adaptive intra error concealment method is proposed in this paper. Different images and different packet loss rates of the video sequences are used in the experiment. The experimental results show that the improved method has the better subjective quality and the higher PSNR than the bilinear interpolation and the semi-adaptive error concealment method.",2010,0,
1793,1794,"FixD : Fault Detection, Bug Reporting, and Recoverability for Distributed Applications","Model checking, logging, debugging, and checkpointing/recovery are great tools to identify bugs in small sequential programs. The direct application of these techniques to the domain of distributed applications, however, has been less effective (mostly owing to the high degree of concurrency in this context). This paper presents the design of a hybrid tool, FixD, that attempts to address the deficiencies of these tools with respect to their application to distributed systems by using a novel composition of several of these existing techniques. The authors first identify and describe the four abstract components that comprise the FixD tool, then conclude with a proposal for how existing tools can be used to implement these components.",2007,0,
1794,1795,A fault-tolerant MPLS-based control and communication network for the Lucent LambdaRouter,"Recently, the importance of out-of-band signaling has been realized for next-generation intelligent optical transport networks (OTNs). This next-generation OTN will be capable of providing services like real-time point-and-click provisioning of optical channels, automatic protection and restoration, network topology auto-discovery, and bandwidth management. However, for successful deployment and usage of these services, reliability of the out-of-band signaling network, referred to as the control and communication network (CCN), is critical. Specifically, it is desired that the signaling network transparently recover from any single failure events and continue providing communication between OTN nodes. In this paper, we propose a fault-tolerant, scalable, and cost-effective architecture for IP-based control plane of next-generation OTNs. The architecture is based on multiprotocol label switching (MPLS) protocols. The essence is to provide diverse paths in the network so that any failure in the OTN, as well as the control network, can be recovered transparently. These diverse paths employ a packet dual-feed and select mechanism for fast recovery from failures. The architecture requires diverse paths between the neighboring OTN nodes, instead of all-pair of nodes, enabling the control plane to seamlessly scale with the size of the OTN network. Moreover, the architecture allows the control plane topology to be independent of the OTN topology. This gives operators considerable flexibility in the design of the CCN in a cost-effective manner by using virtual private networks (VPNs) through public data networks, or by avoiding unnecessary OTN links. We also present an implementation of the proposed architecture that will be used as the highly reliable IP-based CCN in OTNs consisting of the Lucent LambdaRouter, an optical cross-connect product. Although in this paper the architecture is discussed in the context of providing a highly reliable IP-centric CCN for optical cross- connect-based (OXC-based) OTNs, it can be extended to a general purpose fault-tolerant CCN for any next-generation OTN.3",2002,0,
1795,1796,Application of Bayesian Theory in Fault Diagnosis of Turbo-generators,"Building on the analysis of the features of the sealing oil system faults in turbo-generators this paper mainly discusses how to employ Bayesian theory to perform fault diagnosis by providing mathematical formulae concerning the solution to the fault diagnosis and determining the Bayesian network inference methodology based on the prior information of the samples. It is demonstrated that the application of Bayesian theory, combined with the leaky noisy-OR model which helps to reduce the amount of data required, is conducive to improving the diagnosis speed and efficiency. This paper testifies the validity of this approach and realizes a forecast of the faults at early stages and a rapid diagnosis of their possible causes as well",2005,0,
1796,1797,Modeling transformer internal faults using Matlab,"This paper describes a technique for modeling transformer internal faults using MATLAB. In this technique a model for simulating a two-winding-single-phase transformer is modified to be suitable for simulating an internal fault in one of the windings. The transformer is represented by three-windings in this case; one winding is the healthy winding, while the other two represent the faulty windings. Three differential equations representing these windings are simulated and solved using MATLAB/SIMULINK. Simulation results include inrush magnetizing current and internal fault current. A fast Fourier transform (FFT) is used to analyze the simulated currents, and it shows that the second harmonic component of the internal fault current is not predominant which agrees with transformer theories.",2002,0,
1797,1798,"Crouching error, hidden markup [Microsoft Word]","Microsoft has such a pervasive commercial presence that people working professionally in computing must expect that the company's behavior will affect their own work and reputation in many ways. It's difficult to ignore Microsoft and only too easy to snipe at the company, but Microsoft's effect on the computing profession, as distinct from its effect on the industry, might be harmful to a degree that would justify severe criticism. Microsoft Word's lack of a versatile and visible markup language can make using the package a nightmare-and reflects poorly on our profession. In addition, one's struggle to get help from Microsoft technical support can often be fruitless and expensive",2001,0,
1798,1799,A Modified BCE Algorithm for Fault-Tolerance Scheduling of Periodic Tasks in Hard Real-Time Systems,"Fault tolerance is an important aspect of real-time control systems, due to unavoidable timing constraints. In this paper, the timing problem of a set of concurrent periodic tasks is considered where each task has primary and alternate versions. In the literature, probability of fault in the alternate version of a task is assumed to be zero. Here, a fault probability with uniform distribution has been used. In addition, to cover the situations in which both versions are scheduled with some time overlapping, a criterion is defined for prioritizing primary version against the alternate version. A new scheduling algorithm is proposed based on the defined criterion. Simulation results show that an increase in the number of executed primary tasks which improves the efficiency of processor utilization, hence prove the efficiency of the proposed algorithm.",2009,0,
1799,1800,Discrete wavelet and neural network for transmission line fault classification,This paper presents an efficient wavelet and neural network (WNN) based algorithm for fault classification in single circuit transmission line. The first level discrete wavelet transform is applied to decompose the post fault current signals of the transmission line into a series of coefficient components (approximation and detail). The values of the approximation coefficients obtained can accurately discriminate between all types of fault in transmission line and reduce the number of data feeding to the ANN. These coefficients are further used to train an Artificial Neural Network (ANN) fitting function. The trained FFNN clearly distinguishes and classify very accurate and very fast the fault type. A typical generation system connected by single circuit transmission line to many nodes of lodes at the receiving end was simulated using MATLAB simulation software using only one node to do this work. The generated data were used by the MATLAB software to test the performance of the proposed technique. The simulation results obtained show that the new algorithm is more reliable and accurate.,2010,0,
1800,1801,Fault tolerance through redundant COTS components for satellite processing applications,"Satellites and other space systems typically employ rare and expensive radiation tolerant, radiation hardened or at least military qualified parts for computational and other subsystems to ensure reliability in the harsh environment of space. In this paper, a parallel architecture is proposed that allows commercial devices to be incorporated into a computational unit with aggregate reliability figures approaching that of space-qualified alternatives. Apart from the common argument of cost, commercial-off-the-shelf (COTS) devices are attractive due to their relatively low power consumption and high processing performance. This paper describes a COTS-based processing architecture for space-borne computers and compares its reliability to a common alternative radiation hardened configuration.",2003,0,
1801,1802,Experimental evaluation of error-detection mechanisms,"Effective error-detection is paramount for building highly dependable computing systems. A new methodology, based on physical and simulated fault injection, has been developed for assessing the effectiveness of error-detection mechanisms. This approach has 2 steps: (1) transient faults are physically injected at the IC pin level of a prototype, in order to derive the error-detection coverage. Experiments are carried out in a 3-dimensional space of events. Fault location, time of occurrence, and duration of the injected fault are the dimensions of this space. (2) Simulated fault-injection is performed to assess the effectiveness of new error-detection mechanisms, designed to improve the detection coverage. Complex circuitry, based on checking for protocol violations, is considered. A temporal model of the protocol checker is used, and transient faults are injected in signal traces captured from the prototype system. These traces are used as inputs of the simulation engine. s-confidence intervals of the error-detection coverage are derived, both for the initial design and the new detection mechanism. Physical fault-injection, carried out on a prototype server, proved that several signals were sensitive to transient faults and error-detection coverage was unacceptably low. Simulated fault injection shows that an error-detection mechanism, based on checking for protocol violations, can appreciably increase the detection coverage, especially for transient faults longer that 200 nanoseconds. Additional research is required for improving the error-detection of shorter transients. Fault injection experiments also show that error-detection coverage is a function of fault duration: the shorter the transient fault, the lower the coverage. As a consequence, injecting faults that have a unique, predefined duration, as it was frequently done in the past, does not provide accurate information on the effectiveness of the error-detection mechanisms. Injecting only permanent faults leads to unrealistically high estimates of the coverage. These experiments prove that combined physical and simulated fault injection, performed in a 3-dimensional space of events, is a superior approach, which allows the designers to accurately assess the efficacy of various candidate error-detecti- on mechanisms without building expensive test circuits.",2003,0,
1802,1803,PLC Control Logic Error Monitoring and Prediction Using Neural Network,"This paper reviews monitoring and error prediction of PLC-program using Neural Network. In the PLC-device controlled manufacturing line, PLC-program holds place of underlying component. It becomes controlling mechanism. The level of automation in the production line relies on control mechanism practiced. In the modern manufacturing, PLC devices can handle whole production line given that structured and smart PLC-program is executed. In other words, PLC-program can manage whole process structure consisting set of procedures. We present a method to monitor PLC-program and PLC error prediction it using neural network. The neural network method being predictive in nature, it rigorously can monitor process signals from sensors, sensed during operation of PLC devices or execution of PLC-program. Subsequently, a neural network algorithm practiced for the analysis of signals. In this way, thorough monitoring of PLC-program can find possible errors from temporal parameters (e.g. Voltage, bias etc). In addition, possible alterations in program and irregularities can be minimized. That can result, easily to use in fault detection, maintenance, and decision support in manufacturing organization. Similarly, it can lessen down-time of machines and prevent possible risks.",2008,0,
1803,1804,Compact Bandpass Ring Resonator Filter With Enhanced Wide-Band Rejection Characteristics Using Defected Ground Structures,"This letter addresses a modified bandpass ring resonator filter providing compact size, low insertion-loss, wide bandwidth, sharp rejection, and suppressed higher order modes. It is demonstrated that the use of internal folded stubs translates into an overall size reduction of more than 70% through the exploitation of the internal ring area. The introduction of defected ground structures enhances the rejection of higher order modes beside offering a further advantage in terms of size reduction. It is furthermore shown that electromagnetic simulations, transmission line model, as well as measurement results are in very good agreement.",2008,0,
1804,1805,Optimal index-bit allocation for dynamic post-correction of analog-to-digital converters,"Signal processing methods for digital post-correction of analog-to-digital converters (ADCs) are considered. ADC errors are in general signal dependent, and in addition, they often exhibit dynamic dependence. A novel dynamic post-correction scheme based on look-up tables is proposed. In order to reduce the table size and, thus, the hardware requirements, bit-masking is introduced. This is to limit the length of the table index by deselecting index bits. At this point, the problem of which bits to use arises. A mathematical analysis tool is derived, enabling the allocation of index bits to be analyzed. This analysis tool is applied in two optimization problems, optimizing the total harmonic distortion and the signal-to-noise and distortion ratio, respectively, of a corrected ADC. The correction scheme and the optimization problems are illustrated and exemplified using experimental ADC data. The results show that the proposed correction scheme improves the performance of the ADC. They also indicate that the allocation of index bits has a significant impact on the ADC performance, motivating the analysis tool. Finally, the optimization results show that performance improvements compared with static look-up table correction can be achieved, even at a comparable table size.",2005,0,
1805,1806,A neural-network approach to recognize defect spatial pattern in semiconductor fabrication,"Yield enhancement in semiconductor fabrication is important. Even though IC yield loss may be attributed to many problems, the existence of defects on the wafer is one of the main causes. When the defects on the wafer form spatial patterns, it is usually a clue for the identification of equipment problems or process variations. This research intends to develop an intelligent system, which will recognize defect spatial patterns to aid in the diagnosis of failure causes. The neural-network architecture named adaptive resonance theory network 1 (ART1) was adopted for this purpose. Actual data obtained from a semiconductor manufacturing company in Taiwan were used in experiments with the proposed system. Comparison between ART1 and another unsupervised neural network, self-organizing map (SOM), was also conducted. The results show that ART1 architecture can recognize the similar defect spatial patterns more easily and correctly",2000,0,
1806,1807,Understanding the sources of software defects: a filtering approach,"The paper presents a method proposal of how to use product measures and defect data to enable understanding and identification of design and programming constructs that contribute more than expected to the defect statistics. The paper describes a method that can be used to identify the most defect-prone design and programming constructs and the method proposal is illustrated on data collected from a large software project in the telecommunication domain. The example indicates that it is feasible, based on defect data and product measures, to identify the main sources of defects in terms of design and programming constructs. Potential actions to be taken include less usage of particular design and programming constructs, additional resources for verification of the constructs and further education into how to use the constructs",2000,0,
1807,1808,Fault localization in medium voltage networks with compensated and isolated star-point grounding,"This paper presents the experimental investigation of a fault localization algorithm in a network with compensated or isolated star-point grounding. The paper first discusses the theoretical background of the method and then the conclusions drawn from the discussion were tested using a complex network structure (NETOMAC) with the support of models developed in MATLAB. Finally, the complete method is proposed and its usability confirmed.",2010,0,
1808,1809,A Service-Oriented Fault-Tolerant Environment for Telecom Operation Support Systems,"The complexity that telecommunications companies are faced with in their Operations Support Systems (OSSs) is especially apparent in their billing systems. These systems are required to handle large volumes of data located on different legacy systems operating on heterogeneous platforms. These systems are often integrated using service oriented architecture (SOA). The successful completion of billing processes is essential to the operation of a telecommunications company, as it ensure the smooth inflow of income from services rendered. When a billing process fails it is the generally the result of one of two types of faults - availability faults or reliability faults. Unless the system has been designed with fault-tolerance in mind, failures constitute an extremely costly and time consuming occurrence. This paper proposes a Service Oriented Fault Tolerant Environment (SOFTEN) that seeks to address this problem directly. The SOFTEN described here is composed of three key modules: a Fault Tolerant Proxy (FT Proxy), Fault Tolerant Services (FT Services), and an Adaptable Service Bus (ASB). The FT Proxy allows developers to define strategies and responses for dealing with the occurrence of both availability and reliability faults. FT Services are fault handling mechanisms that enact the procedures established in the FT Proxy. The ASB provides intelligent routing and data transform between the billing process and the billing services. Further, the paper presents results that demonstrate the effectiveness of the solution, both in increasing reliability and in reducing calculation time for completion of the billing process. An implementation of this system has been operational at the ChungHwa Telecom Company, in Taiwan since May 2008 and provides complete support for its billing application. As a result, the billing process cycle time has been reduced from 10-16 days to 3-4 days, which cleared the way for further growth of the business.",2008,0,
1809,1810,"Aspect-Oriented Programing Techniques to support Distribution, Fault Tolerance, and Load Balancing in the CORBA-LC Component Model","The design and implementation of distributed High Performance Computing (HPC) applications is becoming harder as the scale and number of distributed resources and application is growing. Programming abstractions, libraries and frameworks are needed to better overcome that complexity. Moreover, when Quality of Service (QoS) requirements such as load balancing, efficient resource usage and fault tolerance have to be met, the resulting code is harder to develop, maintain, and reuse, as the code for providing the QoS requirements gets normally mixed with the functionality code. Component Technology, on the other hand, allows a better modularity and reusability of applications and even a better support for the development of distributed applications, as those applications can be partitioned in terms of components installed and running (deployed) in the different hosts participating in the system. Components also have requirements in forms of the aforementioned non-functional aspects. In our approach, the code for ensuring these aspects can be automatically generated based on the requirements stated by components and applications, thus leveraging the component implementer of having to deal with these non-functional aspects. In this paper we present the characteristics and the convenience of the generated code for dealing with load balancing, distribution, and fault-tolerance aspects in the context of CORBA-LC. CORBA-LC is a lightweight distributed reflective component model based on CORBA that imposes a peer network model in which the whole network acts as a repository for managing and assigning the whole set of resources: components, CPU cycles, memory, etc.",2007,0,
1810,1811,Fault Anticipation Software System architecture for aircraft EMA,"This paper shows a Fault Anticipation Software System (FASS) architecture, for diagnosis and prognosis purposes, in order to be implemented in a FASS unit, that nowadays, is a necessary tool to diagnose aircraft Electro Mechanical Actuators which are been implemented in order to face up to ldquoMore Electric Aircraftrdquo (MEA) technologies. This work shows the requirements in a FAS system, and a fault diagnosis and prognosis structure purposed to solve this challenge.",2009,0,
1811,1812,Feature Extraction of Helicopter Fault Signal Based on HHT,"Since most of helicopter fault signals are non-linear and non-stationary, the method based on Fourier transform (FT) and wavelet analysis are not applicable any longer. The feature extraction method based on Hilbert-Huang transform (HHT), a new method for processing non-stationary and non-linear signals, is applied to analyze helicopter fault signals. The signal is firstly decomposed into intrinsic mode function (IMF) by the empirical mode decomposition (EMD) method. Then Hilbert spectrum and Hilbert marginal spectrum are obtained from instantaneous frequency and amplitude obtained from Hilbert transform (HT). The simulation results are analyzed and compared with FT method, and then suggest the effectiveness of this method based on HHT.",2009,0,
1812,1813,Image-Position Technology of the Digital Circuit Fault Diagnosis Based on Lab Windows/CVI,"This paper presents the different methods and processes of the implement of image-positioning technique, by which the user can locate the probe quickly and accurately during the process of circuit-fault diagnosis.The article introduces the fault diagnosis program that can retrieves information from fault dictionary, presents the image-position technique of BMP and PCB and proposes the usage of the significant function and controls are given which can used to locate the probe accurately. During the test, the real circuit graph guides the user and points out the fault location, thus raising the efficiency of fault removal.",2008,0,
1813,1814,A motion correction system for brain tomography based on biologically motivated models,"A motion correction system for brain tomography is presented, which employs two calibrated video cameras and three vision models (CIECAM'97, RETINA, and BMV). The system is evaluated on the pictures monitoring a subjects head while simulating PET scanning (n=12) and on the face images of subjects with different skin colours (n=31). The results on 2D images with known moving parameters show that motion parameters of rotation and translation along X, Y, and Z directions can be obtained very accurately via the method of facial landmark detection. A feasibility study of the developed algorithms is evaluated by a computer simulation of brain scanning procedures.",2008,0,
1814,1815,OBDD-based evaluation of reliability and importance measures for multistate systems subject to imperfect fault coverage,"Algorithms for evaluating the reliability of a complex system such as a multistate fault-tolerant computer system have become more important. They are designed to obtain the complete results quickly and accurately even when there exist a number of dependencies such as shared loads (reconfiguration), degradation, and common-cause failures. This paper presents an efficient method based on ordered binary decision diagram (OBDD) for evaluating the multistate system reliability and the Griffith's importance measures which can be regarded as the importance of a system-component state of a multistate system subject to imperfect fault-coverage with various performance requirements. This method combined with the conditional probability methods can handle the dependencies among the combinatorial performance requirements of system modules and find solutions for multistate imperfect coverage model. The main advantage of the method is that its time complexity is equivalent to that of the methods for perfect coverage model and it is very helpful for the optimal design of a multistate fault-tolerant system.",2005,0,
1815,1816,Implementation of a modified state estimator for topology error identification,"This paper describes the implementation of a modified state estimation program and its associated user interface. The state estimator is improved by adding the capability of detecting and identifying topology errors, which are caused by the incorrect status information for the circuit breakers at the substations. The developed program is tested using a library of topology error scenarios. A user friendly interface is also implemented in order to facilitate testing of these cases. Some representative cases that are simulated, are presented along with the detailed models for the system and substations.",2003,0,
1816,1817,Fault-Tolerance of Robust Feed-Forward Architecture Using Single-Ended and Differential Deep-Submicron Circuits Under Massive Defect Density,"An assessment of the fault-tolerance properties of single-ended and differential signaling is shown in the context of a high defect density environment, using a robust error-absorbing circuit architecture. A software tool based on Monte-Carlo simulations is used for the reliability analysis of the examined logic families. A benefit of the differential circuit over standard single-ended is shown in case of complex systems. Moreover, analysis of reliability of different circuits and discussion on the optimal granularity of redundant blocks was made.",2006,0,
1817,1818,Bit Error Rate Estimation for Improving Jitter Testing of High-Speed Serial Links,"This paper describes a bit error rate (BER) estimation technique for high-speed serial links, which utilizes the jitter spectral information extracted from the transmitted data and some key characteristics of the clock and data recovery (CDR) circuit in the receiver. In addition to improving the accuracy of BER prediction, the estimation technique can be used to accelerate the jitter tolerance test by eliminating the conventional BER measurement process. Experimental results comparing the estimated BER and the BERT-measured BER on a 2.5 Gbps commercial CDR circuit demonstrate the high accuracy of the proposed technique",2006,0,
1818,1819,SEU-induced persistent error propagation in FPGAs,This paper introduces a new way to characterize the dynamic single-event upset (SEU) cross section of an FPGA design in terms of its persistent and nonpersistent components. An SEU in the persistent cross section results in a permanent interruption of service until reset. An SEU in the nonpersistent cross section causes a temporary interruption of service. These cross sections have been measured for several designs using fault-injection and proton testing. Some FPGA applications may realize increased reliability at lower costs by focusing SEU mitigation on just the persistent cross section.,2005,0,
1819,1820,Eliciting design requirements for maintenance-oriented IDEs: a detailed study of corrective and perfective maintenance tasks,"Several innovative tools have found their way into mainstream use in modern development environments. However, most of these tools have focused on creating and modifying code, despite evidence that most of programmers' time is spent understanding code as part of maintenance tasks. If new tools were designed to directly support these maintenance tasks, what types would be most helpful? To find out, a study of expert Java programmers using Eclipse was performed. The study suggests that maintenance work consists of three activities: (1) forming a working set of task-relevant code fragments; (2) navigating the dependencies within this working set; and (3) repairing or creating the necessary code. The study identified several trends in these activities, as well as many opportunities for new tools that could save programmers up to 35% of the time they currently spend on maintenance tasks.",2005,0,
1820,1821,Ajax-based information publishing system for traveling wave fault location,"In order to real-time issue high precision faults location results for fault treatment, a Web based fault information publishing system for power grid traveling wave fault location is designed in the paper. The system adopts three-layer frame based on Browse/Server (B/S) mode, and uses Microsoft IIS 5.1 as Web Server. Ajax (Asynchronous JavaScript and XML) technique is applied to realize the location results release in real-time, which can create more dynamic and highly available web user interfaces (close to a local desktop application). In addition, a detailed study is carried out for data security. The software has been applied in a real electric power network, practical operation results show that the software can release faults information dynamically and share faults data safely.",2009,0,
1821,1822,Rapid detection of faults for safety critical aircraft operation,"Fault diagnosis typically assumes a sufficiently large fault signature and enough time for a reliable decision to be reached. However, for a class of safety critical faults on commercial aircraft engines, prompt detection is paramount within a millisecond range to allow accommodation to avert undesired engine behavior. At the same time, false positives must be avoided to prevent inappropriate control action. To address these issues, several advanced features were developed that operate on the residuals of a model based detection scheme. We show that these features pick up system changes reliably within the required time. A bank of binary classifiers determines the presence of the fault as determined by a maximum likelihood hypothesis test. We show performance results for four different faults at various levels of severity and show performance results throughout the entire flight envelope on a high fidelity aircraft engine model.",2004,0,
1822,1823,IFRA: Instruction Footprint Recording and Analysis for post-silicon bug localization in processors,"The objective of IFRA, instruction footprint recording and analysis, is to overcome the challenges associated with a very expensive step in post-silicon validation of processors - bug localization in a system setup. IFRA consists of special design and analysis techniques required to bridge a major gap between system-level and circuit-level debug. Special hardware recorders, called footprint recording structures (FRS's), record semantic information about data and control flows of instructions passing through various design blocks of a processor. This information is recorded concurrently during normal operation of a processor in a post-silicon system validation setup. Upon detection of a problem, the recorded information is scanned out and analyzed for bug localization. Special program analysis techniques, together with the binary of the application executed during post-silicon validation, are used for the analysis. IFRA does not require full system-level reproduction of bugs or system-level simulation. Simulation results on a complex super-scalar processor demonstrate that IFRA is effective in accurately localizing bugs with very little impact on overall chip area.",2008,0,
1823,1824,A language-driven tool for fault injection in distributed systems,"In a network consisting of several thousands computers, the occurrence of faults is unavoidable. Being able to test the behavior of a distributed program in an environment where we can control the faults (such as the crash of a process) is an important feature that matters in the deployment of reliable programs. In this paper, we present FAIL (for FAult Injection Language), a language that permits to elaborate complex fault scenarios in a simple way, while relieving the user from writing low level code. Besides, it is possible to construct probabilistic scenarios (for average quantitative tests) or deterministic and reproducible scenarios (for studying the application's behavior in particular cases). We also present FCI, the FAIL cluster implementation, that consists of a compiler, a runtime library and a middleware platform for software fault injection in distributed applications. FCI is able to interface with numerous programming languages without requiring the modification of their source code, and the preliminary tests that we conducted show that its effective impact at runtime is low.",2005,0,
1824,1825,Module size distribution and defect density,"Data from several projects show a significant relationship between the size of a module and its defect density. We address implications of this observation. Does the overall defect density of a software project vary with its module size distribution? Even more interesting is the question can we exploit this dependence to reduce the total number of defects? We examine the available data sets and propose a model relating module size and defect density. It takes into account defects that arise due to the interconnections among the modules as well as defects that occur due to the complexity of individual modules. Model parameters are estimated using actual data. We then present a key observation that allows use of this model for not just estimation of the defect density, but also potentially optimizing a design to minimize defects. This observation, supported by several data sets examined, is that the module sizes often follow exponential distribution. We show how the two models used together provide a way of projecting defect density variation. We also consider the possibility of minimizing the defect density by controlling module size distribution",2000,0,
1825,1826,High-Speed Error-Correction for Leading Zero/One Anticipator,"The algorithm and its implementation of a leading zero anticipators (LZA) are very vital for the performance of a high-speed floating-point adder in current state of art microprocessor design. However, in predicting ""shift amount"" by a conventional LZA design, there may be one-bit error, which is mentioned as the possible error in the result. This paper compares the conventional LZA designs and presents a novel parallel error-detection algorithm for modifying the result of the LZA before it is sent off to improve the performance of the LZA. The novel error-detection algorithm does not depend on any carry-in signals of the adder. Therefore, it makes the error-correction to be parallel with the mantissas addition and increases the speed of the LZA to generate correct results significantly.",2010,0,
1826,1827,Mixed DSP/FPGA implementation of an error-resilient image transmission system based on JPEG2000,"This paper describes a demonstrator of an error-resilient image communication system over wireless packet networks, based on the novel JPEG2000 standard. In particular, the decoder implementation is addressed, which is the most critical task in terms of complexity and power consumption, in view of use on a wireless portable terminal for cellular applications. The system implementation is based on a mixed DSP/FPGA architecture, which allows us to parallelize some computational tasks, thus leading to efficient system operation.",2001,0,
1827,1828,Improved error concealment algorithms based on H.264/AVC non-normative decoder,"We propose several improved error concealment (EC) algorithms based on the H.264/AVC non-normative decoder. The major differences are that motion compensated EC is introduced for intra frames, whereas spatial EC is introduced for inter frames. As for the EC of intra frames, scene change detection, motion activity detection and MV retrieval are hierarchically performed to decide whether spatial or temporal information is to be used. As for the EC of inter frames, scene change is also detected to avoid merging the scenes from different video shots. Therefore, the main idea of the proposed algorithms is that both spatial and temporal correlations are utilized for the EC of intra and inter frames. Both subjective and objective simulations under Internet conditions show that the proposed algorithm greatly outperforms that in the H.264/AVC non-normative decoder.",2004,0,
1828,1829,Empirical scatter correction (esc): A new CT scatter correction method and its application to metal artifact reduction,"Scatter artifacts impair the CT image quality and the accuracy of CT values. Especially in cases with metal implants and in wide cone-angle flat detector CT scans, scatter artifact removal can be of great value. Typical scatter correction methods try to estimate scattered radiation and subtract the estimated scatter from the uncorrected data. Scatter is found either by time-consuming Monte Carlo-based simulations of the photon trajectories, or by rawdata-based modelling of the scatter content using scatter kernels, whose open parameters have to be determined very accurately and for each scanner and type of object individually, and that sometimes even require a data base of typical objects. The procedures are time-consuming and require intimate knowledge about the scanner, in particular about the spectral properties, for which a correction is designed. We propose an empirical scatter correction (ESC) algorithm which does not need lots of prior knowledge for calibration. ESC assumes that a linear combination of the uncorrected image with various ESC basis images is scatter-free. The coefficients for the linear combination are determined in image domain by maximizing a flatness criterion of the combined volume. Here, we minimized the total variation in soft tissue regions using the gradient descent method with a line search. Simulated data and several patient data sets acquired with a clinical cone-beam spiral CT scanner, where scatter was added using a Monte Carlo scatter calculation algorithm, were used to evaluate ESC. Metal implants were simulated into those data sets, too. Our preliminary results indicate that ESC has the potential to efficiently reduce scatter artifacts in general, and metal artifacts in particular. ESC is computationally inexpensive, highly flexible, and does not require know-how of the scanner properties.",2010,0,
1829,1830,3H-5 Photoacoustic Tomographic Characterization of Surface and Undersurface Simulated Defects with a Line-focus Laser Beam,"In this paper, the analyses of both surface and undersurface photoacoustic (PA) tomographic imaging with a line-focus laser beam were formulated. Since PA signal in solid specimen is dominantly proportional to the local absorption coefficient of the specimen alpha (x,y) times laser beam intensity divided by the modulation frequency, illumination of the specimen with a line-focus laser beam in PAT corresponds to the summation of the absorption coefficient along some inclined direction thetas measured from some axis (x-axis). A second harmonic green laser beam of a LD-pumped YAG laser was expanded and focused on a specimen by concave and cylindrical lenses, respectively. The CT scanning (rotation and translation) of a laser beam on the specimen surface was achieved by a mechanical rotating stage and a mechanical stepping stage controlled by a computer. The size of a laser beam on a specimen was 25 mm (length) times 1 mm (width). The measured area was 26 mm times 26 mm, while the reconstructed area was 18 mm times 18 mm. Rotation and translation steps were 2pi/64 and 26 mm/64, respectively. The experiments for the simulated surface defect with shapes of a cross and a semicircle with a diameter of 13 mm resulted in the reasonable reconstruction of the original images",2006,0,
1830,1831,Autonomic Fault-Management and resilience from the perspective of the network operation personnel,"Autonomic networks are an emerging technology which is promising to reduce the complexity of human-driven network management processes and enable a variety of so-called self-* features such as self-configuration, self-optimization, etc, inside the network devices and the network as a whole. Autonomic behaviors are widely understood as a control loop implemented by an autonomic entity that automates management processes and controls diverse aspects of a set of resources. Though automation is necessary and achievable, autonomic decision-making-elements of the network can not fully perform decisions on every task of the network without requiring some degree of a human-in-the-loop in some of the decisions. From the operator's perspective, controllability of the control loop and decision notification from the autonomic network is a vital issue that needs to be addressed. In this paper we present our considerations on how an Autonomic Fault-Management control loop (detect an incident - find the root cause behind it - remove the root cause) can be controlled by the network operation personnel.",2010,0,
1831,1832,Implementing Network Partition-Aware Fault-Tolerant CORBA Systems,"The current standard for fault-tolerance in the Common Object Request Broker Architecture (CORBA) does not support network partitioning. However, distributed systems, and those deployed on wide area networks in particular, are susceptible to network partitions. The contribution of this paper is the description of the design and implementation of a CORBA fault-tolerance add-on for partitionable environments. Our solution can be applied to an off-the-shelf Object Request Broker, without having access to the ORB's source code and with minimal changes to existing CORBA applications. The system distinguishes itself from existing solutions in the way different replication and reconciliation strategies can be implemented easily. Furthermore, we provide a novel replication and reconciliation protocol that increases the availability of systems, by allowing operations in all partitions to continue",2007,0,
1832,1833,Software reliability allocation of digital relay for transmission line protection using a combined system hierarchy and fault tree approach,"Digital relay is a special purpose signal processing unit in which the samples of physical parameters such as current, voltage and other quantities are taken. With the proliferation of computer technology in terms of computational ability as well as reliability, computers are being used for such digital signal processing purposes. As far as computer hardware is concerned, it has been growing steadily in terms of power and reliability. Since power plant technology is now globally switching over to such computer-based relaying, software reliability naturally emerges as an area of prime importance. Recently, some computer-based digital relay algorithms have been proposed based on frequency-domain analysis using wavelet-neuro-fuzzy techniques for transmission line faults. A software reliability allocation scheme is devised for the performance evaluation of a multi-functional, multi-user digital relay that does detection, classification and location of transmission line faults.",2008,0,
1833,1834,Towards measurement of testability of concurrent object-oriented programs using fault insertion: a preliminary investigation,"There is a lack of methods and techniques for measuring testability of concurrent object-oriented programs. Current theory and practice for testing sequential programs do not usually apply to concurrent systems. An approach towards measuring the testability of concurrent Java programs is proposed in this paper. The key idea is to take a program and insert faults that are related to the concurrency aspects. The approach is based on two methods: (1) mutation of the program based on keywords, and (2) creation of conflict graphs based on static analysis of the code.",2002,0,
1834,1835,A new scheduling algorithm for dynamic task and fault tolerant in heterogeneous grid systems using Genetic Algorithm,"In this paper with studying of all parameters in grid environment a new scheduling algorithm for independent task is introduced according to Genetic Algorithm. This algorithm can be more efficient and more dependable than similar previous algorithms. The simulated results and reasons for reaching to better makespan and more efficiency in the grid environment. In the grids with high fault with high fault rate for fault tolerant is used from check point method that has more efficiency that other methods such as retry, migration and replication. This method maintains effective efficiency in these situations. So the servicing quality increases in various grid environments and also the average time of task recoveries decreases considerably The main purpose of this paper is reducing the repeating of the generations in Genetic Algorithm for reaching higher speed and also considering the communications costs (available in fitness function) with maintaining the fitness efficiency. The simulations are done with Gridsim for showing the created improvement at proposed algorithm rather than previous algorithms.",2010,0,
1835,1836,Determining the faulted phase,"In August 1999, a lightning strike caused a misoperation of a relay installed in the late 1980s. The relay misoperation caused a two-minute outage at a petrochemical plant and led to an exhaustive root-cause analysis. The misoperation can be attributed to incorrect fault type selection in a distance element-based, 1980s-era relay. Two separate events in different locations, one in December 2007 and another in March 2009, highlight additional incorrect operations that occurred due to the same problem and root cause. The recent events remind us that this topic is still important and should be reviewed. This paper shares details about three challenging case studies and their root causes. Methodical root-cause analysis techniques are used, including mathematical simulation and testing of old and newer relay designs. This paper contrasts distance and fault identification algorithms, demonstrates methodical analysis techniques, and proposes solutions. Fault type selection logic is discussed, and the evolution and improvement of faulted phase selection logic over several decades is demonstrated. A newer relay design, available since 1993, is proven to have improved performance, namely better security, for these challenging cases.",2010,0,
1836,1837,Fault-tolerant technique in the cluster computation of the digital watershed model,"This paper describes a parallel computing platform using the existing facilities for the digital watershed model. In this paper, distributed multi-layered structure is applied to the computer cluster system, and the MPI-2 is adopted as a mature parallel programming standard. An agent is introduced which makes it possible to be multi-level fault-tolerant in software development. The communication protocol based on checkpointing and rollback recovery mechanism can realize the transaction reprocessing. Compared with conventional platform, the new system is able to make better use of the computing resource. Experimental results show the speedup ratio of the platform is almost 4 times as that of the conventional one, which demonstrates the high efficiency and good performance of the new approach.",2007,0,
1837,1838,The selection and creation of the rules in rules-based optical proximity correction,"Considering the efficiency and veracity of rules-based OPC applied to recent large-scale layout, we firstly point out the importance of the selection and creation of rules in rules-based OPC. Our discussion addresses the crucial factors in selecting and creating rules as well as how we select and create more concise and practical rules-base. Based on our ideas we suggest four primary rules and as a result we show some rule data in table. The automatic construction of the rules-base called OPCL is an important part of the whole rules-based OPC software",2001,0,
1838,1839,The modeling and analysis of geometric error for machining center by homogeneous coordinate transformation method,"In this paper, the collectivity error transformation matrix from the tip of the cutting tool to the workbench is set up for TH6350 machining center, by based on this, the numerical model of the universal geometric error of machining center is built up by applying the homogeneous coordinate transformation principle and the rigid-body hypothesis, and the geometric error of TH6350 machining center is calculated, the numerical model of geometric error is verified and analyzed by the way. The analyzed result plays a very important rule to improving designing, error compensation and practical machining of TH6350 machining center.",2006,0,
1839,1840,Fault tolerant permanent magnet motor drives for electric vehicles,"This paper presents a novel five-phase fault tolerant interior permanent magnet (IPM) motor drive with higher performance and reliability for electric vehicles applications. A new machine design along with efficient control strategy is developed for fault tolerant operation of electric drive without severely compromising the drive performance. Fault tolerance is achieved by employing a five phase fractional-slot concentrated windings configuration IPM motor drive, with each phase electrically, magnetically, thermally and physically independent of all the others. The proposed electric drive system presents higher torque density, negligible cogging torque, and about plusmn0.5% torque ripple. Power converter requirements are discussed and control strategies to minimize the impact of machine or converter fault are developed. Besides, all the requirement of a fault tolerant operation, including high phase inductance and negligible mutual coupling between phases are met. Analytical and finite element analysis and comparison case studies are presented.",2009,0,
1840,1841,Fault-aware scheduling for Bag-of-Tasks applications on Desktop Grids,"Desktop grids have proved to be a suitable platform for the execution of bag-of-tasks applications but, being characterized by a high resource volatility, require the availability of scheduling techniques able to effectively deal with resource failures and/or unplanned periods of unavailability. In this paper we present a set of fault-aware scheduling policies that, rather than just tolerating faults as done by traditional fault-tolerant schedulers, exploit the information concerning resource availability to improve application performance. The performance of these strategies have been compared via simulation with those attained by traditional fault-tolerant schedulers. Our results, obtained by considering a set of realistic scenarios modeled after real desktop grids, show that our approach results in better application performance and resource utilization",2006,0,
1841,1842,Optical Fault Masking Attacks,"This paper introduces some new types of optical fault attacks called fault masking attacks. These attacks are aimed at disrupting of the normal memory operation through preventing changes of the memory contents. The technique was demonstrated on an EEPROM and Flash memory inside PIC microcontrollers. Then it was improved with a backside approach and tested on a PIC and MSP430 microcontrollers. These attacks can be used for the partial reverse engineering of semiconductor chips by spotting the areas of activity in reprogrammable non-volatile memory. This can assist in data analysis and other types of fault injection attacks later, thereby saving the time otherwise required for exhaustive search. Practical limits for optical fault masking attacks in terms of sample preparation, operating conditions and chip technology are discussed, together with possible countermeasures.",2010,0,
1842,1843,Heating of cables due to fault currents,"The metal screen of new type of medium and high voltage single-core underground cables is comprised of helically applied copper wires embedded in a semiconductive layer. The distribution of current and power among the thin wires is non-uniform in a three-phase system, what can lead to their non-uniform heating, especially during fault conditions. The paper investigates this problem with 2D finite element models. The simulations are based on the coupling of the electromagnetic and thermal fields.",2010,0,
1843,1844,"Implementation of error trapping technique in (31,16) cyclic codes for two-bit error correction in 16-bit sound data using Labview Software","This paper considers the implementation of cyclic codes encoder and decoder for multimedia content in the form of sound data using National Instruments LabView software. Cyclic codes can be defined by two parameters, which are code size n and information bit size k. LabView is an easy to use, multipurpose software which has many features for designing and prototyping. This research is a preliminary research on channel coding implementation on LabView. In this research, cyclic codes are used to implement the design. 16-bit sound data are used as test subjects for cyclic code encoding, decoding, and error correction. The result shows that the design works well. The design can correct short two-bit error in last n-k position of the codeword. Authors' next project is to implement more advanced code for error correcting implementation in LabView.",2010,0,
1844,1845,Checkpointing Based Fault Tolerance Patterns for Systems with Arbitrary Deadlines,"This paper aims to provide a fault tolerant scheduling algorithm that have fault tolerance patterns for periodic task sets with arbitrary deadlines. The fault tolerance is achieved by checkpointing where number of checkpoint is decided on the bases of the lemmas proposed. These patterns provide minimum tolerance to all the releases and an improved tolerance to some releases pertaining to the availability of the slack time. They may be binary (i.e., either provide maximum or minimum tolerance to a release) or greedy (i.e., provide an improved tolerance whenever it is possible) in nature. Theorems have been proposed to ensure that the task set is schedulable with at least minimum fault tolerance. The effectiveness of the proposed patterns have been measured through extensive examples and simulations.",2007,0,
1845,1846,Automated Analysis of Faults and Disturbances: Utility Expectations and Benefits - CenterPoint Energy,"Faults and disturbances in a utility power system are typically analyzed using recorded data captured by digital fault recorders, digital protective relays, and other intelligent electronic devices. Several steps are involved in the process including communication to the devices, downloading files, viewing data, and protection engineers performing the actual analysis. This paper discusses automated analysis of this data, and expectations and benefits at centerpoint energy.",2007,0,
1846,1847,Integrating system health management into the early design of aerospace systems using Functional Fault Analysis,"This paper introduces a systematic design methodology, namely the functional fault analysis (FFA), developed with the goal of integrating SHM into early design of aerospace systems. The basis for the FFA methodology is a high-level, functional model of a system that captures the physical architecture, including the physical connectivity of energy, material, and data flows within the system. The model also contains all sensory information, failure modes associated with each component of the system, the propagation of the effects of these failure modes, and the characteristic timing by which fault effects propagate along the modeled physical paths. Using this integrated model, the designers and system analysts can assess the sensor suitepsilas diagnostic functionality and analyze the ldquoracerdquo between the propagation of fault effects and the fault detection isolation and response (FDIR) mechanisms designed to compensate and respond to them. The Ares I Crew Launch Vehicle has been introduced as a case example to illustrate the use of the Functional Fault Analysis (FFA) methodology during system design.",2008,0,
1847,1848,Electrical equipment fault diagnosis system based on the decomposition products of SF6,"This paper presents electrical equipment fault diagnosis system based on the decomposition products of SF6, and makes an introduction of a method that electrical equipment fault diagnosis system of hardware and software implementation. The hardware uses ATmega128 series single-chip platform, and the software uses advanced wavelet neural network fault diagnosis method. To prove the superiority of this algorithm, we make the simulation and comparison with others. A good synergy of hardware and software is used in SF<sub>6</sub> electrical equipment fault diagnosis, and analysis of SF<sub>6</sub> gas content of decomposition products to judge if the electrical equipment fault happens and to make a fault prediction. In this paper we will introduce the hardware design method and the detailed design of the software just because of strong electromagnetic interference environment and it is very important to the system design, finally by giving the experimental data to prove system reliability and practicality.",2009,0,
1848,1849,Design of supervisory control scheme for fault tolerant control of telerobotic system in operational space,"This paper addresses the issues of developing fault tolerant control system for telerobotic systems in operational space. First, the characteristics of operational faults and the relevant sensor signals are studied and classified. Secondly, the framework for FDI and the associated supervisory control scheme are proposed to effectively integrate the FDI approaches into telerobotic systems.",2003,0,
1849,1850,The Back Propagation Neural Network Model of Non-Periodic Defected Ground Structure,"Presently, electromagnetic field numerical value analysis methods such as finite difference time-domain (FDTD) method are generally used to calculate the DGS, although these methods are accurate, they are also computationally expensive. In this paper, a neural network model of a novel defected ground structure is established. Since the neural network model has the advantages of great precision and effectiveness, the developed design model can be used to take the place of the FDTD method of the DGS, being a kind of aid tool of circuit design. The neural network models of two different non-periodic DGS have been developed, at the same time the circuit of the according DGS is designed and manufactured. The result of computer simulation and product measurements are obtained to demonstrate the effectiveness of the method.",2008,0,
1850,1851,A New Approach for the Construction of Fault Trees from System Simulink,"Fault tree analysis is a common method for reliability, safety, and availability assessment of digital systems. Since 70s, a number of construction and analysis methods have been introduced in the literature. The main difference between these methods is the starting model from which the tree is constructed. This paper presents a novel methodology for the construction of fault tree from a system Simulink model, and introduces a fault tree analysis approach in the Simulink environment. The analysis method evaluates static fault tree of a system. The method is introduced and explained in details and its correctness and completeness is validated by using a number of examples. The limitations of the proposed methodology are related to the limitations of the MATLAB-Simulink toolbox. Important advantages of the method are also stated.",2009,0,
1851,1852,Outcomes of overvoltage monitoring and fault location in underground distribution networks,"This paper describes an information-measuring system for surge voltages monitoring and the outcomes of multi-year monitoring in 10 kV underground distribution networks. The statistical analysis of monitoring outcomes has shown a low level of the most often overvoltages in networks originating during restriking earth faults (REFs). For automatic location of REFs, a parametrical method based on the frequency properties of the network is offered.",2002,0,
1852,1853,PRASE: An Approach for Program Reliability Analysis with Soft Errors,"Soft errors are emerging as a new challenge in computer applications. Current studies about soft errors mainly focus on the circuit and architecture level. Few works discuss the impact of soft errors on programs. This paper presents a novel approach named PRASE, which can analyze the reliability of a program with the effect of soft errors. Based on the simple probability theory and the corresponding assembly code of a program, we propose two models for analyzing the probabilities about error generation and error propagation. The analytical performance is increased significantly with the help of basic block analysis. The programAs reliability is determined according to its actual execution paths. We propose a factor named PVF (program vulnerability factor), which represents the characteristic of programAs vulnerability in the presence of soft errors. The experimental results show that the reliability of a program has a connection with its structure. Comparing with the traditional fault injection techniques, PRASE has the advantage of faster speed and lower price with more general results.",2008,0,
1853,1854,Coordinated checkpoint versus message log for fault tolerant MPI,"MPI is one of the most adopted programming models for large clusters and grid deployments. However, these systems often suffer from network or node failures. This raises the issue of selecting a fault tolerance approach for MPI. Automatic and transparent ones are based on either coordinated checkpointing or message logging associated with uncoordinated checkpoint. There are many protocols, implementations and optimizations for these approaches but few results about their comparison. Coordinated checkpoint has the advantage of a very low overhead on fault free executions. In contrary a message logging protocol systematically adds a significant message transfer penalty. The drawbacks of coordinated checkpoint come from its synchronization cost at checkpoint and restart times. In this paper we implement, evaluate and compare the two kinds of protocols with a special emphasis on their respective performance according to fault frequency. The main conclusion (under our experimental conditions) is that message logging becomes relevant for a large scale cluster from one fault every hour for applications with large dataset.",2003,0,
1854,1855,Spatial avoidance of hardware faults using FPGA partial reconfiguration of tile-based soft processors,"This paper presents the design of a many-core computer architecture with fault detection and recovery using partial reconfiguration of an FPGA. The FPGA fabric is partitioned into tiles which contain homogenous soft processors. At any given time, three processors are configured in triple modulo redundancy to detect faults. Spare processors are brought online to replace faulted tiles in real time. A recovery procedure involving partial reconfiguration is used to repair faulted tiles. This type of approach has the advantage of recovering from faults in both the circuit fabric and the configuration RAM of an FPGA in addition to spatially avoiding permanently damaged regions of the chip.",2010,0,
1855,1856,Stable fault-tolerant adaptive fuzzy/neural control for a turbine engine,"Stimulated by the growing demand for improving the reliability and performance of systems, fault-tolerant control has been receiving significant attention since its goal is to detect the occurrence of faults and achieve satisfactory system performance in the presence of faults. To develop an intelligent fault-tolerant control system, we begin by constructing a design model of the system using a hierarchical learning structure in the form of Takagi-Sugeno fuzzy systems. Afterwards, the fault-tolerant control scheme is designed based on stable adaptive fuzzy/neural control, where its online learning capabilities are used to capture the unknown dynamics caused by faults. Finally, the effectiveness of the proposed methods has been studied by extensive analysis of system zero dynamics and asymptotic tracking abilities for both indirect and direct adaptive control cases, and by component level model simulation of the General Electric XTE46 turbine engine",2001,0,
1856,1857,Error Analysis of Explosion-Height Controlling Method Based on Geomagnetism Information,"A new explosion-height controlling scheme of the ballistic missile based on geomagnetism information was put forward aiming at the deficiency of the traditional explosion-height controlling method. The simplified geomagnetism model was presented in this study for the error analysis and precision calculating. Although the scheme was not feasible yet, it could provide the beneficial reference for the explosion-height controlling system design of the ballistic missile.",2009,0,
1857,1858,The research on fault equivalent analysis method in testability experiment validation,"With the existing fault injection techniques, many faults that can fully expose testability design defects can not be injected. To solve this problem, a method of fault equivalent analysis is proposed. By this means, some characteristics are extracted from the faults those unable to be injected, and ldquoyield analysisrdquo or ldquoyielded analysisrdquo is performed. Then the minimal cut sets of atom faults is obtained and selected, which are finally equivalent to the atom faults sequence. Applications show that the method not only solves the problem that many faults are not able to be injected, but also ensures the effect of testability experiment validation.",2009,0,
1858,1859,Fault Tolerant Algorithm for Functional and Data Flow Parallel Programs Performance on Clusters,An approach and algorithm providing fault tolerant performance of functional and data flow parallel processes on clusters is presented in the paper. Main accent in the paper is on decentralized solution supporting of the fault tolerant computations and minimization of time and resources of the recovery process.,2008,0,
1859,1860,Logical method for detecting faults by fault detection table,"Algebro-logic vector method for diagnosing faults of systems and their components based on the use a fault detection table and transactional graph, is proposed. The method allows decreasing the verification time of software model.",2010,0,
1860,1861,Fault-Tolerant 2D Fourier Transform with Checksum Encoding,"Space-based applications increasingly require more computational power to process large volumes of data and alleviate the downlink bottleneck. In addressing these demands, commercial-off-the-shelf (COTS) systems can serve a vital role in achieving performance requirements. However, these technologies are susceptible to radiation effects in the harsh environment of space. In order to effectively exploit high-performance COTS systems in future spacecraft, proper care must be taken with hardware and software architectures and algorithms that avoid or overcome the data errors that can lead to erroneous results. One of the more common kernels in space-based applications is the 2D fast Fourier transform (FFT). Many papers have investigated fault-tolerant FFT, but no algorithm has been devised that would allow for error correction without re-computation from original data. In this paper, we present a new method of applying algorithm-based fault tolerance (ABFT) concepts to the 2D-FFT that will not only allow for error detection but also error correction within memory-constrained systems as well as ensure coherence of the data after the computation. To further improve reliability of this ABFT approach, we propose use of a checksum encoding scheme that addresses issues related to numerical precision and overflow. The performance of the fault-tolerant 2D-FFT will be presented and featured as part of a dependable range Doppler processor, which is a subcomponent of synthetic-aperture radar algorithms. This work is supported by the Dependable Multiprocessor project at Honeywell and the University of Florida, one of the experiments in the Space Technology 8 (ST-8) mission of NASA's New Millennium Program.",2007,0,
1861,1862,Formal Analysis of Fault Recovery in Self-Organizing Systems,"The members of a self-organizing distributed system have ability to automatically organize themselves into a specific structure. The functionality of the system is achieved by collaboration of the members in this structure. Through automatic (re)organization, such a system is able to recover from various temporary faults which may disturb the established structure. In this paper, we propose a technique to identify all recoverable faults as well as to analyze fault tolerance and recovery from temporary faults by reorganization in self-organizing systems.",2009,0,
1862,1863,Fault diagnosis of node in wireless sensor network based on the interval-numbers rough neural network,"This paper proposed a new fault diagnosis method of node in WSN based on the interval-numbers rough neural network. Firstly, this method established the most simple decision-making table of the fault diagnosis by the improved discriminate matrix, then applied rough decision-making analysis method constructed a interval-value information decision-making system of WSN node, and constructed the rough neuron of the input layer; Finally, constructed the fault diagnosis system based on the three-layers feed-forward rough neural network with the interval numbers. The simulation results show that this method made the rate of diagnostic accuracy to 99.24% when the computing time was greatly reduced, and it has high practical value.",2010,0,
1863,1864,Error-related potential recorded by EEG in the context of a p300 mind speller brain-computer interface,The Mind Speller<sup></sup> is a Brain-Computer Interface (BCI) which enables subjects to spell text on a computer screen by detecting P300 Event-Related Potentials in their electroencephalograms (EEG). This BCI application is of particular interest for disabled patients who have lost all means of verbal and motor communication. Error-related Potentials (ErrP) in the EEG are generated by the subject's perception of an error. We report on the possibility of using this ErrP for improving the performance of our Mind Speller<sup></sup>. We tested 6 subjects and recorded several typing sessions for each of them. Responses to correct and incorrect performances of the BCI are recorded and compared. The shape of the received ErrP is compared to other studies. The detection of this ErrP and its integration in the Mind Speller<sup></sup> are discussed.,2010,0,
1864,1865,Analog circuit fault diagnosis using bagging ensemble method with cross-validation,"Neural Network (NN) ensemble approach has been an appealing topic in the field of analog circuit fault diagnosis lately. In this paper, a new method for fault diagnosis of analog circuits with tolerance based on NN ensemble method with cross-validation is proposed. Firstly, bias-variance decomposition shows the theoretical guide on how to choose the component networks when composing the ensemble. Secondly, output voltage signal of the Circuit Under Test (CUT) has been obtained after the stimulus imposed on the CUT. After getting the corresponding fault feature sets, Bagging algorithm is employed to produce the different training sets in order to train the different component networks, and cross-validation technique has been employed to further improve fault diagnosis accuracy. Finally, the outputs of the component ensemble members are combined to isolate the CUT faults. Simulations result shows the superior performance of this proposed approach. This system is able to effectively improve the generalization ability of the analog circuit fault classifier and increase the fault diagnosis accuracy.",2009,0,
1865,1866,"<img src=""/images/tex/16713.gif"" alt=""\Sigma \Delta ""> ADC-Based Frequency-Error Measurement in Single-Carrier Digital Modulations","This paper deals with the functional-block architecture implementing the new method to measure the carrier frequency error of the single-carrier digital modulations. The method is able to operate on the modulations M-ary amplitude shift keying, M-ary phase shift keying, and M-ary quadrature amplitude modulation used in the telecommunication systems. The functional-block architecture obtained from the software-radio (SR) architecture is based on the cascade of the analog-to-digital converter (ADC), the digital down converter, and the base-band processing. Looking at the implementation in measurement instrumentation, the performance of this cascaded architecture is investigated by considering three different ADC architectures. They are based on 1) pipeline; 2) single quantizer loop Sigma-Delta (SigmaDelta) modulator; and 3) multistage noise shaper (MASH) SigmaDelta modulator. Numerical tests confirm the important rule of the ADC in this architecture and highlight the interesting performance of the MASH-based SigmaDelta modulator for use in advanced measurement instruments",2006,0,
1866,1867,Adaptive OSEK Network Management for in-vehicle network fault detection,"Rapid growth in the deployment of networked electronic control units (ECUs) and enhanced software features within automotive vehicles has occurred over the past two decades. This inevitably results in difficulties and complexity in in-vehicle network fault diagnostics. To overcome these problems, a framework for on-board in-vehicle network diagnostics has been proposed and its concept has previously been demonstrated through experiments. This paper presents a further implementation of network fault detection within the framework. Adaptive OSEK Network Management, a new technique for detecting network level faults, is presented. It is demonstrated in this paper that this technique provides more accurate fault detection and the capability to cover more fault scenarios.",2007,0,
1867,1868,Federate Fault Tolerance in HLA-Based Simulation,"A large scale HLA-based simulation (federation) is composed of a large number of simulation components (federates), which may be developed by different participants and executed at different locations. These federates are subject to failures due to various reasons. What is worse, the risk of federation failure increases with the number of federates in the federation. In this paper, a fault tolerance mechanism is proposed to tolerate the crash-stop failures of federates. By exploiting the decoupled federate architecture, federate failures can be masked from the federation and recovery can take place without interrupting the executions of other federates. A basic state recovery protocol is first proposed to recover the state of the failed federate relying on the checkpoint and message logging taken before the failure. Then, an optimized protocol is further developed to accelerate the state recovery procedure. Experiments are carried out to verify that the proposed mechanism provides correct failure recovery. The experimental results also indicate that the optimized protocol can outperform the basic one considerably.",2010,0,
1868,1869,Turbo Coded Modulation for Unequal Error Protection,"A major concern in designing communication systems is to maintain quality of service for a wide range of channel conditions. This is an important issue particularly for the applications where precise characterization of the channel is impossible. For such applications, the source data can be classified into several classes and Unequal Error Protection (UEP) can be used to effectively protect the more important classes even in poor receiving conditions. This paper is focused on the study, design, and performance evaluation of unequal error protecting turbo coded modulation schemes. We first propose several schemes for unequal error protecting using turbo coded modulation. All these schemes provide high performance gains for more important classes that can hardly be achieved using conventional coded modulation schemes. We then study unequal error protecting turbo coded modulation schemes by deriving channel capacity and cutoff rates for different protection levels. We show that for more important classes more room is available for improvement.",2008,0,
1869,1870,Impact of motion correction on parametric images in PET neuroreceptor studies,"The calculation of parametric images in PET studies of neuroreceptors is based on dynamic data which have been recorded over many minutes. It is essential that the subject's head remains unmoved during the PET scan, otherwise the data may become useless in the worst case. This work studies the degrading of parametric images caused by head movements and improvements which is achieved by an appropriate motion correction. The head movements present in PET neuroreceptor studies cause artifacts in the calculation of parametric images. Whereas the activity images look blurred, the parametric images contain discontinuities especially at the cortex. It is concluded that the linear regression is sensitive to the specific errors present in the dynamic images because of the movements",2004,0,
1870,1871,Multi-converter operation of variable speed wind turbine driving permanent magnet synchronous generator during network fault,"The full or partial rating frequency converters, in general, are used widely for the operation of variable speed wind turbine (VSWT) driven wind generators. Among the variable speed wind generators, permanent magnet synchronous generator (PMSG) which uses a full rating frequency converter for grid interfacing is drawing much attention nowadays due to its some salient features. However, considering the factors such as higher-reliability, higher efficiency, and lower harmonics, the multi-converter topology is preferable. This paper proposes a detailed control strategy of multiple parallel connected frequency converter units integrated with VSWT driving PMSG to augments the transient performance during a network disturbance.",2009,0,
1871,1872,Histogram-offset-based color correction for multi-view video coding,"In multi-view video system, variations of different camera setups (for example, camera positions, lighting conditions and camera characteristics) might cause discrepancies on the luminance and chrominance components of different views. From the viewpoint of source compression, this will lead to inaccurate inter-view prediction and lower coding efficiency. In this paper, a histogram-offset-based color correction method is developed for benefitting multi-view video coding. First, disparity estimation is conducted on the rank-transformed domain to identify the maximum matching regions between the reference view and the target view. Within the identified matching regions, the histograms of the reference view and the target view are then calculated, respectively. By using an iterative thresholding approach, a histogram offset is generated and exploited to correct the target view. Experimental results have shown that the proposed color correction method outperforms the histogram matching method on the improvement of coding efficiency.",2010,0,
1872,1873,"Impact of wind farms on electromagnetic transients in 132kV network, with particular reference to fault detection","Adaptive autoreclosure has been extensively researched as a protection methodology for overhead lines, with well-known advantages over conventional autoreclosure. However, the effect of modern wind farms, specifically power electronics, on existing adaptive autoreclosure methods is unknown. Using the DIgSILENT software, a small part of the UK Generic Distribution System network is constructed as a test system and connected to built-in DFIG and full converter wind farm models. EMT simulations are carried out whilst varying the parameters known to affect single phase-ground fault voltage signatures. The Discrete Wavelet Transform is subsequently applied to these waveforms. Results show that adaptive autoreclosing schemes may need particular attention when designed for DFIG connected lines, although the traditional approach of signal processing and AI is validated since the effect of fault parameters have far more significance than the generating technology concerned.",2009,0,
1873,1874,Analysis and comparison of several fault line selective methods in small current grounding power system,"Through a small current grounding system of single-phase grounding to steady state and transient analysis and the existing line selection methods are analyzed and compared, their advantages and shortcomings can be gotten. The line of the selection method in the future was pointed out. The wavelet analysis was introduced to selection methods of faults and the fault lines can be selected in the wavelet analysis method.",2008,0,
1874,1875,Error analysis of Non-contact Weighted-Stretched-Wire System for measuring big structure deflections,"Structure deflections are key parameters to reflect its state, Non-contact Weighted-Stretched-Wire System (NCWSWS) is a novel system for measuring on-line deflections of big structures. This paper presents the configuration of the system and its measurement principle, researches errors due to the stretched-wire component and photography system, and corresponding strategies are given to reduce the errors. The practicality of the system is improved by study the error, and its performance satisfies the requirements of on-line measurement of big structure deflections.",2008,0,
1875,1876,A new approach for real time fault diagnosis in induction motors based on vibration measurement,"Condition monitoring is used to increase machinery availability and machinery performance, reducing consequential damage, increasing machine life, reducing spare parts inventories, and reducing breakdown maintenance. An efficient real time vibration measurement and analysis instruments is capable of providing warning and predicting faults at early stages. In this paper, a new methodology for the implementation of vibration measurement and analysis instruments in real time based on circuit architecture mapped from a MATLAB/Simulink model is presented. In this study, signal processing applications such as FIR filters and fast Fourier transform are treated as systems, which are implemented in hardware using a system generator toolbox, which translates a Simulink model in a hardware description language - HDL for FPGA implementations.",2010,0,
1876,1877,Optical Electric Current Transformer Error Measuring System Based on Virtual Instrument,"Optical electric current transformer is introduced in this paper. Based on virtual instrument and digital signal processing technology, error measuring system is designed by using LabVIEW software and DAQ-2006 data acquisition card. There are harmonic analysis module, error analysis module, and history data analysis module in error measuring system. Functions of these three modules are introduced. Measuring channel's error and protection channel's error of optical electric current transformer, are measured. Random phase error curve and ratio error curve of measuring channel in different test current are plotted. The result shows that, this error measuring system achieves accuracy standard, and is able to measure phase error and ratio error of optical electric current transformer exactly.",2007,0,
1877,1878,An efficient method for compensating the truncation DC-error in a multi-stage digital filter,"Binary two's complement operation in a digital circuit brings increased word length in operation results. In this case, the LSB bit truncation is performed to meet the system requirement. However, truncating the word adds an undesired DC-error to the signal and degrades system performance. To solve this problem, a complementary method is suggested. The proposed scheme improves system performance with a simple method that involves inverting the sign of the truncated signal value. The suggested truncation DC-error reducing method is applied to the multi-stage digital FIR filter implementation on the FPGA and the results are analyzed.",2004,0,
1878,1879,Aspects of the Development of Secure and Fault-Resistant Hardware,"Designing ""secure hardware"" like a chip card controller, is a challenging task for hardware manufacturers: More and more attacks that are also more and more sophisticated generate a need for more and more countermeasures. Developers of these devices have to live with certain additional constraints and this does not make their life easier. The difficulties that the designer of such a system is confronted with, are pointed out. This might give the scientific community some impression of the problems that would be interesting to be solved.",2008,0,
1879,1880,Evaluation of a single accelerometer based biofeedback system for real-time correction of neck posture in computer users,"The worldwide adoption of computers is closely linked to increased prevalence in neck and shoulder pain. Many ergonomic interventions are available; however, the lifetime prevalence of neck pain is still estimated as high as 80%. This paper introduces a biofeedback system using a novel single accelerometer placement. This system allows the user to react and correct for movement into a position of bad posture. The addition of visual information provides artificial proprioceptive information on the cranial-vertebral angle. Six subjects were tested for 5 hours with and without biofeedback. All subjects had a significant decrease in the percentage of time spent in bad posture when using biofeedback.",2009,0,
1880,1881,Optimization of Warpage Defect in Injection Moulding Process Using ABS Material,"Plastic injection moulding process produces various defects such as warpage, sink marks, weld lines and shrinkage. The purpose of present paper is to analyze the warpage defect on Acrylonitrile Butadiene Styrene (ABS) for selected part using FEA simulation. The approach was based on Taguchipsilas Method and Analysis of Variance (ANOVA) to optimize the processing parameters namely packing pressure, mould temperature, melt temperature and packing time for effective process. It was found that the optimum parameters for ABS material are packing pressure at 375 MPa, mould temperature at 40degC, melt temperature at 200degC and packing time at 1 s. Melt temperature was found to be the most significant factor followed by packing time and mould temperature. Meanwhile, packing pressure was insignificant factor contributing to the warpage in present study.",2009,0,
1881,1882,An improved DMVE temporal error concealment,"During video transmission over error-prone networks, the compressed bit stream is often corrupted by channel errors which may cause the video quality degrading suddenly. In this paper, we present a novel temporal error concealment technique as a post-processing tool at the decoder side for recovering the lost information. In order to recover the lost motion vector, an improved decoder motion vector estimation (DMVE) criterion is introduced which considers temporal correlation and motion trajectory together. We utilize pixels in the two previous frames as well as surrounding pixels of the lost block. The best motion vector is determined according to the criterion, and then the lost pixels are recovered using motion compensation. Simulations show that the proposed technique can achieve remarkable objective (PSNR) and subjective gains in the quality of the recovered video.",2008,0,
1882,1883,Current harmonics analysis as a method of electrical faults diagnostic in switched reluctance motors,"Diagnostic method of electrical faults in switched reluctance motors (SRM) based on current harmonics analysis is presented in this paper. The classification of electrical faults in SRM's has been discussed, simulation model and laboratory setup presented. For the SRM motor of 6/4 design the conclusions drawn from the tests conducted both in the simulated and laboratory conditions for regular operation and selected electrical faults. The electrical fault type found in the machine was determined on the content of harmonics of higher order in the source current. The conclusions are included.",2007,0,
1883,1884,Terrorist risk evaluation using a posteriori fault trees,"Analysis of a terrorist attack involves an event that has already occurred and an a posteriori fault tree can be utilized. This differs from a conventional a priori fault tree used for prediction. Since the top event has a known probability of unity, only ratios of the lower event probabilities are needed to solve and these are easier to estimate. If one assumes that probability ratios are the same for both types of fault trees, then a posteriori analysis aids a priori analysis. Use of a sequence diagram helps direct the work of first responders",2006,0,
1884,1885,Looking for Product Line Feature Models Defects: Towards a Systematic Classification of Verification Criteria,"Product line models (PLM) are important artifacts in product line engineering. Due to their size and complexity, it is difficult to detect defects in PLMs. The challenge is however important: any error in a PLM will inevitably impact configuration, generating issues such as incorrect product models, inconsistent architectures, poor reuse, difficulty to customize products, etc. Surveys on feature-based PLM verification approaches show that there are many verification criteria, that these criteria are defined in different ways, and that different ways of working are proposed to look for defect. The goal of this paper is to systematize PLM verification. Based on our literature review, we propose a list of 23 verification criteria that we think cover those available in the literature.",2009,0,
1885,1886,A novel 20Hz power injection protection scheme for stator ground fault of pumped storage generator,"Based on the analysis of the start-up of pumped storage generator under motor circumstance, it is found out that the conventional 20 Hz injection component is influenced by the low frequency component when ground faults take place in the stator windings for static frequency converter (SFC) start-up generator. The conventional relay is just simply blocked in case mal-operation during 20 Hz working condition. To overcome the disadvantage, a novel scheme is proposed to improve grounding- resistance(R) calculation accuracy and enhance ability to limit impact of 20 Hz component from generator. The simulation results proved its effectiveness and practicability of the scheme.",2008,0,
1886,1887,Noise Reduction and Confidence Level Analysis in MMG-based Transient Fault Location,"A multi-resolution morphological gradient (MMG) filtering technique is applied successfully into the transient fault locator. The performance of MMG will inevitably deteriorate when various disturbances are imposed on the transient signals. These disturbances can be considered as noise. In this paper, median filter is applied to fault location for noise reduction in the transient signals and to improve the performance of accurate fault location using MMG. The confidence analysis is introduced to discuss the reliabilities of fault location under different signal-noise-ratios in noisy environments. Since the accuracy of fault location relies on the accurate extraction of the maxima of MMG, a confidence level index (CLI) is defined based on the analysis of these maxima. A hypothesis testing is used to determine whether an assertion of maxima of time-tags is reasonable. By using the CLI, the reliability of fault location is able to be measured. The analysis of receiver operating characteristic (ROC) curves with respect to CLI has shown that the median filter can improve the discrimination of accurate and inaccurate locations and the performance of the MMG detection scheme has been improved therefore",2005,0,
1887,1888,Fault Detection in Distributed Climate Sensor Networks Using Dynamic Bayesian Networks,"The Atmospheric Radiation Measurement (ARM) program operated by the U.S. Department of Energy is one of the largest climate research programs dedicated to the collection of long-term continuous measurements of cloud properties and other key components of the earth's climate system. Given the critical role that collected ARM data plays in the analysis of atmospheric processes and conditions and in the enhancement and evaluation of global climate models, the production and distribution of high-quality data is one of ARM's primary mission objectives. Fault detection in ARM's distributed sensor network is one critical ingredient towards maintaining high quality and useful data. We are modeling ARM's distributed sensor network as a dynamic Bayesian network where key measurements are mapped to Bayesian network variables. We then define the conditional dependencies between variables by discovering highly correlated variable pairs from historical data. The resultant dynamic Bayesian network provides an automated approach to identifying whether certain sensors are malfunctioning or failing in the distributed sensor network. A potential fault or failure is detected when an observed measurement is not consistent with its expected measurement and the observed measurements of other related sensors in the Bayesian network. We present some of our experiences and promising results with the fault detection dynamic Bayesian network.",2010,0,
1888,1889,Towards a Defect Prevention Based Process Improvement Approach,"Defect causal analysis (DCA) is a means of product focused software process improvement. A systematic literature review to identify the DCA state of the art has been undertaken. The systematic review gathered unbiased knowledge and evidence and identified opportunities for further investigation. Moreover, some guidance on how to efficiently implement DCA in software organizations could be elaborated. This paper describes the initial concept of the DBPI (Defect Based Process Improvement) approach. It represents a DCA based approach for process improvement, designed considering the results of the systematic review and the obtained guidance. Its main contributions are tailoring support for DCA based process improvement and addressing an identified opportunity for further investigation by integrating organizational learning mechanisms regarding cause-effect relations into the conduct of DCA.",2008,0,
1889,1890,Novel modular fault tolerant switched reluctance machine for reliable factory automation systems,"Electrical machines and drives used in diverse critical fields like advanced factory automation systems, automotive and aerospace applications, military, energy and medical equipment, etc. require both special motor and converter topologies to achieve high level fault tolerance. In the paper a novel modular fault tolerant switched reluctance machine is proposed. Its stator is built up of simply to manufacture and to replace modules. The machine is able to have continuous operation despite winding faults of diverse severity. It is fed by a special power converter having separate half H-bridge leg for each coil. Thus a complex and high reliable electrical system is obtained. By advanced dynamic co-simulations (using a coupled Flux 2D and Simulink<sup></sup> program) the behaviour of the drive system under five winding fault conditions are studied. The obtained results prove the fault tolerant capacity of the proposed machine.",2010,0,
1890,1891,A neural net based approach for fault diagnosis in distribution networks,"This paper discusses the application of field data to a new supervised clustering-based arcing distribution fault diagnosis method. The fault diagnosis method can perform three functions that provide preliminary fault location information for grounded and ungrounded power distribution systems: fault detection, faulted type classification, and faulted phase identification. It contains two main modules: a preprocessor and a pattern classifier which was implemented as a supervised clustering-based neural net. The inputs to the fault diagnosis method are the three phase and neutral currents for a feeder. The preprocessor computes a vector of statistical features from the phase currents and passes them to the neural net pattern classifier. The neural net determines the features pattern as normal or faulted. If detected as faulted, the neural net also identifies the fault type and classifies the faulted phase. Field studies were conducted in which the fault diagnosis method was trained and tested with normal and faulted phase currents generated from data recorded by events staged in the field for two, four-wire systems. The fault diagnosis method was highly successful during tests to validate the fault detection and identification functions. Also the fault diagnosis method was able to recognize the difference between faulted test patterns and fault-like test patterns representing line switching and load tap changer operations. Further the clustering-based fault diagnosis approach was evaluated using simulated data generated for a 3-feeder ungrounded system",2000,0,
1891,1892,Experiences with formal specification of fault-tolerant file systems,"Fault-tolerant, replicated file systems are a crucial component of todaypsilas data centers. Despite their huge complexity, these systems are typically specified only in brief prose, which makes them difficult to reason about or verify. This paper describes the authorspsila experience using formal methods to improve our understanding of and confidence in the behavior of replicated file systems. We wrote formal specifications for three real-world fault-tolerant file systems and used them to: (1) expose design similarities and differences; (2) clarify and mechanically verify consistency properties; and (3) evaluate design alternatives. Our experience showed that formal specifications for these systems were easy to produce, useful for a deep understanding of system functions, and valuable for system comparison.",2008,0,
1892,1893,Error-Rate Analysis of FHSS Networks Using Exact Envelope Characteristic Functions of Sums of Stochastic Signals,"In this paper, we present a novel approach to analytically study and evaluate the error probability of frequency- hopping spread-spectrum (FHSS) systems with intentional or nonintentional interference over the additive white Gaussian noise and Rayleigh-fading channels. The new approach is based on the derivation of new formulas for the exact envelope characteristic function (EECF) of the general sum of n stochastic sinusoidal signals, with each of the n signals having different random amplitude and phase angle. The envelope probability density function (pdf) is obtained from the characteristic function (CF), which, in the important cases of interest, is shown to also give simpler formulas in terms of the Fourier transform (FT) of the Bessel functions. Previously, the Ricean envelope density had only been verified for the very special case where n = 1, and the phase is uniform and independent of amplitude. Here, a new formula for the exact density of the envelope of noisy stochastic sinusoids (EDENSS) is presented, which leads to the generalization of the Ricean envelope-density (GRED) formula under the most general conditions, namely, n ges 1 signals, dependent amplitudes, and phases having an arbitrary joint pdf. The EDENSS and GRED formulas are applied to compute the pdfs needed in noncoherent detection under noise. The derived formulas also lead to the exact formulas for the error probability of the FHSS networks using M-ary amplitude shift keying (MASK) without setting limits on the number of interferers or the symbol alphabet. The power of our EECF and FT methods is further demonstrated by their ability to give an alternative derivation of the exact general envelope-density (EGED) formula, which has previously been reported by Maghsoodi. The comparative numerical results also support the analytical findings.",2008,0,
1893,1894,Learning from errors: A bio-inspired approach for hypothesis-based machine learning,"This contribution present an approach extending existing learning strategies based on situation-operator-modeling (SOM), which can be used to model interactions with the environment and to represent the knowledge of cognitive systems. The approach proposes a planning process using hypotheses to bridge the gap of knowledge, which is refined by a following check of the applied hypothesis. The hypotheses are inspired by human errors according to Dornerpsilas classification, which is related to the interaction within complex dynamic systems. The programmed implementation of the approach is based on an experimental environment using a software tool for high-level Petri nets.",2008,0,
1894,1895,Adaptive routing strategies for fault-tolerant on-chip networks in dynamically reconfigurable systems,"An investigation into an effective and low-complexity adaptive routing strategy based on stochastic principles for an asynchronous network-on-chip platform that includes dynamically reconfigurable computing nodes is presented. The approach is compared with classic deterministic routing and it is shown to have good properties in terms of throughput and excellent fault-tolerance capabilities. The challenge of how to deliver reliability is one of the problems that multiprocessor system architects and manufactures will face as feature sizes and voltage supplies shrink and deep-submicron effects reduce the ability to carry out deterministic computing. It is likely that a new type of deep-submicron complex multicore systems will emerge which will be required to deliver high performance within strict energy and area budgets and operate over unreliable silicon. Within this context, the paper studies an on-chip communication infrastructure suitable for these systems.",2008,0,
1895,1896,Effects of fault arcs on insulating walls in electrical switchgear,This paper deals with test methods to evaluate the effects of high current fault arcs occurring in electrical switchgear compartments stressing insulating walls made of different materials. These partitions are stressed mechanically and thermally. To study these effects several test arrangements were developed and relevant experimental studies performed. The performance of the investigated materials is presented and discussed,2000,0,
1896,1897,Diagnosis of Multiple Scan Chain Timing Faults,"A diagnosis technique is presented to locate multiple timing faults in scan chains. Jump simulation is a novel parallel simulation technique which quickly searches for the upper and the lower bounds of every individual fault. The proposed technique takes into account the interaction of multiple faults so the diagnosis results are deterministic, not probabilistic. This technique is very useful in the production test environment because it requires only regular automated test pattern generator patterns, not specialized diagnosis patterns. Experiments on ISCAS'89 benchmark circuits show that this technique can successfully pinpoint almost every single one of 16 hold-time faults in a scan chain of more than 800 scan cells. The proposed technique is still effective when failure data are limited or faults are clustered.",2008,0,
1897,1898,An approach for analysing and improving fault tolerance in radio architectures,"We present an approach for analysing and improving fault-tolerance aspects in radio architectures. This is a necessary step to be taken in order to implement reliable radio systems in future nanoscale technologies. We present problem formulation, optimisation approach and implementation methodology. We are adding fault tolerance at architecture level by taking advantage of existing parallel structures and using a spare module approach in order to minimise hardware overhead needed. These issues have been analysed and demonstrated using two radio case studies: a UMTS MIMO and a GSM diversity receivers",2006,0,
1898,1899,Analysis and Implementation of LMS Algorithm with Coding Error in the DSP TMS320C6713,"In this work we present the analysis and implementation in a digital signal processor (DSP), of a variant of the least mean square (LMS) algorithm. Modification is based on codifying the error of the algorithm, in order to reduce the design complexity for its implementation in digital adaptive filters, because the error is made up of whole values. The results demonstrate an increase in the convergence speed; it's affected indirectly by the convergence factor, and to obtain a floating point operation reduction, which accelerates processing. These, to demonstrate the results obtained from the implementation of the algorithm in the digital signal processor TMS320C6713 by Texas instruments.",2008,0,
1899,1900,Quasi-Renewal Time-Delay Fault-Removal Consideration in Software Reliability Modeling,"Software reliability growth models based on a nonhomogeneous Poisson process (NHPP) have been considered as one of the most effective among various models since they integrate the information regarding testing and debugging activities observed in the testing phase into the software reliability model. Although most of the existing NHPP models have progressed successfully in their estimation/prediction accuracies by modifying the assumptions with regard to the testing process, these models were developed based on the instantaneous fault-removal assumption. In this paper, we develop a generalized NHPP software reliability model considering quasi-renewal time-delay fault removal. The quasi-renewal process is employed to estimate the time delay due to identifying and prioritizing the detected faults before actual code change in the software reliability assessment. Model formulation based on the quasi-renewal time-delay assumption is provided, and the generalized mean value function (MVF) for the proposed model is derived by using the method of steps. The general solution of the MVFs for the proposed model is also obtained for some specific existing models. The numerical examples, based on a software failure data set, show that the consideration of quasi-renewal time-delay fault-removal assumption improves the descriptive properties of the model, which means that the length of time delay is getting decreased since testers and programmers adapt themselves to the working environment as testing and debugging activities are in progress.",2009,0,
1900,1901,Evaluating the Effect of the Number of Naturally Occurring Faults on the Estimates Produced by Capture-Recapture Models,"Project managers can use the capture-recapture models to estimate the number of faults in a software artifact. The capture-recapture estimates are calculated using the number of unique faults and the number of times each fault is found. The accuracy of the estimates is affected by the number of inspectors and the number of faults. Our earlier research investigated the effect that the number of inspectors had on the accuracy of the estimates. In this paper, we investigate the effect of the number of faults on the performance of the estimates using real requirement artifacts. These artifacts have an unknown amount of naturally occurring faults. The results show that while the estimators generally underestimate, they improve as the number of faults increases. The results also show that the capture-recapture estimators can be used to make correct re-inspection decisions.",2009,0,
1901,1902,Transmission surveillance and self-restoration against fibre fault for time division multiplexing using passive optical network,"This study proposes a practical transmission surveillance and self-protection scheme for time division multiplexing using passive optical network (TDM-PON) with centralised monitoring and self-restorable apparatus. Troubleshooting a TDM-PON involves locating and identifying the source of an optical problem in what may be a complex optical network topology that includes several optical line terminals (OLTs), optical splitters, fibres and optical network units (ONUs). Since most components in the network are passive, a large part of the issues are due to dirty/damaged/misaligned connectors or breaks/macrobends in optical fibre cables. These will affect one, some or all subscribers in the network, depending on the location of the problems. The proposed scheme is able to prevent and detect the occurrence of fibre faults in a network system through centralised monitoring and remotely operate from a central office via Ethernet connection. Even with fibre fault prevention mechanisms, failures will still occur. Therefore fibre fault detection is required in order to detect potential faults and precisely localise the exact failure location. Whenever any failure occurs on the primary entity, the proposed system can protect and switch the failure line to the protection line to ensure that traffic flows continuously. Meanwhile, the failure information will be delivered to field engineers for taking appropriate recovery action to treat the fibre fault and failure link. One suggestion in point-to-multipoint (P2PM) applications has been proposed with the experimental results as the feasibility approach. This approach has bright prospects for improving the survivability and reliability as well as increasing the efficiency and monitoring capabilities in TDM-PON.",2009,0,
1902,1903,Transient fault-tolerance through algorithms,"This article describes that single-version enhanced processing logic or algorithms can be very effective in gaining dependable computing through hardware transient fault tolerance (FT) in an application system. Transients often cause soft errors in a processing system resulting in mission failure. Errors in program flow, instruction codes, and application data are often caused by electrical fast transients. However, firmware and software fixes can have an important role in designing an ESD, or EMP-resistant system and are more cost effective than hardware. This technique is useful for detecting and recovering transient hardware faults or random bit errors in memory while an application is in execution. The proposed single-version software fix is a practical, useful, and economic tool for both offline and online memory scrubbing of an application system without using conventional N versions of software (NVS) and hardware redundancy in an application like a frequency measurement system",2006,0,
1903,1904,Expert fault diagnosis using rule models with certainty factors for the leaching process,"A key point in the operation of the leaching process is to ensure the safe running of the process. The paper proposes an expert fault diagnosis strategy for the leaching process, which is based on rule models with certainty factors. A diagnosis procedure is first presented. Then, the rule models are constructed based on empirical knowledge, empirical data and statistical results on past fault countermeasures, and an expert reasoning method is used which employs the rule models and forward chaining. A real-word application is expounded finally",2000,0,
1904,1905,Experimental Risk Assessment and Comparison Using Software Fault Injection,"One important question in component-based software development is how to estimate the risk of using COTS components, as the components may have hidden faults and no source code available. This question is particularly relevant in scenarios where it is necessary to choose the most reliable COTS when several alternative components of equivalent functionality are available. This paper proposes a practical approach to assess the risk of using a given software component (COTS or non-COTS). Although we focus on comparing components, the methodology can be useful to assess the risk in individual modules. The proposed approach uses the injection of realistic software faults to assess the impact of possible component failures and uses software complexity metrics to estimate the probability of residual defects in software components. The proposed approach is demonstrated and evaluated in a comparison scenario using two real off-the-shelf components (the RTEMS and the RTLinux real-time operating system) in a realistic application of a satellite data handling application used by the European Space Agency.",2007,0,
1905,1906,The effect of real-time software reuse in FPGAs and microcontrollers with respect to software faults,"Reuse is considered as an important aspect in software design, but certain challenges have to be met if software reuse is applied in embedded systems. In these systems, specific requirements, as for example safety or real-time requirements, have to be considered, which typically complicate the reuse of software. Moreover, a large variety of hardware platforms is present in embedded systems. Those hardware platforms have different properties, which might affect the reuse of the corresponding software. In this paper, the different impacts of microcontrollers and FPGAs on software reuse are considered by empirical investigations. In particular, the investigations focus on the effect of this reuse on faults in real-time software. As a result, different benefits and drawbacks of software reuse were identified for microcontrollers and FPGAs.",2008,0,
1906,1907,Effects of rotor bar and end-ring faults over the signals of a position estimation strategy for induction motors,"The effect of rotor faults, such as broken bars and end-rings, over the signals of a position estimation strategy for induction motor drives is analyzed using a multiple coupled circuit model. The objective of this analysis is to establish the possibility of using the estimation strategy signals for fault diagnosis in variable-speed electric drives. This strategy is based on the effect produced by inductance variation on the zero-sequence voltage, when exciting the motor with a predefined inverter switching pattern. Experimental results illustrate the feasibility of the proposal.",2005,0,
1907,1908,Glass auto alignment mechanism on LCD Defect repair stage system,"These Mechanism provide Auto Align for LCD Defect repair stage system. While glass floated on glass chuck by air, this system what located at side of glass chuck will blow air to glass side through many holes. This air blow will bring on floating glass side on side of glass chuck, so, at that times, glass will enter to guide while sliding, which guide is fixed by fixed glass size on side of glass chuck. This mechanism has a lot of profits that low cost, past align time, easy maintenance, no cylinder, etc.",2009,0,
1908,1909,Applying object-orientation and IEC 61850 standard to architecture design of power system fault information processing,"With the maturing of the IEC 61850, utilities are beginning to implement substation automation systems (SAS) that are based on this new international standard. This paper describes such an implementing research for power fault information processing system on East China electric power group in China. In particular, it presents the idea of applying object-oriented methodology to architecture design and providing an open interface of IEC 61850. Based on the idea and technique, some benefits are brought.",2004,0,
1909,1910,Safety optimization: a combination of fault tree analysis and optimization techniques,"We present a new form of quantitative safety analysis -safety optimization. This method is a combination of fault tree analysis (FTA) and mathematical optimization techniques. With the use of the results of FTA, statistics, and a quantification of the costs of hazards, it allows to find the optimal configuration of a given system with respect to opposed safety requirements. Furthermore, the system may not only be examined for safety, but usability as well. We illustrate this method on a real-world case study: the height control system of the Elbtunnel in Hamburg. Safety optimization showed some significant problems in trustworthiness of the system, yielded optimal values for configuration of free parameters and showed possible modifications to improve the system.",2004,0,
1910,1911,Vibration Fault Diagnosis of Steam Turbine Shafting Based on Probability Neural Networks,"Information entropy is an effective description for the uncertainty of a system, and could be used for the symptom to detect the vibration changes of steam turbine. Based on the faulty signals collected from rotor test rig, three information entropy: singular spectrum entropy, power spectrum entropy, wavelet energy spectrum entropy were calculated as information entropy data. Probability neural networks(PNNs) was explored to fuse the three information entropy. Research shows that with the advantages of Bayes classifier and neural networks, PNNs has good classification ability to typical vibration faults of turbine, the classification accuracy is 100% for training data, 80% for unseen data. Compared with the classification accuracy of minimum distance classifier(MDC) and improved MDC, PNNs has higher classification accuracy. It can be deduced that PNNs is a practical fusion diagnosis method for typical fault identification of turbine rotor.",2008,0,
1911,1912,Parfait - A Scalable Bug Checker for C Code,"Parfait is a bug checker of C code that has been designed to address developers' requirements of scalability (support millions of lines of code in a reasonable amount of time), precision (report few false positives) and reporting of bugs that may be exploitable from a security vulnerability point of view. For large code bases, performance is at stake if the bug checking tool is to be integrated into the software development process, and so is precision, as each false alarm (i.e., false positive) costs developer time to track down. Further, false negatives give a false sense of security to developers and testers, as it is not obvious or clear what other bugs were not reported by the tool. A common criticism of existing bug checking tools is the lack of reported metrics on the use of the tool. To a developer it is unclear how accurate the tool is, how many bugs it does not find, how many bugs get reported that are not actual bugs, whether the tool understands when a bug has been fixed, and what the performance is for the reported bugs. In this tool demonstration we show how Parfait fairs in the area of buffer overflow checking against the various requirements of scalability and precision.",2008,0,
1912,1913,Fault Slip Through measurement in software development process,"The pressure to improve software development process is not new, but in today's competitive environment there is even greater emphasis on delivering a better service at lower cost. In market-driven development where time-to-market is of crucial importance, software development companies seek improvements that can decrease the lead-time and improve the delivery precision. One way to achieve this is by analyzing the test process since rework commonly accounts for more than half of the development time. A large reason for high rework costs is fault slippage from earlier phases where they are cheaper to find and remove. As an input to improvements, this article introduces a measure that can quantify this relationship. That is, a measure called faults-slip-through, which determines the faults that would have been more cost-effective to find in an earlier phase.",2010,0,
1913,1914,Research on Neural Network Integration Fusion Method and Application on the Fault Diagnosis of Automotive Engine,"A new fusion model is proposed, which is the combination of integration BP neural networks models and D-S evidence reasoning model, to solve the problems of low precision rate in automotive engine fault diagnosis by traditional expert system. The method of this paper not only realizes feature level fusion of all subjective observation data and expert experiments on different parts of engineer, but also realizes the predominance compensation of different models. In simulation experiment, by comparison between the two methods, this method proposed in the paper can improve diagnosis precision 7.1%more than expert system and reduce time complication degree.",2007,0,
1914,1915,Fault diagnosis of motor bearing using self-organizing maps,"This paper focuses on the application of self-organizing maps (SOM) in motor bearing fault diagnosis and presents an approach for motor rolling bearing fault diagnosis using SOM neural networks and time/frequency-domain bearing analysis. The SOM is a neural network algorithm which is based on unsupervised learning and combines the tasks of vector quantization and data projection. The objective of this paper is to detect and diagnose faults to motor adoptively, with emphasis on faults occurred in the bearing part of the motor. The experiment results show that the SOM is an efficient tool for the fault visualization and diagnosis of motor bearing",2005,0,
1915,1916,On Scan Chain Diagnosis for Intermittent Faults,"Diagnosis is increasingly important, not only for individual analysis of failing ICs, but also for high-volume test response analysis which enables yield and test improvement. Scan chain defects constitute a significant fraction of the overall digital defect universe, and hence it is well justified that scan chain diagnosis has received increasing research attention in recent years. In this paper, we address the problem of scan chain diagnosis for intermittent faults. We show that the conventional scan chain test pattern is likely to miss an intermittent fault, or inaccurately diagnose it. We propose an improved scan chain test pattern which we show to be effective. Subsequently, we demonstrate that the conventional bound calculation algorithm is likely to produce wrong results in the case of an intermittent fault. We propose a new lower bound calculation method which does generate correct and tight bounds, even for an intermittence probability as low as 10%.",2009,0,
1916,1917,An optimization to automatic Fault Tree Analysis and Failure Mode and Effect Analysis approaches for processes,"There are two issues to be addressed in the automatic Fault Tree Analysis (FTA) and Failure Mode and Effect Analysis (FMEA) approaches: resource fault and channel fault are not considered when generating the artifact flow graph (AFG) from Little-JIL processes. The AFG is incomplete, thus the fault trees and FMEA reports automatically generated partially based on AFG is incomplete. In this paper, we put forward to an approach of introducing resource instances and channel instances into fault propagation in Little-JIL processes. Thus, it makes the FTA and FMEA for Little-JIL processes to be much more effective. That is to say, how resource instances and channel instances might affect the fault propagation in Little-JIL processes is taken into account while performing these two automatic safety analysis techniques.",2010,0,
1917,1918,Transition Faults Testing Based on Functional Delay Tests,"Rapid advances of semiconductor technology lead to higher circuit integration as well as higher operating frequencies. The statistical variations of the parameters during the manufacturing process as well as physical defects in integrated circuits can sometimes degrade circuit performance without altering its logic functionality. These faults are called delay faults. In this paper we consider the quality of the tests generated for two types of delay faults, namely, functional delay and transition faults. We compared the test quality of functional delay tests in regard to transition faults and vice versa. We have performed various comprehensive experiments with combinational benchmark circuits. The experiments exhibit that the test sets, which are generated according to the functional delay fault model, obtain high fault coverages of transition faults. However, the functional delay fault coverages of the test sets targeted for the transition faults are low. It is very likely that the test vectors based on the functional delay fault model can cover other kinds of the faults. Another advantage of test set generated at the functional level is that it is independent of and effective for any implementation and, therefore, can be generated at early stages of the design process.",2007,0,
1918,1919,Optimization of regularization of attenuation and scatter-corrected <sup>99m</sup>Tc cardiac SPECT studies for defect detection using hybrid images,"Through means of a receiver-operating-characteristics study, we optimize the iteration number and three-dimensional (3-D) Gaussian postfiltering of <sup>99m</sup>Tc cardiac emission ordered-subset expectation-maximization (OSEM) reconstructions that implement corrections for both attenuation and scatter. Hybrid images, wherein artificial perfusion defects were added to clinical patient studies that were read as being normally perfused, were used for this optimization. The test conditions included three different iteration numbers of OSEM (1, 5 and 10) using four angles per subset, followed by 3-D Gaussian low-pass filtering at each iteration level. The level of Gaussian low-pass filtering was varied using standard deviations () of 0.6, 0.75, 1 and 1.25 pixels, in addition to a case where no postfiltering was applied. Four observers read 80 images for each of the 15 test conditions being investigated, providing confidence ratings as to the presence or absence of perfusion defects. Results indicate that at all iterations, optimum detection performance is obtained for a broad plateau or range of postfilters (=0.6 to 1 pixel). As expected, a gradual reduction in performance is seen at either end of this broad maximum where the images either have been very heavily smoothed or have very little postfiltering. Finally, one iteration of OSEM appears to be the appropriate choice since no significant improvement in detection accuracy was observed with increasing iteration number as long as the reconstructions are postfiltered with  in the range of 0.6 to 1 pixel",2001,0,
1919,1920,Research on real-time error measurement in curve grinding process based on machine vision,"A machine vision image measurement system for online monitoring of the wheel wear degree during the curve grinding process is designed and developed. The measurement apparatus and its principle of operation are introduced in detail. Real-time image of work piece and wheel in the grinding zone is gathered by CCD camera installed in the grinder. For the purpose of increasing the measurement precision, a new edge detection approach combining Zernike moments operator with Prewitt operator is proposed. The edge of the finished work piece is located with sub-pixel level accuracy, and then the machining error of the work piece is calculated on-line by comparing with the theoretical curve of the work piece. An application of its validity and the experimental results are also given. Experimental results demonstrate the proposed measurement method in this paper is effective, and its detection precision and results are reasonable.",2006,0,
1920,1921,Research on the Gear Fault Diagnosis Using Order Envelope Spectrum,"The speed-up and speed-down of the gearbox are non-stationary process and the vibration signal can not be processed by traditional processing method. In order to process the non-stationary vibration signals such as speed-up or speed-down signals effectively, the order envelope analysis technique is presented. This new method combines order tracking technique with envelope spectrum analysis. Firstly, the vibration signal is sampled at constant time increments and then uses software to resample the data at constant angle increments. Therefore, the time domain non-stationary signal is changed into angle domain stationary signal. In the end, the resampled signals are processed by envelope spectrum analysis. The experimental results show that order envelope spectrum analysis can effectively diagnosis the faults of the gear crack.",2009,0,
1921,1922,Errors Estimation Models for Embedded Software Development Projects,"In this study, we analyze and evaluate development data obtained in the course of ""OMRON software Co."", that develops equipments used in financial markets, as well as data obtained during the development of software. Because of this, it is becoming very important for the software- development corporations to find how to develop software efficiently while guaranteeing delivery time and quality, and holding down developing costs. Especially, estimating manpower of new projects and guaranteeing quality of software are important, because the estimation relates to costs and the quality relates to the reliability of corporations. In the field of embedded software, development techniques, management techniques, tools, testing techniques, reusing techniques, real time operating systems and so on have already been studied. However, there is few studies about the relationship between the development scales and the total errors using the data accumulated by past projects. Hence, we establish a model for the first trial to estimate the errors of a new project by using regression analysis.",2007,0,
1922,1923,Ultra-Wideband Bandpass Filter With Improved Upper Stopband Performance Using Defected Ground Structure,"A novel ultra-wideband (UWB) bandpass filter (BPF) with improved upper stopband performance using a defected ground structure (DGS) is presented in this letter. The proposed BPF is composed of seven DGSs that are positioned under the input and output microstrip line and coupled double step impedance resonator (CDSIR). By using CDSIR and open loop defected ground structure (OLDGS), we can achieve UWB BPF characteristics, and by using the conventional CDGSs under the input and output microstrip line, we can improve the upper stopband performance. Simulated and measured results are found in good agreement with each other, showing a wide passband from 3.4 to 10.9 GHz, minimum insertion loss of 0.61 dB at 7.02 GHz, a group delay variation of less than 0.4 ns in the operating band, and a wide upper stopband with more than 30 dB attenuation up to 20 GHz. In addition, the proposed UWB BPF has a compact size (0.27??<i>g</i> ~ 0.29??<i>g</i> , ??<i>g</i> : guided wavelength at the central frequency of 6.85 GHz).",2010,0,
1923,1924,Implementing fault hierarchy to trace failures in home gateways,"A home gateway (HG) acts as border router by selecting the correct route to different appliances using devices address and supervising resource assignment between the local and access networks. From the quality of service standpoint, minimizing the number of failures for this equipment is a way to gain customerpsilas confidence. To reach this goal, when a service terminates abnormally, the overall failure cause and impact need to be analyzed and quantified. This paper describes a fault hierarchy modeling for HGs: starting from test and field failure data, we analyzed the different failures gravity, their originpsilas correlation and their impact on the overall service architectural elements. The hierarchy interpreting the transition from faults to failures, a designer, or tester, can thus predict the consequences of a particular fault from this model. Our study yielded some test sets that help to trace potential failures, and to enhance the design specifications.",2008,0,
1924,1925,An in-site system based-on ARM of faults diagnostic with the amplitude recovery method,"A new harmonic analysis method - the amplitude recovery method - is addressed and is used in the in-site faults diagnostic in induction motors. To practice this method, an in-site and portable system of faults diagnostic in induction motors on ARM (advanced RISC machines) is also addressed. S3C2410 ARM is chosen as the core of this system. Its faster and multi-channel ADC subsystem and small size can meet the needs of the real-time and in-situation test on induction motors. With the help of the PC and the current spectrum analysis, this ARM system can be used to diagnose the faults in induction motors. Even better, having the aid of the small-sized PC, Vortex86-6082, a reliable and portable system can be realized to meet the purpose of testing and diagnosing the faults of induction motors in-situation.",2008,0,
1925,1926,Semiconductor production schedule check and correction technique through mobile agent system,"A novel agent system that can check and correct machine schedules in semiconductor production was proposed The system consists of stationary machine agents that control their machine schedules, mobile agents that move between the machines, machine schedule files described in XML, and a time buffer stage where lots wait to keep the machine schedules. The real-time scheduling system proceeds according to the rules of mobile agent generation, machine scheduling, and machine and mobile agents collaboration. The system was confirmed through the computer simulation, supposing a small-scaled semiconductor production line",2005,0,
1926,1927,On the Error-Free Realization of a Scaled DCT Algorithm and Its VLSI Implementation,"This brief is concerned with the efficient and error-free implementation of the order-8 Linzer-Feig (L-F) scaled discrete cosine transform (sDCT). We present a novel 3-D algebraic integer encoding scheme which maps the transform basis functions (transcendental functions such as cosine and tangent) with integer values, so that the quantization errors can be minimized and the cross-multiplications (in the signal path) can be avoided. This scheme also allows the separable computation of a 2-D DCT in multiplication-free, fast, and efficient architectures with a process rate of 80 mega-samples/sec. The proposed scheme also reduces the latency and the power consumption compared to previously employed designs for DCT implementations.",2007,0,
1927,1928,Realize a self-recovering function during phase-sequence fault of a digital servo system,"As for a servo control system of the permanent magnet synchronous motor (PMSM), this paper analyzes fault causation of motor start-up that is on account of mistaken connection with motor phase-sequence, and kinds of faults have been classified in the paper. In addition, a method that can realize motor rotor oriented control using TMS320F240 digital signal processor (DSP) is proposed. With the motor rotor position can be acquired by an incremental photo-electricity encoder, the connected sequence of motor phases can be judged by using stator voltage vector oriented control technology, also it can realize self-recovering function by adjusting output phase-sequence of the controller by programming. Experiment results verify that motor can self-recover fast when phase-sequence faults happened and can start rapidly, and the proposed method can be used in servo control system.",2006,0,
1928,1929,Analysis of Packet Loss for Compressed Video: Effect of Burst Losses and Correlation Between Error Frames,"Video communication is often afflicted by various forms of losses, such as packet loss over the Internet. This paper examines the question of whether the packet loss pattern, and in particular, the burst length, is important for accurately estimating the expected mean-squared error distortion resulting from packet loss of compressed video. We focus on the challenging case of low-bit-rate video where each P-frame typically fits within a single packet. Specifically, we: 1) verify that the loss pattern does have a significant effect on the resulting distortion; 2) explain why a loss pattern, for example a burst loss, generally produces a larger distortion than an equal number of isolated losses; and 3) propose a model that accurately estimates the expected distortion by explicitly accounting for the loss pattern, inter-frame error propagation, and the correlation between error frames. The accuracy of the proposed model is validated with H.264/AVC coded video and previous frame concealment, where for most sequences the total distortion is predicted to within plusmn0.3 dB for burst loss of length two packets, as compared to prior models which underestimate the distortion by about 1.5 dB. Furthermore, as the burst length increases, our prediction is within plusmn0.7 dB, while prior models degrade and underestimate the distortion by over 3 dB. The proposed model works well for video-telephony-type of sequences with low to medium motion. We also present a simple illustrative example, of how knowledge of the effect of burst loss can be used to adapt the schedule of video streaming to provide improved performance for a burst loss channel, without requiring an increase in bit rate.",2008,0,
1929,1930,Rapid: Identifying Bug Signatures to Support Debugging Activities,"Most existing fault-localization techniques focus on identifying and reporting single statements that may contain a fault. Even in cases where a fault involves a single statement, it is generally hard to understand the fault by looking at that statement in isolation. Faults typically manifest themselves in a specific context, and knowing that context is necessary to diagnose and correct the fault. In this paper, we present a novel fault-localization technique that identifies sequences of statements that lead to a failure. The technique works by analyzing partial execution traces corresponding to failing executions and identifying common segments in these traces, incrementally. Our approach provides developers a context that is likely to result in a more directed approach to fault understanding and a lower overall cost for debugging.",2008,0,
1930,1931,4K-1 Volume Visualization and Error Analysis Using 3D ARFI Imaging Data,The utility and accuracy of volume visualization using 3D ARFI image data was investigated. Volumes of ARFI data were collected of custom tissue-mimicking phantoms by using a translation stage to increment the position of the imaging transducer through the elevation plane. Both vessel phantoms with stiff or soft plaques and a liver phantom with a stiff inclusion were used in the study. Simple displacement threshold techniques were used to segment inclusions from phantom background materials. ARFI-based measurements of inclusion volume were accurate to within 4% error for the liver phantom and 10.6% error for the vessel phantoms,2006,0,
1931,1932,On the practicality of using intrinsic reconfiguration for fault recovery,"Evolvable hardware (EHW) combines the powerful search capability of evolutionary algorithms with the flexibility of reprogrammable devices, thereby providing a natural framework for reconfiguration. This framework has generated an interest in using EHW for fault-tolerant systems because reconfiguration can effectively deal with hardware faults whenever it is impossible to provide spares. But systems cannot tolerate faults indefinitely, which means reconfiguration does have a deadline. The focus of previous EHW research relating to fault-tolerance has been primarily restricted to restoring functionality, with no real consideration of time constraints. In this paper, we are concerned with EHW performing reconfiguration under deadline constraints. In particular, we investigate reconfigurable hardware that undergoes intrinsic evolution. We show that fault recovery done by intrinsic reconfiguration has some restrictions, which designers cannot ignore.",2005,0,
1932,1933,Fault recovery for a distributed SP-based delay constrained multicast routing algorithm,"This paper proposes a new distributed shortest path (SP) based delay constrained multicast routing algorithm which is capable of constructing a delay constrained multicast tree when node,failures occur during the tree construction period and recovering from any node failure in a multicast tree during the on-going multicast session without interrupting the running traffic on the unaffected portion of the tree. The proposed algorithm performs the failure recovery efficiently, which gives better performance in terms of the number of exchanged messages and the convergence time than the existing distributed SP-based delay constrained multicast routing algorithms in a network where node failures occur.",2002,0,
1933,1934,Detection and Visualization of Defects in 3D Unstructured Models of Nematic Liquid Crystals,"A method for the semi-automatic detection and visualization of defects in models of nematic liquid crystals (NLCs) is introduced; this method is suitable for unstructured models, a previously unsolved problem. The detected defects - also known as disclinations - are regions were the alignment of the liquid crystal rapidly changes over space; these defects play a large role in the physical behavior of the NLC substrate. Defect detection is based upon a measure of total angular change of crystal orientation (the director) over a node neighborhood via the use of a nearest neighbor path. Visualizations based upon the detection algorithm clearly identify complete defect regions as opposed to incomplete visual descriptions provided by cutting-plane and isosurface approaches. The introduced techniques are currently in use by scientists studying the dynamics of defect change",2006,0,
1934,1935,The recovery language approach for software-implemented fault tolerance,"We describe a novel approach for software-implemented fault tolerance that separates error detection from error recovery and offers a distinct programming and processing context for the latter. This allows the application developer to address separately the non-functional aspects of error recovery from those pertaining to the functional behaviour that the user application is supposed to have in the absence of faults. We conjecture that this way only a limited amount of non-functional code intrusion affects the user application, while the bulk of the strategy to cope with errors is to be expressed by the user in a recovery script, conceptually as well physically distinct from the functional application layer. Such script is to be written in what we call a recovery language, i.e. a specialised linguistic framework devoted to the management of the fault tolerance strategies that allows to express scenarios of isolation, reconfiguration, and recovery. These are to be executed on meta-entities of the application with physical or logical counterparts (processing nodes, tasks, or user-defined groups of tasks). The developer is therefore made able to modify the fault tolerance strategy with only a few or no modifications in the application part, or vice-versa, tackling more easily and effectively any of these two fronts. This can result in a better maintainability of the target fault-tolerant application and in support for reaching portability of the service while moving the application to different unfavourable environments. The paper positions and discusses the recovery language approach and a prototypal implementation for embedded applications developed within project TIRAN on a number of distributed platforms",2001,0,
1935,1936,Analyzing and Extending MUMCUT for Fault-based Testing of General Boolean Expressions,"Boolean expressions are widely used to model decisions or conditions of a specification or source program. The MUMCUT, which is designed to detect seven common faults where Boolean expressions under test are assumed to be in Irredundant Disjunctive Normal Form (IDNF), is an efficient fault-based test case selection strategy in terms of the fault-detection capacity and the size of selected test suite. Following up our previous work that reported the fault-detection capacity of the MUMCUT when it is applied to general form Boolean expressions, in this paper we present the characteristic of the types of single faults committed in general Boolean expressions that a MUMCUT test suite fails to detect, analyze the certainty why a MUMCUT test suite fails to detect these types of undetected faults, and provide some extensions to enhance the detection capacity of the MUMCUT for these types of undetected faults.",2006,0,
1936,1937,Resistive superconducting fault current limiter simulation and design,"Resistive superconducting fault current limiters (SFCL) are characterized by very fast transition to the resistive state, automatic recovery after current fault and simple structure. SFCL simulations include thermal and electrical nonlinear model analysis. In this paper few mathematical models for the effective analysis of thermo-electrical processes in resistive superconducting fault current limiter are presented. Basing on the developed models an effective algorithm for optimization model was established.",2008,0,
1937,1938,Approximating Deployment Metrics to Predict Field Defects and Plan Corrective Maintenance Activities,"Corrective maintenance activities are a common cause of schedule delays in software development projects. Organizations frequently fail to properly plan the effort required to fix field defects. This study aims to provide relevant guidance to software development organizations on planning for these corrective maintenance activities by correlating metrics that are available prior to release with parameters of the selected software reliability model that has historically best fit the product's field defect data. Many organizations do not have adequate historical data, especially historical deployment and field usage information. The study identifies a set of metrics calculable from available data to approximate these missing predictor categories. Two key metrics estimable prior to release surfaced with potentially useful correlations, (1) the number of periods until the next release and (2) the peak deployment percentage. Finally, these metrics were used in a case study to plan corrective maintenance efforts on current development releases.",2009,0,
1938,1939,3D Visualization of Stratum with Faults Based on VTK,"3D stratum can become one useful auxiliary geological tool. Based on this tool, geologist can analyze stratum more accurately and scientific. VTK is an object-oriented visualization class library. 3D stratum is interpolated with Kriging method. Faults are common geological phenomenon. In order to visualize stratum, with Faults, one new interpolation method is proposed.",2009,0,
1939,1940,Research of Refractive Correction Forecasting Software Technology Based on WEB Database,"DTK (Diode Thermal Keratoplasty) technology is a new type of laser refractive technology. This thesis adopts web-based database for the multi-layer architecture mode, ADO.NET technology for the realization of data access, and B/S (Browser /Server) mode for the resolution of network method to accumulate large samples data and improve forecast accuracy of refractive laser correction. The thesis also develops refractive correction prediction software based on Web database to fulfill the publishing, updating of and access to information through web so that a more flexible device management tool can adapt to the information age, and it gives the achievement results, including data management interface, the results of data analysis and so on. Refractive correction prediction software based on Web database has realized data accumulation of laser refractive and improvement of prediction accuracy.",2009,0,
1940,1941,Induction motor stator faults diagnosis by a current Concordia pattern-based fuzzy decision system,"This paper deals with the problem of detection and diagnosis of induction motor faults. Using the fuzzy logic strategy, a better understanding of heuristics underlying the motor faults detection and diagnosis process can be achieved. The proposed fuzzy approach is based on the stator current Concordia patterns. Induction motor stator currents are measured, recorded, and used for Concordia patterns computation under different operating conditions, particularly for different load levels. Experimental results are presented in terms of accuracy in the detection of motor faults and knowledge extraction feasibility. The preliminary results show that the proposed fuzzy approach can be used for accurate stator fault diagnosis if the input data are processed in an advantageous way, which is the case of the Concordia patterns.",2003,0,
1941,1942,Chip-level soft error estimation method,This paper gives a review of considerations necessary for the prediction of soft error rates (SERs) for microprocessor designs. It summarizes the physics and silicon process dependencies of soft error mechanisms and describes the determination of SERs for basic circuit types. It reviews the impact of logical and architectural filtering on SER calculations and focuses on the structural filtering of soft radiation events by nodal timing mechanisms.,2005,0,
1942,1943,Geometrical model to drive vision systems with error propagation,"Localization with respect to a reference model is a key feature for mobile robots. Urban environment offers numerous landmarks that can be used for the localization process. This paper deals with the use of an environment model stored in a geographic information system, to drive a vision system i.e. highlights what to look for and where to look for. This task is achieved by propagating uncertainties along the image acquisition system to highlight some region of interest in the image.",2004,0,
1943,1944,Fault detection and control in superheater using electronic simulator,This paper presents the design of an electronic simulator in order to reproduce the real function of the superheater in normal condition and in fault condition. With this electronic simulator there was studied the possibility to control the superheater when the perturbations action and the control of the system structure which will be implemented on the real system.,2010,0,
1944,1945,Fault tolerant strategies under open phase fault for doubly salient electro-magnet motor drives,"In this paper, the fault tolerant system for doubly salient electro-magnet motor(DSEM) drives have been proposed to maintain the control performance under open phase faults of inverter. The proposed fault tolerant system provides compensation for open-circuit faults in power inverter. The fault identification is quickly achieved by phase current and voltage changes between lower legs and middle point of DC-link. The drive system after fault identification is reconstructed by the FSTP(four-switch three-phase) topology connecting a faulty leg to the middle point of DC-link using bidirectional switches. The fault tolerant system quickly recovers the control performance by short detecting time. Experiments confirm the feasibility of the proposed fault tolerant system.",2007,0,
1945,1946,Intelligent fault diagnosis system based on UML,"In this paper, it is united that the ordinary software project method and the object-oriented method. The object-oriented analysis, the object-oriented design and the object-oriented modeling in the intelligent fault diagnosis system with UML is introduced, which decreases the complexity of the intelligent fault diagnosis system that is more manageable. The diagnostic reasoning adopts the technique of expert system and reasoning under uncertainty. The intelligent fault diagnosis system is a very important part of the power station simulation system. With the data gathered from the devices or the converted parameters, it can find the devices which are out of order by reasoning. The diagnostic reasoning adopts the technique of expert system and reasoning under uncertainty.",2009,0,
1946,1947,Sensor placement and fault detection using an efficient fuzzy feature selection approach,"Process monitoring and fault diagnosis are of great importance for operation safety and efficiency of complex industrial plants. The present article proposes a novel methodology to address the sensor location problem for fault detection. Firstly, all the process situations are identified based on a fuzzy learning algorithm using measurements generated from the whole available set of sensors. Then, a fuzzy feature selection approach is used to select the optimal number of sensors that characterize accurately the set of process situations (abnormal and normal). This method optimizes the performance of the learning algorithm within a membership margin framework, and thereby, it is capable to address correlation and redundancy issues. A behavioral pattern of the process is constructed with the selected sensors and is used to associate new online observations to previously characterized process situations. The proposed strategy has been applied for fault diagnosis to a pharmaceutical synthesis carried out in a new intensified heat-exchanger reactor.",2010,0,
1947,1948,Fault-Tolerant Control for SSSC Using Neural Networks and PSO,"This paper presents a fault-tolerant indirect adaptive neuro-controller (FTNC) for controlling a static synchronous series compensator (SSSC), which is connected to a power network. The FTNC consists of a sensor evaluation and restoration scheme (SERS), a radial basis function neuro-identifier (RBFNI) and a radial basis function neuro-controller (RBFNC). The SERS is designed using the auto-associative neural networks (auto-encoder) and the particle swarm optimizer (PSO). This FTNC is able to provide efficient control to the SSSC when single or multiple crucial sensor measurements are unavailable. The validity of the proposed FTNC model is examined by simulations in PSCAD/EMTDC environment",2006,0,
1948,1949,Kalman Predictive Redundancy System for Fault Tolerance of Safety-Critical Systems,"The dependence of intelligent vehicles on electronic devices is rapidly increasing the concern over fault tolerance due to safety issues. For example, an x-by-wire system, such as electromechanical brake system in which rigid mechanical components are replaced with dynamically configurable electronic elements, should be fault-tolerant because a critical failure could arise without warning. Therefore, in order to guarantee the reliability of safety-critical systems, fault-tolerant functions have been studied in detail. This paper presents a Kalman predictive redundancy system with a fault-detection algorithm using the Kalman filter that can remove the effect of faults. This paper also describes the detailed implementation of such a system using an embedded microcontroller to demonstrate that the Kalman predictive redundancy system outperforms well-known average and median voters. The experimental results show that the Kalman predictive redundancy system can ensure the fault-tolerance of safety-critical systems such as x-by-wire systems.",2010,0,
1949,1950,Fault Location and Voltage Estimation in Transmission Systems by Evolutionary Algorithms,"This paper aims at presenting the development and implementation of an approach to deal with fault location in transmission systems by Evolutionary Algorithms. The algorithm allows for the estimation of voltage sags and swells in non-monitored buses as well. An Evolutionary Strategy was applied to address the optimization problem, from the generation of a population of individuals that represent possible solutions to the problem to the evolutionary operators such as selection, mutation and crossover. Some variants of the basic approach were also considered in such a way to determine the fault location with better performance. A case study, based on the 30 bus IEEE study system, demonstrates the potential of the proposed methodology.",2009,0,
1950,1951,A Rule-Based Expert System for Automatic Error Interpretation Within Ultrasonic Flaw Model Simulators,"This paper describes the development of a novel rule-based expert system application that automatically runs a set of theoretical models used to simulate test procedures for ultrasonic testing methods in nondestructive evaluation and interprets their results. Theoretical modeling is an essential tool in verifying that the test procedures are fit for their intended purpose of defect detection. Four validated models are available to simulate theoretical ultrasonic flaw modeling scenarios. Under certain conditions, the models may break down and produce warning flags indicating that results may not be considered accurate. A considerable level of expertise in the theoretical background of the models is required to interpret these flags. The expert system addresses any warning flags encountered by adjusting the original simulation parameters and rerunning the test in order to produce a valid simulation. Warning flags are addressed by a rule file, which contains formal rules developed from knowledge-elicitation sessions with suitably qualified engineers. The rule file represents the action an engineer would adopt to counter highlighted warning flags. A description of the system and rule-base design is given as well as how the system and its performance were validated",2006,0,
1951,1952,"Yield Improvement, Fault-Tolerance to the Rescue?","With the technology entering the nano dimension, manufacturing processes are less and less reliable, thus drastically impacting the yield. A possible solution to alleviate this problem in the future could consist in using fault tolerant architectures to tolerate manufacturing defects. In this paper, we analyze the conditions that make the use of a classical triple modular redundancy (TMR) architecture interesting for a yield improvement purpose.",2008,0,
1952,1953,An Adaptive and Flexible Fault Tolerance Mechanism Designed on Multi-behavior Agents for Wireless Sensor/Actuator Network,"In the last few years, WSN has been object of an intense research activity that has determined an important improvement by technologic and computation point of view both. The notable level got and the increasing request of applications designed over Sensor Networks make WSN commercial diffusion next to be a fact. One of key issues for commercial diffusion of WSN is related to the robustness of architectures. An adaptive and flexible fault tolerant mechanism for WSN is proposed in the paper. Considering the tradeoffs between robustness and energy efficiency as central issue, a programming model based on multi-behavior agents that can guarantee an efficient, dynamic and extendible implementation is proposed too.",2007,0,
1953,1954,Probability of error metrics for best-basis selection,In this paper we derive a metric for selecting the best-basis of a wavelet packet used to compress a classifier database. The metric will choose a basis that minimizes the probability of error given that the database must be represented with a finite number of bits. We also solve the corresponding bit allocation problem.,2000,0,
1954,1955,Error Corrections in Outdoor Cylindrical near Field Radar Antenna Measurement System,"An outdoor cylindrical near field antenna measurement system was designed and fabricated in Guadalajara (Spain) for measuring large L-band RADAR antennas. This design was presented in EuCap 2006 by Martin, F., et al. (2006), where the theoretical error analysis and the main description of the complete system were presented. This paper presents the solutions adopted for solving some of the problems presented in the outdoor design: the first of them due to temperature variations, the second one to the effect of the wind and the third one to the reflections in the ground.",2007,0,
1955,1956,Dynamic Field Estimation Using Wireless Sensor Networks: Tradeoffs Between Estimation Error and Communication Cost,"This paper concerns the problem of estimating a spatially distributed, time-varying random field from noisy measurements collected by a wireless sensor network. When the field dynamics are described by a linear, lumped-parameter model, the classical solution is the Kalman-Bucy filter (KBF). Bandwidth and energy constraints can make it impractical to use all sensors to estimate the field at specific locations. Using graph-theoretic techniques, we show how reduced-order KBFs can be constructed that use only a subset of the sensors, thereby reducing energy consumption. This can lead to degraded performance, however, in terms of the root mean squared (RMS) estimation error. Efficient methods are presented to apply Pareto optimality to evaluate the tradeoffs between communication costs and RMS estimation error to select the best reduced-order KBF. The approach is illustrated with simulation results.",2009,0,
1956,1957,Consider of fault propagation in architecture-based software reliability analysis,"Software reliability models are used to estimation and prediction of software reliability. Existing models either use black-box approach that based on test data during software test phase or white-box approach that based on software architecture and individual component reliability, which is more suited to assess the reliability of modern software system. However, most of the reliability models based on architecture assumed that a failure occurring within one component will not cause any other component to fail, which is inconsistent with the facts. This paper introduces a reliability model and a reliability analysis technique for architecture-based reliability evaluation. Our approach extend existing reliability model by considering fault propagation. We believe that this model can be used to effectively improve software quality.",2009,0,
1957,1958,Multiple Fault Models for Timed FSMs,"An implementation under test (IUT) can be formally described using finite-state machines (FSMs). Due to the presence of inherent timing constraints and variables in a communication protocol, an IUT is modeled more accurately by using extended finite-state machines (EFSMs). However, infeasible paths due to the conflicts among timing condition and action variables of EFSMs can complicate the test generation process. The fault detection capability of the graph augmentation method given in M. U. Uyar et al. (2005) and M. A. Fecko et al. (2000) are analyzed in the presence of multiple timing faults. The complexity increases with the consideration of the concurrent running and expiring of timers in a protocol. It is proven that, by using our graph augmentation models, a faulty IUT will be detected for the multiple occurrences of pairwise combinations of a class of timing faults",2006,0,
1958,1959,Study of hybrid intelligent fault diagnosis,"A hybrid intelligent fault diagnosis method is presented for the diversity, uncertainty and complexity of device faults. This method integrates respective advantages of fault tree, fuzzy theory, neural networks and genetic algorithms to form a hybrid approach and is applied to fault diagnosis of fan. Experiments show that this method is simple and effective. It can also be applied to other fault diagnosis of complex systems and has certain portability.",2010,0,
1959,1960,Defect detection and identification in textile fabrics using Multi Resolution Combined Statistical and Spatial Frequency Method,"In textile industry, reliable and accurate quality control and inspection becomes an important element. Presently, this is still accomplished by human experience, which is more time consuming and is also prone to errors. Hence automated visual inspection systems become mandatory in textile industries. This Paper presents a novel algorithm of fabric defect detection by making use of multi resolution combined statistical and spatial frequency method. Defect detection consists of two phases, first is the training and next is the testing phase. In the training phase, the reference fabric images are cropped into non-overlapping sub-windows. By applying MRCSF the features of the textile fabrics are extracted and stored in the database. During the testing phase the same procedure is applied for test fabric and the features are compared with database information. Based on the comparison results, each sub-window is categorized as defective or non-defective. The classification rate obtained by the process of simulation using MATLAB was found to be 99%.",2010,0,
1960,1961,Modeling of the distance error for indoor geolocation,"This paper uses the results of a calibrated ray tracing software in a sample office environment to analyze and model the distance error measured from the estimated time of arrival (TOA) of the direct line-of-sight (LOS) path in a typical indoor environment. First, we analyze the effect of bandwidth on the measured distance error using TOA and then, we propose a model for simulation of the distance error in LOS and OLOS indoor areas.",2003,0,
1961,1962,Accelerating error correction in high-throughput short-read DNA sequencing data with CUDA,"Emerging DNA sequencing technologies open up exciting new opportunities for genome sequencing by generating read data with a massive throughput. However, produced reads are significantly shorter and more error-prone compared to the traditional Sanger shotgun sequencing method. This poses challenges for de-novo DNA fragment assembly algorithms in terms of both accuracy (to deal with short, error-prone reads) and scalability (to deal with very large input data sets). In this paper we present a scalable parallel algorithm for correcting sequencing errors in high-throughput short-read data. It is based on spectral alignment and uses the CUDA programming model. Our computational experiments on a GTX 280 GPU show runtime savings between 10 and 19 times (for different error-rates using simulated datasets as well as real Solexa/Illumina datasets).",2009,0,
1962,1963,Symbol Error Rate of Wireless Multiuser Relay Networks in Nakagami-m Fading Channels,"This paper analyzes the performance of wireless multiuser relay networks (MRN) in unbalanced Nakagami-m fading channels. For such networks, we consider a single channel state information (CSI)-based amplify-and-forward (AaF) relay. We derive a new exact expression for the symbol error rate (SER), which is in closed-form and applies to a wide variety of modulations. Subsequently we present a simplified asymptotic expression for the SER in the high signal-to-noise ratio (SNR) regime to identify key performance metrics such as the diversity order and array gain. Our asymptotic result explicitly reveals the direct relationship between the diversity order and both the number of destinations and the per-hop fading parameters. Moreover, we highlight the effect of the number of destinations on the optimal relay location aiming at minimizing the SER. The validity of our analysis is substantiated by numerical results.",2010,0,
1963,1964,Fault tolerant control for unstable systems: a linear time varying approach,"In (passive) fault tolerant control design, the objective is to find a fixed compensator, which maintains a suitable performance - or at least stability - in the event that a fault should occur. A major theoretical obstacle to obtain this objective, is that even if the system models corresponding to the occurrence of various faults are simultaneously stabilizable by a linear, time-invariant compensator, this compensator might have to be of a very high order, as shown in a recent publication. In this paper, we propose a design procedure for a time-varying compensator, which overcomes the obstacle for any finite number of faults with a controller order of no more than the plant order. The performance of this compensator might be poor, but a heuristic procedure for improving the performance is also shown, and an example demonstrates that this improvement can be truly significant.",2004,0,
1964,1965,The ASDMCon Project: The Challenge of Detecting Defects on Construction Sites,"Techniques for three dimensional (3D) imaging and analysis of as-built conditions of buildings are gaining acceptance in the architecture, engineering, and construction (AEC) community. Early detection of defects on construction sites is one domain where these techniques have the potential to revolutionize an industry, since construction defects can consume a significant portion of a project's budget. The ASDMCon project is developing methods to aid site managers in detecting and managing construction defects using 3D imaging and other advanced sensor technologies. This paper presents an overview of the project, its 4D visualization environment, and the 3D segmentation and recognition strategies that are being employed to automate defect detection.",2006,0,
1965,1966,Correction of image coordinate using landmark for setting error,"In recently years, the production method has changed from the diversification of a sense of values from mass production to diverse-types-and-small-quantity production on the production site. Therefore, a productive system that can correspond to changes in the environment is required. In a productive system that performs handling tasks, we use a hand-eye system with a camera and a manipulator and perform hand-eye calibration from two dimension images obtained by camera to the manipulator coordinates a three dimension system of in the hand-eye system. However, the camera and the manipulator positions in the previous system changed, and great error from the position, which performed the calibration, was caused even when the error was minute. Therefore, in this hand-eye system, changing this production system is difficult or re-calibration is needed. The projection matrix must be calculated so that the image coordinates obtained with the camera and the manipulator coordinates of three dimensions correspond again in the re-calibration. However, calculating the projection matrix requires great care. We propose a technique that simply calculates the manipulator coordinates of three dimensions from image coordinates without calculating the projection matrix again by correcting the image coordinates. Our experiment results show that the image coordinates can be corrected without calculating the projection matrix again using a landmark. In addition, a three-dimensional position was corrected simply, and we were able to handle the object.",2009,0,
1966,1967,Achieving Fault Tolerance on Grids with the CPPC Framework and the GridWay Metascheduler,"Grids have brought a significant increase in the number of available resources that can be provided to applications. In the last decade, an important effort has been made to develop middleware that provides grids with functionalities related to application execution. However, support for fault-tolerant executions is either lacking or limited. This paper presents an experience to endow with fault tolerance support parallel executions on grids through the integration of CPPC, a check pointing tool for parallel applications, and Grid Way, a well-known met scheduler provided with the Globus Toolkit. Since both tools are not immediately compatible, a new architecture, called CPPC-GW, has been designed and implemented to allow for the transparent execution of CPPC applications through Grid Way. The performance of the solution has been evaluated using the NAS Parallel Benchmarks. Detailed experimental results show the low overhead of the approach.",2010,0,
1967,1968,"VRL, a Novel Environment for Control Engineering Practicing: An Application to a Fault Tolerant Control System",Virtual remote laboratory (VRL) is a powerful tool for an effective active learning in control engineering formation because it gives the opportunity of testing remotely control laws both by simulations within a virtual reality framework and by remote experiments. In this paper the virtual environment VRL is described and an application of a fault tolerant control law on an inverted pendulum is shown,2006,0,
1968,1969,Efficient techniques for reducing error latency in on-line periodic built-in self-test,"Due to the high cost of failure, verification and testing now account for more than half of the total lifetime cost of an integrated circuit (IC). Increasing emphasis needs to be placed on finding design errors and physical faults as early as possible in the life of a digital system, new algorithms need to be devised to create tests for logic circuits, and more attention should be paid to synthesis for test and on-line testing. On-line testing requires embedding logic that continuously checks the system for correct operation. Built-in self-test (BIST) is a technique that modifies the IC by embedding test mechanisms directly into it. BIST is often used to detect faults before the system is shipped and is potentially a very efficient way to implement on-line testing. Error latency is the elapsed time between the activation of an error and its detection. Reducing the error latency is often considered a primary goal in on-line testing.",2010,0,
1969,1970,Systematic approach to error budget analysis for integrated sensor systems,"Multi-sensor integration (fusion) has the potential to provide increased detection volume, complementary coverage, improved track accuracy and track continuity, and is a key feature being investigated for TIS-B and proposed for the overall Safe Flight 21 architecture. Tracking accuracy is a critical characteristic of any proposed system concept and is the focus of the analysis discussed in this paper. Methods for determining whether or not a particular system concept can produce tracking accuracies sufficient to meet the requirements for the anticipated applications must be developed. This paper describes a high level, end-to-end error budget analysis approach for statistically quantifying tracking errors at the end user. The approach uses Monte Carlo techniques to provide quantifiable statistical results that can be used for performance prediction, requirement refinement, and error budget allocation. The paper also discusses the need to carefully develop and articulate the track accuracy requirements for applications so that a system concept evaluation can be completed. An example analysis for the SF-21 Enhanced Visual Acquisition application using TIS-B illustrates this approach and how it can be extended to the development of other SF-21 applications.",2002,0,
1970,1971,Evaluating Throughput of a Wormhole-Switched Routing Algorithm in NoC with Faults,"A famous wormhole-switched routing algorithm for mesh interconnection network called f-cube3 uses three virtual channels to pass faulty blocks, while only one virtual channel is used when a message does not encounter by fault. Routing with faults usually uses virtual channels to conquer faulty regions. One of the key issues in the design of Network-on-Chips (NoC) is the development of a well-organized communication system to provide high throughput interconnection network. We have evaluated if-cube3 - a fault-tolerant routing algorithm based on f-cube3 - for increasing the throughput of the network. Moreover, simulation of both f-cube3 and if-cube3 algorithm for the same conditions presented. Modifications of the use of virtual channels per each physical link without adding new extra virtual channel illustrated by results obtained from simulation. As the simulation results show, if-cube3 has a higher performance than f-cube3. The results also show that if-cube3 has less exist packets in network with improved performance at high traffic load in Network-on-Chip.",2009,0,
1971,1972,Research on fault detection for satellite attitude control systems based on sliding mode observers,"This paper investigates the design and application of a sliding mode observer (SMO) strategy for fault detection problem for satellite attitude control systems. A particular design of sliding mode observer is presented for which the parameters can be obtained by a linear change of coordinates under some conditions. Instead of generating residuals for other observer-based fault detection methods, the sliding mode observer discussed is designed based on the so-called equivalent output estimation error injection concept to reconstruct the faults. Both the actuator and sensor fault detection and reconstruct for satellite attitude control systems are taken into account. A mathematical simulation is given to illustrate the effectiveness of the proposed approach.",2009,0,
1972,1973,Performance analysis of HTS fault current limiter combined with a ZnO varistor against transient overvoltages,"The superconducting technology nowadays is an innovation in the field of the electrical power supply. Using HTS in fault current limiter (FCL) represents a new category of electrical equipment and a novel configuration of electrical network. In fact the high temperature superconductivity (HTS) makes a relatively sharp transition to a highly resistive state when the critical current density is exceeded, and this effect has suggested their use for resistive fault current limiters. FCL is an important element in order to reduce system impedance, which permits an increase of power transmission. Furthermore, it allows additional meshing of a power system, which increases the power availability. The most significant features of the SCFCLs requested from the power system operating conditions are limiting impedance, a trigger current level and a recovery time. In this paper, a model of HTS FCL using ZnO varistor is proposed. The effectiveness of this model is investigated through results of simulation in MATLAB/Simulink software. In addition it is illustrated the limiting feature of HTS FCL and protected role of ZnO varistor for transient overvoltage",2006,0,
1973,1974,Implementation of a sensor fault reconstruction scheme on an inverted pendulum,"This paper presents a robust sensor fault reconstruction scheme, using an unknown input observer, applied to an inverted pendulum. The scheme is adapted from existing work in the literature. A suitable interface between the pendulum and a computer enabled the application. Very good results were obtained.",2004,0,
1974,1975,SFIDA: a software implemented fault injection tool for distributed dependable applications,"SFIDA, a new software implemented fault injection tool is described in this paper which can be used to test for dependability of distributed applications on the Linux platform. This has been integrated with a general debugging tool so that it has the functionality of both debugging and fault injection. It is assumed that the target application is composed of multiple components (programs) which cooperate for the result, and that its successful completeness is determined by a failure condition. SFIDA is capable of injecting transient and permanent hardware faults with emulating the error state incurred by hardware faults in the runtime environment of each program. It can also collect test results from all components and determine the soundness of the final result based on the failure condition.",2000,0,
1975,1976,Error Reduction Based on Error Categorization in Arabic Handwritten Numeral Recognition,"In practical applications, errors should not be treated equally, but conditionally. In this paper, errors are categorized based on different costs in misclassification. Accordingly, the characteristics of the error categorization and the corresponding strategies for correcting them are proposed. Verification based on Arabic Handwritten Numeral Recognition is considered as one application to utilize these definitions and strategies. As a result, the recognition results improved from 98.47% to 99.05%, and errors were significantly reduced by over 35% compared to previous studies. When a rejection measurement was applied, and the rejection threshold was adjusted to maintain the same error rate, both the recognition rate and reliability increased from 96.98% to 97.89% and from 99.08% to 99.28%, respectively.",2010,0,
1976,1977,EVAL: Utilizing processors with variation-induced timing errors,"Parameter variation in integrated circuits causes sections of a chip to be slower than others. If, to prevent any resulting timing errors, we design processors for worst-case parameter values, we may lose substantial performance. An alternate approach explored in this paper is to design for closer to nominal values, and provide some transistor budget to tolerate unavoidable variation-induced errors. To assess this approach, this paper first presents a novel framework that shows how microarchitecture techniques can trade off variation-induced errors for power and processor frequency. Then, the paper introduces an effective technique to maximize performance and minimize power in the presence of variation-induced errors, namely High-Dimensional dynamic adaptation. For efficiency, the technique is implemented using a machine-learning algorithm. The results show that our best configuration increases processor frequency by 56% on average, allowing the processor to cycle 21% faster than without variation. Processor performance increases by 40% on average, resulting in a performance that is 14% higher than without variation - at only a 10.6% area cost.",2008,0,
1977,1978,A new three-phase inverter power-factor correction (PFC) scheme using field programmable gate array,"In this paper, a new three-phase Inverter power-factor correction PFC scheme is proposed using field programmable gate array (FPGA) technology all the functions can be implemented in a single chip. The implementation tool fit the entered design into the target device (XC4008E). Design verification includes functional simulator, in circuit testing and timing simulation the main function is to verify the proper operation of the designed circuit. It will compile a design file into a configuration file that is optimized in terms of use of logic gates and interconnections for the target device. Power factor measures how effective electrical power is being used. A high power factor means that electrical power is being utilized effectively, while a low power factor indicates poor utilization of electrical power. The simplest way to improve power factor is to add power factor correction capacitors to your plant distribution system. In this paper a new technique is proposed to improve the power factor, experimental results are presented to show the effectiveness of the proposed technique.",2002,0,
1978,1979,An energy flow approach to fault propagation analysis,"A complex system such as an aircraft engine, is composed of several components or subsystems which interact with each other in several ways. These constituent components/subsystems and their interactions make up the whole system. When a fault condition arises in one of the components, not only that component behavior changes but the interaction of that component with the other system constituents may also change. This might result in spreading the effect of that fault to other components in a domino like effect, until the overall system fails. This paper presents a modular methodology to analyze propagation of faults from one subsystem to the other subsystems. The application domain focuses on an aero propulsion system of the turbofan type.",2009,0,
1979,1980,Self-organizing and fault-tolerant behaviors approach in bio-inspired hardware redundant network structures,"It's well-known, biological organisms offer the ability to grow with fault-tolerance and self-organization behaviors. By adapting basic properties and capabilities from nature, scientific approaches have helped researches understand related phenomena and associated with principles to engine complex novel digital systems and improve their capability. Founded by these observations, the paper is focused on modeling and simulation artificial embryonic structures, with the purpose to develop VLSI hardware architectures able to imitate cells or organism operation mode, with similar robustness like their biological equivalents from nature. Self-healing algorithms and artificial immune properties implementation is investigated and experimented on the developed models. The presented theoretical and simulation approaches were tested on a FPGA-based embryonic network architecture (embryonic machine), built with the purpose to implement on silicon fault-tolerant and surviving properties of living organisms.",2010,0,
1980,1981,A Physics-Based Engineering Methodology for Calculating Soft Error Rates of Bulk CMOS and SiGe Heterojunction Bipolar Transistor Integrated Circuits,"This paper describes a new methodology for characterizing the electrical behavior and soft error rate (SER) of CMOS and SiGe HBT integrated circuits that are struck by ions. A typical engineering design problem is to calculate the SER of a critical path that commonly includes several circuits such as an input buffer, several logic gates, logic storage, clock tree circuitry, and an output buffer. Using multiple 3D TCAD simulations to solve this problem is too costly and time-consuming for general engineering use. The new and simple methodology handles the problem with ease by simple SPICE simulations. The methodology accurately predicts the measured threshold linear energy transfer (LET) of a bulk CMOS SRAM. It solves for circuit currents and voltage spikes that are close to those predicted by expensive 3D TCAD simulations. It accurately predicts the measured event cross-section vs. LET curve of an experimental SiGe HBT flip-flop. The experimental cross section vs. frequency behavior and other subtle effects are also accurately predicted.",2010,0,
1981,1982,GIS based multilevel intelligent fault diagnosis system on electric power equipment,"Along with the application which AM/FM/GIS has been used in distribution network strengthening, at present it has become a technique development direction that SCADA should integrate with AM/FM/GIS. In pace with electrified wire netting scale growing immensely, the equipment requirements are increasing and their function is more and more advanced. It can not satisfy the requirements of a practical system to adopt traditional concentrated and single fault diagnosis methods. We designed a distributed multilevel intelligent fault diagnosis method in this paper. First resolve the equipment fault diagnosis of the whole system into a fault diagnosis of each subsystem, then, according to the characteristic of the equipment type, transfer many sorts of fault diagnosis means from the knowledge-base to carry on cooperation fault diagnosis under the control of multilevel intelligent fault diagnosis tactics. Leave the final diagnosis result with a fault database to offer system-making-policy and alarm information.",2004,0,
1982,1983,Data Mining and Analysis of Tree-Caused Faults in Power Distribution Systems,"The reliability and quality of power distribution systems are affected by different distribution faults. Trees are one of the major fault causes. In this paper, four different measures: actual measure, normalized measure, relative measure, and likelihood measure are used to data mine the Duke Energy Distribution Outage Database for meaningful data features and to analyze the characteristics of tree-caused distribution faults. This paper also applies statistical techniques to analyze tree-caused faults with respect to several selected influential factors. The results can be used to assist power distribution engineers to provide a more effective fault restoration system and design a more effective tree-fault prevention strategy",2006,0,
1983,1984,Dynamic displacement control error of multifunction and all-electric rheometer,"A new multi-function and all-electric rheometer (MAR) is designed by the authors. A sine vibration displacement is superimposed on the steady-state displacement of the piston in parallel. In order to study the dynamic displacement control error of MAR, its frequency and amplitude was obtained through decomposing the measured resultant displacement into steady-state displacement and the dynamic displacement, and transforming the time-domain signal into frequency-domain presentation. The results show that both the frequency and amplitude control errors of MAR are small enough for the test requirement. However, the repeatability of measured values of amplitude is not as well as the frequency. And they are always smaller than the set values. The mean values of amplitude relative errors are about 2%, so it may be a useful way to set the amplitude 2% higher than the wanted values. The relative errors of amplitude in 5 Hz are slightly larger than the errors in the other cases. It may caused by the reason that 5 Hz is closed to the lower limitation of the vibration table rated frequency.",2010,0,
1984,1985,Bayesian Networks for Fault Detection under Lack of Historical Data,In this paper we propose a Bayesian Network approach as a promissory data fusion technique for surveillance of sensors accuracy. We prove the usefulness of this method even in case when there is not enough feasible data to construct the model in traditional way. In presence of this data constrains we suggest an inversion of the causal relationship. This approach proves to be a possible solution to help the expert in the conditional probabilities assessment process. As a result a working model is constructed what would not be possible using traditional Bayesian Network approach.,2009,0,
1985,1986,Improved Evaluation Algorithm for Performance Prediction with Error Analysis,"With the help of a proper performance model and evaluation algorithm, the performance metrics of information systems can be determined at the early stages of the development process. In our work, the response time and the throughput performance metrics of multi-tier Web applications have been predicted based on a queueing model. The goal of our work is to extend the mean-value analysis (MVA) algorithm according to the investigation of the thread pool. Web applications have been tested with concurrent user sessions in order to validate the proposed algorithm in different versions of ASP.NET environment. Moreover, error analysis has been performed to demonstrate the accuracy of the proposed algorithm.",2007,0,
1986,1987,Control-oriented errors quantification under measurement disturbance,"This paper considers the problem of control-oriented errors quantification for a known linear discrete-time SISO model under coprime factor perturbations and measurement disturbance. Upper bounds on exogenous disturbance, measurement disturbance and perturbations in output and control are assumed to be unknown to the controller designer. The problem under consideration is to compute data-consistent upper bounds on steady-state tracking errors in the framework of the lscr<sub>1</sub> robust control theory.",2009,0,
1987,1988,Average error rate of linear diversity reception schemes over generalized gamma fading channels,"We study the performance of M-ary modulation schemes in the presence of additive white Gaussian noise (AWGN) and slow fading. Selection combining (SC), equal gain combining (EGC), and maximal ratio combining (MRC) diversity schemes are considered. The fading channel is modeled by the generalized gamma distribution, which includes the Rayleigh, Nakagami, Weibull, and log-normal distributions as special or limiting cases. The Suzuki distribution can be adequately approximated by the generalized gamma distribution. The exact average symbol error rates (ASER) for coherent multilevel modulation schemes with SC and MRC are presented by using the moment generating function (MGF) based approach while that of EGC is obtained by employing a characteristic function (CHF) based approach. The analysis results for the three combiners are compared and discussed. Simulation results are also provided.",2005,0,
1988,1989,Use of substation IED data for improved alarm processing and fault location,"With the advent of technology, substations of modern days are being equipped with different types of IEDs (Intelligent Electronic Devices) such as Digital Protective Relay (DPR), Digital Fault Recorders (DFR), Phasor Measurement Units (PMU), etc. These devices are capable of recording huge amount of data and thus integration and appropriate use of those data can be beneficial to the power industry. There are several issues to be solved in this regard: (1) Which data to be used and when (for what application), (2) Accuracy of such data (in the measurement process from the place of data capture to where it is used), (3) Extraction of useful information from captured data and (4) Use of the information in applications. This paper focuses on these issues and also some new applications which can use those substation IED data.",2008,0,
1989,1990,Hardware-in-the-loop test for fault diagnosis system of tilt rotor UAV,"The tilt rotor UAV developed in KARI has fault diagnosis functions to enhance system reliability, which were implemented using the operational flight program (OFP) of the flight control computer (FCC). Basically, they conduct built-in-test (BIT) between the FCC and onboard systems such as the communication system, navigation system, and actuators, etc. If a system has no BIT function, fault diagnosis can be performed by checking if the corresponding physical data exit within the available range. In order to test each function, a test device for fault diagnosis was developed and used for hardware-in-the-loop simulation (HILS). The test apparatus consists of the operation control computer (OCC) and the verification computer (VC), which can be interfaced with the simulation computer via Ethernet. The OCC creates faults into the VC and displays the result of fault diagnosis. The VC has simulation models for each onboard system and provides output information to FCC. The FCC performs fault diagnosis and returns the result of fault diagnosis. This paper describes the fault diagnosis functions of the tilt rotor UAV, the test environment used to evaluate them and the test result in hardware-in-the-loop simulation environment.",2008,0,
1990,1991,Survey on Fault Operation on Multilevel Inverters,"This paper is related to faults that can appear in multilevel (ML) inverters, which have a high number of components. This is a subject of increasing importance in high-power inverters. First, methods to identify a fault are classified and briefly described for each topology. In addition, a number of strategies and hardware modifications that allow for operation in faulty conditions are also presented. As a result of the analyzed works, it can be concluded that ML inverters can significantly increase their availability and are able to operate even with some faulty components.",2010,0,
1991,1992,Exact fault simulation for systems on silicon that protects each core's intellectual property (IP),"We present a fault simulation approach for multicore systems on silicon (SOC) (a) that provides exact fault coverage for the entire SOC, (b) does so without revealing any intellectual property (IP) of core vendors, and (c) whose run time is comparable to that required by the existing approaches that require all IP to be revealed. This fault simulator assumes a full scan SOC design and is first in a suite of simulation, test generation, and DFT tools that are currently under development. The proposed approach allows flexibility in selection of a test methodology for SOC, reduces test application cost and area and performance overheads, and allows more comprehensive testing",2001,0,
1992,1993,Increasing register file immunity to transient errors,"Transient errors are a major reason for system downtime in many systems. In prior research, the register file has largely been neglected, but since it is accessed very frequently, the probability of transient errors is high. These errors can quickly spread to different parts of the system, and cause an application crash or silent data corruption. The paper addresses the reliability of register files in superscalar processors. We propose to duplicate actively used physical registers in unused physical registers. If the protection mechanism (parity or ECC) used for the primary copy indicates an error, the duplicate can provide the data, as long as it is not corrupted. We implement two strategies based on register duplication. In the ""conservative strategy"", we limit ourselves with the given register usage behavior, and duplicate register contents only on otherwise unused registers. Consequently, there is no impact on the original performance when there is no error, except for the protection mechanism used for the primary copy. Experiments with two different versions of this strategy show that, with the more powerful conservative scheme, 78% of the accesses are to the physical registers with duplicates. The ""aggressive strategy"" sacrifices some performance to increase the number of register accesses with duplicates. It does so by marking the registers not used for a long time as ""dead"" and using them for duplicating actively used registers. Experiments with this strategy indicate that it takes the fraction of reliable register accesses to 84%, and degrades the overall performance by only 0.21% on average.",2005,0,
1993,1994,Transient fault sensitivity analysis of analog-to-digital converters (ADCs),"Reliability of systems used in space, avionic and biomedical applications is highly critical. Such systems consist of an analog front-end to collect data, an ADC to convert the collected data to digital form and a digital unit to process it. It is important to analyze the fault sensitivities of each of these to effectively gauge and improve the reliability of the system. This paper addresses the issue of fault sensitivity of ADCs. A generic methodology for analyzing the fault sensitivity of ADCs is presented. A novel concept of node weights specific to -particle induced transient faults is introduced to increase the accuracy of such an analysis",2001,0,
1994,1995,Networked vehicles for automated fault detection,"Creating fault detection software for complex mechatronic systems (e.g. modern vehicles) is costly both in terms of engineer time and hardware resources. With the availability of wireless communication in vehicles, information can be transmitted from vehicles to allow historical or fleet comparisons. New networked applications can be created that, e.g., monitor if the behavior of a certain system in a vehicle deviates compared to the system behavior observed in a fleet. This allows a new approach to fault detection that can help reduce development costs of fault detection software and create vehicle individual service planning. The COSMO (consensus self-organized modeling) methodology described in this paper creates a compact representation of the data observed for a subsystem or component in a vehicle. A representation that can be sent to a server in a backoffice and compared to similar representations for other vehicles. The backoffice server can collect representations from a single vehicle over time or from a fleet of vehicles to define a norm of the vehicle condition. The vehicle condition can then be monitored, looking for deviations from the norm. The method is demonstrated for measurements made on a real truck driven in varied conditions with ten different generated faults. The proposed method is able to detect all cases without prior information on what a fault looks like or which signals to use.",2009,0,
1995,1996,A Similar Resource Auto-Discovery Based Adaptive Fault-tolerance Method for Embedded Distributed System,"Because of the resource constraints and high reliability requirement of Embedded Distributed System (EDS), some new fault-tolerance means, which are different from the traditional hardware- redundancy ones, should be studied. In this article, a fault-tolerance method that based on similar resources and related technologies are proposed and discussed. First, several mathematical models of key elements, such as computing nodes, similar nodes and tasks, are constructed. Then, the similarity computation methods and evaluation criteria are evinced by two different views: tasks and resources. Supported by theories above, numerous methods, such as similar nodes auto- discovery (SNAD) and its optimization one (oSNAD), redundant tasks auto-deployment, and reconfiguration policies of fault tasks and nodes are highlighted respectively. Simulation results show that these approaches and schemes can improve the adaptive fault-tolerance abilities of complicated embedded distributed systems.",2007,0,
1996,1997,Fault detection in dynamic systems via decision fusion,"Due to the growing demands for system reliability and availability of large amounts of data, efficient fault detection techniques for dynamic systems are desired. In this paper, we consider fault detection in dynamic systems monitored by multiple sensors. Normal and faulty behaviors can be modeled as two hypotheses. Due to communication constraints, it is assumed that sensors can only send binary data to the fusion center. Under the assumption of independent and identically distributed (1ID) observations, we propose a distributed fault detection algorithm, including local detector design and decision fusion rule design, based on state estimation via particle filtering. Illustrative examples are presented to demonstrate the effectiveness of our approach.",2008,0,
1997,1998,Low voltage fault detection and localisation using the TOPAS 1000 disturbance recorder,"During the past 20 years an increasingly competitive power industry has recognised the importance of addressing the issue of power quality. Many companies now take an active roll in addressing the problems associated with power quality. They are investing in the research and development of equipment to overcome power dips, surges and interruptions, and offering the customer a wide and varied choice of solutions and services to their power quality needs. These solutions are usually based on overcoming the limitations of individual equipment, which is being used by the customer, rather than making improvement to the quality of power supplied.",2005,0,
1998,1999,FPGA-based Ultra-Low Latency HIL Fault Testing of a Permanent Magnet Motor Drive using RT-LAB-XSG,"Real-time simulation of PMSM drives enables thorough testing of control strategies & software protection routines and therefore allows rapid deployment of vehicular or industrial applications. The proposed PMSM model is a phase domain model with sinusoidal flux induction. A 3-phase IGBT inverter drives the PMSM machine. Both models are implemented on an FPGA chip, without any VHDL coding, with the RT-LAB real-time simulation platform of Opal-RT Technologies using a Simulink blockset called Xilinx System Generator (XSG). The paper explains various aspects of the design of the motor drive models in fixed-point representation in XSG, as well as simulation validation against a standard PMSM drive model built in Simulink. The phase-domain PMSM drive model runs with an equivalent 10 nanosecond time step (100 MHz FPGA card) and has a latency of 300 nanoseconds (PMSM machine and inverter). The motor drive has a resulting total hardware-in-the-loop latency of 1.3 microseconds.",2008,0,
1999,2000,Evaluation of a Fault-Tolerance Mechanism for HLA-Based Distributed Simulations,"Successful integration of Modeling and Simulation (M&S) in the future Network-Based Defence (NBD) depends, among other things, on providing faulttolerant (FT) distributed simulations. This paper describes a framework, named Distributed Resource Management System (DRMS), for robust execution of simulations based on the High Level Architecture. More specifically, a mechanism for FT in simulations synchronized according to the time-warp protocol is presented and evaluated. The results show that utilization of the FT mechanism, in a worst-case scenario, increases the total number of generated messages by 68% if one fault occurs. When the FT mechanism is not utilized, the same scenario shows an increase in total number of generated messages by 90%. Considering the worst-case scenario a plausible requirement on an M&S infrastructure of the NBD, the overhead caused by the FT mechanism is considered acceptable.",2006,0,
2000,2001,Vibration monitoring and faults detection using wavelet techniques,"This paper introduces an efficient approach for fault detection in rotating machinery by analyzing its vibration signals using wavelet techniques. Specifically our approach uses the wavelet packet transform (WPT) to decompose the vibration signals in the wavelet packet space, in order to reveal the transient information in these signals. Faults are efficiently detected by exploiting the mean values of the energy in the detail signals. The wavelet-based approach is also compared with the traditional Fourier-based one. Both analysis and an extensive simulation of the two approaches clearly show the superiority of the WPT-based approach over the Fourier-based one, in efficiently diagnosing faults from vibration signals.",2007,0,
2001,2002,Stochastic error-correcting parsing for OCR post-processing,"In this paper, stochastic error-correcting parsing is proposed as a powerful and flexible method to post-process the results of an optical character recognizer (OCR). Deterministic and nondeterministic approaches are possible under the proposed setting. The basic units of the model can be words or complete sentences, and the lexicons or the language databases can be simple enumerations or may convey probabilistic information from the application domain",2000,0,
2002,2003,Error Resilient Decoding of JPEG2000,"In this letter, we investigate the error resilience properties of JPEG2000. We identify the dependencies among coding passes of a codeblock codestream, and determine the sections of the codestream that can be salvaged in the presence of errors. In our analysis, we consider the effects of mode variations provided by the standard. The proposed methods are derived using the existing dependency structure of coding passes and do not require a substantial increase in computational complexity of the decoder. Experimental results indicate that the proposed methods can improve the error resilience performance substantially.",2007,0,
2003,2004,Fast computation of maximum time interval error for telecommunications clock stability characterization,"In telecommunications standards, maximum time interval error (MTIE) is one of the main time domain quantities for characterizing clock stability. However, the direct computation of MTIE, defined in ITU-T Recommendation G.810, tends to be unmanageable given large number of samples. This work proposes a fast computation approach for MTIE. The proposed approach is based on the recursive algorithm and the computation exactly conforms to the ITU-T G.810 MTIE definition. Compared with the direct computation approach, the computational complexity of the proposed approach is reduced by a factor of N, the number of samples. This study, thus, demonstrates that the real-time MTIE measurement or monitoring employing proposed computation approach is feasible.",2005,0,
2004,2005,Detecting type errors and secure coding in C/C++ applications,"The programming languages such as C/C++ suffer from memory management and security of code especially when their codes are used in critical systems. Therefore, we need an efficient mechanism to detect memory and type errors. Some researches have been done and many tools have been developed to detect these errors and to secure C/C++ code. However, theses tools have some drawbacks such as memory management and leak, and type errors in static and dynamic analysis. Generally speaking, this paper proposes a dynamic analysis mechanism to detect type errors in modules of C/C++ code using aspect-oriented programming. We illustrate problems by examples and discuss their solutions.",2010,0,
2005,2006,Error detection in addition chain based ECC Point Multiplication,"In this paper the problem of error detection in elliptic curve point multiplication is faced. Elliptic Curve Point Multiplication is often used to design cryptographic algorithms that use fewer bits than other methods with the same security level. One of the mode used to break the security of cryptosystem is the injection of a fault in the hardware realizing the cryptographic algorithm. Therefore, to avoid this kind of attack, is very important to develop cryptosystems that are able to detect errors induced by a fault. The paper takes into account the algorithm for elliptic Curve Point Multiplication based on a sequence of additions called ldquoaddition chainrdquo and shows how suitable modifications of the algorithms used for computing the point multiplication adds the error detection property to the algorithm.",2009,0,
2006,2007,Sequential Bayesian bit error rate measurement,"As bit error rates decrease, the time required to measure a bit error rate (BER) or perform a BER test (i.e., to determine that a particular communications device's BER is less than some acceptable limit) increases dramatically. One cause of long measurement times is the difficulty of deciding a priori how many bits to measure to establish the BER to within a predetermined confidence interval width. This paper explores a new approach to deciding how many bits to measure, namely a sequential Bayesian approach. As measurement proceeds, the posterior distribution of BER is checked to see if the conclusion can be made that the BER rate is known to be within the desired range with high enough probability. Desired properties of the posterior distribution such as the maximum a postiori estimate and confidence limits can be computed quickly using off-the-shelf numerical software. Examples are given of using this method on bit error data measured with an Agilent 81250 parallel BER tester.",2004,0,
2007,2008,Monitoring power quality disturbances under frequency changes using Complex Least Error Squares algorithm,"This paper presents a new complex least error squares (CLES) algorithm for off nominal frequency power quality disturbances detection. First, a new complex least error squares structure is introduced that compresses three phase signals into a complex vector and produces a tuning vector. In the second layer of the filter, estimation of symmetrical components in off nominal frequency condition is carried out by use of the produced tuning vector and least error squares (LES) algorithm. Presented simulations evaluate the accuracy and efficiency of the proposed method.",2008,0,
2008,2009,SAFE: Scalable Autonomous Fault-tolerant Ethernet,"In this paper, we present a new fault-tolerant Ethernet scheme called SAFE (scalable autonomous fault-tolerant ethernet). SAFE scheme is based on software approach which takes place in layer 2 and layer 3 of the OSIRM. The goal of SAFE is to provide scalability, autonomous fault detection and recovery. SAFE divides a network into several subnets and limits the number of nodes in a subnet. Network can be extended by adding additional subnets. All nodes in a subnet automatically detect faults and perform fail-over by sending and receiving Ethernet based heartbeat each other. For inter-subnet fault recovery, SAFE manages master nodes in each subnet. Master nodes communicate each other using IP packets to exchange the subnet status. We also propose a master election algorithm to recover faults of master node automatically. Proposed SAFE performs efficiently for large scale network and provides fast and autonomous fault recovery.",2009,0,
2009,2010,A new approach to solve dynamic fault trees,"The traditional static fault trees with AND, OR and voting gates cannot capture the dynamic behavior of system failure mechanisms such as sequence-dependent events, spares and dynamic redundancy management and priorities of failure events. Therefore, researchers introduced dynamic gates into fault trees to capture these sequence-dependent failure mechanisms. Dynamic fault trees are generally solved using automatic conversion to Markov models; however, this process generates a huge state space even for moderately sized problems. In this paper, the authors propose a new method to analyze dynamic fault trees. In most cases, the proposed method solves the fault trees without converting them to Markov models. They use the best methods that are applicable for static fault tree analysis in solving dynamic fault trees. The method is straightforward for modular fault trees; and for the general case, they use conditional probabilities to solve the problem. In this paper, the authors concentrate only on the exact methods. The proposed methodology solves the dynamic fault tree quickly and accurately.",2003,0,
2010,2011,Error Correction in 3D Coordinate Measurement,"The increasing use of micromechanical systems in industry and the permanent increase for higher measuring accuracy has led to ongoing development in the field of 3D coordinate measurement. This development includes new designs embodied in unconventional machine structures and also the application software error compensation techniques. The paper over view in case of a novel design the error sources, their influence on the measuring accuracy and the suitable compensation algorithms",2006,0,
2011,2012,An industrial environment for high-level fault-tolerant structures insertion and validation,"When designing a VLSI circuits, most of the efforts are now performed at levels of abstractions higher than gate. Correspondingly to this clear trend, there is a growing request to tackle safety-critical issues directly at the RT-level. This paper presents a complete environment for considering safety issues at the RT level. The environment was implemented and tested by an industry for devising a sample safety-critical device. Designers were permitted to assess the effects of transient faults, automatically add fault-tolerant structures, and validate the results working on the same circuit descriptions and acting in a coherent framework. The evaluation showed the effectiveness of the proposed environment.",2002,0,
2012,2013,Dynamic Multi-mode Switching Error Concealment Algorithm for H.264/AVC Video Applications,"In video communication based consumer device applications, the compressed video is extremely fragile to transmission errors due to channel noise. Error concealment (EC) is an efficient way to recover a damaged video sequence. The existing EC algorithms for compressed video typically apply uniform EC mode to entire video sequence regardless of frame features, or lack in sensible mode-switching considerations. This paper proposes a new dynamic multi-mode switching (DMS) EC algorithm. The proposed algorithm provides several EC modes for intra- and inter- frames. The proposed smart decoder is able to adaptively apply these modes in different situations. The mode switching in DMS depends on the spatial/temporal correlation, estimated edge features of the corrupted Macroblock (MB), the smoothness of the reconstructed MB and etc. The proposed DMS algorithm has been evaluated and compared with 3 classical EC algorithms. The experimental results show that the DMS algorithm can achieve significant gains in peak signal noise ratio (PSNR) and therefore improves the decoded picture quality in video applications.",2008,0,
2013,2014,Automated design flaw correction in object-oriented systems,"Software inevitably changes. As a consequence, we observe the phenomenon referred to as ""software entropy"" or ""software decay"": the software design continually degrades making maintenance and functional extensions overly costly if not impossible. There exist a number of approaches to identify design flaws (problem detection) and to remedy them (refactoring). There is, however, a conceptual gap between these two stages: There is no appropriate support for the automated mapping of design flaws to possible solutions. Here we propose an integrated, quality-driven and tool-supported methodology to support object-oriented software evolution. Our approach is based on the novel concept of ""correction strategies"". Correction strategies serve as reference descriptions that enable a human-assisted tool to plan and perform all necessary steps for the safe removal of detected design flaws, with special concern towards the targeted quality goals of the restructuring process. We briefly sketch our tool chain and illustrate our approach with the help of a medium-sized real-world case-study.",2004,0,
2014,2015,Switch fault diagnosis of PM brushless DC motor drive using adaptive fuzzy techniques,An adaptive neuro-fuzzy inference system (ANFIS) is developed to diagnose open switch faults of PM brushless dc motor drives. Features extracted under healthy and faulty operations using wavelet transform are used to train ANFIS. Testing of the proposed diagnostic system shows it could not only diagnose the fault but identify the faulty switch as well. Good agreement between experimentation and simulation is obtained.,2004,0,
2015,2016,Cryptographic Test Correction,"Multiple choice questionnaires (MCQs) are an assessment procedure invented in 1914. Today, they're widely used in education, opinion polls, and elections. When we first encountered MCQs in the university environment, we faced the daunting challenge of having to grade 600 of them. This article explores the possibility of safely transferring part of an MCQ's correction burden to the examinee - in this case, students - when sophisticated technological means such as optical character recognition systems aren't available. The MCQ grader uses a scoring algorithm C to compute the student's final mark. We call such a procedure a cryptographic test correction (CTC) scheme.",2008,0,
2016,2017,Zero-Hardened SRAM Cells to Improve Soft Error Tolerance in FPGA,"Soft errors due to charged particle strikes at the sensitive cell nodes could modify the functionality of the design by changing the configuration bits of an SRAM based FPGA. However, with the development of very-deep-sub-micron (VDSM) or even the nano-technologies, aggressive device size has impacted severely the soft error rate of integrated circuits. In this paper, three new SRAM cell designs are proposed which mainly aim at reducing the soft error rate in FPGA. We verify the soft error tolerance and the power dissipation of these three designs using HSPICE simulation with Berkeley Predictive Technology Model (PTM) of the 65 nm, 1.0 V technology. The simulation results of our three designs are compared with that of standard 6-transistor SRAM cell and an existing increased soft error tolerance cell - ASRAM0. Comparison result shows that our new cells, especially the 0-hardened SRAM cell, have triple the critical charge of the standard 6-transistor SRAM cell, when the cell is storing 0.",2008,0,
2017,2018,Determining the Amount of Audio-Video Synchronization Errors Perceptible to the Average End-User,"<para> The Media and Acoustics Perception Lab (MAPL) designed a study to determine the minimum amount of audio-visual synchronization (a/v sync) errors that can be detected by end-users. Lip synchronization is the most noticeable a/v sync error, and was used as the testing stimuli to determine the perceptual threshold of audio leading errors. The results of the experiment determined that the average audio leading threshold for a/v sync detection was 185.19 ms, with a standard deviation of 42.32 ms. This threshold determination of lip sync error (with audio leading) will be widely used for validation and verification infrastructures across the industry. By implementing an objective pass/fail value into software, the system or network under test is held against criteria which were derived from a scientific subjective test. </para>",2008,0,
2018,2019,Nanolab: a tool for evaluating reliability of defect-tolerant nano architectures,"As silicon manufacturing technology reaches the nanoscale, architectural designs need to accommodate the uncertainty inherent at such scales. These uncertainties are germane in the miniscule dimension of the device, quantum physical effects, reduced noise margins, system energy levels reaching computing thermal limits, manufacturing defects, aging and many other factors. Defect tolerant architectures and their reliability measures gain importance for logic and micro-architecture designs based on nano-scale substrates. Recently, a Markov random field (MRF) has been proposed as a model of computation for nanoscale logic gates. In this paper, we take this approach further by automating this computational scheme and a belief propagation algorithm. We have developed MATLAB based libraries and toolset for fundamental logic gates that can compute output probability distributions and entropies for specified input distributions. Our tool eases evaluation of reliability measures of combinational logic blocks. The effectiveness of this automation is illustrated in this paper by automatically deriving various reliability results for defect-tolerant architectures, such as triple modular redundancy (TMR), cascaded triple modular redundancy (CTMR) and multi-stage iterations of these. These results are used to analyze trade-offs between reliability and redundancy for these architectural configurations.",2004,0,
2019,2020,Adaptive Fault-Tolerance by Exposing Service Request Process as First-Class Object in Pervasive Computing,"In the open and dynamic pervasive computing environment, it is challenging to detect and handle the frequently occurred failures of service request. The widely used transparent mechanisms Remote Procedure Call and Object Request Broker have a great impact on the adaptive fault-tolerance, because they make it difficult for service requester to sense, configure and control the service request process. In this paper, we explicitly encapsulate and expose the service request process as the first-class object. By the exposed service request object, both the pervasive computing platform and the user applications are able to acquire the adaptive and systematic fault-tolerance ability during the service request process.",2010,0,
2020,2021,Computation Error Analysis in Digital Signal Processing Systems With Overscaled Supply Voltage,"It has been recently demonstrated that digital signal processing systems may possibly leverage unconventional voltage overscaling (VOS) to reduce energy consumption while maintaining satisfactory signal processing performance. Due to the computation-intensive nature of most signal processing algorithms, the energy saving potential largely depends on the behavior of computer arithmetic units in response to overscaled supply voltage. This paper shows that different hardware implementations of the same computer arithmetic function may respond to VOS very differently and result in different energy saving potentials. Therefore, the selection of appropriate computer arithmetic architecture is an important issue in voltage-overscaled signal processing system design. This paper presents an analytical method to estimate the statistics of computer arithmetic computation errors due to supply voltage overscaling. Compared with computation-intensive circuit simulations, this analytical approach can be several orders of magnitude faster and can achieve a reasonable accuracy. This approach can be used to choose the appropriate computer arithmetic architecture in voltage-overscaled signal processing systems. Finally, we carry out case studies on a coordinate rotation digital computer processor and a finite-impulse-response filter to further demonstrate the importance of choosing proper computer arithmetic implementations.",2010,0,
2021,2022,Evaluating the accuracy of defect estimation models based on inspection data from two inspection cycles,"Defect content estimation techniques (DCETs), based on defect data from inspection, estimate the total number of defects in a document to evaluate the development process. For inspections that yield few data points DCETs reportedly underestimate the number of defects. If there is a second inspection cycle, the additional defect data is expected to increase estimation accuracy. In this paper we consider 3 scenarios to combine data sets from the inspection-reinspection process. We evaluate these approaches with data from an experiment in a university environment where 31 teams inspected and reinspected a software requirements document. Main findings of the experiment were that reinspection data improved estimation accuracy. With the best combination approach all examined estimators yielded on average estimates within 20% around the true value, all estimates stayed within 40% around the true value.",2001,0,
2022,2023,A method of probe refinement for fault diagnosis,"In order to obtain the information of the actual execution sequences, probes are required to be deployed in the system, which may influence the operation of system. In this paper, a method of probe refinement is proposed to reduce the cost of probes. By taking the distinction capacity of component as heuristic information, and considering the actual constraints such as that the probes in different positions impact the system differently, the algorithm of reduction is applied to remove these components which can not affect the recognition of the execution sequences. Then the necessary information of the sequences can be obtained with fewer probes deployed in the rest components. Meanwhile, the strategy is discussed and implemented to ensure the completeness of the result. Finally, the experiment shows that the refinement method can reduce the overheads of introducing probes effectively.",2010,0,
2023,2024,Design and test of HTS coils for resistive fault current limiter,"This paper presents the design, manufacturing and test results of HTS coils for single-phase resistive current limiters using two different Bi-2223/Ag tapes. Two coils were separately wound on cylindrical G-10 tubes with helical path. One of the coil was wound in a bifilar conductor arrangement aiming at reducing the self-inductance. The maximum flux density is 40 mT on the HTS tape for both coils and the calculated inductance is 5.76H for the non-inductively wound coil and 28.3H for the conventional one. The two coils were individually tested under DC and AC currents. The DC power loss was calculated from the measured voltage-current values product and the AC power loss was determined by the liquid nitrogen mass boil-off measurement. The acquired voltage AC waveforms were analyzed through the separation of distinct contributions due to magnetic flux and to a resistive component in-phase with the current. It was observed an inflection point common to the DC and AC power losses curves close to I<sub>c</sub> attributed to the AC transition current.",2004,0,
2024,2025,SCEMIT: A SystemC error and mutation injection tool,"As high-level models in C and SystemC are increasingly used for verification and even design (through high-level synthesis) of electronic systems, there is a growing need for compatible error injection tools to facilitate further development of coverage metrics and automated diagnosis. This paper introduces SCEMIT, a tool for the automated injection of errors into C/C++/SystemC models. A selection of `mutation' style errors are supported, and injection is performed though a plugin interface in the GCC compiler, which minimizes the impact of SCEMIT on existing simulation flows. Experimental injected error detection results are presented for the set of OSCI SystemC Example Models as well as the CHStone C High-Level-Synthesis benchmark set. Aside from demonstrating compatibility with these models, the results show the value of high-level error injection as a coverage measure compared to conventional code coverage measures.",2010,0,
2025,2026,Forward error correction codes to reduce intercarrier interference in OFDM,"Orthogonal Frequency Division Multiplexing (OFDM) is sensitive to the carrier frequency offset (CFO), which destroys orthogonality and causes intercarrier interference (ICI). Recently, a simple rate 1/2 repeat coding scheme has been shown to be effective in suppressing ICI. That such a simple coding scheme is so effective raises an interesting question. Can more powerful error correcting codes with less redundancy be used just as effectively for the same purpose? In this paper, we propose the use of rate-compatible punctured convolutional (RCPC) codes",2001,0,
2026,2027,A real-time hardware fault detector using an artificial neural network for distance protection,"A real-time fault detector for the distance protection application, based on artificial neural networks, is described. Previous researchers in this field report use of complex filters and artificial neural networks with large structure or long training times. An optimum neural network structure with a short training time is presented. Hardware implementation of the neural network is addressed with a view to improve the performance in terms of speed of operation. By having a smaller network structure the hardware complexity of implementation reduces considerably. Two preprocessors are described for the distance protection application which enhance the training performance of the artificial neural network many fold. The preprocessors also enable real-time functioning of the artificial neural network for the distance protection application. Design of an object oriented software simulator, which was developed to identify the hardware complexity of implementation, and the results of the analysis are discussed. The hardware implementation aspects of the preprocessors and of the neural network are briefly discussed",2001,0,
2027,2028,Design of a Window Comparator with Adaptive Error Threshold for Online Testing Applications,This paper presents a novel window comparator circuit whose error threshold can be adaptively adjusted according to its input signal levels. It is ideal for analog online testing applications. Advantages of adaptive comparator error thresholds over constant or relative error thresholds in analog testing applications are discussed. Analytical equations for guiding the design of proposed comparator circuitry are derived. The proposed comparator circuit has been designed and fabricated using a CMOS 0.18mu technology. Measurement results of the fabricated chip are presented,2007,0,
2028,2029,Neuro-Fuzzy Fabric Defect Detection and Classification for Knitting Machine,"The conventional method in textile industries for fault detection is human inspection. In this method, the classification of produced fabrics and online identification of fault in production line is done by human vision. The goal of this study is to replace the current human visual inspection methods with an automated visual inspection system that provide high accuracy of defect detection and classification. The proposed system consists of a computer based vision system (for capturing an image), image processing tools (for adjusting the image and extracting the features) and a system for detecting and classifying fabric defects. Two types of features are extracted (tonal and texture). Two types of systems are used. The first system is based on fuzzy clustering using fuzzy c-means clustering (FCM). The second system is adaptive neural-fuzzy inference system (ANFIS). Experimental results show that, the proposed systems are capable of detecting and classifying fabric defect, also it shows that using both tonal and texture features gives higher detection and classification rate than using each one individually",2006,0,
2029,2030,Optimizing device size for soft error resilience in sub-micron logic circuits,"As technology nodes are being scaled down, soft errors induced by particle strikes are becoming a troublesome reliability issue in logic circuits. Various sizing techniques commonly used to reduce soft error rate in the past are expensive in terms of area, performance, and energy consumption. These methods require changes to adapt to sub-micron technologies. This study introduces two novel sizing methods that selectively upsize transistor networks of a circuit. Our first proposed methodology formulates the soft error rate minimization as a mathematical optimization problem and searches for the best area distribution such that maximum reliability gain is obtained. This methodology assures that optimal solutions are achieved within given area budget provided to the designer. However, generating optimal solution requires very high CPU time. Therefore, we propose a heuristic based methodology which upsizes only selected transistor network in sensitive gates based on soft error sensitivity of each gate. With proper sensitive gate selection and area distribution algorithms proposed in this technique, we show through experimental results that our heuristic driven method gives satisfactory reliability improvement compared to our first method, while requiring relatively small computation time.",2010,0,
2030,2031,The role of defects on CdTe detector performance,"We present results from a characterisation of bulk defects in CdTe wafers, and their role in the degradation of charge transport performance of CdTe radiation detectors. Sub-bandgap IR microscopy and X-ray Lang topography have been used to characterise material quality prior to device processing. IR microscopy clearly identifies extended defects such as tellurium precipitates in the material bulk, whilst Lang topography characterises stacking faults, crystallite boundaries and other crystallographic features in the near-surface region. After fabrication of contacts onto the material, ion beam induced charge imaging is used to investigate the correlations between material defects and charge transport. Digital ion beam induced charge imaging is used to produce high resolution maps of charge signal amplitude, carrier drift time, and carrier drift mobility.",2003,0,
2031,2032,Spatial error concealment for H.264 using sequential directional interpolation,"Error concealment at the decoder restores erroneous macroblocks (MBs) caused by channel errors. In this paper, we propose a novel spatial error concealment algorithm based on prediction modes of intra-blocks which are included in a H.264-coded stream and highly correlated to the direction of local edge within the block. The key contribution is to sequentially interpolate each pixel in a lost MB by utilizing edge directions and strengths efficiently estimated from the neighboring blocks, preserving local edge continuity for more visually acceptable images. The proposed scheme is simple to implement and more reliably recover high-detailed content in corrupted MBs. The experimental results shows the proposed method achieves reduction in speed by 14%<sub>~</sub>39% as compared to existing method, and outperforms them in PSNR by 0.5<sub>~</sub>1 dB as well as in subjective visual evaluation.",2008,0,
2032,2033,A self-correcting active pixel sensor using hardware and software correction,Active pixel sensor (APS) CMOS technology reduces the cost and power consumption of digital imaging applications. We present a highly reliable system for the production of high-quality images in harsh environments. The system is based on a fault-tolerant architecture that effectively combines hardware redundancy in the APS cells and software correction techniques.,2004,0,
2033,2034,A condition monitoring vector database approach for broken bar fault diagnostics of induction machines,"In this paper, a condition monitoring vector database (CMVDB) approach for broken bar fault diagnostics of squirrel-cage induction machines is presented. In this approach, a database of so-called ""condition monitoring vectors"" (CMVs) is generated for healthy and broken bar fault conditions using the time-stepping finite-element method. The CMV consists of the negative sequence components of winding voltages, currents, and impedances, the frequency spectrum sideband components of motor currents, and the space-vectors of motor terminal quantities (currents and voltages) from which the motor magnetic field pendulous oscillations are derived, as well as the motor speed and developed torque. This CMV will serve as the fault index (signature) for the faults under investigation in this work. This database is intended for use as a reference database in an on-line condition monitoring and fault diagnostic system. In this work, artificial intelligence (AI) techniques based on a statistical machine learning approach are used to detect and distinguish the type of fault and its severity based on the on-line measurements of the motor terminal voltages and currents, as well as the motor speed and developed torque, in comparison to the available CMVDB. To demonstrate the proof-of-principle of the database approach, simulation and experimental results for a 2-hp induction motor are given here to verify the viability of this approach",2005,0,
2034,2035,Fault-tolerant solutions for a MPI compute intensive application,"The running times of large-scale computational science and engineering parallel applications, executed on clusters or grid platforms, are usually longer than the mean-time-between-failures (MTBF). Hardware failures must be tolerated by the parallel applications to ensure that no all computation done is lost on machine failures. Checkpointing and rollback recovery is a very useful technique to implement fault-tolerant applications. Although extensive research has been carried out in this field, there are few available tools to help parallel programmers to enhance with fault tolerant capability their applications. This work presents two different approaches to endow with fault tolerance the MPI version of an air quality simulation. A segment-level solution has been implemented by means of the extension of a checkpointing library for sequential codes. A variable-level solution has been implemented manually in the code. The main differences between both approaches are portability, transparency-level and checkpointing overheads. Experimental results comparing both strategies on a cluster of PCs are shown in the paper",2007,0,
2035,2036,Implementation and error performance evaluation of an iterative decoding algorithm,"This paper presents new experimental results about the error correction performance of an iterative threshold decoder at relatively high signal to noise ratio. To accomplish this task, an accelerated characterization platform has been developed. Without this platform, it would take approximately 37.23 years in computational time with a software version of the decoder to get the error correction performances over an extended signal to noise range. An acceleration factor of 4812 is obtained by using the accelerated characterization platform. The platform constitutes a new way for characterizing quickly and efficiently a new error correction algorithm. The acceleration characterization platform has allowed verifying the error correction performance at high SNR which will require a prohibitive computational time on a conventional computer.",2005,0,
2036,2037,Reliability analysis of large fault trees using the Vesely failure rate,"Fault trees provide a compact, graphical, intuitive method to analyze system reliability. However, combinatorial fault tree analysis methods, such as binary decision diagrams, cannot be used to find the reliability of systems with repairable components. In such cases, the analyst should use either Markov models explicitly or generate Markov models from fault trees using automatic conversion algorithms. This process is tedious and generates huge Markov models even for moderately sized fault trees. In this paper, the use of the Vesely failure rate as an approximation to the actual failure rate of the system to find the reliability-based measures of large fault trees is demonstrated. The main advantage of this method is that it calculates the reliability of a repairable system using combinatorial methods such as binary decision diagrams. The efficiency of this approximation is demonstrated by comparing it with several other approximations and provide various bounds for system reliability. The usefulness of this method in finding the other reliability measures such as MTBF, MTTR, MTTF, and MTTFF is shown. Finally, extending this method to analyze complex fault trees containing static and dynamic modules as well as events represented by other modeling tools.",2004,0,
2037,2038,Spectral Smile Correction of CRISM/MRO Hyperspectral Images,"The Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) is affected by a common artifact to pushbroom-type imaging spectrometers, the so-called spectral smile. For this reason, the central wavelength and the width of the instrument spectral response vary according to the spatial dimension of the detector array. As a result, the spectral capabilities of CRISM get deteriorated for the off-axis detector elements while the distortions are minimal in the center of the detector array, the so-called sweet spot. The smile effect results in a data bias that affects hyperspectral images and whose magnitude depends on the column position (i.e., the spatial position of the corresponding detector element) and the local shape of the observed spectrum. The latter is singularly critical for images that contain chemical components having strong absorption bands, such as carbon dioxide on Mars in the gas or solid phase. The smile correction of CRISM hyperspectral images is addressed by the definition of a two-step method that aims at mimicking a smile-free spectral response for all data columns. First, the central wavelength is uniformed by resampling all spectra to the sweet-spot wavelengths. Second, the nonuniform width of the spectral response is overcome by using a spectral sharpening which aims at mimicking an increase of the spectral resolution. In this step, only spectral channels particularly suffering from the smile effect are processed. The smile correction of two CRISM images by the proposed method show remarkable results regarding the correction of the artifact effects and the preservation of the original spectra.",2010,0,
2038,2039,Automated methods for atmospheric correction and fusion of multispectral satellite data for national monitoring,"The Earth Observation for Sustainable Development of Canada's forests (EOSD) project monitors Canada's forests from space. Canada contains ten-percent of the world's forests. Initial EOSD products are land cover, forest change, forest biomass, and automated methods. There are more than 500 LANDSAT TM or ETM+ scenes required for a single coverage of Canada's forests. Multi-temporal analysis using satellite data requires automation for conversion of these data to common units of exoatmospheric radiance or ground reflectance. During the next ten years the EOSD project will use a variety of Landsat optical and Radarsat sensors. A diverse set of ancillary and satellite data formats exist which require the development of adaptable data ingest and processing streams. Legacy LANDSAT TM and ETM+ data are available in a number of different formats from several national and US suppliers. In this paper, we present an automated system for managing processing streams for calibration and atmospheric correction of LANDSAT TM and ETM+ data to create data sets ready to analyze for EOSD products. Using known forest attributes from GIS data and field measurements, we validated our results of studies undertaken to assess spectral signal variability using both at-sensor radiance and ground reflectance for LANDSAT TM and ETM+ for a test site on Vancouver Island, BC. We present a strategy for correcting and fusing multi-source and multitemporal satellite data for meeting EOSD requirements.",2002,0,
2039,2040,Instrument fault detection and isolation: state of the art and new research trends,This paper presents the current state-of-the-art of residual generation techniques adopted in instrument fault detection and isolation. Both traditional and innovative methods are described with their advantages and their limits. The improvement of analytical redundancy technique performances for better dealing with high-dynamics systems and/or with online applications is pointed out as the most interesting need to focus the research efforts,2000,0,
2040,2041,Isolating Suspiciousness from Spectrum-Based Fault Localization Techniques,"Spectrum-based fault localization (SBFL) is one of the most promising fault localization approaches, which normally uses the failed and passed program spectrum to evaluate the risks for all program entities. However, it does not explicitly distinguish the different degree in definiteness between the information associated with the failed spectrum and the passed spectrum, which may result in an unreliable location of faults. Thus in this paper, we propose a refinement method to improve the accuracy of the predication by SBFL, through eliminating the indefinite information. Our method categorizes all statements into two groups according to their different suspiciousness, and then uses different evaluation schemes for these two groups. In this way, we can reduce the use of the unreliable information in the ranking list, and finally provide a more precise result. Experimental study shows that for some SBFL techniques, our method can significantly improve their performance in some situations, and in other cases, it can still remain the techniques' original performance.",2010,0,
2041,2042,Fault diagnosis on analog circuits based on Integrated Learning Method,"The Integrated Learning Method (ILM) uses multiple learners to solve the same problem, which can greatly improve the generalization ability of learning systems. To address the fault diagnosis on analog circuits, aiming at the shortcomings of diagnosis and model stability with single RBF neural network to diagnose faults of analog circuit system, the paper discussed method to improve model diagnosis accuracy with Bagging algorithm of ILM to integrated multiple neural networks. The experiment results show the adoption of this scheme can significantly improve the performance of neural network diagnostic model.",2010,0,
2042,2043,Error-Pooling Empirical Bayes Model for Enhanced Statistical Discovery of Differential Expression in Microarray Data,"A number of statistical approaches have been proposed for evaluating the statistical significance of a differential expression in microarray data. The error estimation of these approaches is inaccurate when the number of replicated arrays is small. Consequently, their resulting statistics are often underpowered to detect important differential expression patterns in the microarray data with limited replication. In this paper, we propose an empirical Bayes (EB) heterogeneous error model (HEM) with error-pooling prior specifications for varying technical and biological errors in the microarray data. The error estimation of HEM is thus strengthened by and shrunk toward the EB priors that are obtained by the error-pooling estimation at each local intensity range. By using simulated and real data sets, we compared HEM with two widely used statistical approaches, significance analysis of microarray (SAM) and analysis of variance (ANOVA), to identify differential expression patterns across multiple conditions. The comparison showed that HEM is statistically more powerful than SAM and ANOVA, particularly when the sample size is smaller than five. We also suggest a resampling-based estimation of Bayesian false discovery rate to provide a biologically relevant cutoff criterion of HEM statistics.",2008,0,
2043,2044,Dynamic Fault Tree Analysis Using Input/Output Interactive Markov Chains,"Dynamic fault trees (DFT) extend standard fault trees by allowing the modeling of complex system components' behaviors and interactions. Being a high level model and easy to use, DFT are experiencing a growing success among reliability engineers. Unfortunately, a number of issues still remains when using DFT. Briefly, these issues are (1) a lack of formality (syntax and semantics), (2) limitations in modular analysis and thus vulnerability to the state-space explosion problem, and (3) lack in modular model-building. We use the input/output interactiveMarkov chain (I/O-IMC) formalism to analyse DFT. I/O-IMC have a precise semantics and are an extension of continuous-time Markov chains with input and output actions. In this paper, using the I/OI-MC framework, we address and resolve issues (2) and (3) mentioned above. We also show, through some examples, how one can readily extend the DFT modeling capabilities using the I/O-IMC framework.",2007,0,
2044,2045,Production Evaluation of Automated Reticle Defect Printability Prediction Application,"The growing complexity of reticles and continual tightening of defect specifications causes the reticle defect disposition function to become increasingly difficult. No longer can defect specifications be distilled to a single number, nor can past simple classification rules be employed due to the effects of MEEF on actual printing behavior. The mask maker now requires lithography-based rules and capabilities for making these go/no-go decisions at the reticle inspection step. We have evaluated an automated system that predicts the lithographic significance of reticle defects using PROLITH(TM) technology. This printability prediction tool was evaluated and tested in a production environment using both standard test reticles and production samples in an advanced reticle manufacturing environment. Reference measurements on Zeiss AIMS(TM) systems were used to assess the accuracy of predicted results. The application, called the Automated Mask Defect Disposition System, or AMDD, models defective and non-defective test and reference images generated by a high-resolution inspection system. The results were calculated according to the wafer exposure conditions given at setup such that the reticle could be judged for its `fitness-for-use?? from a lithographic standpoint rather than from a simple physical measurement of the film materials. We present the methods and empirical results comparing 1D and 2D Intensity Difference Metrics (IDMs) with respect to AIMS and discuss the results of usability and productivity studies as they apply to manufacturing environments.",2007,0,
2045,2046,Development of ANN-based virtual fault detector for Wheatstone bridge-oriented transducers,This paper reports on the development of a new artificial neural network-based virtual fault detector (VFD) for detection and identification of faults in DAS-connected Wheatstone bridge-oriented transducers of a computer-based measurement system. Experimental results show that the implemented VFD is convenient for fusing intelligence into such systems in a user-interactive manner. The performance of the proposed VFD is examined experimentally to detect seven frequently occurring faults automatically in such transducers. The presented technique used an artificial neural network-based two-class pattern classification network with hard-limit perceptrons to fulfill the function of an efficient residual generator component of the proposed VFD. The proposed soft residual generator detects and identifies various transducer faults in collaboration with a virtual instrument software-based inbuilt algorithm. An example application is also presented to demonstrate the use of implemented VFD practically for detecting and diagnosing faults in a pressure transducer having semiconductor strain gauges connected in a Wheatstone bridge configuration. The results obtained in the example application with this strategy are promising.,2005,0,
2046,2047,Navigation error analysis for the rocketplane XP,"bd Systems has supported the navigation architecture development of the Rocketplane XP as part of its responsibility as the vehicle guidance, navigation, and control (GN&C) subsystem lead contractor. The Rocketplane XP is a reusable sub-orbital space-plane being designed by Rocketplane Limited, Inc. for the commercial space tourism market. The horizontal take-off/horizontal landing vehicle, which will be one of the world's first true manned aerospace vehicles, has a short development schedule and began operation in 2007. bd Systems performed a detailed trade study to select the navigation subsystem and developed high-fidelity error models for use in the overall vehicle six-degree-offreedom (6DOF) simulation in order to determine the effects of navigation errors on vehicle performance.",2009,0,
2047,2048,Study on 3D Modeling Method of Faults Based on GTP Volume,"3D fault modeling is one of the key issues of three-dimensional geosciences modeling system (3DGMS). Summarized the 3D modeling methods of faults at present, a new method, named as partition recursively modeling of the rock pillar body (RPB), for 3D modeling of faults (3DMF) based on generalized tri-prism (GTP) volume is put forward. This method takes borehole data as its original data source. The process for 3DMF includes: a) to generate the constrained delaunay TIN (CD-TIN) of terrain according to the collar data of borehole and the outcrops of faults; b) to construct the limit triangles in each PRB; c) to create 3D faults models according to the limit triangles. The test shows that the RPB method can accurately describe 3D complicated faults system involving obverse faults and reverse faults.",2006,0,
2048,2049,Algorithm-based fault tolerance for many-core architectures,"Modern many-core architectures with hundreds of cores provide a high computational potential. This makes them particularly interesting for scientific high-performance computing and simulation technology. Like all nano scaled semiconductor devices, many-core processors are prone to reliability harming factors like variations and soft errors. One way to improve the reliability of such systems is software-based hardware fault tolerance. Here, the software is able to detect and correct errors introduced by the hardware. In this work, we propose a software-based approach to improve the reliability of matrix operations on many-core processors. These operations are key components in many scientific applications.",2010,0,
2049,2050,Microprocessor fault-tolerance via on-the-fly partial reconfiguration,"This paper presents a novel approach to exploit FPGA dynamic partial reconfiguration to improve the fault tolerance of complex microprocessor-based systems, with no need to statically reserve area to host redundant components. The proposed method not only improves the survivability of the system by allowing the online replacement of defective key parts of the processor, but also provides performance graceful degradation by executing in software the tasks that were executed in hardware before a fault and the subsequent reconfiguration happened. The advantage of the proposed approach is that thanks to a hardware hypervisor, the CPU is totally unaware of the reconfiguration happening in real-time, and there's no dependency on the CPU to perform it. As proof of concept a design using this idea has been developed, using the LEON3 open-source processor, synthesized on a Virtex 4 FPGA.",2010,0,
2050,2051,Fault tolerant event localization in sensor networks using binary data,"This paper investigates the use of wireless sensor networks for estimating the location of an event that emits a signal which propagates over a large region. In this context, we assume that the sensors make binary observations and report the event if the measured signal at their location is above a threshold; otherwise they remain silent. Based on the sensor binary beliefs we use 4 different estimators to localize the event: CE (centroid estimator), ML (maximum likelihood), SNAP (subtract on negative add on positive) and AP (add on positive). The main contribution of this paper is the fault tolerance analysis of the proposed estimators. Furthermore, the analysis shows that SNAP is the most fault tolerant of all estimators considered.",2008,0,
2051,2052,Toward Fault-Tolerant Atomic Data Access in Mutable Distributed Hash Tables,"Though the peer-to-peer network based on distributed hash table have scalability and availability features, but most of the distributed hash tables lack the support for the atomic data accessing, this ability is important to build data structures on distributed system. This paper gives the design and implementation of an atomic accessible mutable distributed hash table - FamDHT. The FamDHT use the Paxos distributed consensus protocol to ensure that the simultaneous access to data is executed under a total order agreement. The FamDHT is build on top of the Bamboo DHT, a robust, open-source DHT implementation. At the end of the paper, an evaluation was given",2006,0,
2052,2053,Modeling fault-tolerant mobile agent execution as a sequence of agreement problems,"Fault tolerance is fundamental to the further development of mobile agent applications. In the context of mobile agents, fault tolerance prevents a partial or complete loss of the agent, i.e. ensures that the agent arrives at its destination. Simple approaches such as checkpointing are prone to blocking. Replication can in principle improve solutions based on checkpointing. However existing solutions in this context either assume a perfect failure detection mechanism (which is not realistic in an environment such as the Internet), or rely on complex solutions based on leader election and distributed transactions, where only a subset of solutions prevents blocking. The paper proposes a novel approach to fault tolerant mobile agent execution, which is based on modeling agent execution as a sequence of agreement problems. Each agreement problem is one instance of the well understood consensus problem. Our solution does not require a perfect failure detection mechanism, while preventing blocking and ensuring that the agent is executed exactly once",2000,0,
2053,2054,Supporting fault-tolerance in streaming grid applications,"This paper considers the problem of supporting and efficiently implementing fault-tolerance for tightly-coupled and pipelined applications, especially streaming applications, in a grid environment. We provide an alternative to basic checkpointing and use the notion of light-weight summary structure(LSS) to enable efficient failure-recovery. The idea behind LSS is that at certain points during the execution of a processing stage, the state of the program can be summarized by a small amount of memory. This allows us to store copies of LSS for enabling failure-recovery, which causes low overhead fault-tolerance. Our work can be viewed as an optimization and adaptation of the idea of application-level checkpointing to a different execution environment, and for a different class of applications. Our implementation and evaluation of LSS based failure- recovery has been in the context of the GATES (grid-based adaptive execution on streams) middleware. An observation we use for providing very low overhead support for fault-tolerance is that algorithms analyzing data streams are only allowed to take a single pass over data, which means they only perform approximate processing. Therefore, we believe that in supporting fault-tolerant execution for these applications, it is acceptable to not analyze a small number of packets of data during failure-recovery. We show how we perform failure-recovery and also demonstrate how we could use additional buffers to limit data loss during the recovery procedure. We also present an efficient algorithm for allocating a new computation resource for failure-recovery at runtime. We have extensively evaluated our implementation using three stream data processing applications, and shown that the use of LSS allows effective and low-overhead failure-recovery.",2008,0,
2054,2055,Attenuation correction for the NIH ATLAS small animal PET scanner,"We evaluated two methods of attenuation correction for the NIH ATLAS small animal PET scanner: 1) a CT-based method that derives 511 keV attenuation coefficients () by extrapolation from spatially registered CT images; and 2) an analytic method based on the body outline of emission images and an empirical . A specially fabricated attenuation calibration phantom with cylindrical inserts that mimic different body tissues was used to derive the relationship to convert CT values to  for PET. The methods were applied to three test data sets: 1) a uniform cylinder phantom, 2) the attenuation calibration phantom, and 3) a mouse injected with [<sup>18</sup>F] FDG. The CT-based attenuation correction factors were larger in nonuniform regions of the imaging subject, e.g. mouse head, than the analytic method. The two methods had similar correction factors for regions with uniform density and detectable emission source distributions.",2003,0,
2055,2056,Analysis and Proposition of Error-Based Model to Predict the Minimum Reliability of Software,"Although, there are several software reliability models existing today, yet for software in the useful life where no upgradation, debugging or modification is done and also where no efforts are taken to avoid the errors, there exists several contradicting opinions. Some people say that in the useful life the reliability is time dependent others say that they are independent of time. In the literature, many software reliability models had been developed in compatible with hardware reliability models which are directly a function of time. This paper analyzes these aspects with the existing software reliability models, determines their deficiencies and effectiveness, and finally proposes an innovative error based generalized model for predicting the minimum reliability of software.",2009,0,
2056,2057,Inhomogeneity correction of magnetic resonance images by minimization of intensity overlapping,"This work presents a new algorithm (NIC; nonuniform intensity correction) for the correction of intensity inhomogeneities in magnetic resonance images. The algorithm has been validated by means of realistic phantom images and a set of 24 real images. Evaluation using previously proposed phantom images for inhomogeneity correction algorithms allowed us to obtain results fully comparable to the previous literature on the topic. This new algorithm was also compared, using a real image dataset, to other widely used methods which are freely available in the Internet (N3, SPM'99 and SPM2). Standard quality criteria have been used for determining the goodness of the different methods. The new algorithm showed better results removing the intensity inhomogeneities and did not produce degradation when used on images free from this artifact.",2003,0,
2057,2058,Smart energy meters for energy conservation & minimizing errors,"The monitoring of the power quality helps to lower the energy costs and to prolong the machine's life. Smart metering is such a complete end to end solution which minimizes the several errors and helps in distributing Quality Power. It is an energy policy for consumers to provide them a user friendly face in dealing with utility (especially the Electricity) bills. It provides the users, a Digital Meter which displays the real time power consumption every time in very friendly and detailed format, and a website to check & analyze their consumption and expenses on energy, using different types of graphs, tabulated and manipulated data. It not only comforts their users but also give relief to the distribution company by minimizing power losses by using automatic Power factor maintenance technique, and providing anti-power theft capability. It also gives a control of power distribution through which Distribution Company can limit the user from exceeding usage of power in specific time duration.",2010,0,
2058,2059,Automatic Generation of Instructions to Robustly Test Delay Defects in Processors,"We present a technique for generating instruction sequences to test a processor functionally. We target delay defects with this technique using an ATPG engine to generate delay tests locally, a verification engine to map the tests globally, and a feedback mechanism that makes the entire procedure faster. We demonstrate nearly 96% coverage of delay faults with the instruction sequences generated. These instruction sequences can be loaded into the cache to test the processor functionally.",2007,0,
2059,2060,Fabric defect detection by Fourier analysis,"Many fabric defects are very small and undistinguishable, which makes them very difficult to detect by only monitoring the intensity change. Faultless fabric is a repetitive and regular global texture and Fourier transforms can be applied to monitor the spatial frequency spectrum of a fabric. When a defect occurs in fabric, its regular structure is changed so that the corresponding intensity at some specific positions of the frequency spectrum would change. However, the three-dimensional frequency spectrum is very difficult to analyze. In this paper, a simulated fabric model is used to understand the relationship between the fabric structure in the image space and in the frequency space. Based on the three-dimensional frequency spectrum, two significant spectral diagrams are defined and used for analyzing the fabric defect. These two diagrams are called the central spatial frequency spectrums. The defects are broadly classified into four classes: (1) double yarn; (2) missing yarn; (3) webs or broken fabric; and (4) yarn densities variation. After evaluating these four classes of defects using some simulated models and real samples, seven characteristic parameters for a central spatial frequency spectrum are extracted for defect classification",2000,0,
2060,2061,Global CD uniformity improvement using dose modulation pattern correction of pattern density-dependent and position-dependent errors,"The specification of mask global CD uniformity (GCDU) is ever tightening. There is no exception at the 65-nm node. Some of the key contributors affecting GCD non-uniformity are pattern-density effects such as fagging effect from the e-beam writer and macro loading effect from the etcher. In addition, the contributions from position-dependent effects are significant, and these contributions included resist developing, baking, as well as aberrations of the wafer-imaging lens. It is challenging to quantify these effects and even more so to correct them to improve the GCDU. Correction of the fogging and etch loading effects had been reported by various authors. In addition to correction for these effects, we are reporting the position-dependent effects in this paper.",2004,0,
2061,2062,Sequential decision fusion for controlled detection errors,"Information fusion in biometrics has received considerable attention. The architecture proposed here is based on the sequential integration of multi-instance and multi-sample fusion schemes. This method is analytically shown to improve the performance and allow a controlled trade-off between false alarms and false rejects when the classifier decisions are statistically independent. Equations developed for detection error rates are experimentally evaluated by considering the proposed architecture for text dependent speaker verification using Hidden Markov Model (HMM) based digit dependent speaker models. The tuning of parameters, n classifiers and m attempts/samples, is investigated and the resultant detection error trade-off performance is evaluated on individual digits. Results show that performance improvement can be achieved even for weaker classifiers (FRR-19.6%, FAR-16.7%). The architectures investigated apply to speaker verification from spoken digit strings such as credit card numbers in telephone or VOIP or internet based applications.",2010,0,
2062,2063,A comparison of sliding mode and unknown input observers for fault reconstruction,"This paper compares the use of a recently proposed sliding mode fault detection and isolation scheme with a linear scheme based on an unknown input observer. Both methods seek to reconstruct actuator and sensor fault signals for a class of uncertain systems. Although the explicit details of the two approaches appear quite different, an underlying link between them is exposed and investigated. The methods are compared on data collected from a laboratory scale crane rig on which (known) faults were deliberately introduced.",2004,0,
2063,2064,Speech Enhancement Based on Minimum Mean-Square Error Estimation and Supergaussian Priors,"This paper presents a class of minimum mean-square error (MMSE) estimators for enhancing short-time spectral coefficients of a noisy speech signal. In contrast to most of the presently used methods, we do not assume that the spectral coefficients of the noise or of the clean speech signal obey a (complex) Gaussian probability density. We derive analytical solutions to the problem of estimating discrete Fourier transform (DFT) coefficients in the MMSE sense when the prior probability density function of the clean speech DFT coefficients can be modeled by a complex Laplace or by a complex bilateral Gamma density. The probability density function of the noise DFT coefficients may be modeled either by a complex Gaussian or by a complex Laplacian density. Compared to algorithms based on the Gaussian assumption, such as the Wiener filter or the Ephraim and Malah (1984) MMSE short-time spectral amplitude estimator, the estimators based on these supergaussian densities deliver an improved signal-to-noise ratio.",2005,0,
2064,2065,Using a square-wave signal for fault diagnosis of analog parts of mixed-signal embedded systems controlled by microcontrollers,"The paper presents a new method of single soft fault detection and localisation of analog parts in embedded mixed-signal electronic systems controlled by microcontrollers. The method consists of three stages: a pre-testing stage of a fault dictionary creation using identification curves, a measurement stage based on stimulating the tested circuit by a square-wave signal generated by the microcontroller and measurements of voltage samples of the circuit response by the internal ADC of the microcontroller. At a final stage the faults detection and localisation are performed by the microcontroller. The BIST consists only of internal devices of the microcontroller mounted in the system. Hence, this approach simplifies the structure of BISTs, which allows to reduce test costs.",2007,0,
2065,2066,Design and implementation of fault tolerant CORBA-based middleware platform,"Distributed middleware can reduce the development cycle of distributed application system, and provide a good development environment and transparent support for distributed application. Developing the fault-tolerant distributed application platform based on middleware, it not only greatly simplifies the development of the fault-tolerant distributed system itself, but also improves the flexibility, scalability and reliability of distributed application, and is easier to deploy and manage the fault-tolerant system. From this perspective, we have designed and implemented a fault-tolerant platform based on active replication by using CORBA. Comparing the result in different systems, we know the middle tier is the bottleneck of the platform performance.",2010,0,
2066,2067,Toward the detection of interpretation effects and playing defects,"Precise automatic music transcription requires accurate modeling and identification of the spectral content of the audio signal. But music can not be reduced to a succession of notes, and an accurate transcriptor should be able to detect other performance characteristics, such as slow tempo variations or, depending on the instrument detecting some interpretation effects. In a pedagogic way a student could want to improve his level and a good challenge will be to estimate the quality of play of musician. In this paper we present some of the most common playing defects and interpretation effects and we propose a way for detecting them.",2009,0,
2067,2068,Comparison of Error Estimators in Eddy Current Testing,In this paper we present the comparison of error estimators using in 3-D magnetoharmonic FE model dedicated to eddy-current testing. The first error estimator is based on the nonverification of the behaviour law. This estimator is very accurate but requires the solution of both complementary formulations. The second estimator is based on an energetic approach using the Poynting vector in conductive regions and using the variation of the magnetic energy in nonconductive regions. Both estimators are tested within an adaptive meshing procedure and the results are compared with experimental results.,2009,0,
2068,2069,New Error Containment Schemes for H.264 Decoders,"This paper focuses on new error containment schemes for the H.264 advanced video codec (AVC). A lossless flexible macroblock ordering (FMO) removal scheme that allows playback of FMO-encoded videos on any H.264 player and a novel error concealment method have been developed. H.264 introduces powerful error resilience tools such as FMO to mitigate the effect of errors on the decoded videos. However, many commercial H.264 players cannot handle FMO. We have developed a new method to remove the FMO structure, thereby allowing the video to be decoded on any commercial player. We also present a model that accurately predicts the average overheads incurred by our scheme. At the same time, we developed a new error concealment method for I-frames to enhance video quality without relying on channel feedback. This method is shown to be superior to existing methods, including that from the JM reference software.",2009,0,
2069,2070,Improving Fault Management Using Voting Mechanism in Wireless Sensor Networks,Wireless Sensor Networks (WSN) have the potential of significantly enhancing our ability to monitor and interact with our physical environment. Realizing a fault management operation is critical to the success of WSN. The main challenge is providing fault-tolerance (FT) while conserving the limited resources of the network. In this work we propose a new method for fault management that do the fault detection and fault recovery in decentralized. Simulation result shows that proposed method have better performance.,2010,0,
2070,2071,Modeling and Measuring Error Contributions in Stepwise Synthesized Josephson Sine Waves,"Synthesizing Josephson stepwise approximated sine waves to produce an AC quantum standard is limited by errors in the waveform transitions between quantized voltages. Many parameters have an impact on the shape of the transients. A simple model with a single equivalent time constant can be used to evaluate the influence of these parameters. Measurements of the transients allow establishment of the value of the equivalent time constant and prediction of the response to variations of the parameters. We have experimentally confirmed the influence of changes in the bias current used for the quantized voltages and in the microwave power applied. Under usual operating conditions, the model predicts that increasing the number of samples per period nonlinearly reduces the difference between measured and ideal root-mean-square (rms) values. This behavior was confirmed through measurements with thermal converters.",2009,0,
2071,2072,Prony-Based Optimal Bayes Fault Classification of Overcurrent Protection,"The development of deregulation and demand for high-quality electrical energy has lead to a new requirement in different fields of power systems. In the protection field, this means that high sensitivity and fast operation during the fault are required while maltripping of relay protection is not acceptable. One case that may lead to a maltrip of the high-sensitive overcurrent relay is the starting current of the induction motor or inrush current of the transformer. This transient current has the potential to affect the correct operation of protection relays close to the component being switched. In the case of switching events, such transients must not lead to overcurrent relay operation; therefore, a reliable and secure relay response becomes a critical matter. Meanwhile, proper techniques must be used to prevent maltripping of such relays, due to transient currents in the network. In this paper, the optimal Bayes classifier is utilized to develop a method for discriminating the fault from nonfault events. The proposed method has been designed based on extracting the modal parameters of the current waveform using the Prony method. By feeding the fundamental frequency damping and ratio of the 2nd harmonic amplitude over the fundamental harmonic amplitude to the classifier, the fault case is discriminated from the switching case. The suitable performance of this algorithm is demonstrated by simulation of different faults and switching conditions on a power system using PSCAD/EMTDC software.",2007,0,
2072,2073,EGNOS test bed ionospheric corrections under the October and November 2003 storms,"Two severe geomagnetic storms were experienced on October 29-31 and November 20, 2003, degrading significantly the European Geostationary Navigation Overlay Service (EGNOS) Test Bed (ESTB) performance in Europe. Such storms reached extreme values of Kp=9 during the most severe periods. The analysis of the ESTB ionospheric corrections and their effect on the ESTB integrity and accuracy is presented in this work. The ESTB performance during those storms was monitored from a network of global positioning system (GPS) receivers widely distributed over Europe, including the ESTB reference stations, and the geographical degradation of the accuracy is analyzed in this paper. The correlation between the Kp index and the misleading information (MI) events is also shown. During the most severe stormy periods, the errors in the ESTB ionospheric corrections and its integrity bounds are analyzed to explain the peaks in the navigation system error, which produces MIs. This analysis has been carried out by comparing with direct dual-frequency GPS measurements and global ionospheric maps.",2005,0,
2073,2074,The Application of Travelling Wave Fault Locators in China,"At present there are more than 1500 TWFL equipments in operation in China. The fault location accuracy obtained with TWFL equipment in practice has been better than 500 m. The TWFL systems and the methods of detecting the travelling waves used in China are described. Some typical fault location results from high voltage AC and DC transmission lines are presented. The ""seamless recording"" of transient signals to ensure the capture of fault transients during severe thunderstorm activity is introduced. The application of TWFL to distribution lines is also described.",2008,0,
2074,2075,Correction of smart antennas receiving channels characteristics for 4G mobile communications,The paper considers a way of correction of the reception channels characteristics for smart-antennas in 4G mobile communications.,2003,0,
2075,2076,Fault detection for uncertain sampled-data systems,Studies the fault detection (FD) problem for uncertain sampled-data systems. A direct design approach of the discrete-time FD system is presented which takes the intersample behavior into account. The resulting FD system achieves the best compromise between the sensitivity to the faults and the robustness to the unknown disturbances and model uncertainty.,2002,0,
2076,2077,Towards the next generation of bug tracking systems,"Developers typically rely on the information submitted by end-users to resolve bugs. We conducted a survey on information needs and commonly faced problems with bug reporting among several hundred developers and users of the APACHE, ECLIPSE and MOZILLA projects. In this paper, we present the results of a card sort on the 175 comments sent back to us by the responders of the survey. The card sort revealed several hurdles involved in reporting and resolving bugs, which we present in a collection of recommendations for the design of new bug tracking systems. Such systems could provide contextual assistance, reminders to add information, and most important, assistance to collect and report crucial information to developers.",2008,0,
2077,2078,Enhanced Error Vector Magnitude (EVM) Measurements for Testing WLAN Transceivers,"As wireless LAN devices become more prevalent in the consumer electronics market, there is an ever increasing pressure to reduce their overall cost. The test cost of such devices is an appreciable percentage of the overall cost, which typically results from the high number of specifications, the high number of distinct test set-ups and equipment pieces that need to be used, and the high cost of each test set-up. In this paper, we investigate the versatility of EVM measurements to test the variable-envelope WLAN (wireless local area networks) receiver and transmitter characteristics. The goal is to optimize EVM test parameters (input data and test limits) and to reduce the number of specification measurements that require high test times and/or expensive test equipment. Our analysis shows that enhanced EVM measurements (optimized data sequence and limits, use of RMS, scale, and phase error vector values) in conjunction with a set of simple path measurements (input-output impedances) can provide the desired fault coverage while eliminating lengthy spectrum mask and noise figure tests",2006,0,
2078,2079,"GPR studies in the piano di pezza area of the ovindoli-pezza fault, Central Apennines, Italy: Extending palacoselsmic trench investigations with high resolution GPR profiling","A high-resolution 200 MHz frequency GPR profile with 0.1 m station spacing was collected using a Pulse EKKO 100 system, adjacent to a trench site previously excavated and interpreted by Pantosti et al., 1996. The profile intersected the fault scarp on one of the alluvial fans on the northern slope of the Piano di Pezza; this fault scarp was thought to be associated with repe:ated earthquake activity along the Ovindoli-Pezza fault. The trench excavation exposed a 4 metre sedimentary section formed principally of palaeosols and alluvial fan gravel dipping gently toward the south west, into the Piano di Pezza basin. The deposits exposed in the trench are strongly deformed in a 7 metre complex fault zone, located beneath the main topographic scarp. The scarp is 6.8 metres high at this location. Two palaeo-earthquake events were recognised in the trench, the most recent (circa. 1300 A.D.?) with a vertical displacement of 2.8 to 3.0 metres, and thoucght to be responsible for the current scarp formation, surface rupture, and deformation of the sedimentary section exposed in the upper part of the trench. The second event (circa. 1900 B.C.?) is thought to be represented in the lower part of the trench, and estimated to have a minimum vertical displacement of 2.5 metres. The GPR data were processed using Win EKKO radar data processing software and included the application of SEC gain and time-depth conversion utilising a field derived CMP analysis which provided a velocity of 0.06 dns. In the upper part of the GPR section a strong continuous radar reflector, thought to represent the top of the carbonate scarp, is clearly imaged beneath 1.5 m of surficial cover, and is abruptly down faulted vertically by approximately 2.8 metres, to the south west. The complex, 7 - metre wide deformation zone, recognised in the trench and associated with the most recent catastrophic event, is clearly imaged in the hanging wall of the fault and exhibits a diminishing deformation intensity towa- ds the south west, terminating with a vertical sequence of antithetic faults with a local cumulative offset of 1.5 metres, thus forming a low angle, south west tilted graben structure. In the lower part of the GPR section, the down thrown carbonate surface forms the north eastern margin of a rotated basement block, above which an asymmetric graben has developed within the sedimentary section, with maximum antithetic fault offset of the order of 0.6 metres. This graben is imaged beneath the sedimentary units exposed by the trenching operations, and is interpreted as representing the base of an approximately 12 metres cumulative vertical offset from the original carbonate surface. The GPR data has thus provided a framework for estimating the extent of extensional and episodic catastrophic activity along the fault, and elucidated the geometric relationship between the faulting in response to recent tectonic events.",2004,0,
2079,2080,Experimental Results with Forward Erasure Correction and Real Video Streaming in Hybrid Wireless Networks,"In a heterogeneous MANET, based on wireless LANs linked together by satellite, the overall channel efficiency is impaired by multiple effects, because of multipath fading in the terrestrial segment and atmospheric fading on the satellite link. In this paper we address this issue by applying forward erasure correction codes (FZC) to MPEG-4 video sequences exchanged by the hosts of a hybrid network, made of a satellite link and a wireless LAN using 802.11b devices. A standard video streaming application runs on one end of the satellite link while, at the other end, a wireless ad hoc network receives the multicast video stream. This work aims at demonstrating the improvement in quality of service (QoS) of the video transmitted in the hybrid network. The main parameters measured are the packet loss, the delivery delay, and the overhead in bandwidth occupancy imposed by the use of FZC. The received video is then evaluated by using a MOS (mean opinion score) procedure",2005,0,
2080,2081,Real-Time Error-Feedback Output Regulation of Nonhyperbolically Nonminimum Phase System,A real time implementation of an error feedback output regulation problem for the gyroscopical platform is presented here. It is based on a numerical method for the solution of the so-called regulator equation. The regulator equation consists of partial differential equations combined with algebraic ones and arises when solving the output-regulation problem. Error-feedback output regulation problem aims to find a dynamic feedback compensator using only tracking error measurements to ensure tracking given reference and/or rejecting unknown disturbance. Solving the regulator equation is becoming difficult especially for the non-minimum phase systems where reducing variables against algebraic part leads to possible unsolvable differential part. The proposed numerical method is based on the successive approximation of the differential part of the regulator equation by the finite-element method while trying to minimize functional expressing the error of its algebraical part. This solution is then used to design real-time controller which is successfully experimentally tested.,2007,0,
2081,2082,Soft error detection and correction for FFT based convolution using different block lengths,"The structure of radix-2 Fast Fourier Transforms of length 2<sup>n</sup> where n is an integer is used to propose a new soft error detection and correction scheme for transform based convolution. The scheme can provide up to 100% detection and correction of isolated soft errors for, in many cases, approximately double the original system cost in terms of area and/or computational complexity. This is a substantial reduction when compared with conventional Triple Modular Redundancy. The method can be used for both hardware and software implementations of transform-based convolution.",2009,0,
2082,2083,Quantifying the effects of placement errors on WSN connectivity in grid-based deployments,"Device deployment plays a key role in the performance of any Wireless Sensor Network (WSN) application. WSN device deployment (i.e. the numbers and positions of the devices) must consider several design factors, like coverage, connectivity, lifetime, etc. However, connectivity remains the most fundamental factor especially in harsh environments. Extensive work has been applied on connectivity in WSN deployments. However, realistic physical deployment errors have been ignored in the majority of that work. In this paper, we explore an efficient grid-based deployment planning for connectivity when sensors placement is affected by random bounded errors around their corresponding grid vertices. We propose a new approach to evaluate the average connectivity percentage of the deployed sensor nodes. We apply this approach to practical 3D deployment scenario, namely, the cubic grid-based deployment with bounded uniform random errors. The average connectivity percentage is computed numerically and verified by extensive simulation results. Based on the results, quantified effects of placement errors on the connectivity percentage are outlined.",2010,0,
2083,2084,Notice of Retraction<BR>A channel quality based unequal error protection method for wireless video transmission,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>A channel quality based unequal error protection (UEP) method for wireless video transmission is proposed in this paper. The importance of different partitions of the compressed video bitstream is classified by the corresponding impact on the decoding quality, and has been taken into consideration in designing the transmission system. In order to avoid mapping the important parts into the subcarrier in deep fading, a subcarrier mapping rule combined with the hierarchical QAM (HQAM) is also proposed. Simulation results demonstrate that the proposed UEP method outperforms the equal error protection method (EEP) and the previously developed UEP methods by an average improvement of the received video of 4.0 dB.",2010,0,
2084,2085,Design of arc fault detection system based on CAN bus,"Arc fault detection system (AFDS) is a device intended to protect the power system against the arc fault that may cause fire. When there is an arc fault, the scale of fault current is lower than the initialization of most of the protection devices installed in the lowers, hence AFDS is an effective device to detect the arc fault successfully and interrupt the circuit in time. The characteristics of arc, how it ignites, and what losses it may cause were discussed. The basic structure of AFDS and the primary principles of that AFDS detect arc fault were proposed. For efficiently realizing the global optimization, the single function problem of conventional arc fault circuit interrupter (AFCI) was solved by setting up a detection system. Composed with detectors, controllers and host computer database, system achieved the automatic detection of arc fault, high temperature fault and leakage current fault to protect the conductors, the equipments and ensure human's safety. Detectors, controllers and host computer database were communicated with CAN bus. Device realized the expectations of reducing the communication cost and improving the communication quality.",2009,0,
2085,2086,Computer-aided fault to defect mapping (CAFDM) for defect diagnosis,"Defect diagnosis in random logic is currently done using the stuck-at fault model, while most defects seen in manufacturing result in bridging faults. In this work we use physical design and test failure information combined with bridging and stuck-at fault models to localize defects in random logic. We term this approach computer-aided fault to defect mapping (CAFDM). We build on top of the existing mature stuck-at diagnosis infrastructure. The performance of the CAFDM software was tested by injecting bridging faults into samples of a Streaming audio controller chip and comparing the predicted defect locations and layers with the actual values. The correct defect location and layer was predicted in all 9 samples for which scan-based diagnosis could be performed. The experiment was repeated on production samples that failed scan test, with promising results",2000,0,
2086,2087,Detecting Defects in Golden Surfaces of Flexible Printed Circuits Using Optimal Gabor Filters,"This paper studies the application of advanced computer image processing techniques for solving the problem of automated defect detection for golden surfaces of flexible printed circuits (FPC). A special defect detection scheme based on semi-supervised mechanism is proposed, which consists of an optimal Gabor filter and a smoothing filter. The aim is to automatically discriminate between ""known"" non-defective background textures and ""unknown"" defective textures of golden surfaces of FPC. In developing the scheme, the parameters of the optimal Gabor filter are searched with the help of the genetic algorithm based on constrained minimization of a Fisher cost function. The performance of the proposed defect detection scheme is evaluated off-line by using a set of golden images acquired from CCD. The results exhibit accurate defect detection with low false alarms, thus showing the effectiveness and robustness of the proposed scheme.",2008,0,
2087,2088,"System architecture for error-resilient, embedded JPEG2000 wireless delivery","With new, third generation mobile terminals several multimedia-based applications will be soon available. The reliable transmission of audiovisual content will probably become one of the most asked services. On the other hand, when wireless delivery is addressed, it would be desirable to reach very high compression ratio keeping a good image perceptual quality. With these requirements the choice of JPEG2000 as the source encoding stage assures excellent results. However a non-ideal wireless network could seriously affect the received image decoding, despite JPEG2000's error resilience capabilities. In this paper a flexible DSP/FPGA system with high error-resilience features is proposed. Experimental results show very promising visual quality, with a limited complexity overhead, even in the presence of a mean loss rate of 10%.",2002,0,
2088,2089,An offset self-correction sample and hold circuit for precise applications in low voltage CMOS,"This work describes a new topology for CMOS sample-and-hold circuits, in low voltage, with self-correction of the offset voltage caused by mismatches in the differential input pair of the operational amplifier. The charge injection of the NMOS switches is an important factor and it is minimized in this topology. The results were obtained using the ACCUSIM II simulator on the AMS CMOS 0.8 m CYE and they reveal the circuit has a reduced error of just 0.03% at the output.",2002,0,
2089,2090,Defect-oriented testing and defective-part-level prediction,"After an integrated circuit (IC) design is complete, but before first silicon arrives from the manufacturing facility, the design team prepares a set of test patterns to isolate defective parts. Applying this test pattern set to every manufactured part reduces the fraction of defective parts erroneously sold to customers as defect-free parts. This fraction is referred to as the defect level (DL). However, many IC manufacturers quote defective part level, which is obtained by multiplying the defect level by one million to give the number of defective parts per million. Ideally, we could accurately estimate the defective part level by analyzing the circuit structure, the applied test-pattern set, and the manufacturing yield. If the expected defective part level exceeded some specified value, then either the test pattern set or (in extreme cases) the design could be modified to achieve adequate quality. Although the IC industry widely accepts stuck-at fault detection as a key test-quality figure of merit, it is nevertheless necessary to detect other defect types seen in real manufacturing environments. A defective-part-level model combined with a method for choosing test patterns that use site observation can predict defect levels in submicron ICs more accurately than simple stuck-at fault analysis",2001,0,
2090,2091,Traveling wave-based fault location experiences,"Transmission utility companies continuously strive for high availability of their lines. When a line is off service following a permanent fault, a significant amount of the time during the restore the line can be attributed to locating the point of the fault. This paper shows how this time can be significantly reduced using highly accurate traveling wave (TW) fault locators. It describes location results of several faults in three separated transmissions lines. The collected data are compared to those obtained by considering one- and two-end impedance location algorithms for the same faults. It also highlights the unique characteristics of Reason fault locators to overcome common problems of other TW solutions existing in the market.",2010,0,
2091,2092,Cross-layer WLAN measurement and link analysis for low latency error resilient wireless video transmission,This work introduces a cross-layer measurement and link analysis strategy for video transport over IEEE 802.11. Field trial measurement data is presented for streamed H.264 video over ad-hoc 802.11g links. The data is used to analyze the interactions between the physical/network/transport and application layers. The development of cross layer optimized low latency error resilient video transmission schemes is discussed.,2005,0,
2092,2093,Shannon Capacity and Symbol Error Rate of Space-Time Block Codes in MIMO Rayleigh Channels With Channel estimation Error,"Space-time block coding (STBC) is an attractive solution for improving quality in wireless links. In this paper, we analyze the impact of channel estimation error on the ergodic capacity and symbol error rate (SER) for space-time block coded multiple-input multiple-output (MIMO) systems. We derive a closed-form capacity expression over MIMO Rayleigh channels with channel estimation error. Moreover, we derive an exact closed-form SER for general PAM/PSK/QAM with channel estimation error. We show that, as expected, channel estimation error introduces the capacity loss and diversity gain loss. Furthermore, as the number of transmit and receive antennas increases, the sensitivity of the STBC system to channel estimation error increases. Simulation results demonstrate the accuracy of our analysis.",2008,0,
2093,2094,A Method of simple adaptive control using neural networks with offset error reduction for an SISO magnetic levitation system,"This paper proposes the implementation of the method of SAC using neural network with offset error reduction to control an SISO magnetic levitation system. In this paper, the control input for the SISO magnetic levitation system is given by the sum of the output of a simple adaptive controller and the output of neural networks. The role of neural networks is to compensate for constructing a linearized model so as to minimize the output error caused by nonlinearities in the magnetic levitation system. The neural networks use the backpropagation algorithm for the learning process. The role of simple adaptive controller is to perform the model matching for the linear system with unknown structures to a given linear reference model. In this method, only part of the control input is fed to the PFC. Thus, the error will be reduced using this method, and the output of the magnetic levitation system can follow significantly closely the output of the reference model. Finally, the effectiveness of this method is confirmed through experiments to the real SISO magnetic levitation system.",2010,0,
2094,2095,Defect Classification Algorithm for IC Photomask Based on PCA and SVM,"During IC photomask vision inspection, considering problem that fine image defectpsilas fineness, complex shape, extraction feature difficultly, and effect by noise easily, presented defect identification classification algorithm based on PCA (principal components analysis) and SVM (support vector machine). It resolved the problem that fine and complex defect was difficult to classify, by merits of the extracting image global feature with PCA, and high accuracy and generalization capability with SVM. Regard class distance as criterion to construct the binary tree in multi-class SVM classification algorithm. It resolved the problem that the structure of binary tree affected the accuracy of classifier, and upgraded defect classification accuracy finally. Experiments show that six defects classification accuracy by this method is up to 97.8%, higher than best accuracy 93.3% by BP network and 83.3% by method based on region. And the training and inspecting time is few. In result, itpsilas an effective method for fineness defect identification and classification.",2008,0,
2095,2096,A Dynamic Fault Tolerant Algorithm Based on Active Replication,"To the wide area network oriented distributed computing such as Web services, a slow service is equivalent to an unavailable service. It makes demands of effectively improving the performance without damaging availability to the replication algorithms. Aim for improving the performance of the active replication algorithm, we propose a new replication algorithm named AAR (adaptive active replication). Its basic idea is: all replicas receive requests, but only the fastest one returns the response. Its main advantages are: (1) The response is returned by the fastest replica; (2) The algorithm is based on the active replication algorithm, but it avoids the redundant nested invocation problem. We prove the advantages by analyzing and experiments.",2008,0,
2096,2097,FPGA based realization of a reduced complexity high speed decoder for error correction,"A chip for high speed two bit error correction in the received signal has been designed and implemented on a Xilinx FPGA using VHDL. The design is based on a modified step-by-step decoding algorithm which does not require the calculation of the error location polynomial. The use of complex computation intensive inverse operations is also avoided. Efforts have been made for reducing the complexity of the decoder. A modified circuit has been used for multiplication of field elements within the Galois field. For squaring the field elements within the Galois field, a modified square circuit with much less complexity has been successfully designed. The average operation cycle for decoding each received word is just equal to the block length of the coded word.",2003,0,
2097,2098,Optimal task ordering for troubleshooting systems faults,"Automated troubleshooting of system faults is an essential element of modern aerospace equipment. The increased efficiency and accuracy helps in dramatically lowering maintenance costs. The Prognostic Health Management (PHM) group at Pratt & Whitney is responsible for developing automated systems for fault detection, isolation, and accommodation for the Joint Strike Fighter (JSF) propulsion system. A fundamental question that arises in this context is the following: Given a list of suspected components that have been identified a priori as possible causes for failure symptom(s), what is the optimal troubleshooting task assignment strategy? This paper introduces an approach to optimal task ordering. We show that the correct strategy is to order the tasks based on an easily calculated metric - which we call the mean utility function - that takes into consideration the mean troubleshooting time, or cost, or a combination of the two, depending on what is considered to be most critical. A mathematical proof is given for this. The approach shown in the paper can also be applied, as a troubleshooting strategy, for any other machinery health management system",2005,0,
2098,2099,Using Duplication with Compare for On-line Error Detection in FPGA-based Designs,"It is well known that SRAM-based FPGAs are susceptible to single-event upsets (SEUs) in radiation environments. A variety of mitigation strategies have been demonstrated to provide appropriate mitigation and correction of SEUs in these environments. While full mitigation of SEUs is appropriate for some situations, some systems may tolerate SEUs as long as these upsets are detected quickly and correctly. These systems require effective error detection techniques rather than costly error correction methods. This work leverages a well-known error detection technique for FPGAs called duplication with compare (DWC). This technique has been shown to be very effective at quickly and accurately detecting SEUs using fault injection and radiation testing.",2008,0,
2099,2100,Self-checking and fault tolerance quality assessment using fault sampling,"The computational effort associated with fault simulation (FS) processes in digital systems can become overwhelming, due to circuit complexity, test pattern size or fault list size. The same applies when safety properties (such as fault tolerance or fail-safe) need to be verified in a new product development, in the design environment. If a bridging fault model replaces the simple stuck-at fault model, the fault list size easily becomes very large. If the product needs to comply to safety standards, such as EN298, these properties need to be verified in the presence of double faults, which explodes the fault list dimension. In this paper, a novel method is proposed to deal with this problem, based on fault sampling. A model to compute the confidence level that the global fault coverage, FC, is within the interval [FC<sub>min</sub> 100%] is proposed. A case study, an ASIC for a safety-critical gas burner control system, is used to ascertain the usefulness of the proposed methodology.",2002,0,
2100,2101,Minimizing Latency in Fault-Tolerant Distributed Stream Processing Systems,"Event stream processing (ESP) applications target the real-time processing of huge amounts of data. Events traverse a graph of stream processing operators where the information of interest is extracted. As these applications gain popularity, the requirements for scalability, availability, and dependability increase. In terms of dependability and availability, many applications require a precise recovery, i.e., a guarantee that the outputs during and after a recovery would be the same as if the failure that triggered recovery had never occurred. Existing solutions for precise recovery induce prohibitive latency costs, either by requiring continuous checkpoint or logging (in a passive replication approach) or perfect synchronization between replicas executing the same operations (in an active replication approach). We introduce a novel technique to guarantee precise recovery for ESP applications while minimizing the latency costs as compared to traditional approaches. The technique minimizes latencies via speculative execution in a distributed system. In terms of scalability, the key component of our approach is a modified software transactional memory that provides not only the speculation capabilities but also optimistic parallelization for costly operations.",2009,0,
2101,2102,Compact Delay Test Generation with a Realistic Low Cost Fault Coverage Metric,This paper proposes a realistic low cost fault coverage metric targeting both global and local delay faults. It suggests the test strategy of generating a different number of the longest paths for each line in the circuit while maintaining high fault coverage. This metric has been integrated into the CodGen ATPG tool. Experimental results show significant reductions in test generation time and vector count on ISCAS89 and industry designs.,2009,0,
2102,2103,Bug Hunt: Making Early Software Testing Lessons Engaging and Affordable,"Software testing efforts account for a large part of software development costs. However, as educators, we struggle to properly prepare students to perform software testing activities. This struggle is caused by multiple factors: (1) it is challenging to effectively incorporate software testing into an already over-packed curriculum, (2) ad-hoc efforts to teach testing generally happen too late in the students' career, after bad habits have already been developed, and (3) these efforts lack the necessary institutional consistency and support to be effective. To address these challenges we created Bug Hunt, a web-based tutorial to engage students in learning software testing strategies. In this paper we describe the most interesting aspects of the tutorial including the lessons and feedback mechanisms, and the facilities for instructors to configure the tutorial and obtain automatic student assessment. We also present the lessons learned after two years of deployment.",2007,0,
2103,2104,Improvement of myocardial perfusion defect severity quantitation in cardiac SPECT: A simulation study,"We aim at improving the quantitative assessment of the severity of myocardial perfusion defects in cardiac SPECT imaging. The idea of a numerical heart template is utilized, which enables a patient specific measurement of defect severity as opposed to the more traditional population-based approaches. Using NCAT we developed three male thorax phantoms with different orientations and sizes of the left ventricle. Each heart contained a small (5%) inferior wall defect with a severity of 20-80%. The SimSET code was used to perform 21 simulations modeling cardiac SPECT acquisitions with a Tc-99m radiotracer, LEHR collimator, 64A64 matrix, and 60 camera stops. A conventional method (CM) of defect severity assessment included the MLEM reconstruction with 40 iterations and a calculation of the ratio, RCM, of the average activity concentrations in the defect and the normal heart. Our template method (TM) calculates a new ratio, RTM, which is a correction to RCM by rescaling it between two reference levels corresponding to a completely non-perfused defect and a healthy myocardium. These levels are calculated by projecting and reconstructing two numerical heart templates (may be based on CT in clinical studies) with activity ratios in defect to normal heart set to zero and unity, respectively. The proposed TM method was more accurate and more sensitive to small changes in defect severity than CM. While CM showed no defect (RCM was equal to 0.98-1.02) in the case with 20% severity (true ratio is 0.8), TM led to RTM values of 0.84-0.92. On average, our TM technique exhibited a 17% improvement in defect to normal ratios relative the CM method. Our proposed method offers a patient-specific assessment of perfusion defect severity in SPECT without the limitations intrinsic to traditional methodologies (e.g. extreme heart geometries).",2009,0,
2104,2105,Improving error resilience of scalable H.264 (SVC) via drift control,"Common error concealment schemes mitigate errors for frames in which losses occur only, even though errors propagate to future frames. Drift control is generally challenging due to lack of reliable basis for determining what needs to be corrected and how. In this paper, we show that for scalable or multi-layer video, an available base-layer can serve as such basis to allow continous error drift checking and correction of higher layers even when the base-layer is of much lower spatial resolution. The associated algorithm is low-complexity, incurs no additional bit cost, and experiments using SVC reference software show PSNR improvement of up to 5 dB over concealment methods without drift control.",2010,0,
2105,2106,"Predicting Future States With <formula formulatype=""inline""> <img src=""/images/tex/388.gif"" alt=""n""> </formula> -Dimensional Markov Chains for Fault Diagnosis","This paper introduces a novel method of predicting future concentrations of elements in lubrication oil, for the aim of identifying possible anomalies in continued operation aboard a large marine vessel. The research carried out is supported by a discussion of previous work in the field of fault detection in tribological mechanisms, although with a focus upon two stroke marine diesel engines. The approach taken implements an <i>n</i>-dimensional Markov chain model with a singular weighted connection between layers. The approach leverages the computational simplicity of the Markov chain and combines this with a weighted decision calculated from the correlational coefficients between variables, with the notable assumption that interconnectivity between elements is not constant. The approach is compared to an established method, which is the Kalman filter, with promising results for future work and extension of the method to include expert knowledge in the decision making process.",2009,0,
2106,2107,A large signal dynamic model for single-phase AC-to-DC converters with power factor correction,"This paper presents a model for average current control that can be applied to DC-to-DC converters and AC-to- DC power factor correction (PFC) circuits. The proposed DC-to-DC model consists of two parts: 1) an averaged DC-to-DC converter topology with all the switching elements replaced by dependent sources 2) an average current control scheme with a pulse width modulation (PWM) model, which determines the duty cycles. Similarly, the AC-to-DC PFC model is obtained by combining an averaged boost converter model with the PFC control scheme using average current control. To verify the proposed model, simulated results were compared to experimental waveforms. The experimental results demonstrate that the model can correctly predict the steady-state and large signal dynamic behavior for average current controlled DC-to-DC and AC-to-DC PFC converters.",2004,0,
2107,2108,The global fault-tolerance of interconnection networks,"In this paper, we introduce a new concept in fault-tolerance, namely the global fault-tolerance of interconnection networks. We pose the problem of characterizing the fault-tolerance of an interconnection network, modelled as an undirected unweighted graph, by a scalar, in a global manner. This can be achieved by defining an adequate metric. In this paper, we propose such a metric and we apply it on two comparative analysis: for three infinite families of minimum broadcast graphs (hypercubes, recursive circulants, and Knodel graphs), and for five families of hypercubic graphs (butterfly, wrapped butterfly, shuffle exchange, de Bruijn, and cube connected cycles)",2006,0,
2108,2109,Improving the Table Boundary Detection in PDFs by Fixing the Sequence Error of the Sparse Lines,"As the rapid growth of PDF documents, recognizing the document structure and components are useful for document storage, classification and retrieval. Table, a ubiquitous document component, becomes an important information source. Accurately detecting the table boundary plays a crucial role for many applications, e.g., the increasing demand on the table data search. Rather than converting PDFs to image or HTML and then processing with other techniques (e.g., OCR), extracting and analyzing texts from PDFs directly is easy and accurate. However, text extraction tools face a common problem: text sequence error. In this paper, we propose two algorithms to recover the sequence of extracted sparse lines, which improve the table content collection. The experimental results show the comparison of the performance of both algorithms, and demonstrate the effectiveness of text sequence recovering for the table boundary detection.",2009,0,
2109,2110,Inter-defect charge exchange in silicon particle detectors at cryogenic temperatures,"Silicon particle detectors in the next generation of experiments at the CERN Large Hadron Collider will be exposed to a very challenging radiation environment. The principal obstacle to long-term operation arises from changes in detector doping concentration (N<sub>eff</sub>), which lead to an increase in required operating voltage. We have previously presented a model of inter-defect charge exchange between closely-spaced centres in the dense terminal clusters formed by hadron irradiation. This manifestly non-Shockley-Read-Hall mechanism leads to a marked increase in carrier generation rate and negative space charge over the SRH prediction. We present here measurements of spectra from <sup>241</sup>Am alpha particles and 1064 nm laser pulses as a function of bias over a range of temperatures. Values of N<sub>eff</sub> and substrate type are extracted from the spectra and compared with the model. The model is implemented in both a commercial finite-element device simulator (ISE-TCAD) and a purpose-built simulation of inter-defect charge exchange. Deviations from the model are explored, and conclusions drawn as to the feasibility of operating silicon particle detectors at cryogenic temperatures.",2001,0,
2110,2111,Fault Models and Injection Strategies in SystemC Specifications,"This paper presents fault models and fault injection strategies designed in a simulation platform with reflection capabilities, used for simulating complex systems specified by using SystemC and by adopting a platform-based design approach. The approach allows the designer to work at different levels of abstraction and to take into account permanent and transient faults, and -- most important -- it features a transparent and dynamic mechanism for both injecting faults and analyzing the produced errors, in order to evaluate possible fault detection and/or tolerance design techniques.",2008,0,
2111,2112,Fault Tolerant Network on Chip Switching With Graceful Performance Degradation,"The structural redundancy inherent to on-chip interconnection networks [networks on chip (NoC)] can be exploited by adaptive routing algorithms in order to provide connectivity even if network components are out of service due to faults, which will appear at an increasing rate with future chip technology nodes. This paper is based on a new, fine-grained functional fault model and a corresponding distributed fault diagnosis method that facilitate determining the fault status of individual NoC switches and their adjacent communication links. Whereas previous work on network fault-tolerance assume switches to be either available or fully out of service, we present a novel adaptive routing algorithm that employs the remaining functionality of partly defective switches. Using diagnostic information, transient faults are handled with a retransmission scheme that avoids the latency penalty of end-to-end repeat requests. Thereby, graceful degradation of NoC communication performance can be achieved even under high failure rates.",2010,0,
2112,2113,Performance evaluation of color correction approaches for automatic multi-view image and video stitching,"Many different automatic color correction approaches have been proposed by different research communities in the past decade. However, these approaches are seldom compared, so their relative performance and applicability are unclear. For multi-view image and video stitching applications, an ideal color correction approach should be effective at transferring the color palette of the source image to the target image, and meanwhile be able to extend the transferred color from the overlapped area to the full target image without creating visual artifacts. In this paper we evaluate the performance of color correction approaches for automatic multi-view image and video stitching. We consider nine color correction algorithms from the literature applied to 40 synthetic image pairs and 30 real mosaic image pairs selected from different applications. Experimental results show that both parametric and non-parametric approaches have members that are effective at transferring colors, while parametric approaches are generally better than non-parametric approaches in extendability.",2010,0,
2113,2114,Fault detection of eccentricity and bearing damage in a PMSM by means of wavelet transforms decomposition of the stator current,"This paper presents the study of permanent magnet synchronous machines (PMSM) running with eccentricity and bearings damage. The objective is to detect and identify the fault through the current signature analysis. The stator current has been analyzed by means of both Fourier (FFT) and Discrete Wavelet (DWT) transforms. Simulations have been carried out with a two-dimensional (2-D) finite element analysis (FEA), and they have been also compared with experiments. The results prove that the method proposed can be used to identify mechanical faults in a PMSM.",2008,0,
2114,2115,Error Detection Reliability of LTE CRC Coding,"The error detection performance of CRC coding in LTE with general two-level early stopping algorithms for turbo decoding is investigated. Analytical models for the probability of block error and undetected block error at the code block and transport block levels were developed. Simulations were used to verify the model for shorter CRC lengths and block sizes. The analytical models show the setting of 24-bit CRCs in LTE allows low-complexity, robust and reliable early stopping algorithms to reduce turbo decoding complexity.",2008,0,
2115,2116,A CPW bandpass filter using defected ground structure with shorting stubs for 60 GHz applications,"In this paper, we design and fabricate a coplanar waveguide (CPW) bandpass filter operating at 60 GHz. A new DGS pattern is proposed and serves as the unit cell. We cascade two unit cells to construct a second-order bandpass filter. The equivalent lumped element circuit model was extracted form the full-wave electromagnetic simulation. Moreover, we obtain good agreement between circuit model analysis and simulation. The measurement results for the proposed bandpass filter reveals a 3 dB pass band with a bandwidth of 6.8 GHz. Furthermore, the insertion loss at 60 GHz is less than 2 dB.",2010,0,
2116,2117,A fault tolerance approach for distributed systems using monitoring based replication,"High availability is a desired feature of a dependable distributed system. Replication is a well-known technique to achieve fault tolerance in distributed systems, thereby enhancing availability. We propose an approach relying on replication techniques and based on monitoring information to be applied in distributed systems for fault tolerance. Our approach uses both active and passive strategies to implement an optimistic replication protocol. Using a proxy to handle service calls and relying on service replication strategies, we effectively deal with the complexity and overhead issues. This paper presents an architecture for implementing the proxy based on monitoring data and the replication management. Experimentation and application testing using an implementation of the architecture is presented. The architecture is demonstrated to be a viable technique for increasing dependability in distributed systems.",2010,0,
2117,2118,Implementation of e-beam proximity effect correction using linear programming techniques for the fabrication of asymmetric bow-tie antennas,"Antenna-coupled tunnel junction diodes have recently been offering great advantages for IR and Terahertz detection applications. Fabrication has been a major constraint in our ability to field these devices. The first obstacle is the relatively small size of the antenna. As the length of the wave to be detected gets smaller, the size of the antenna shrinks according to the A/4 rule. This eliminates the use of traditional photolithographic fabrication techniques, which fails in the nanometer geometry range. For this reason, e-beam lithographic technique is used. The second challenge appears in the fabrication of the tunnel junction. The tunnel junction part of the device is formed by sandwiching an insulation layer in between two conductor antenna parts. Previously, many fabrication techniques were offered for the vertical conductor-insulator-conductor (CIC) structures where two metal layers overlap each other forming a tunnel junction vertical to the antenna surface. However, planar CIC structures have become more popular because they enable the surface plasmon excitement across the tunnel junction barrier. The fabrication of planar tunnel junction requires the patterning of a nano-size gap that will enable the tunneling of the electrons in between two conductor antenna wings. At this critical location, e-beam proximity effect (pixel-to-pixel beam interactions) becomes a very important issue to be addressed in order to create a nanometer-range accuracy gap.",2009,0,
2118,2119,Fault detection of Air Intake Systems of SI gasoline engines using mean value and within cycle models,"This paper addresses the detection of faults in Air Intake Systems (AIS) of SI gasoline engines based on realtime measurements. It presents comparison of two classes of models for fault detection, namely those using a Mean Value Engine Model (MVEM) involving variables averaged over cycles and Within Cycle Crank-angle-based Model (WCCM) involving instantaneous values of variables changing with crank angle. Numerical simulation results of intake manifold leak and mass air flow sensor gain faults, obtained using the industry standard software called AMESim<sup>TM</sup>, have been used to demonstrate the fault detection capabilities of individual approaches. Based on these results it is clear that the method using WCCM has a higher fault detection sensitivity compared to one that uses MVEM, albeit at the expense of increased computational and modeling complexity.",2009,0,
2119,2120,Extension of Power Line Fault Location Techniques to Pressurized Line Diagnostics,"Researchers in power systems have investigated fault location methods for many years and have developed mature techniques. Analogies between hydraulic systems and electrical circuits are long established, and are useful for pipeline system analysis. Based on an electric model of the pressurized lines, this paper provides a feasibility study of extending power line fault location techniques to pressurized line leakage diagnostics. Theoretical derivations are presented in the paper.",2006,0,
2120,2121,Analysis of mechanical stresses developed on T-G shaft during faults,"The instability of an interconnected power system means a condition denoting loss of synchronism or falling out of phase. Stability considerations have been recognized as an essential part of power system planning for a long time. With interconnected systems continually growing in size and extending over vast geographical regions, it is becoming increasingly more difficult to maintain synchronism between various parts of power system. It is found that due to transient disturbance generator speeds up causing motor angle to increase resulting in the development of varying torque on the rotor of generator. This varying torque develops the varying stresses on the motor shaft which when crosses the endurance limit i.e 45 times 10<sup>7</sup> N/m<sup>2</sup> leads to damage of the shaft. In this paper a strategy is presented which seeks the coordination of generator drooping, fast valving and resynchronising the dropped generator by 120deg phase rotation. The implementation of this strategy will enable more effective use of generator dropping without load shedding which is normally associated with generator dropping.",2007,0,
2121,2122,Thermal and magnetic characteristics of bulk superconductor and performance analysis of magnetic shielding type of superconducting fault current limiter,"Superconducting fault current limiter (SCFCL) is expected to be the first application of a high-temperature superconductor (HTS) to electric power systems. The authors have been developing a magnetic shielding type of SCFCL that uses a cylindrical Bi-2223 HTS bulk. Short-circuit fault tests in a small SCFCL model were performed experimentally. A computer program based on the finite element method (FEM) taking the voltage-current (E-J) characteristics of the bulk material into account was developed to analyze the performance in the short-circuit fault tests and to investigate the dynamic electromagnetic behavior within a bulk superconductor. Because the E-J characteristic of HTS bulk depends on temperature and magnetic field, they investigated experimentally the E-J characteristics of a bulk superconductor in various operating temperatures and magnetic fields. The computer program considering the measured E-J characteristics simulated the electromagnetic behaviors in an SCFCL test model successfully (BiPb)<sub>2</sub>Sr<sub>2</sub>Ca<sub>2</sub>Cu<sub>3</sub>O<sub>10 </sub>",2001,0,
2122,2123,A taxonomy of software security defects for SST,"Software security test (SST) is a useful way to validate software system security attribute. Defects based testing technologies are more effective than traditional specification testing technologies, and more and more researchers pay their attention to the testing methods. Before testing, an organized list of actual defects is especially essential. But at present the only existing suitable taxonomies are mostly for software designers or tool-builders, and do not adequately represent security defects that are found in modern software. In our work, we have coalesced previous efforts to categorize security errors as well as problem reports in order to create a kind of security defects taxonomy. We correlate this taxonomy with available information about current Top 10 software dangerous errors, which come from CWE, SANS and other authoritative vulnerabilities enumerations. We suggest that this taxonomy is suitable for software security testers and to outline possible areas of future research.",2010,0,
2123,2124,Fault tolerance techniques for high capacity RAM,"As the complexity and size of the embedded memories keep increasing, improving the yield of embedded memories is the key step toward improving the overall chip yield of a SOC design. The most well known way to improve the memory yield is by using redundant elements to replace the faulty cells. However, the repair efficiency mainly depends on the type, and the amount of redundancy; and on the redundancy analysis algorithms. Therefore, new types of redundancy based on divided bit-line (DBL), and divided word-line (DWL) techniques are proposed in this work. A memory column (row), including the redundant column (row), is partitioned into column blocks (row blocks), respectively. A row/column block is used as the basic replacement element instead of a row/column for the traditional approaches. Based on the new types of redundancy, three types of fault-tolerant memory (FTM) systems are also proposed. If a redundant row/column block is used as the basic replacement element, then the row block-based FTM (RBFTM)/column block-based (CBFTM) system is used. If both the DWL, and DBL techniques are implemented onto a memory chip, then the hybrid FTM (HFTM) system is achieved. The storage and remapping of faulty addresses can be implemented with a CAM (content addressable memory) block. To achieving better repair efficiency, a novel hybrid block-repair (HBR) algorithm is also proposed. This algorithm is suitable for hardware implementation with negligible overhead. For the HFTM system, the hardware overheads are less than 0.65%, and 0.7% for 64-Kbit SRAM, and 8-Mbit DRAM, respectively. Moreover, the repair rate can be improved significantly. Experimental results show that our approaches can improve the memory fabrication yield significantly. The characteristics of low power and fast access time of DBL and DWL techniques are also preserved.",2006,0,
2124,2125,Online Failure Forecast for Fault-Tolerant Data Stream Processing,"In this paper, we present a new online failure forecast system to achieve predictive failure management for fault-tolerant data stream processing. Different from previous reactive or proactive approaches, predictive failure management employs failure forecast to perform informed and just-in-time preventive actions on abnormal components only. We employ stream-based online learning methods to continuously classify runtime operator state into normal, alert, or failure, based on collected feature streams. We have implemented the online failure forecast system as part of the IBM system S stream processing system. Our experiments show that the on-line failure forecast system can achieve good prediction accuracy for a range of stream processing software failures, while imposing low overhead to the stream system.",2008,0,
2125,2126,Multilevel-converter-based VSC transmission operating under fault AC conditions,"A study of a floating-capacitor (FC) multilevel-converter-based VSC transmission system operating under unbalanced AC conditions is presented. The control strategy is based on the use of two controllers, i.e. a main controller, which is implemented in the synchronous d-q frame without involving positive and negative sequence decomposition, and an auxiliary controller, which is implemented in the negative sequence d-q frame with the negative sequence current extracted. Automatic power balancing during AC fault is achieved without communication between the two converters by automatic power modulation on the detection of abnormal DC voltages. The impact of unbalanced floating capacitor voltages of the FC converter on power devices is described. A software-based method, which adds square waves whose amplitudes vary with the capacitor voltage errors to the nominal modulation signals for fast capacitor voltage balancing during faults, is proposed. Simulations on a 300 kV DC, 300 MW VSC transmission system based on a four-level FC converter show good performance of the proposed control strategy during unbalanced conditions caused by single-phase to ground fault.",2005,0,
2126,2127,Influence of the nonlinearity of measurement transformers on effectiveness of module error correction,The article presents the program spectral line method of module error correction in linear input circuits. The method requires the knowledge of the frequency characteristics of the module errors and the phase errors of the input circuit to be corrected. The method was used for frequency error correction of measurement transformers applied in electronic power transducers. Such input circuits are nonlinear and distort the measured signals. The article presents the analysis of the influence of the nonlinearity of the input circuits upon the effectiveness of correction of frequency errors of the input circuit module at measurements of the root-mean-square value.,2008,0,
2127,2128,A novel technique for coupling three dimensional mesh adaptation with an a posteriori error estimator,"We present a novel error estimation driven 3D unstructured mesh adaptation technique based on a posteriori error estimation techniques with upper and lower error bounds. In contrast to other work (Oden, 2002; Prudhomme et al., 2003) we present this approach in three dimensions using unstructured meshing techniques to potentiate an automatically adaptation of 3D unstructured meshes without any user interaction. The motivation for this approach, the applicability and usability is presented with real-world examples.",2005,0,
2128,2129,Fault-tolerant routing on Complete Josephus Cubes,"This paper introduces the Complete Josephus Cube, a fault-tolerant class of the recently proposed Josephus Cube and proposes a cost-effective, fault-tolerant routing strategy for the Complete Josephus Cube. For a Complete Josephus Cube of order r, the routing algorithm can tolerate up to (r+1) encountered component faults in its message path and generates routes that are both deadlock-free and livelock-free. The message is guaranteed to be optimally (respectively, sub-optimally) delivered within a maximum of r (respectively, 2r+1) hops. The message overhead incurred is only a single (r+2)-bit routing vector accompanying the message to be communicated",2001,0,
2129,2130,Design of monitoring and Fault Diagnosis System on-line for certain gas turbo-generator set,"Since its parameters can not been test about the certain gas turbo-generator set in local rapidly after repairing, a monitoring and fault diagnosis system based on IPC is designed in this paper. The system adopts advanced measuring technology based on PCI bus, intelligent appearance and VI technology and can check voltage, electric current, power and temperature of key parts on-line about the set etc. Simultaneously it can give an alarm when the parameter is abnormal and diagnose basic fault by FDES (Fault Diagnosis Expert System). Its results has characteristics of high reliability, high speed and intuitive. So it can improve efficiency of test system maximum and save repair expenditure. The system can helps to improve reliability of gas turbo-generator set and power quality.",2008,0,
2130,2131,Hierarchical fault diagnosis and health monitoring in multi-platform space systems,"Current spacecraft health monitoring and fault diagnosis practices that involve around-the-clock limit-checking and trend analysis on large amount of telemetry data, do not scale well for future multi-platform space missions due to the presence of larger amount of telemetry data and an increasing need to make the long-duration missions cost-effective by limiting the size of the operations team. The need for efficient utilization of telemetry data by employing machine learning and rule-based reasoning has been pointed out in the literature in order to enhance diagnostic performance and assist the less-experienced personnel in performing monitoring and diagnosis tasks. In this research we develop a systematic and transparent fault diagnosis methodology within a hierarchical fault diagnosis framework for multiplatform space systems. Our proposed Bayesian network-based hierarchical fault diagnosis methodology allows fuzzy rule-based reasoning at different components in the hierarchy. Due to the unavailability of real formation flight data, we demonstrate the effectiveness of our proposed methodology by using synthetic data of a leader-follower formation flight. Our proposed methodology is likely to enhance the level of autonomy in ground support based spacecraft health monitoring and fault diagnosis.",2009,0,
2131,2132,Fast error resilient H.263 to H.264 video transcoding using RD optimized intra fresh,"A key issue of error resilient transcoding using ldquointra freshrdquo algorithm is how to decide the intra coding rate and adjust corresponding bits for error resilience, so that jointly optimization between video source and error resilience bit can be achieved. To provide practical solution for this issue, we propose to use rate distortion (RD) optimization criterion to decide the application of intra coding mode at MB level. To reduce the implementation complexity of RD optimization intra fresh method, we also present a fast mode decision algorithm that takes advantage of error propagation distortion estimation as assistance to decide if an early termination of existing coding mode choices is necessary. The algorithm is implemented to realize fast error resilient H.263 to H.264 video transcoding. Compared with error resilient transcoding method using random intra update, the proposed algorithm can enhance speed up to 7.2 times while maintaining better picture quality.",2008,0,
2132,2133,Error rates for Nakagami-m fading multichannel reception of binary and M-ary signals,"This paper derives new closed-form formulas for the error probabilities of single and multichannel communications in Rayleigh and Nakagami-m (1960) fading. Closed-form solutions to three generic trigonometric integrals are presented as part of the main result, providing a unified method for the derivation of exact closed-form average symbol-error probability expressions for binary and M-ary signals with L independent channel diversity reception. Both selection-diversity and maximal-ratio combining (MRC) techniques are considered. The results are generally applicable for arbitrary two-dimensional signal constellations that have polygonal decision regions operating in a slow Nakagami-m fading environments with positive integer fading severity index. MRC with generically correlated fading is also considered. The new expressions are applicable in many cases of practical interest. The closed-form expressions derived for a single channel reception case can be extended to provide an approximation for the error rates of binary and M-ary signals that employ an equal-gain combining diversity receiver",2001,0,
2133,2134,Fault Tolerance Middleware for Cloud Computing,"The Low Latency Fault Tolerance (LLFT) middleware provides fault tolerance for distributed applications deployed within a cloud computing or data center environment, using the leader/follower replication approach. The LLFT middleware consists of a Low Latency Messaging Protocol, a Leader-Determined Membership Protocol, and a Virtual Determinizer Framework. The Messaging Protocol provides are liable, totally ordered message delivery service by employing a direct group-to-group multicast where the ordering is determined by the primary replica in the group. The Membership Protocol provides a fast reconfiguration and recovery service when a replica becomes faulty and when a replica joins or leaves a group. The Virtual Determinizer Framework captures ordering information at the primary replica and enforces the same ordering at the backup replicas for major sources of non-determinism. The LLFT middleware maintains strong replica consistency, offers application transparency, and achieves low end-to-end latency.",2010,0,
2134,2135,Using Web services for atmospheric correction of remote sensing data,"We present the technical implementation details for a prototype central atmospheric correction parameter server called NOMAD (Networked On-line Mapping of Atmospheric Data). Using a Web service, aerosol optical depth (AOD) values are transmitted via the network to a Web service aware atmospheric correction application. Information about the date and location of the image are extracted automatically from the image, allowing a best estimate of AOD at 500 nm to be retrieved from the central server database. A Web service approach was adopted to allow easy cross platform development in multiple software languages. Using the Web Services Definition Language (WSDL) description of the atmospheric correction parameter server Web service, application developers can easily make their atmospheric correction applications ""Web aware"". On the server side, researchers maintaining the central database are free to concentrate on providing the best quality data available.",2002,0,
2135,2136,Shape-space based negative selection algorithm and its application on power transformer fault diagnosis,"Dissolved gas analysis is an effective and important method for power transformer fault diagnosis. The negative selection algorithm has much advantage for some faults which lack a great deal of training sample data in the power transformer faults. It can examine the abnormality of the infinite categories by using fewer detectors, covering with wide space. However, the existing negative selection algorithm also exists some shortages. For these shortages, this paper researches a shape-space based negative selection algorithm, which uses the shape-space model in the mathematics theory, mapped the data of detector, self space and non-self space to the n dimension space, using the calculation of affinity to carry out matching. Experiments show that the method can make use of few fault data (i.e. antigen) to obtain the mature training set, so it is suitable for the small sample fault diagnosis of which the failure data is difficult to get.",2007,0,
2136,2137,Generating Compact Robust and Non-Robust Tests for Complete Coverage of Path Delay Faults Based on Stuck-at Tests,"A new rest generation method of fully scanned or combinational circuits is proposed for complete coverage of path delay faults based on single stuck-at tests. The proposed method adds the target path into the original circuit, where all off inputs of the path are connected with corresponding nodes in the original circuit. Test generation of the path delay fault is reduced to that of the single stuck-at fault at the fanout branch, where the additional path connects with its source node in the original circuit. A disjoint dynamic test compaction scheme is proposed to reduce the size of the test set in the process of test generation. A conjoint test compaction scheme is proposed based on fanout counts of the paths. The proposed method presents a very compact test set for complete coverage of robustly and non-robustly testable path delay faults.",2006,0,
2137,2138,Fault secure datapath synthesis using hybrid time and hardware redundancy,A fault-secure datapath either generates a correct result or signals an error. This paper presents a register transfer level concurrent error detection (CED) technique that uses hybrid time and hardware redundancy to optimize the time and area overhead associated with fault security. The proposed technique combines the idle computation cycles in a datapath with selective breaking of data dependences of the normal computation. Designers can tradeoff time and hardware overhead by varying these design parameters. We present an algorithm to synthesize fault secure designs and validate it using Synopsys' behavioral compiler.,2004,0,
2138,2139,Fault-Tolerant Scheduling for Periodic Tasks based on DVFS,"Dynamic voltage and frequency scaling (DVFS) technique is emerging in various battery-operated embedded systems to reduce the energy consumption and prolong the working life of the system. However, DVFS technology has been proved to have some direct and negative effects on the reliability of the system. Most existing schedulers of real-time tasks based on DVFS only focus on minimizing energy consumption without taking the fault-tolerant into account. To solve this problem, in this study, we developed a novel energy-aware fault-tolerant (EAFT) technique that was tailored for the real-time periodic tasks. The heuristic EAFT balances the allocation of slacks used for reducing energy consumption and used for re-executing the failed tasks. The simulation results showed that the proposed reliability-aware schemes could guarantee the system reliability and significantly save energy comparing to the existing allocating schemes.",2008,0,
2139,2140,Probability of Error Analysis for Hidden Markov Model Filtering With Random Packet Loss,"This paper studies the probability of error for maximum a posteriori (MAP) estimation of hidden Markov models, where measurements can be either lost or received according to another Markov process. Analytical expressions for the error probabilities are derived for the noiseless and noisy cases. Some relationships between the error probability and the parameters of the loss process are demonstrated via both analysis and numerical results. In the high signal-to-noise ratio (SNR) regime, approximate expressions which can be more easily computed than the exact analytical form for the noisy case are presented",2007,0,
2140,2141,Dynamic Derivation of Application-Specific Error Detectors and their Implementation in Hardware,"This paper proposes a novel technique for preventing a wide range of data errors from corrupting the execution of applications. The proposed technique enables automated derivation of fine-grained, application-specific error detectors. An algorithm based on dynamic traces of application execution is developed for extracting the set of error detector classes, parameters, and locations in order to maximize the error detection coverage for a target application. The paper also presents an automatic framework for synthesizing the set of detectors in hardware to enable low-overhead runtime checking of the application execution. Coverage (evaluated using fault injection) of the error detectors derived using the proposed methodology, the additional hardware resources needed, and performance overhead for several benchmark programs are also reported",2006,0,
2141,2142,Design of safety monitor schemes for a fault tolerant flight control system,"For a research aircraft, ""conventional"" control laws (CLs) are implemented on a ""baseline"" flight computer (FC) while research CLs are typically housed on a dedicated research computer. Therefore, for an experimental aircraft used to test specific fault tolerant flight control systems, a safety logic scheme is needed to ensure a safe transition from conventional to research CLs (while at nominal conditions) as well as from research CLs at nominal conditions to conditions with ""simulated"" failures on specific control surfaces. This paper describes the design of such a safety scheme for the NASA Intelligent Flight Control System (IFCS) F-15 Program. The goals of the IFCS F-15 program are to investigate the performance of a set of fault tolerant CLs based on the use of dynamic inversion with neural augmentation. The different transitions are monitored using information relative to flight conditions and controller-related performance criteria. The testing of the scheme is performed with a Simulink-based flight simulation code and interface developed at West Virginia University for the NASA IFCS F-15 aircraft.",2006,0,
2142,2143,Identification of mechanical defects in MEMS using dynamic measurements for application in the production monitoring,In this paper investigations for the non-destructive characterization of MEMS (Micro-Electro-Mechanical- Systems) are presented that can be applied in the production monitoring in early stages. Different aspects and experimental results are shown for silicon membrane structures with artificial faults. The structures were characterized by their resonant frequencies and associated mode shapes measured via laser-Doppler vibrometry. The consequence of the artificial faults is investigated on the basis of the ratios of measured resonant frequencies and the quantified comparison of mode shapes using the MAC-value. The artificial faults have a significant influence on the dynamic properties which depends on their size. According to the results a systematical approach for the dynamic characterization is derived from the results of the investigations for a possible application in the production monitoring.,2009,0,
2143,2144,Design and Analysis of Three-Phase Reversible High-Power-Factor Correction Based on Predictive Current Controller,"A predictive current control for three-phase PWM rectifiers is proposed to simplify the design of the current loop with less memory and interrupt resource occupation in a digital signal processor. Together with the decomposition-matrix method, the total algorithm can be completed with only one timer and its underflow interrupt subroutine. The predictive current controller helps to design the voltage regulator and make the input currents track the input voltages so well that high power factor is achieved. Finally, the experimental results are given to verify the proposed predictive current control.",2008,0,
2144,2145,Extraction error modeling and automated model debugging in high-performance custom designs,"In the design cycle of high-performance integrated circuits, it is common that certain components are designed directly at the transistor level. This level of design representation may not be appropriate for test generation tools that usually require a model expressed at the gate level. Logic extraction is a key step in test model generation to produce a gate-level netlist from the transistor-level representation. This is a semi-automated process which is error-prone. Once a test model is found to be erroneous, manual debugging is required, which is a resource-intensive and time-consuming process. This paper presents an in-depth analysis of typical sets of extraction errors found in the test model representations of the pipelines in high-performance designs today. It also develops an automated debugging solution for single extraction errors for pipelines with no state equivalence information. A suite of experiments on circuits with similar architecture to that found in the industry confirms the fitness and practicality of the solution",2006,0,
2145,2146,Quantitative analysis of first-pass contrast-enhanced myocardial perfusion multidetector CT using a Patlak plot method and extraction fraction correction during adenosine stress,"The purpose of this study was to develop a quantitative method for myocardial blood flow (MBF) measurement that can be used to derive accurate myocardial perfusion measurements from dynamic multidetector computed tomography (MDCT) images by using a compartment model for calculating the first-order transfer constant (K1) with correction for the capillary transit extraction fraction (E). Six canine models of left anterior descending (LAD) artery stenosis were prepared and underwent first-pass contrast-enhanced MDCT perfusion imaging during adenosine infusion (0.14-0.21 mg/kg/min). K<sub>1</sub>, which is the first-order transfer constant from left ventricular (LV) blood to myocardium, was measured using the Patlak plot method applied to time-density curve data of the LV blood pool and myocardium. The results were compared against microsphere MBF measurements, and the extraction fraction of contrast agent was calculated. K<sub>1</sub> is related to the regional MBF as K<sub>1</sub> = EF, E = (1-exp(-PS/F)), where PS is the permeability-surface area product and F is myocardial flow. Based on the above relationship, a look-up table from K<sub>1</sub> to MBF can be generated and Patlak plot-derived K<sub>1</sub> values can be converted to the calculated MBF. The calculated MBF and microsphere MBF showed a strong linear association. The extraction fraction in dogs as a function of flow (F) was E = (1- exp(-(0.253 F + 0.7871)/F)). Regional MBF can be measured accurately using the Patlak plot method based on a compartment model and look-up table with extraction fraction correction from K<sub>1</sub> to MBF.",2009,0,
2146,2147,The hybrid video error concealment algorithm with low complexity approach,"The transmission of the multimedia data under Internet environment has been wildly used in many applications in the recent. However, the video data is very sensitive to the transmission error caused by packet lost. This induces decoded video data with error propagation and makes the video quality very poor. In this paper we proposed the hybrid error concealment algorithm with low complexity approach. By use of the bidirectional frame as the referenced frame, error propagation in inter frame coding mode can be reduced largely. Our approach has the advantage of better video quality within a small search range and low complexity requirement.",2003,0,
2147,2148,On the effect of word error rate on automated quality monitoring,"This paper studies the effect of word-error-rate (WER) on an automated quality monitoring application for call centers. The system consists of a speech recognition module and a call ranking module. The call ranking module combines direct question answering with a maximum-entropy classifier to automatically monitor the calls that enter a call center, and label them as ""good"" or ""bad"". We find that, in the monitoring regime where only a small fraction of the calls are monitored, we achieve 80% precision and 50% recall in classifying whether a call belongs to the bottom 20%. Additionally, the correlation between human and computer-generated scores turns out to be highly sensitive to word error rate.",2006,0,
2148,2149,A practical system for online diagnosis of control valve faults,"In this paper, we present a new control valve monitoring system using the nonparametric statistical hypothesis tests for the diagnosis of backlash, deadband, leakage, and blocking, four common faults found in many control systems. To make our system practical and inexpensive, we utilize the sensor measurements available in most control systems. For the classification of individual faults, we extract the geometric features from the measurement signals and detect the faults from their temporal trends. Based on the features, the statistical hypothesis tests are applied to discriminate the faults.",2007,0,
2149,2150,Analysis and Design for a Novel Single-Stage High Power Factor Correction Diagonal Half-Bridge Forward AC&#8211;DC Converter,"By means of components placement, the buck-boost and diagonal half-bridge forward converters are combined to create a novel single-stage high power factor correction (HPFC) diagonal half-bridge forward converter. When both the PFC cell and dc-dc cell operate in DCM, the proposed converter can achieve HPFC and lower voltage stress of the bulk capacitor. The circuit analysis of the proposed converter operating in DCM+DCM mode is presented. In order to design controllers for the output voltage regulation, the ac small-signal model of the proposed converter is derived by the averaging method. Based on the derived model, the proportional integral (PI) controller and minor-loop controller are then designed. The simulation and experimental results show that the proposed converter with the minor-loop controller has faster output voltage regulation than that with the PI controller despite the variations of line voltage and load. Finally, a 100-W prototype of the proposed ac-dc converter is implemented and the theoretical result is experimentally verified",2006,0,
2150,2151,"Fault Tolerance using ""Parallel Shadow Image Servers (PSIS)"" in Grid Based Computing Environment","This paper presents a critical review of the existing fault tolerance mechanism in grid computing and the overhead involved in terms of reprocessing or rescheduling of jobs, if in case a fault arisen. For this purpose we suggested the parallel shadow image server (PSIS) copying techniques in parallel to the resource manager for having the check points for rescheduling of jobs from the nearest flag, if in case the fault is detected. The job process is to be scheduled from the resource manager node to the worker nodes and then its' submitted back by the worker nodes in serialized form to the parallel shadow image servers from the worker nodes after the pre-specified amount of time, which we call the recent spawn or the flag check point for rescheduling or reprocessing of job. If the fault is arisen then the rescheduling is done from the recent check point and submitted to the worker node from where the job was terminated. This will not only save time but will improve the performance up to major extent",2006,0,
2151,2152,Effect of Implantation Defects and Carbon Incorporation on Si/SiGe Bipolar Characteristics,"Incorporation of carbon in SiGe has attracted great interest, which makes SiGeC based heterojunction transistors as an attractive device for high frequency applications. Carbon addition in SiGe dramatically reduces out-diffusion of boron caused by excess of interstitials generated by extrinsic base implantation. However carbon incorporation negatively influences the electrical device characteristics. In this paper we investigate active implantation defects, the aim was to the specify space localization and parasitic effects of this ones. Second, we investigate the impact of carbon content on the electrical characteristics of device, the results show that indeed C contents A 1% severely degrade transistor performances.",2009,0,
2152,2153,A New Method for Discriminating Between Internal Faults and Inrush Current Conditions in Power Transformers Based on Neuro Fuzzy,"in this paper, a new algorithm based on Neuro Fuzzy for differential protection of the power transformer is proposed. This algorithm consists of considering the ratio and the difference phase angle of the second harmonic to the fundamental component of differential currents under various conditions. These two protection functions are computed and the protective system operates in less than one cycle after the occurrence of disturbance. Another advantage of this algorithm is that fault detection doesn't depend on the selection of thresholds and one of the best advantages of this algorithm is that all kinds of internal fault can be recognized from magnetizing inrush current even those which are mixed with inrush currents. The proposed Nerue Fuzzy is trained by obtained data from simulation of power system under different faults and switching conditions. The suitable operation of this algorithm by simulation of faults and various switching conditions on a power transformer is shown.",2007,0,
2153,2154,Modeling by groups for faults tolerance based on multi agent systems,"The mobile Ad hoc networks are distributed environments characterized by a high mobility and limited battery resources. In these networks, mobiles nodes are subject to many errors. In this paper, we present our approach of modeling by groups for faults tolerance based in MAS, which predicts a problem and provide decisions in relation to critical nodes. Our work contributes to the resolution of two points. First, we propose an algorithm for modeling by groups in wireless network Ad hoc. Secondly, we study the fault tolerance by prediction of disconnection and partition in network; therefore we provide an approach which distributes efficiently the information in the network by selecting some objects of the network to be duplicates of information.",2010,0,
2154,2155,Basic Fault Tree Analysis for use in protection reliability,Fault tree analysis (FTA) is a tool originally developed in 1962 by Bell Labs for use in studying failure modes in the launch control system of the Minuteman missile project. The tool now finds wide use in numerous applications from accident investigation to design prototyping and is also finding use for protection and control related applications. This paper provides an elementary background to the application of fault tree analysis for use in protection applications. The construction of the fault tree as well as the use of reliability data is considered. A simple example is presented. The intention is to provide a brief introduction to the concept to allow users to at least understand how a fault tree is constructed and what can be done with it.,2007,0,
2155,2156,"Building an adaptable, fault tolerant, and highly manageable web server on clusters of non-dedicated workstations","Clustered server architecture is increasingly being viewed as a successful and cost-effective approach to building a high-performance Web server. Existing server-clustering schemes have typically concentrated on the following issues: scalability, high availability, and user transparency. In this paper, we argue that the design goals of the Web server cluster should include adaptability, fault tolerance, and high manageability. In the presence of the Internet's highly unpredictable workload, the server system should be self-adapting to changing circumstances. We address this problem by building a Web server on a cluster of non-dedicated workstations. Such a server can easily recruit non-dedicated nodes dynamically in response to load bursts. Based on such a scheme, we designed and implemented an innovative approach that enables an ongoing request to be smoothly migrated to another node either in response to a node failure or overload. We also designed and implemented a management system that enables the Web site manager to manage and maintain the distributed server as a single large system",2000,0,
2156,2157,Double-fed three-phase induction machine model for simulation of inter-turn short circuit fault,"The aim of this paper is to develop a doubly-fed induction machine (DFIM) model suitable for the simulation of this machine in the healthy mode and faulty mode. Indeed, the developed model allows the simulation of the inter-turn short circuit in the stator or the rotor of the machine in any system with control circuits and/or connections to the grid by means of power electronics converters. The circuit-oriented approach has been chosen in order to represent the DFIM model as a rotating transformer. Of course, since this model doesn't need any transformation, it is possible to simulate any kind of asymmetry in both stator and rotor sides with or without variations of the machine parameters. In addition, the first turn of one phase of the stator and the rotor was modeled using the circuit-oriented approach to simulate an inter-turn short circuit in both stator and rotor. A specific wound-rotor induction machine model, using only resistances, inductances and controlled voltage sources, has been developed. The coupling effects between stator and rotor have been taken into account stator-rotor mutual inductances. The performances of the model have been verified by comparison between simulation and experimental results on a 5.5 kW-50 Hz-8 poles DFIM working in generating mode at different load conditions.",2009,0,
2157,2158,Design of the Autonomous Fault Manager for learning and estimating home network faults,"This paper proposes a design of software autonomous fault manager (AFM) for learning and estimating faults generated in home network. Most of the existing researches employ rule-based fault processing mechanism, but those works depend on the static characteristics of rules for a specific home environment. Therefore, we focus on a fault estimating and learning mechanism that autonomously produces a fault diagnosis rule and predicts an expected fault pattern in the mutually different home environment. For this, the proposed AFM extracts the home network information with a set of training data using the 5W1H (Who, What, When, Where, Why, How) based contexts to autonomously produce a new fault diagnosis rule. The fault pattern with high correlations can then be predicted for the current home network operation pattern.",2009,0,
2158,2159,Exploring the Maintenance Process through the Defect Management in the Open Source Projects - Four Case Studies,"While Open Source Software are becoming evermore widespread and used these days, their maintenance is coming important issue. Earlier studies have shown that defect and version management systems are rich and valuable sources for evaluation of maintenance but they have not studied the use of separate management system for support and feature request. Therefore, in this research we study defect reports, support and feature requests of Open Source Software projects through four case studies from SourceForge. Results showed that most of the case studies used actively those systems but discussion forums were even more active. Although reports and requests were submitted, most of them did not cause any changes or further actions because they were closed shortly as duplicates, invalid or without any resolution.",2006,0,
2159,2160,Geolocation Error Analysis of the Special Sensor Microwave Imager/Sounder,"Geolocation errors in excess of 20-30 km have been observed in the special sensor microwave imager/sounder (F-16 SSMIS) radiometer observations when compared with accurate global shoreline databases. Potential error sources include angular misalignment of the sensor spin axis with the spacecraft zenith, sample timing offsets, nonuniform spin rate, antenna deployment offsets, spacecraft ephemeris, and approximations of the geolocation algorithm in the Ground Data Processing Software. An analysis methodology is presented to automate the process of quantifying the geolocation errors rapidly in terms of partial derivatives of the radiometer data in the along-scan and along-track directions and is applied to the SSMIS data. Angular and time offsets are derived for SSMIS that reveal the root cause(s) of the geolocation errors, while yet unresolved, are systematic, correctable in the ground processing software, and may be reduced to less than 4-5 km (1-sigma).",2008,0,
2160,2161,Fast algorithm for distortion-based error protection of embedded image codes,"We consider a joint source-channel coding system that protects an embedded bitstream using a finite family of channel codes with error detection and error correction capability. The performance of this system may be measured by the expected distortion or by the expected number of correctly decoded source bits. Whereas a rate-based optimal solution can be found in linear time, the computation of a distortion-based optimal solution is prohibitive. Under the assumption of the convexity of the operational distortion-rate function of the source coder, we give a lower bound on the expected distortion of a distortion-based optimal solution that depends only on a rate-based optimal solution. Then, we propose a local search (LS) algorithm that starts from a rate-based optimal solution and converges in linear time to a local minimum of the expected distortion. Experimental results for a binary symmetric channel show that our LS algorithm is near optimal, whereas its complexity is much lower than that of the previous best solution.",2005,0,
2161,2162,Thermal Behavior of a Three-Phase Induction Motor Fed by a Fault-Tolerant Voltage Source Inverter,"This paper presents the results of an investigation regarding the thermal behavior of a three-phase induction motor when supplied by a reconfigured three-phase voltage source inverter with fault-tolerant capabilities. For this purpose, a fault-tolerant operating strategy based on the connection of the faulty inverter leg to the dc link middle point was considered. The experimentally obtained results show that, as far as the motor thermal characteristics are concerned, it is not necessary to reinforce the motor insulation properties since it is already prepared for such an operation",2007,0,
2162,2163,Adaptive runtime fault management for service instances in component-based software applications,"The Trust4All project aims to define an open, component-based framework for the middleware layer in high-volume embedded appliances that enables robust and reliable operation, upgrading and extension. To improve the availability of each individual application in a Trust4All system, a runtime configurable fault management mechanism (FMM) is proposed, which detects deviations from given service specifications by intercepting interface calls. When repair is necessary, FMM picks a repair action that incurs the best tradeoff between the success rate and the cost of repair. Considering that it is rather difficult to obtain sufficient information about third party components during their early stage of usage, FMM is designed to be able to accumulate knowledge and adapts its capability accordingly",2007,0,
2163,2164,Defect coverage of boundary-scan tests: what does it mean when a boundary-scan test passes?,"A new coverage definition and metric, called the 'PCOLA/SOQ"" model, introduced in K. Hird et al. (2002), has great utility in allowing the test coverage of defects on boards to be measured and compared rationally. This paper discusses the general topic of measuring test coverage of boundary-scan tests within this framework. A conclusion is that boundary-scan tests offer a large amount of test coverage when boundary-scan is implemented on a board, even if that implementation is partial. However, coverage is not perfect for certain defects in the PCOLA/SOQ model, as shown by example.",2003,0,
2164,2165,Relative error measures for evaluation of estimation algorithms,"This paper is part of a series of publications that deal with evaluation of estimation algorithms. This series introduces and justifies a variety of metrics useful for evaluating various aspects of the performance of an estimation algorithm, among other things. This paper focuses on relative error measures, i.e., those with respect to some references, including the magnitude of the quantity to be estimated, its prior mean, and/or measurement error. It proposes several relative metrics that are particularly good for measuring different aspects of estimation performance. They often reveal the inherent error characteristics of an estimator better than widely used metrics of the absolute error. The metrics are illustrated via an example of target localization with radar measurements.",2005,0,
2165,2166,A comparison of techniques to optimize measurement of voltage changes in electrical impedance tomography by minimizing phase shift errors,"In electrical impedance tomography, errors due to stray capacitance may be reduced by optimization of the reference phase of the demodulator. Two possible methods, maximization of the demodulator output and minimization of reciprocity error have been assessed, applied to each electrode combination individually, or to all combinations as a whole. Using an EIT system with a single impedance measuring circuit and multiplexer to address the 16 electrodes, the methods were tested on resistor-capacitor networks, saline-filled tanks and humans during variation of the saline concentration of a constant fluid volume in the stomach. Optimization of each channel individually gave less error, particularly on humans, and maximization of the output of the demodulator was more robust. This method is, therefore, recommended to optimize systems and reduce systematic errors with similar EIT systems.",2002,0,
2166,2167,"Small Errors in ""Toward Formalizing Domain Modeling Semantics in Language Syntax'",A recent paper on domain modeling had State Charts with semantic errors.,2005,0,
2167,2168,Investigation of induction machine phase open circuit faults using a simplified equivalent circuit model,"An induction motor model based on the per-phase equivalent circuit is used to simulate operation with an open circuit fault. The model considers both spatial field harmonics as well as saturation effects to correctly model the motor's behaviour under faulty conditions, where the non-linearities may produce problematic torque pulsations due to the unbalanced nature of the winding distribution. A model is presented which takes into account the most prominent nonlinearities, however keeping simulation times low in order to enable the development of fault tolerant control strategies. In addition, this paper presents a study of the behaviour of an induction motor drive with a phase open circuit fault. A new fault remedial control strategy for this type of fault will also be described. Experimental tests on an instrumented vector controlled rig have been used to verify simulation results.",2008,0,
2168,2169,An Intelligent Error Detection Model for Reliable QoS Constraints Running on Pervasive Computing,"We propose an intelligence predictive model for reliable QoS constraints running on pervasive computing. FTA is a system that is suitable for detecting and recovering software error based on pervasive computing environment as RCSM(Reconfigurable Context-Sensitive Middleware) by using software techniques. One of the methods to detect error for session's recovery inspects process database periodically. But this method has a weak point of inspecting all processes without regard to session. Therefore, we propose FTA. This method detects error by inspecting by hooking method. If an error is found, FTA informs GSM of the error. GSM informs Daemon or SA-SMA of the error. Daemon creates SA-SMA and so on. SA-SMA creates Video Service Provide Instance and so on.",2006,0,
2169,2170,An Asymmetric Checkpointing and Rollback Error Recovery Scheme for Embedded Processors,"This paper presents a checkpointing scheme for rollback error recovery, called Asymmetric Checkpointing and Rollback Recovery (ACRR) which stores the processor states in an asymmetric manner. In this way, error recovery latency and the number of checkpoints are reduced to increase the probability of timely task completion for soft real-time applications. To evaluate the ACRR, this scheme was studied analytically. The analytical results show that the recovery latency is reduced as non-uniformity of the checkpoint increases. As a case study, the ACRR is implemented and simulated on a behavioral VHDL model of LEON2 processor. The simulation results follow the results obtained in the analytical study.",2008,0,
2170,2171,An infrastructure for adaptive fault tolerance on FT-CORBA,"The fault tolerance provided by FT-CORBA is basically static, that is, once the fault tolerance properties of a group of replicated processes are defined, they cannot be modified in runtime. A support for dynamic reconfiguration of the replication would be highly advantageous since it would allow the implementation of mechanisms for adaptive fault tolerance, enabling FT-CORBA to adapt to the changes that can occur in the execution environment. In this paper, we propose a set of extensions to the FT-CORBA infrastructure in the form of interfaces and object service implementations, enabling it to support dynamic reconfiguration of the replication",2006,0,
2171,2172,Dynamic Frequency Analysis on Industrial Electric Power Network under External Fault Condition,"The condition of thermal power plant of some SINOPEC is simply introduced first, after choosing its receiving operation, enterprise internal power's dynamic frequency characteristic mechanism has been analyzed under external fault condition, and then arriving at a conclusion that the dynamic frequency characteristic has something to do with active power shortage and frequency adjustment effect modulus of load. At last, having selected the reasonable failure mode and mathematical models in software PSASP7.0 to simulate this dynamic frequency characteristics curve of the system, we can verify the validity of the conclusion. The above is good for revealing the weak link of the plant in future and laying the foundation for finding the way to improve the reliability of the system. At the same time, it also has some reference value for other enterprise grid.",2010,0,
2172,2173,Ultrasonic nondestructive detection for defects in epoxy/mica insulation,"The detection and recognition of defects in insulation is a significant investigation for insulation diagnosis. In our laboratory, an ultrasonic testing system has been developed and used for insulation diagnosis of large generators. Using this system with lower frequency transducers, experiments were performed on actual stator bars. The results showed that, the 0.5~1.2 MHz frequency transducers could be used for the manufacturing quality test of stator bars. The 0.5~0.8 MHz transducers are more effective for aging assessment of aged stator bars. They are sensitive to the degradation and the internal mechanical damages of aged stator insulation",2001,0,
2173,2174,Statistical analysis of random errors from calibration standards,"This paper presents the results of error estimation caused by the VNA calibration standards using StatistiCAL designed at NIST. A GCPW-stripline-GCPW type transition and a test fixture for transceiver module testing are chosen to do the study over 8 to 12 GHz. Calibration standards, that is through, lines and shorts are fabricated in-house. The electromagnetic modeling needed in the development of the calibration standards is done through Ansoft HFSS. The results are used to verify HFSS simulations and they are effectively applied to module testing and performance screening.",2005,0,
2174,2175,Fault Management for Self-Healing in Ubiquitous Sensor Network,"This work concerns the development of a fault model of sensor for detecting and isolating sensor, actuator, and various faults in USNs (Ubiquitous Sensor Network). USN are developed to create relationships between humans, objects and computers in various fields. A management research of sensor nodes is very important because the ubiquitous sensor network has the numerous sensor nodes. However, Self-healing technologies are insufficient to restore when an error event occurs in a sensor node in a USN environment. A layered healing architecture for each node layer (3-tier) is needed, because most sensor devices have different capacities in USN. In this paper, we design a fault model and architecture of the sensor and sensor node separately for self-healing in USN. In order to evaluate our approach, we implement prototype of the USN fault management system to evaluate our approach. We compare the resource use of self-healing components in the general distributed computing (wired network) and the USN.",2008,0,
2175,2176,Fault-Tolerant Distributed Computing in Full-Information Networks,"In this paper, we use random-selection protocols in the full-information model to solve classical problems in distributed computing. Our main results are the following: An O(log n)-round randomized Byzantine agreement (BA) protocol in a synchronous full-information network tolerating t < n/(3+epsi) faulty players (for any constant epsi > 0). As such, our protocol is asymptotically optimal in terms of fault-tolerance. An O(1)-round randomized BA protocol in a synchronous full-information network tolerating t = O(n/((log n)<sup>1.58</sup>)) faulty players. A compiler that converts any randomized protocol Pi<sub>in</sub> designed to tolerate t fail-stop faults, where the source of randomness of Pi<sub>in</sub> is an SV-source, into a protocol Pi<sub>out</sub> that tolerates min(t, n/3) Byzantine faults. If the round-complexity of Pi<sub>in</sub> is r, that of Pi<sub>out</sub> is O(r log* n). Central to our results is the development of a new tool, ""audited protocols"". Informally ""auditing"" is a transformation that converts any protocol that assumes built-in broadcast channels into one that achieves a slightly weaker guarantee, without assuming broadcast channels. We regard this as a tool of independent interest, which could potentially find applications in the design of simple and modular randomized distributed algorithms",2006,0,
2176,2177,Temporal error concealment for video transmission,"In this paper, we propose a temporal error concealment algorithm for video transmission in an error-prone environment. The error concealment algorithm employed an edge detection algorithm and progressive median motion vector concealment. First, the edges are detected and concealed portion by portion. Then, the corrupted MB is partitioned by the reconstructed edges and each partition is concealed by progressive median motion vector individually. The proposed algorithm shows better performance on both objective and subjective quality over the existing temporal error concealment algorithm",2004,0,
2177,2178,Application of BP neural network fault diagnosis in solar photovoltaic system,"This paper introduces fault diagnosis modes and points out the source of trouble in grid-connected solar photovoltaic systems. It analyses and researches the structure and algorithm of BP neural network. After that, the paper brings forward fault diagnosis method based on BP neural network for the grid-connected solar photovoltaic system. It shows this method is efficacious and earthly and attains the expected results, it can be applied to fault diagnosis of grid-connected solar photovoltaic system definitely.",2009,0,
2178,2179,Intelligent Fault Diagnosis System in Large Industrial Networks,"Traditional fault diagnosis system in large industrial networks is not intelligent enough and cannot predict faults. It is too expensive for industrial corporations. This paper brings forward an intelligent fault diagnosis system-IFDS, which uses new types of intelligent database technology, and has the ability of effectively solving the fault diagnosis and prediction issue of current industrial Ethernet network. In addition, this paper discusses some methods which can be implemented in IBM DB2 database.",2008,0,
2179,2180,An simple and efficient fault tolerance mechanism for divide-and-conquer systems,"Summary form only given. We study if fault tolerance can be made simpler and more efficient by exploiting the structure of the application. More specifically, we study divide-and-conquer parallelism, which is a popular and effective paradigm for writing parallel Grid applications. We have designed a novel fault tolerance mechanism for divide-and-conquer applications that reduces the amount of redundant computation by storing results of the discarded in a global (replicated) table. These results can later be reused, thereby minimizing the amount of work lost as a result of a crash. The execution time overhead of our mechanism is close to zero. Our mechanism can handle crashes of multiple processors or entire clusters at the same time.. It can also handle crashes of the root node that initially started the parallel computation. We have incorporated our fault tolerance mechanism in Satin, which is a Java-based divide-and-conquer system. Satin is implemented on top of the Ibis communication library. The core of Ibis is implemented in pure Java, without using any native libraries. The Satin runtime system and our fault tolerance extension also are written entirely in Java. The resulting system therefore is highly portable allowing the software to run unmodified on a heterogeneous Grid. We evaluated the performance of our fault tolerance scheme on a cluster of the Distributed ASCI Supercomputer 2 (DAS-2). In the first part of our tests, we show that the execution time overhead of our mechanism is close to zero. The results of the second part of our tests show that our algorithm salvages most of the work done by alive processors. Finally, we carried out tests on the European GridLab testbed. We ran one of our applications on a set of six heterogeneous parallel machines (four different operating systems, four different architectures) located in four different European countries. After manually killing one of the sites, the program recovered and finished normally.",2004,0,
2180,2181,The Application of Topological Gradients to Defect Identification in Magnetic Flux Leakage-Type NDT,An inverse problem is formulated to indentify the shape and size of the defects in a nonlinear ferromagnetic material using the signal profile from magnetic flux leakage-type NDT. This paper presents an efficient algorithm based on topological shape optimization which exploits the topological gradient to accelerate the process of shape optimization to identify the defect. Topological gradient images for the cracks are obtained using 2-D and 3-D finite element models. Robustness of this imaging method in the presence of noise is also evaluated.,2010,0,
2181,2182,Identification of Cross-Country Fault of Power Transformer for Fast Unblocking of Differential Protection,"Due to the different ratings of the current transformers (CT) located on different sides of power transformer, only the CT of low ratings will saturate when the power transformer experiences a heavy through fault, leading to false differential current. Such type of external fault can be identified from the internal one if the differential protection is equipped with the percentage restraint characteristic together with the method using operation time difference between pickup element and differential protection. However, the differential protection will be wrongly blocked as well if a cross-country fault occurs. According to the investigations in this paper, in the event of an external fault accompanied by the CT saturation, the variation of most samples of the secondary current of the saturated CT is inversely proportional to the variation of the differential current. Comparatively, this law cannot be followed on the occasion of an internal fault. In this case, the locus of the variation of the saturated secondary current with the differential current can be used to dynamically discriminate if an external fault develops to an internal fault. With the method proposed in this paper, the ability of the differential protection immune to cross-country fault can be improved further. The effectiveness of the proposed method is verified with the simulation tests.",2009,0,
2182,2183,Comparative investigation of diagnostic media for induction motors: a case of rotor cage faults,"Results of a comparative experimental investigation of various media for noninvasive diagnosis of rotor faults in induction motors are presented. Stator voltages and currents in an induction motor were measured, recorded, and employed for computation of the partial and total input powers and of the estimated torque. Waveforms of the current, partial powers p<sub>AB</sub> and p<sub>CB</sub>, total power, and estimated torque were subsequently analyzed using the fast Fourier transform. Several rotor cage faults of increasing severity were studied with various load levels. The partial input power p<sub>CB</sub> was observed to exhibit the highest sensitivity to rotor faults. This medium is also the most reliable, as it includes a multiplicity of fault-induced spectral components",2000,0,
2183,2184,"Effectiveness and limitations of various software techniques for ""soft error"" detection: a comparative study",Deals with different software based strategies allowing the on-line detection of bit flip errors arising in microprocessor-based digital architectures as the consequence of the interaction with radiation. Fault injection experiments put in evidence the detection capabilities and the limitations of each of the studied techniques,2001,0,
2184,2185,Outlier correction in image sequences for the affine camera,"It is widely known that, for the affine camera model, both shape and motion can be factorized directly from the so-called image measurement matrix constructed from image point coordinates. The ability to extract both shape and motion from this matrix by a single SVD operation makes this shape-from-motion approach attractive; however, it can not deal with missing feature points and, in the presence of outliers, a direct SVD to the matrix would yield highly unreliable shape and motion components. Here, we present an outlier correction scheme that iteratively updates the elements of the image measurement matrix. The magnitude and sign of the update to each element is dependent upon the residual robustly estimated in each iteration. The result is that outliers are corrected and retained, giving improved reconstruction and smaller reprojection errors. Our iterative outlier correction scheme has been applied to both synthesized and real video sequences. The results obtained are remarkably good.",2003,0,
2185,2186,Ortho-rectification and terrain correction of polarimetric SAR data applied in the ALOS/Palsar context,Methods for terrain correction of polarimetric SAR data were studied and developed. Ortho-rectification resampling and amplitude correction utilized Stokes matrix data. The Stokes matrix of thermal noise was subtracted before amplitude normalization. Application of an azimuth-slope correction algorithm resulted in slightly narrower distribution of orientation angles compared to input data.,2007,0,
2186,2187,Study on Adaptation of Traveling Waves Based on Wavelet Transform for Fault Location in Automatic Blocking and Continuous Power Transmission Lines,"The theory of traveling waves and its wavelet representation were presented. The characteristics and fault location research actuality in railway automatic blocking and continuous power transmission line were introduced. Considering the influence of the bus bar, conductor's architecture and the load along the line, the adaptation of traveling waves was analyzed. A method to locate fault by combining zero modal and aerial modal was brought forth, i.e., to distinguish the reflected wave of the faulty point from that of the bus at the opposite terminal by zero mode components and to locate the faulty position by aerial mode component. As for the mixed line, some characteristics can be obtained from the combination of aerial modal and zero modal of fault current, therefore the fault section can be determined. The modeling and simulation of the proposed method are conducted by software PSCAD/EMTDC. The simulation results show that the proposed method is feasible",2005,0,
2187,2188,Flexible docking mechanism using combination of magnetic force with error-compensation capability,"An auto-recharging system for a mobile robot can help the robot to perform its tasks constantly and without human intervention. For implementing the system, a docking mechanism is required. This paper presents a new docking mechanism with a localization error-compensation capability. The proposed mechanism uses the combination of mechanical structure and magnetic forces between the docking connectors. It is a structure to improve the allowance ranges of lateral and directional docking errors, in which the robot is able to dock into the docking station. Consequently, this mechanism reduces dependency of a robot control and allows easy docking with only mechanical configuration. In this paper, the superiority of the proposed mechanism is verified with experimental results.",2008,0,
2188,2189,On the Selection of Error Model(s) for OS Robustness Evaluation,"The choice of error model used for robustness evaluation of operating systems (OSs) influences the evaluation run time, implementation complexity, as well as the evaluation precision. In order to find an ""effective"" error model for OS evaluation, this paper systematically compares the relative effectiveness of three prominent error models, namely bit-flips, data type errors and fuzzing errors using fault injection at the interface between device drivers OS. Bit-flips come with higher costs (time) than the other models, but allow for more detailed results. Fuzzing is cheaper to implement but is found to be less precise. A composite error model is presented where the low cost of fuzzing is combined with the higher level of details of bit-flips, resulting in high precision with moderate setup and execution costs.",2007,0,
2189,2190,Simulation of single-phase nonlinear and hysteretic transformer with internal faults,This paper presents a complete scheme for simulation of single-phase transformers with internal faults. Traditional methods may not be effective in displaying hysteretic characteristics in the transformer core. The approach in this paper uses transmission line method (TLM) incorporating Jiles-Atherton model to introduce nonlinear hysteresis. A small 25 kVA 11 kV/220 V power transformer with turn-to-earth fault is simulated by applying such a scheme. By comparing these results consistency is shown and by total harmonic distortion (THD) analysis the nonlinear hysteretic features of terminal voltages and currents when fault occurs are investigated,2006,0,
2190,2191,Random and systematic defect analysis using IDDQ signature analysis for understanding fails and guiding test decisions,"This work demonstrates IDDQ signature analysis of random and systematic defects, including yield detractors and reliability defects. Application is demonstrated for both understanding failure root cause and guiding test decisions. IDDQ signatures contain rich information about the circuit, defect behavior and processing conditions. This paper describes capturing that information by classifying signatures into different categories and using the classifications to learn about the nature of the defects occuring on a variety of ASIC chips.",2004,0,
2191,2192,Virtual Multiresolution Screen Space Errors: Hierarchical Level-of-Detail (HLOD) Refinement Through Hardware Occlusion Queries,"We present a novelty metric to perform the refinement of a HLOD-based system that takes into account visibility information. The information is gathered from the result of a hardware occlusion query (HOQ) performed on the bounding volume of a given node in the hierarchy. Although the advantages of doing this are clear, previous approaches treat refinement criteria and HOQ as independent subjects. For this reason, HOQs have been used restrictively as if their result were Boolean. In contrast to that, we fully exploit the results of the queries to be able to take into account visibility information within refinement conditions. We do this by interpreting the result of a given HOQ as the virtual resolution of a screen space where the refinement decision takes place. Our new error metric is general enough to be employed in any HLOD-based system as the quantity that guides its refinement. Despite its simplicity, in our experiments we obtained a meaningful performance boost (compared to previous approaches) in the frame-rate with almost no loss in image quality",2006,0,
2192,2193,Clinical evaluation of real-time phase-aberration correction system [medical ultrasound],"We have developed a phase-aberration correction system that correlates signals in real time. In this paper we evaluate the clinical performance of the system in vivo. This system 1) constructs a cross-sectional image and, 2) calculates the correlation of signals at neighboring sensor elements. Both 1) and 2) are carried out in real time. The system was used for imaging of living tissue. First the beam was formed using initial focus delay settings and a real-time cross-sectional image was constructed. The correlation between neighboring signals was calculated simultaneously with the beam formation and the time difference between the pairs of signals was acquired. The time difference was then used to compensate for the initial delay. The image of the living tissue was substantially improved after the compensation. A further experiment is in progress to collect a statistically significant number of clinical results",2000,0,
2193,2194,Automatic Detection of In-field Defect Growth in Image Sensors,"Characterization of in-field defect growth with time in digital image sensors is important for measuring the quality of sensors as they age. While more defects were found in cameras exposed to high cosmic ray radiation environments, comparing the collective growth rate of different sensor types has shown that CCD imagers develop twice as many defects as APS imagers, indicating that CCD imagers may be more sensitive to radiation. The defect growth of individual imagers can be estimated by analyzing historical image sets captured by individual cameras. This paper presents a defect tracing algorithm, which determines the presence or absence of defects by accumulating Bayesian statistics collected over a sequence of images. Recognizing the complexity of image scenes, camera settings, and local clustering of defects in color images (due to demosaicing), refinements of the algorithm have been explored and the resulting detection accuracy has increased significantly. In-field test results from 3 imagers with a total of 26 defects have shown that 96% of the defects' dates were identified with less than 10 days difference compared to visual inspection. In addition to our continuous study of in-field defects in high-end digital SLRs, this paper presents a preliminary study of 10 cellphone cameras. Our test results address the comparison of defects types, distribution and growth found in low-end and high-end cameras with significantly different pixel sizes.",2008,0,
2194,2195,Open Switch Fault Diagnosis for a Doubly-Fed Induction Generator,This paper addresses the analysis and detection of open switch faults in back-to-back PWM converters used in doubly-fed induction generators (DFIG). Several methods have previously been proposed to detect open switch faults in either the machine-side or line-side converter. The operating conditions that can cause possible false alarms with these methods are investigated. The proposed method detects open switch faults more reliably than any of the existing methods and hence improves overall system reliability. The performance of the existing methods and proposed methods has been verified by both simulation and experiment.,2007,0,
2195,2196,Using transient/persistent errors to develop automated test oracles for event-driven software,"Today's software-intensive systems contain an important class of software, namely event-driven software (EDS). All EDS take events as input, change their state, and (perhaps) output an event sequence. EDS is typically implemented as a collection of event-handlers designed to respond to individual events. The nature of EDS creates new challenges for test automation. In this paper, we focus on those relevant to automated test oracles. A test oracle is a mechanism that determines whether a software executed correctly for a test case. A test case for an EDS consists of a sequence of events. The test case is executed on the EDS, one event at a time. Errors in the EDS may ""appear"" and later ''disappear"" at several points (e.g., after an event is executed) during test case execution. Because of the behavior of these transient (those that disappear) and persistent (those that don't disappear) errors, EDS require complex and expensive test oracles that compare the expected and actual output multiple times during test case execution. We leverage our previous work to study several applications and observe the occurrence of persistent/transient errors. Our studies show that in practice, a large number of errors in EDS are transient and that there are specific classes of events that lead to transient errors. We use the results of this study to develop a new test oracle that compares the expected and actual output at strategic points during test case execution. We show that the oracle is effective at detecting errors and efficient in terms of resource utilization",2004,0,
2196,2197,Fault Diagnosis of Power Transformer Using SVM and FCM,"In this study, we are concerned with fault diagnosis of power transformer. The objective is to explore the use of some advanced techniques such as SVM and FCM and quantify their effectiveness when dealing with dissolved gases extracted from power transformers. The proposed fault diagnosis system consists of data acquisition, fault/normal diagnosis, identification of fault and analysis of aging degree parts. In data acquisition part, concentrated gases are extracted from transformer for data gas analysis. In fault/normal diagnosis part, SVM is performed to separate normal state from fault types. The determination of fault type is executed by multi-class SVM in identification part. Although the inputted data is normal state, the analysis of aging degree is performed by considering the distance measure calculated by comparing with reference model constructed by FCM and input data. Our approach makes it possible to measure the possibility and degree of aging in normal transformer as well as the identification of faults in abnormal transformer. As the simulation results to verify the effectiveness, the proposed method showed more improved classification results than conventional methods.",2008,0,
2197,2198,A serial unequal error protection code system using trellis coded modulation and an adaptive equalizer for fading channels,"We propose a serial unequal error protection (UEP) scheme using trellis coded modulation and an adaptive equalizer for use in mobile fading channel communication environments. We propose two types of signal constellations, TRAP and RING, to realize unequal error protection and show their performance in fading channels using computer simulations.",2008,0,
2198,2199,Experimental checking of fault susceptibility in a parallel algorithm,We deal with the problem of analyzing fault susceptibility of a parallel algorithm designed for a multiprocessor array (MIMD structure). This algorithm realizes quite a complex communication protocol in the system. We present an original methodology of the analysis based on the use of a software implemented fault injector. The considered algorithm is modeled as a multithreaded application. The experiment set up and results are presented and commented. The performed experiments proved relatively high natural robustness of the analyzed algorithm and showed further possibilities of its improvement.,2002,0,
2199,2200,Modeling and Fault Diagnosis of a Polymer Electrolyte Fuel Cell Using Electrical Equivalent Analysis,"Fuel cell systems are complex systems and a high degree of competence is needed in different areas of knowledge such as thermodynamics, fluid dynamics, electrochemistry, and others, for their comprehension. This paper is a contribution to global modeling and fault diagnosis of these systems. More precisely, the goal of this paper is twofold. First, an electrical equivalent model, which could be used as a unifying approach to fuel cell systems will be resented. Second, a methodology to use the electrical model for fuel cell system diagnosis will be introduced, making special emphasis on fuel cell flooding detection. In order to illustrate the relevance of the proposed approach, experimental validations of the model, and the diagnosis methodology are proposed.",2010,0,
2200,2201,Multi agent-based DCS with fault diagnosis,"The multi agent-based DCS (MADCS) with fault diagnosis function is proposed for the purpose of enhancing both system's flexibility and reliability. Through cooperation among agents, MADCS can diagnose its own state and attempt to recover itself. Commonly, these agents are driven by time or by event, and do not have the same structure. At the same time, the general structure of agent implementation was shown in this paper. To achieve better performance of the fault diagnosis, two models (serial model and parallel model) of data fusion based on multi-sensor are discussed and used in the MADCS.",2004,0,
2201,2202,Research and realization of a network fault locating algorithm,"In complex IP networks, faults usually have various uncertain causes. Therefore, it is essential for the fault locating system to realize accurate and quick fault locating. This paper puts forward a new symptom-fault map as the fault locating method for the fault propagation model-a fault locating method based on the Bayesian network, and a fault locating system is designed based on this method. This system, capable of simultaneously utilizing the characterization network and symptom information on whether the service application is normal or abnormal, has good resistance to noise, and can not only locate usability faults at low layers of the protocol stack, but also well locate service application faults at upper layers of the protocol stack, capable of accurately and quickly locating faults in a large IP network.",2009,0,
2202,2203,A Novel Scheme to Identify Symmetrical Faults Occurring During Power Swings,"A novel, fast unblocking scheme for distance protection to identify symmetrical faults occurring during power swings has been proposed in this paper. First of all, the change rate of three-phase active power and reactive power being the cosine function and the sine function with respect to the phase difference between the two power systems during power swings has been demonstrated. In this case, they cannot be lower than the threshold of 0.7 after they are normalized. However, they will level off to 0 when a three-phase fault occurs during power swings. Thereafter, the cross-blocking scheme is conceived on the basis of this analysis. By virtue of the algorithm based on instantaneous electrical quantities, the calculation of the active and reactive power is immune to the variation of the system power frequency. As the integration-based criterion, it has high stability. Finally, simulation results show that this scheme is of high reliability and fast time response. The longest time delay is up to 30 ms.",2008,0,
2203,2204,A method for reduction of TDOA measurement error in UWB leading edge detection receiver,Leading edge detection receivers are the simplest devices intended for reception of ultra-wideband pulses. However they application for time difference of arrival (TDOA) measurements can be a source of errors which values depend on the amplitudes of received pulses. The paper describes a method for reduction of these errors. It presents necessary receiver modifications and an algorithm for TDOA value correction. The method was experimentally tested with a positioning UWB receiver. The paper contains results of measurements made in indoor environment.,2010,0,
2204,2205,SUDS: An Infrastructure for Creating Bug Detection Tools,SUDS is a powerful infrastructure for creating dynamic bug detection tools. It contains phases for both static analysis and dynamic instrumentation allowing users to create tools that take advantage of both paradigms. The results of static analysis phases can be used to improve the quality of dynamic bug detection tools created with SUDS and could be expanded to find defects statically. The instrumentation engine is designed in a manner that allows users to create their own correctness models quickly but is flexible to support construction of a wide range of different tools. The effectiveness of SUDS is demonstrated by showing that it is capable of finding bugs and that performance is improved when static analysis is used to eliminated unnecessary instrumentation.,2007,0,
2205,2206,Fault-Tolerant Earliest-Deadline-First Scheduling Algorithm,The general approach to fault tolerance in uniprocessor systems is to maintain enough time redundancy in the schedule so that any task instance can be re-executed in presence of faults during the execution. In this paper a scheme is presented to add enough and efficient time redundancy to the earliest-deadline-first (EDF) scheduling policy for periodic real-time tasks. This scheme can be used to tolerate transient faults during the execution of tasks. We describe a recovery scheme which can be used to re-execute tasks in the event of transient faults and discuss conditions that must be met by any such recovery scheme. For performance evaluation of this idea a tool is developed.,2007,0,
2206,2207,Impact of quantization and roundoff errors on the performance of a noise radar correlator,"This paper evaluates the influence of quantization effects on the performance of correlators. The problem is decomposed into two smaller ones, each dealing with difference source of errors (quantization of input signals and finite-precision arithmetics) separately. A discussion of the first type of errors is held on a general level and applicable to any type of correlator. However, roundoff effects depend strongly on details of computations. Therefore, a case study is performed using a fixed-point FFT-based correlator, implemented using LogiCORE IP FFT v. 7.0 engine.",2010,0,
2207,2208,A guide to digital fault recording event analysis,"Proper interpretation of fault and disturbance data is critical for the reliability and continuous operation of the power system. A correct interpretation gives you valuable insight into the conditions and performance of various power system protective equipment. Analyzing records is not an intuitive process and requires system protection knowledge and experience. Having an understanding of the fundamental guidelines for the event analysis process is imperative for new power engineers to properly evaluate faults. As senior power engineers retire, knowledge of how to decipher fault records could be lost with them. This paper addresses aspects of power system fault analysis and provides the new event analyst with a basic foundation of the requirements and steps to analyze and interpret fault disturbances.",2010,0,
2208,2209,ED<sup>4</sup>I: error detection by diverse data and duplicated instructions,"Errors in computing systems can cause abnormal behavior and degrade data integrity and system availability. Errors should be avoided especially in embedded systems for critical applications. However, as the trend in VLSI technologies has been toward smaller feature sizes, lower supply voltages and higher frequencies, there is a growing concern about temporary errors as well as permanent errors in embedded systems; thus, it is very essential to detect those errors. Software-implemented hardware fault tolerance (SIHFT) is a low-cost alternative to hardware fault-tolerance techniques for embedded processors: It does not require any hardware modification of commercial off-the-shelf (COTS) processors. ED<sup>4</sup>I (error detection by data diversity and duplicated instructions) is a SIHFT technique that detects both permanent and temporary errors by executing two ""different"" programs (with the same functionality) and comparing their outputs. ED<sup>4</sup>I maps each number, x, in the original program into a new number x', and then transforms the program so that it operates on the new numbers so that the results can be mapped backwards for comparison with the results of the original program. The mapping in the transformation of ED<sup>4</sup>I is x' = kx for integer numbers, where k<sub>f </sub> determines the fault detection probability and data integrity of the system. For floating-point numbers, we find a value of k<sub>f</sub> for the fraction and k<sub>e</sub> for the exponent separately, and use k = k<sub>f</sub>2<sup>k</sup> for the value of k. We have demonstrated how to choose an optimal value of k for the transformation. This paper shows that, for integer programs, the transformation with k = -2 was the most desirable choice in six out of seven benchmark programs we simulated. It maximizes the fault detection probability under the condition that the data integrity is highest",2002,0,
2209,2210,An empirical study on testing and fault tolerance for software reliability engineering,"Software testing and software fault tolerance are two major techniques for developing reliable software systems, yet limited empirical data are available in the literature to evaluate their effectiveness. We conducted a major experiment to engage 34 programming teams to independently develop multiple software versions for an industry-scale critical flight application, and collected faults detected in these program versions. To evaluate the effectiveness of software testing and software fault tolerance, mutants were created by injecting real faults occurred in the development stage. The nature, manifestation, detection, and correlation of these faults were carefully investigated. The results show that coverage testing is generally an effective means to detecting software faults, but the effectiveness of testing coverage is not equivalent to that of mutation coverage, which is a more truthful indicator of testing quality. We also found that exact faults found among versions are very limited. This result supports software fault tolerance by design diversity as a creditable approach for software reliability engineering. Finally we conducted domain analysis approach for test case generation, and concluded that it is a promising technique for software testing purpose.",2003,0,
2210,2211,Development of diagnostic systems for the fault tolerant operation of Micro-Grids.,"The progressive penetration level of Distributed Generation (DG) is destined to cause deep changes in the existing distribution networks no longer considered as passive terminations of the whole electrical system. A possible solution is the realization of small networks, namely the Micro-Grids, reproducing in themselves the structure of the main production and distribution of the electrical energy system. In order to gain an adequate reliability level of the micro-grids the individuation and the management of the faults with the goal of maintaining the micro-grid operation (fault tolerant operation) is quite important. In the present paper after the introduction of the aims of several diagnostic systems, the main available diagnostic techniques are examined with particular references to those applied to the fault diagnosis of the electrical machines and finally the Authors also present an approach for the fault tolerant exercise of the micro-grid.",2010,0,
2211,2212,Software fault injection for survivability,"In this paper, we present an approach and experimental results from using software fault injection to assess information survivability. We define information survivability to mean the ability of an information system to continue to operate in the presence of faults, anomalous system behavior, or malicious attack. In the past, finding and removing software flaws has traditionally been the realm of software testing. Software testing has largely concerned itself with ensuring that software behaves correctly-an intractable problem for any non-trivial piece of software. In this paper, we present off-nominal testing techniques, which are not concerned with the correctness of the software, but with the survivability of the software in the face of anomalous events and malicious attack. Where software testing is focused on ensuring that the software computes the specified function correctly, we are concerned that the software continues to operate in the presence of faults, unusual system events or malicious attacks",2000,0,
2212,2213,"GosSkip, an Efficient, Fault-Tolerant and Self Organizing Overlay Using Gossip-based Construction and Skip-Lists Principles","This paper presents GosSkip, a self organizing and fully distributed overlay that provides a scalable support to data storage and retrieval in dynamic environments. The structure of GosSkip, while initially possibly chaotic, eventually matches a perfect set of Skip-list-like structures, where no hash is used on data attributes, thus preserving semantic locality and permitting range queries. The use of epidemic-based protocols is the key to scalability, fairness and good behavior of the protocol under churn, while preserving the simplicity of the approach and maintaining O(log(N)) state per peer and O(log(N)) routing costs. In addition, we propose a simple and efficient mechanism to exploit the presence of multiple data items on a single physical node. GosSkip's behavior in both a static and a dynamic scenario is further conveyed by experiments with an actual implementation and real traces of a peer to peer workload",2006,0,
2213,2214,Cause-effect modeling and simulation of power distribution fault events,"Modeling and simulation are important to study power distribution faults due to the limited actual data and high cost of experimentations. Although a number of software packages are available to simulate the electric signals, approaches for simulating fault events in different environments are not well developed yet. In this paper, we propose a framework for modeling and simulating fault events in power distribution systems based on environmental factors and cause-effect relations among them. The spatial and temporal aspects of significant environmental factors leading to various faults are modeled as raster maps and probability distributions, respectively. The cause-effect relations are expressed as fuzzy rules and a hierarchical fuzzy inference system is built to infer the probability of faults given the simulated environments. This work will be helpful in fault diagnosis for different local systems and provide a configurable data source to other researchers and engineers in similar areas as well. A sample fault simulator we have developed is used to illustrate the approaches.",2010,0,
2214,2215,Coupled Equilibrium Model of Hybridization Error for the DNA Microarray and TagAntitag Systems,"In this work, a detailed coupled equilibrium model is presented for predicting the ensemble average probability of hybridization error per chip-hybridized input strand, providing the first ensemble average method for estimating postannealing microarray/TAT system error rates. Following a detailed presentation of the model and implementation via the software package NucleicPark, under a mismatched statistical zipper model of duplex formation, error response is simulated for both mean-energy and randomly encoded TAT systems versus temperature and input concentration. Limiting expressions and simulated model behavior indicate the occurrence of a transition in hybridization error response, from a logarithmically convex function of temperature for excess inputs (high-error behavior), to a monotonic, log-linear function of temperature for dilute inputs (low-error behavior), a novel result unpredicted by uncoupled equilibrium models. Model scaling behavior for random encodings is investigated versus system size and strand-length. Application of the model to TAT system design is also undertaken, via the in silico evolution of a high-fidelity 100-strand TAT system, with an error response improved by nine standard deviations over the performance of the mean random encoding",2007,0,
2215,2216,Design of IIR digital filters with prescribed phase error and reduced group-delay error,"Digital filters are often required to have constant group delays in many applications. Existing designs of IIR digital filters with no explicit constraints on the filters' group-delay responses usually lead to large group-delay errors, especially near the band edges. In design methods with constraints on group-delay error, much reduction of group-delay error have been obtained, but the phase error may not be small enough. In this paper, we design the IIR filter by imposing constraints on its frequency-response error and phase error, and using a sigmoid upper-bound function to shape the phase error. With this method, we have obtained both small phase error and group-delay error. In order to implement the design method, we combine the Steiglitz-McBride strategy with a relaxation scheme to convert the nonconvex design problem into a series of feasible quadratically constrained quadratic programming problems. Two example filters with specifications given in the literature are provided to compare the proposed design method with several existing methods. Design results demonstrate the effectiveness of the proposed method and good properties of the designed filters.",2010,0,
2216,2217,A Multiple-Weight-and-Neuron-Fault Tolerant Digital Multilayer Neural Network,"This paper introduces an implementation method of multiple weight as well as neuron fault-tolerant multilayer neural networks. Their fault-tolerance is derived from our extended back propagation learning algorithm called the deep learning method. The method can realize a desired weight as well as neuron fault-tolerance in multilayer neural networks where weight values are floating-point and the sigmoid function is used to calculate neuron output values. In this paper, fault-tolerant multilayer neural networks are implemented as digital circuits where weight values are quantized and the step function is used to calculate neuron output values using the deep learning method, the VHDL notation, and the logic design software QuartusII of Altera Inc. The efficiency of our method is shown in terms of fabrication-time cost, hardware size, neural computing time, generalization property, and so on",2006,0,
2217,2218,Desktop supercomputing technology for shadow correction of color images,"The paper deals with information technology for correction of shadow artifacts on color digital images obtained by photographing of paintings with the purpose of their reproduction. Shadow artifacts are caused by differences of light intensity. The problem of shadow detection and subsequent color correction is solved. The architecture of heterogeneous CPU/GPU - system implementing the elaborated technology is considered, examples of real images processing are given.",2010,0,
2218,2219,Influence of current measurement errors on parallel-connected UPS inverters,"Redundancy is a common approach for ensuring reliability on applications using UPS inverters. However, overall system's reliability and efficiency is very dependent on the power measurement precision, which if not properly done can lead to system failure. Current transformers, that are widely used to measure the output current of UPS inverters, introduce a phase shift in the output signal that negatively influences the parallelism circuits, leading to a waste in the generated power and therefore reducing system's efficiency and reliability. This paper analyses the influence of current measurement problems that may occur when two or more uninterruptible power supplies are connected together. Current measurement errors are experimentally obtained with two UPS inverters connected in parallel using the drooping method and the results, presented at the end, suggest that the power measurement needs to be carefully designed.",2009,0,
2219,2220,A network distribution power system fault location based on neural eigenvalue algorithm,"A new approach to fault location for distribution network power system is presented. This approach uses the eigenvalue and an artificial neural network based learning algorithm. The neural network is trained to map the nonlinear relationship existing between fault location and characteristic eigenvalue The proposed approach is able to identify, to classify and to locate different types of faults such as: single-line-to-ground, double-line-to-ground, double-line and three-phase. Using the eigenvalue as neural network inputs the proposed algorithm is able to locate the fault distance. The results presented show the effectiveness of the proposed algorithm for correct fault diagnosis and fault location on a distribution power system networks.",2003,0,
2220,2221,A robust technique for motion correction in fMRI,"A major source of error in the analysis of functional Magnetic Resonance images is the presence of spurious activation arising on account of patient head movement at the time of image acquisition. This makes it imperative for the images to be subjected to motion correction through registration. A number of solutions to the problem currently do exist though there is always the need for faster approaches which produce better estimates of the motion parameters. In this paper, we propose a signal model for fMRI images with possible relative movement between scans and show how the least trimmed squares estimator, which is well-known in the statistical literature for its robustness, can be used. Since data obtained from actual fMRI studies do not provide the ldquocorrectrdquo values of the motion parameters with which the performances of various estimators may be compared, computer simulations are set up where these parameters may be controlled. Our simulations indicate that the proposed method produces smaller error in the estimated motion parameters, when compared to the existing estimators, including another robust estimator.",2009,0,
2221,2222,Error investigation of models for improved detection of masses in screening mammography,This study analyzes the performance of a computer aided detection (CAD) scheme for mass detection in mammography. We investigate the trained parameters of the detection scheme before any further testing. We use an extended version of a previously reported mass detection scheme. We analyze the detection parameters by using linear canonical discriminants (LCD) and compare results with logistic regression and multi layer perceptron neural network models. Preliminary results suggest that regression and multi layer perceptron neural network showed the best receiver operator characteristics (ROC). The LCD analysis predictive function showed that the trained CAD scheme performance can maintain 99.08% sensitivity (108/109) with false positive rate (FPI) of 8 per image with ROC Az= 0.74plusmn0.01. The regression and the multi layer perceptron neural network ROC analysis showed stronger backbone for the CAD algorithm viewing that the extended CAD scheme can operate at 96% sensitivity with 5.6 FPI per image. These preliminary results suggest that further logic to reduce FPI is needed for the CAD algorithm to be more predictive,2005,0,
2222,2223,Managing Faults for Distributed Workflows over Grids,"Grid applications composed of multiple, distributed jobs are common areas for applying Web-scale workflows. Workflows over grid infrastructures are inherently complicated due to the need to both functionally assure the entire process and coordinate the underlying tasks. Often, these applications are long-running, and fault tolerance becomes a significant concern. Transparency is a vital aspect to understanding fault tolerance in these environments.",2010,0,
2223,2224,Middleware of real-time object based fault tolerant distributed computing systems: issues and some approaches,At this turn of the century the object-oriented (OO) distributed real-time (RT) programming movement is growing rapidly along with the networked embedded systems market. The motivations are reviewed and then a brief overview is given of the particular programming scheme which this author and his collaborators have been establishing. The scheme is called the time-triggered message triggered object (TMO) programming scheme and it is used to make specific illustrations of the issues and potentials of OO RT programming. Fault tolerance capabilities are required in many distributed RT computing applications. At this time the development of middleware which is capable of supporting reliable fault-tolerant execution of application level RT distributed objects is an important challenge to the research community. Some major issues that need to be resolved and some promising approaches are discussed,2001,0,
2224,2225,The uniform formula of single phase earth - fault distance relay with compensation,"The single-phase earth fault is the most common fault type on high voltage transmission line. But the sensitivity of single-phase distance relays can't satisfy the requirement of power system. In order to improve the sensitivity of distance relays to protect the single-phase faults with large earth resistance, one uniform formula is provided to express most of the single-phase distance relays in this paper. More than ten kinds of relays that have been used widely can be expressed by the uniform formula through giving different coefficients. This simplifies the expressions of conventional relays. The uniform formula is helpful to analysis the performance of these relays. It also provides an approach to create new relays with better performance or to lucubrate the correlative principles. Two new relays based on the uniform formula are proposed in this paper. Simulations show that the new relays have better performance than the old ones. At last, the fundamental about the choice of polarization voltage is discussed.",2003,0,
2225,2226,Design of a fault-tolerant parallel processor,"The Charles Stark Draper Laboratory, under contract to the NASA Johnson Space Center, has developed a Fault-Tolerant Parallel Processor (FTPP) for use on the NASA X-38 experimental vehicle. Using commercial processor boards and the industry-standard VME backplane, the system is configured as a quadruplet Flight-Critical Processor (FCP) and five simplex Instrumentation Control Processors (ICPs). The FCP is Byzantine resilient for any two non-simultaneous permanent faults, and for any number of non-simultaneous recoverable faults, as long as a maximum of one other fault condition occurs during the recovery process (only two recoveries can be in progress at once). This paper focuses on some of the hardware and software design of the Fault-Tolerant System Services (FTSS) that isolate, as much as possible, the redundancy of the FCP from the application software, such as the guidance, navigation and flight control software, on the X-38 FTPP. FTSS also performs reconfiguration and recovery functions.",2002,0,
2226,2227,Novel method for detection of transformer winding faults using Sweep Frequency Response Analysis,"Sweep frequency response analysis (SFRA) is an established tool for determining the core deformations, bulk winding movement relative to each other, open circuits and short circuit turns, residual magnetism, deformation within the main transformer winding, axial shift of winding etc. This test is carried out on the transformer without actual opening it and is an off line test. This paper explains the fundamental studies of SFRA measurement on basic electrical circuits, which can be extended for studying the mechanical integrity of a transformer after short circuit fault, transportation etc.",2007,0,
2227,2228,A fast and efficient H.264 error concealment technique based on coding modes,"A new error concealment technique based on the coding modes is proposed for H.264 video sequences. The motion-compensation modes (i.e., block-partitioning types) of surrounding macroblocks are employed to predict the mode of a lost macroblock. This adaptive selection mechanism is combined with a refined set of candidate motion vectors. Experimental results show that the proposed method, as compared to the technique used in the JM reference software, provides 1 to 2 dB gain in PSNR with only 50% increase in computation time.",2010,0,
2228,2229,Study of the Test Flow Optimization Method in Radar Fault Isolation,"In order to optimize test flow after the default flow was modified by hand, a new software framework for the radar fault isolation was illustrated. This framework separated all mapping algorithms from the test flow graph so as to modify flow and to insert mapping algorithm dynamically in testing process. Based on this framework, a kind of optimization method of test flow was proposed and studied. By defining an objective function, we could evaluate all candidate test flows so as to get a optimizing flow. An example explained how to search the flow from candidate flows.",2009,0,
2229,2230,On node state reconstruction for fault tolerant distributed algorithms,"One of the main methods for achieving fault tolerance in distributed systems is recovery of the state of failed components. Though generic recovery methods like checkpointing and message logging exist, in many cases the recovery has to be application specific. In this paper we propose a general model for a node state reconstruction after crash failures. In our model the reconstruction operation is defined only by the requirements it fulfills, without referring to the specific application dependent way it is performed. The model provides a framework for formal treatment of algorithm-specific and system-specific recovery procedures. It is used to specify node state reconstruction procedures for several widely used distributed algorithms and systems, as well as to prove their correctness.",2002,0,
2230,2231,Dependability analysis of a fault-tolerant processor,"Advances in semiconductor technology have improved the performance of integrated circuits, in general, and microprocessors, in particular, at a dazzling pace. Although, smaller transistor dimensions, lower power voltages and higher operating frequencies have significantly increased the circuit sensitivity to transient and intermittent faults. In this paper we present the architecture of a fault-tolerant processor and analyze its dependability with the aid of a generalized stochastic Petri net (GSPN) model. The effect of transient and intermittent faults is evaluated. It is concluded that fault tolerance mechanisms, usually employed by custom designed systems, have to be integrated into commercial-off-the-shelf (COTS) devices, in order to mitigate the impact of higher rates of occurrence of the transient and intermittent faults",2001,0,
2231,2232,Proxy-Based SNR Scalable Error Tracking for Real-Time Video Transmission Over Wireless Broadband,"In this paper, we propose a proxy-based SNR scalable error tracking framework for real-time video transmission where the server is wired connected to Internet and the client is connected to Internet through wireless broadband networks. We assume that all errors (packet losses) result from wireless links, and wired links are assumed to be error-free. The client sends back NACKs with the information about base layer lost packets to the proxy via a feedback channel. Once the NACK is received, the proxy uses the motion data and the side information received from the streaming server to perform error tracking. We compare our approach to the original proxy-based error tracking scheme without scalability support at the same bitrate and bit error rate. Experimental results show that the proposed method can effectively improve performance",2006,0,
2232,2233,An analysis on the effect of transmission errors in real-time H.264-MVC Bit-streams,"This paper studies the quality of transmitted multi-view video when the corrupted packets are not discarded by the underlying protocols of the decoder. It assumes a wireless channel where the errors can be significant and implements solutions within the current H.264-MVC to reduce their impact on the video quality perceived by the user. The results show that transmission errors drastically reduce the quality of the reconstructed 3D video and confirm that a new type of error propagation between views exists. Furthermore, employing the Context Adaptive Variable Length Coding (CAVLC) entropy encoder, coding and transmitting the video streams in smaller packets, and having a small cyclic-Intra coded period, all improve the error resilience of the system.",2010,0,
2233,2234,Miniaturized Microstrip Lowpass Filter With Wide Stopband Using Suspended Layers and Defected Ground Structure (DGS),"In this paper, a novel compact wideband rejection LPF using defected ground structure (DGS) is presented. The proposed LPF consists of hi-lo etched slots in ground metallic plane as defected ground structure(DGS) and of microstrip hi-lo structure, which corresponding to capacitance and Inductance on the top layer. The effect of the DGS slot on the characteristics of the investigated filter is examined. In this work we have proposed a simple method to realize a compact DGS LPF with good characteristics . In order to prove the efficiency of the method, a comparison is made between the new DGS LPF and conventional filters , which shows that the proposed filter with etched cells is enough to obtain better performance by suppressing ripples and a very large stop band. Measured results are found to be in good agreement with the simulation results.",2008,0,
2234,2235,Multiple Description Coding of 3D Geometrywith Forward Error Correction Codes,"This work presents a multiple description coding (MDC) scheme for compressed three dimensional (3D) meshes based on forward error correction (FEC). It allows flexible allocation of coding redundancy for reliable transmission over error-prone channels. The proposed scheme is based on progressive geometry compression, which is performed by using wavelet transform and modified SPIHT algorithm. The proposed algorithm is optimized for varying packet loss rates (PLR) and channel bandwidth. Modeling distortion-rate function considerably decreases computational complexity of bit allocation.",2007,0,
2235,2236,Mapping a group of jobs in the error recovery of the Grid-based workflow within SLA context,"The error recovery mechanism receives an important position in the system supporting service level agreements (SLAs) for the grid-based workflow. If one sub-job of the workflow is late, a group of directly affected sub-jobs should be re-mapped in a way that does not affect the start time of other sub-jobs in the workflow and is as inexpensive as possible. With the distinguished workload and resource characteristics as well as the goal of the problem, this problem needs new method to be solved. This paper presents a mapping algorithm, which can cope with the problem. Performance measurements deliver good evaluation results on the quality and efficiency of the method.",2007,0,
2236,2237,Estimation of parametric sensitivity for defects size distribution in VLSI defect/fault analysis,The parametric sensitivity of defect size distribution in VLSI defect/fault analysis is evaluated. The use of special software tool FIESTA for the computational experiment aimed at estimation of the significance of parameters in expressions approximating the actual defect distribution is considered. The obtained experimental results and their usefulness have been analysed,2002,0,
2237,2238,Enhanced reliability of finite-state machines in FPGA through efficient fault detection and correction,"SRAM based FPGA are subjected to ion radiation in many operating environments. Following the current trend of shrinking device feature size & increasing die area, newer FPGA are more susceptible to radiation induced errors. Single event upsets (SEU), (also known as soft-errors) account for a considerable amount of radiation induced errors. SEU are difficult to detect & correct when they affect memory-elements present in the FPGA, which are used for the implementation of finite state machines (FSM). Conventional practice to improve FPGA design reliability in the presence of soft-errors is through configuration memory scrubbing, and through component redundancy. Configuration memory scrubbing, although suitable for combinatorial logic in an FPGA design, does not work for sequential blocks such as FSM. This is because the state-bits stored in flip-flops (FF) are variable, and change their value after each state transition. Component redundancy, which is also used to mitigate soft-errors, comes at the expense of significant area overhead, and increased power consumption compared to nonredundant designs. In this paper, we propose an alternate approach to implement the FSM using synchronous embedded memory blocks to enhance the runtime reliability without significant increase in power consumption. Experiments conducted on various benchmark FSM show that this approach has higher reliability, lower area overhead, and consumes less power compared to a component redundancy technique.",2005,0,
2238,2239,Fault analysis expert system for power system,"Internet not only brings opportunities to power utilities, but also takes challenges. By virtue of internet and data warehouse, we developed support software which collected and stored data from devices in substations such as relay protect devices, dynamic process recorder and so on. Based on intelligent agent theory, knowledge discovery theory & rough set theory, we developed a distributed intelligent system (Fault Analysis Expert System based on Intelligent Agent), which can analyze reasons that cause any failures. In the system, intelligent agent is the distributed intelligent unit providing services for other units; knowledge discovery theory based on rough set theory can acquire and optimize rules from fact information.",2004,0,
2239,2240,Tolerance to unbounded Byzantine faults,"An ideal approach to deal with faults in large-scale distributed systems is to contain the effects of faults as locally as possible and, additionally, to ensure some type of tolerance within each fault-affected locality. Existing results using this approach accommodate only limited faults (such as crashes) or assume that fault occurrence is bounded in space and/or time. In this paper, we define and explore possibility/impossibility of local tolerance with respect to arbitrary faults (such as Byzantine faults) whose occurrence may be unbounded in space and in time. Our positive results include programs for graph coloring and dining philosophers, with proofs that the size of their tolerance locality is optimal. The type of tolerance achieved within fault-affected localities is self-stabilization. That is, starting from an arbitrary state of the distributed system, each non-faulty process eventually reaches a state from where it behaves correctly as long as the only faults that occur henceforth (regardless of their number) are outside the locality of this process.",2002,0,
2240,2241,Novel Fiber Bragg Grating sensor applicable for fault detection in high voltage transformers,"In this paper, a fiber optic based sensor capable of fault detection in the high voltage transformers is investigated. Bragg wavelength shift is used to detect fault in power systems. Magnetic fields generated by fault currents in the transformer cause a strain in magnetostrictive material which is then detected by Fiber Bragg Grating (FBG). Fiber Bragg interrogator senses the reflected FBG signals, and the Bragg wavelength shift is calculated and signals are processed. It is shown that the faults in both primary and secondary of transformers cause a detectable wavelength shift in the Fiber Bragg Grating.",2010,0,
2241,2242,"Atmospheric correction and oceanic constituents retrieval, with a neuro-variational method","Ocean color sensors on board satellite measure the solar radiation reflected by the ocean and the atmosphere. This information, denoted reflectance, is affected for 90% by air molecules and aerosols in the atmosphere and for only 10% by water molecules and phytoplankton cells in the ocean. Our method focuses on the chlorophyll-a concentration (chl-a) retrieval, which is commonly used as a proxy for phytoplankton concentration. Our algorithm, denoted NeuroVaria, computes relevant atmospheric (Angstrom coefficient, optical thickness, single-scattering albedo) and oceanic parameters (chl-a, oceanic particulate scattering) by minimizing the difference over the whole spectrum (visible + near infrared) between the observed reflectance and the reflectance computed from artificial neural networks that have been learned with a radiative transfer model. NeuroVaria has been applied to SeaWiFS (sea-viewing wide field-of-view sensor) imagery in the Mediterranean sea. A comparison with in-situ measurements of the water-leaving reflectance shows that NeuroVaria enables to better reconstruct this component at 443 nm than the standard SeaWiFS processing. This leads to an improvement of the retrieval of the chl-a for the oligotrophic sea. This result is generalized to the entire Mediterranean sea through weekly maps of chl-a.",2005,0,
2242,2243,A Cost-Effective Dependable Microcontroller Architecture with Instruction-Level Rollback for Soft Error Recovery,"A cost-effective, dependable microcontroller architecture has been developed. To detect soft errors, we developed an electronic design automation (EDA) tool that generates optimized soft error-detecting logic circuits for flip-flops. After a soft error is detected, the error detection signal goes to a developed rollback control module (RCM), which resets the CPU and restores the CPU's register file from the backup register file using a rollback program routine. After the routine, the CPU restarts from the instruction executed before the soft error occurred. In addition, there is a developed error reset module (ERM) that can restore the RCM from soft errors. We also developed an error correction module (ECM) that corrects ECC errors in RAM after error detection with no delay overheads. Testing on a 32- bit RISC microcontroller and EEMBC benchmarks showed that the area overhead was under 59% and frequency overhead was under 9%. In a soft error injection simulation, the MTBF of random logic circuits, and the MTBF of RAM were 30 and 1.34 times longer, respectively, than those of the original microcontroller.",2007,0,
2243,2244,Fault analysis study using modeling and simulation tools for distribution power systems,"This article describes a fault analysis study using some of the best available simulation and modeling tools for electrical distribution power systems. Several software tools were identified and assessed in L. Nastac, et al (2005). The fault analysis was conducted with the assessed software tools using the recorded fault data from a real circuit system. The recorded fault data including the topology and the line data with more than 1000 elements were provided by Detroit Edison (DTE) Energy for validation purposes. The effects of pro-fault loading and arcing impedance on the predicted fault current values were also investigated. Then, to ensure that the validated software tools are indeed capable of analyzing circuits with DCs, fault management and relay protection problems were developed and solved using a modified IEEE 34-bus feeder with addition of DCs.",2005,0,
2244,2245,Comparison of MV-grid structures on fault ride through behavior of MV-connected DG-units,Nowadays the amount of distributed generation units is increasing rapidly. At this moment in the Netherland small combined heat and power (CHP) plants are most dominant. All CHP-plants are equipped with under-voltage protections which switch-off the CHP-plants at a dip of 0.8 p.u. with a duration of 100200 ms. In this paper the voltage dip propagation of two existing distribution grid structures are compared. With the aid of simulations per grid structure it is determined what amount of CHP-plants disconnects during a voltage dip because of these settings.,2009,0,
2245,2246,Topology Error Identification for the NEPTUNE Power System,"The goal of the North Eastern Pacific Time-Series Undersea Networked Experiment (NEPTUNE) is to provide the infrastructure necessary for scientific exploration and investigation on the floor of the Pacific Ocean in an area encompassing the Juan de Fuca Tectonic Plate. In order to achieve this goal, the power delivery capabilities of the terrestrial distribution system will be extended into the Pacific Ocean. The power system associated with the proposed observatory is unlike conventional terrestrial power systems due to the unique under ocean operating conditions. The operating requirements of the system dictate hardware and software applications that are not found in terrestrial power systems. This paper describes a method for topology error identification in the presence of various forms of measurement errors for NEPTUNE, a highly interconnected DC power system. Hardware and reliability requirements have led to a power system configuration that includes a large number of unmeasured sections. Using previously developed state estimation algorithms as a starting point, a methodology for system topology identification is proposed in this paper.",2005,0,
2246,2247,"Corrections to Design and Performance Analysis of a Unified, Reconfigurable HMAC-Hash Unit [Dec 07 2683-2695]","In the above titled paper (ibid., vol 54, no. 12, pp. 2683-2695), the first paragraph of Section I (p. 2683), was incomplete. The correct paragraph is presented here.",2008,0,
2247,2248,"Experiences, strategies, and challenges in building fault-tolerant CORBA systems","It has been almost a decade since the earliest reliable CORBA implementation and, despite the adoption of the fault-tolerant CORBA (FT-CORBA) standard by the Object Management Group, CORBA is still not considered the preferred platform for building dependable distributed applications. Among the obstacles to FT-CORBA's widespread deployment are the complexity of the new standard, the lack of understanding in implementing and deploying reliable CORBA applications, and the fact that current FT-CORBA do not lend themselves readily to complex, real-world applications. We candidly share our independent experiences as developers of two distinct reliable CORBA infrastructures (OGS and Eternal) and as contributors to the FT-CORBA standardization process. Our objective is to reveal the intricacies, challenges, and strategies in developing fault-tolerant CORBA systems, including our own. Starting with an overview of the new FT-CORBA standard, we discuss its limitations, along with techniques for best exploiting it. We reflect on the difficulties that we have encountered in building dependable CORBA systems, the solutions that we developed to address these challenges, and the lessons that we learned. Finally, we highlight some of the open issues, such as nondeterminism and partitioning, that remain to be resolved.",2004,0,
2248,2249,Reliability Analysis of Embedded Applications in Non-Uniform Fault Tolerant Processors,"Soft error analysis has been greatly aided by the concept of Architectural vulnerability Factor (AVF) and Architecturally Correct Execution (ACE). The AVF of a processor is defined as the probability that a bit flip in the processor architecture will result in a visible error in the final output of a program. In this work, we exploit the techniques of AVF analysis to introduce a software-level vulnerability analysis. This metric allows insight into the vulnerability of instruction and software to hardware faults with a micro-architectural involved fault injection method. The proposed metric can be used to make judgments about the reliability of different programs on different processors with regard to architectural and compiler guidelines for improving the processor reliability.",2010,0,
2249,2250,Multi-fault diagnosis of rolling-element bearings in electric machines,"This paper deals with the diagnosis of faults in roller-element bearings as the core of a dedicated Condition based Maintenance (CBM) system. Vibration signals recorded by accelerometers feed into a classification model in charge of monitoring and evaluating bearing wear. The chosen feature extraction technique is based on the computation of the Discrete Fourier Transform (DFT) and on the estimation of the normalized frequency content in each of the considered spectrum sub-bands as an indicative measure of the state of health of the roller-element bearing. Three different damage attributes have been investigated. For each attribute, both Support Vector Machines (SVM) and neurofuzzy Min-Max classifiers have been employed as the core of the diagnostic system. Test results show that it is possible to achieve high accuracy in all diagnostic problems considered. The pre-processing procedure and the classification stage, especially in the case of Min-Max fuzzy networks, do not require demanding computational hardware resources, and as a result, a simple and effective diagnostic system can be designed by feeding the synthesized Min-Max classifiers with the spectral features computed from vibration sensor outputs.",2010,0,
2250,2251,Spectral RTL Test Generation for Gate-Level Stuck-at Faults,"We model RTL faults as stuck-at faults on primary inputs, primary outputs, and flip-flops. Tests for these faults are analyzed using Hadamard matrices for Walsh functions and random noise level at each primary input. This information then helps generate vector sequences. At the gate-level, a fault simulator and an integer linear program (ILP) compact the test sequences. We give results for four ITC'99 and four ISC AS'89 benchmark circuits, and an experimental processor. The RTL spectral vectors performed equally well on multiple gate-level implementations. Compared to a gate-level ATPG, RTL vectors produced similar or higher coverage in shorter CPU times",2006,0,
2251,2252,Masking Does Not Protect Against Differential Fault Attacks,"Over the past ten years, cryptographic algorithms have been found to be vulnerable against side-channel attacks such as power analysis attacks, timing attacks, electromagnetic radiation attacks and fault attacks. These attacks capture leaking information from an implementation of the algorithm in software or in hardware and apply cryptanalytical and statistical tools to recover the secret keys. A very well-known countermeasure against these attacks is to randomize every execution of the algorithm and every intermediate piece of data with a so-called masking method. In this paper we demonstrate that traditional countermeasures such as masking methodsfor symmetric cryptosystems are completely inefficient against fault attacks. In other words, differential fault attacks still apply on masked data. As an example we show how to recover secret keys from two masked AES implementations using a basic differential fault attack.",2008,0,
2252,2253,Exact bit-error probability for optimum combining with a Rayleigh fading Gaussian cochannel interferer,"We derive expressions for the exact bit-error probability (BEP) for the detection of coherent binary phase-shift keying signals of the optimum combiner employing space diversity when both the desired signal and a Gaussian cochannel interferer are subject to flat Rayleigh fading. Two different methods are employed to reach two different, but numerically identical, expressions. With the direct method, the conditional BEP is averaged over the fading of both signal and interference, With the moment generating function based method, expressions are derived from an alternative representation of the Gaussian Q-function",2000,0,
2253,2254,DCE-MRI Segmentation and Motion Correction Based on Active Contour Model and Forward Mapping,"This paper presents an automatic method to segment and correct motion artifact on dynamic contrast enhanced magnetic resonance imaging (DCE-MRI). The breast region is segmented from DCE-MRI using mathematical morphology, region growing, and active contour models. The motion artifact presented in the image is then corrected by applying B-spline curve fitting, active contour model and forward mapping algorithm. Our segmentation method has been tested on 72 DCR-MRI studies from 33 patients. The average segmentation accuracy was 96.76%, and the confidence interval was [95.55%, 97.97%] with p<0.05. Simulation and validation experiments for motion correction are working in progress. The detail experimental results were presented at the conference. The paper represents work-in-progress in our effort to build a CAD system for breast DCE-MRI",2006,0,
2254,2255,Monitoring a tunneling in an urbanized area with Terrasar-X interferometry  Surface deformation measurements and atmospheric error treatment,"We present results from a deformation monitoring to demonstrate potential and limitations of TerraSAR-X interferometry to measure vertical displacements due to the tunneling of main sewerage pipes along the river Emscher in Germany. In spite of higher sensitivity for deformation gradients the potential for deformation monitoring benefits from high spatial and temporal resolution of the TerraSAR-X data. We analyzed a large stack of TerraSAR-X stripmap scenes to derive regional pattern of vertical displacements with differential SAR Interferometry and small-scale displacements and deformation of objects (infrastructure and houses) in time series of SAR-scenes with Persistent Scatterer Interferometry (PSI). First results from PSI are promising with a great number of detected PS. We show deformation measurements with Artificial Corner Reflectors. Short-time interferograms (11 or 22 days) show high coherence for large areas and therefore are likely less infected by unwrapping errors. Atmospheric errors are important for X-Band SAR. Expected deformation in our application is in the range of mm to cm, similar to tropospheric delay features in their spatial and temporal extent. The atmospheric phase screen in PSI and stacking procedures are smoothing the nonuniform deformation history of progressing tunneling.",2009,0,
2255,2256,Model Structures Used in Rotor Defect Identification of a Squirrel Cage Induction Machine,"In this paper a method of detection of broken bars in squirrel cage induction machine is presented. This method is based on the determination of discrete parameters of the transfer functions of the induction machine by model structures such as ARMAX, ARX, IV and OE model structures",2006,0,
2256,2257,Fault detection in reactive ion etching systems using one-class support vector machines,"A robust method to detect faults in reactive ion etching systems using optical emission spectroscopy data is proposed. The approach is based on one-class support vector machines (SVMs). Unlike previously proposed fault detection methods, this approach only requires data collected during normal equipment operation to be trained. The results obtained suggest that this technique can detect equipment faults with exceptional accuracy. The SVM used detected all faults, yielding a detection accuracy of 100% with zero false alarms",2005,0,
2257,2258,Recent Developments in Fault Detection and Power Loss Estimation of Electrolytic Capacitors,"This paper proposes a comparative study of current-controlled hysteresis and pulsewidth modulation (PWM) techniques, and their influence upon power loss dissipation in a power-factor controller (PFC) output filtering capacitors. First, theoretical calculation of low-frequency and high-frequency components of the capacitor current is presented in the two cases, as well as the total harmonic distortion of the source current. Second, we prove that the methods already used to determine the capacitor power losses are not accurate because of the capacitor model chosen. In fact, a new electric equivalent scheme of electrolytic capacitors is determined using genetic algorithms. This model, characterized by frequency-independent parameters, redraws with accuracy the capacitor behavior for large frequency and temperature ranges. Thereby, the new capacitor model is integrated into the converter, and then, software simulation is carried out to determine the power losses for both control techniques. Due to this model, the <i>equivalent series resistance</i> (ESR) increase at high frequencies due to the skin effect is taken into account. Finally, for hysteresis and PWM controls, we suggest a method to determine the value of the series resistance and the remaining time to failure, based on the measurement of the output ripple voltage at steady-state and transient-state converter working.",2010,0,
2258,2259,Simple switch open fault detection method of voltage source inverter,"Recently, permanent magnet synchronous motors are applied to various applications such as electric vehicle, aerospace, medical service, and military applications due to several outstanding characteristics. Because of the importance of high reliable operation in these areas, many researches which are related to the fault detection and diagnosis of inverter systems are conducted. In this paper, a new simple fault detection method of voltage source inverter for permanent magnet synchronous motor is proposed. The feasibility of the proposed method is proved by simulation and experiment. By the simulation and experiments, rapid detection characteristic of the proposed method has been proved without any additional voltage sensor.",2009,0,
2259,2260,A neural network approach for fault diagnosis of large-scale analogue circuits,"An approach for fault diagnosis of large-scale analogue circuits using neural networks is presented in the paper. This method is based on the fault dictionary technique, but it can deal with soft faults due to the robustness of neural networks. Because the neural networks can create the fault dictionary, memorize and verify it simultaneously, computation time is drastically reduced. Rather than dealing with the whole circuit directly, the proposed approach partitions a large-scale circuit into several small sub-circuits and then tests each sub-circuit using the neural network method. The principle and diagnosis procedure of the method are described. Two examples are given to illustrate the method for both small and large-scale circuits.",2002,0,
2260,2261,Research on dynamic simulation of the resonance fault current limiter,"The resonance fault current limiter (RFCL), which avoids the disadvantages of the series reactor, is feasible for EHV and UHV grid. To contribute to project application of the RFCL, and study its running characteristic and impact on relay protection, the dynamic simulation lab of State Grid simulation center carries out the research on simulation technique for RFCL. The principle and technical properties of RFCL applied in EHV power transmission line are analyzed. Considering the features of dynamic simulation system in the lab, the approach to choose parameters of simulation model and corresponding structural design are described. This simulation model has been connected into dynamic simulation system and the simulation experiments for inspecting the control and protection function of RFCL are performed. Experimental results show that the performances of the developed simulation model for RFCL can meet the design requirement, and can be applied in dynamic simulation tests and researches.",2010,0,
2261,2262,High-speed forward error correction IP blocks for system-on-chip design,"High-speed forward error correction IP (intellectual property) blocks suitable for system-on-chip were designed for a wireless communications transceiver. Synthesizable HDL (hardware description language) code was written for the blocks including a differential encoder/decoder, a Reed-Solomon encoder/decoder and a convolutional interleaver/deinterleaver. The code was compiled, tested and verified for timing and function in both FPGA and 0.18 m CMOS.",2002,0,
2262,2263,Testing Flash Memories for Tunnel Oxide Defects,"Testing non volatile memories for tunnel oxide defects is one of the most important aspects to guarantee cell reliability. Defective tunnel oxide layer in core memory cells can result in various disturb faults. In this paper, we study various defects in the insulating layers of a IT flash cell and analyze their impact on cell performance. Further, we present a test methodology and test algorithms that enable the detection of tunnel oxide defects in an efficient manner.",2008,0,
2263,2264,RI2N/UDP: High bandwidth and fault-tolerant network for a PC-cluster based on multi-link Ethernet,"PC-clusters with high performance/cost ratio have been one of the typical platforms for high performance computing. To lower costs, Gigabit Ethernet is often used for intercommunication networks. However, the reliability of Ethernet is limited due to hardware failures and tentative errors in the network switches. To solve this problem, we propose an interconnection network system based on multi-link Ethernet named RI2N. In this paper, we developed a user level implementation of RI2N using UDP/IP that is called RI2N/UDP. When this new system was evaluated for performance and fault tolerance, the bandwidth on a 2-link Gigabit Ethernet was 246 MB/s, and the system could remain active during network link failure to provide high system reliability.",2007,0,
2264,2265,Active incipient fault detection with more than two simultaneous faults,"The problem of detecting small parameter variations in linear uncertain systems due to incipient faults, with the possibility of injecting an input signal to enhance detection, is considered. Most studies assume that there is only one fault developing. Recently an active approach for two simultaneous faults has been introduced. In this paper we extend this approach to allow for more than two simultaneous faults. Having more than two simultaneous incipient faults is sometimes a natural assumption. A computational method for the construction of an input signal for achieving guaranteed detection with specified precision is presented for discrete time systems. The method is an extension of a multi-model approach used for the construction of auxiliary signals for failure detection, however, new technical issues must be addressed. A case study is examined.",2009,0,
2265,2266,A Robot Fault-Tolerance Approach Based on Fault Type,"As the field of robot service expands, reliability has become one of the highest priorities in robot development. Fault-tolerance is an important characteristic for robots to increase their reliability levels. However, the literature on fault-tolerance in robotics has focused mainly on developing a single fault-tolerance technique that is focused on improving reliability for a limited set of context and situations. To meet the demands for reliable robots, a set of appropriate fault-tolerance techniques should be used in the given context and situation. In this paper, we present a systematic approach to facilitate the selection of appropriate robot fault-tolerance techniques on the basis of the context in the robot domain. We have applied the approach to build a fault-tolerance architecture for a robot platform.",2009,0,
2266,2267,Magnetic equivalent circuit modeling of induction machines under stator and rotor fault conditions,"In this paper, stator and rotor failures in squirrel-cage induction machines are modeled using the magnetic equivalent circuit (MEC) approach. Failures associated with stator winding and rotor cage are considered. More specifically, stator inter-turn short circuit and broken rotor bar failures are modeled. When compared to conventional modeling techniques, the MEC modeling approach offers two main advantages: 1) relatively high speed of execution, and 2) high accuracy. The developed MEC model is validated here with respect to the experimental tests and time-stepping finite-element simulations for healthy and various faulty conditions.",2009,0,
2267,2268,Fault tolerant two-level pyramid networked control systems,"In this paper, a pyramid control hierarchy is proposed. It is based on the presence of a supervisor controller on top of separate controller nodes. A simulation study is conducted to test the functionality of the system. The proposed model is an enhancement of machine modeled in form of networked control systems (NCS). Two models are tested: one supervisor/two sub-controllers, one supervisor/three sub-controllers. All possible combinations of supervisor-controller intercommunication are tested. Also, all supervisor/controller inter-changeability possibilities are taken into consideration. Results are illustrated and discussed. Recommendations are drawn out. All machine models of this study are built using switched gigabit Ethernet in star topology",2005,0,
2268,2269,Enhancing Fault Injection Testbench,Fault injection techniques are widely used in system dependability evaluation. In the paper we deal with the problem of enhancing classical fault injection tools in two aspects: improvement of experiment effectiveness and result analysis capabilities. In particular we discuss the problem of distributing fault injection processes within a computer network and collecting the simulation results in a data warehouse with data mining capabilities. The presented considerations are illustrated with some experimental results performed with simulation tools developed in our institute,2006,0,
2269,2270,Estimation of the output error statistics of space-time equalization in an antenna array EGPRS receiver with soft-decision decoding,"The use of antenna arrays can help combat cochannel interference (CCI) in wireless cellular systems. In this paper, we consider an enhanced general packet radio service diversity receiver based on least squares spatio-temporal equalization and soft-decision decoding in the presence of decision feedback and/or asynchronous CCI. We compare known and novel estimators of the error mean and variance at the output of the deterministic space-time equalizer. The collected simulation data indicate that the estimation of the error mean and variance is critical to the performance of soft-in/hard-out Viterbi decoding in the presence of nonstationary input disturbance. Moreover, the use of short-term error statistics provides receiver performance gains of up to 15-20 dB in terms of signal-to-interference ratio, with respect to the use of burst statistics based on the training sequence midamble and tentative decisions on the payload symbols.",2005,0,
2270,2271,A novel pulse echo correlation tool for transmission path testing and fault finding using pseudorandom binary sequences,"In this paper, a novel pulse sequence testing methodology is presented as an alternative to time domain reflectometry (TDR) for transmission line 'health' condition monitoring, faultfinding and location. This scheme uses pseudo random binary sequence (PRBS) injection with cross correlation (CCR) techniques to build a unique response profile, as a characteristic signature, to identify the type of fault, if any, or load termination present as well as its distance from the point of stimulus insertion. This fault characterization strategy can be applied to a number of industrial application scenarios embracing high frequency (HF) printed circuit board (PCB) and integrated circuit (IC) device operation, overhead lines and underground cables in inaccessible locations, which rely on a transmission line pathway or 'via' common to all cases either for signal propagation or power conveyance. In this paper a lumped parameter circuit model is presented to emulate generalized transmission line behaviour, using the well-known pSpice simulation package, for a range of known load-terminations mimicking fault conditions in a range of application scenarios encountered in practice. Numerous line behavioural simulations for various fault conditions, known a priori, with measured CCR response demonstrate the capability of and establishes confidence in the effectiveness of the PRBS test method in fault type identification and location. The accuracy of the method is further validated through theoretical calculation using known lumped parameters, fault termination conditions and link distance in transmission line simulation.",2005,0,
2271,2272,Reliability evaluation of transmission line protective schemes using static fault tree,"Transmission line protective schemes are sometimes very complex, incorporating many different devices. Reliability of such complex systems is a concern to the power system protection engineer and presents a significant analytical problem. This paper describes the use of fault tree analysis as one method of analyzing the reliability of these complex systems. A MATLAB-based software is developed and reliability of different protection schemes using various devices are studied.",2004,0,
2272,2273,Using the Number of Faults to Improve Fault-Proneness Prediction of the Probability Models,"The existing fault-proneness prediction methods are based on unsampling and the training dataset does not contain the information on the number of faults of each module and the fault distributions among these modules. In this paper, we propose an oversampling method using the number of faults to improve fault-proneness prediction. Our method uses the information on the number of faults in the training dataset to support better prediction of fault-proneness. Our test illustrates that the difference between the predictions of oversampling and unsampling is statistically significant and our method can improve the prediction of two probability models, i.e. logistic regression and naive Bayes with kernel estimators.",2009,0,
2273,2274,A New Mesh Simplification Algorithm Based on Quadric Error Metrics,"This paper proposes a mesh simplification algorithm base on quadric error metric. Most of the simplification algorithms use the geometric distance as their simplification criteria, the distance metric is very efficient to measure geometric error, but it is difficult to distinguish important shape features such as a high-curvature region even though it has a small distance metric. The curvature is one of the good criteria of simplification to preserve the shape of an original model, if the curvature of the vertex is larger, it can present the geometric features of model well. Besides curvature, the size of the incident edges around the vertex can also reflect the geometric feature, if the edge lengths that adjoin the vertex are larger, it infects larger area on the surface of the model. We considered both the local curvature and the size of the incident edges around the vertex on the basis of the quadric error metrics, it can reflect changes on the model surface and still maintain many important geometric features after large scale simplified.",2008,0,
2274,2275,A Merge Method for Decentralized Discrete-Event Fault Diagnosis,This paper proposes a generic component model (GCM) to give a functional representation of large scale systems. The GCM rests on the notion of services provided by components and their organization into operating modes. To each operating mode corresponds a behavior model uses as reference to diagnose fault. Faults are detected by local diagnosers built on subsystems using a merging procedure without any global model and global diagnoser.,2008,0,
2275,2276,Robustness with respect to error specifications,"Summary form only given. Formal specifications used in automatic verification typically describe the desired behavior of a system only in absence of environment failures. That is, specifications are often of the form A ->; G, where A is an assumption on the environment and G is the guarantee, the system should provide. This approach leaves the behavior of the system unspecified when A is not fulfilled and neither verification tools nor synthesis tools take such behavior into account. In practice, however, the environment may fail, due to incomplete specifications, operator errors, faulty implementations, transmission errors, and the like. Thus, a system should not only be correct, it should also be robust, meaning that it ""behaves 'reasonably' even in circumstances that were not anticipated in the requirements specification. In this talk we present a formal notion of robustness through graceful degradation for discrete functional safety properties: A small error by the environment should induce only a small error by the system, where the error is defined quantitatively as part of the specification, for instance, as the number of failures. Given such an error specification, we define a system to be robust if a finite environment error induces only a finite system error. As a more fine-grained measure of robustness, we define the notion of k-robustness, meaning that on average, the number of system failures is at most k times larger than the number of environment failures. We show that the synthesis question for robust systems can be solved in polynomial time as a one-pair Streett game and that the synthesis question for k-robust systems can be solved using ratio games. Ratio games are a novel type of graph games in which edges are labeled with a cost for each player, and the aim is to minimize the ratio of the sum o",2010,0,
2276,2277,Development and research on fault diagnosis system of solar power tower plants,"According to the system configuration and operating characteristic of a constructing solar power tower (SPT) plant in China in this paper, the fault diagnosis system (FDS) was researched and developed. Furthermore, evaluation system of fault grade was established by the method of fuzzy comprehensive evaluation. In this FDS, the fault diagnosis structure was designed to adopt the expert system for priority and the radial basis function (RBF) neural network for assistant. The monitoring index of diagnosis object was built in expert system to set the fault symptom threshold and represent the fault symptom in quantification with the comprehensive methods of expert knowledge, fuzzy mathematics, and low probability identification and so on. The model of neural networks is established based on the structure of RBF multi-neural subnets. According to the analysis and verification results of a fault case, the structure design is reasonable and diagnosis methods are feasible in this FDS. Moreover, the fault could be accurately diagnosed and the evaluation of the fault grade could be made reliably with the great practical value.",2009,0,
2277,2278,Definition and Extraction of Causal Relations for QA on Fault Diagnosis of Devices,"Causal relations in ontology should be defined based on the inference types necessary to solve the tasks specific to application, as well as domain. In this paper, we present a model to define and to extract causal relations for application ontology, which is targeted, as a case study, to serve a question-answering (QA) system on fault-diagnosis of electronic devices. In the first phase, causal categories are defined by identifying the generic inference patterns of QA on fault-diagnosis. In the second, the semantic relations between concepts in the corpus denoting the causal categories are defined as causal relations. In the third, instances of causal relations are extracted using the lexical patterns from the definitional statements of terms in domain, and extended with information from thesaurus. On the evaluation by domain experts, our model shows precision of 92.3% in classifying relations at the definition phase and precision of 80.7% in identifying causal relations at the extraction phase.",2008,0,
2278,2279,Fault identification of double circuit lines,"The technique for identifying fault types on single circuit lines, suggested by Ferrero et al. (1993) is modified to be suitable for protecting double circuit lines. The modified technique is based on estimating the Fourier coefficients of double circuit line currents using the recursive least square identifier. The sequence components are obtained by a linear transformation of the fundamental frequency phase currents of each circuit. The fuzzy logic inference system processes these components to identify fault type. The computer simulation test results indicate the possibility of using the suggested technique as an effective tool for high speed digital relaying",2001,0,
2279,2280,Minimal March tests for unlinked static faults in random access memories,"New minimal March test algorithms are proposed for detection of (all) unlinked static faults in random access memories. In particular, a new minimal March MSS test of complexity I8N is introduced detecting all realistic simple static faults, as March SS (22N), (S. Hamdioui, van de Goor, Rodgers, MTDT 2002).",2005,0,
2280,2281,Baselining network traffic and online faults detection,"This paper addresses the problem of normal operation baselining for automatic detection of network anomalies. A model of network traffic is presented in which studied variables are viewed as sampled from finite mixture model. Based on the stochastic approximation of the maximum likelihood function, we propose baselining network normal operation, using the asymptotic distribution of the difference between successive estimates of model parameters. The baseline random variable is shown to be stationary, with mean zero under normal operation. Anomalous events are shown to induce an abrupt jump in the mean. Detection is formulated as an online change point problem, where the task is to process the baseline random variable realizations, sequentially, and raise alarms as soon as anomalies occur. An analytical expression of false alarm rate allows us to choose the design threshold, automatically. Extensive experimental results on a real network showed that our monitoring agent is able to detect unusual changes in the characteristics of network traffic, adapt to diurnal traffic patterns, while maintaining a low alarm rate. Despite large fluctuations in network traffic, this work proves that tailoring traffic modeling to specific goals can be efficiently achieved.",2003,0,
2281,2282,Robust hierarchical mobile IPv6 (RH-MIPv6): an enhancement for survivability and fault-tolerance in mobile IP systems,"In wireless networks, system survivability is one of the most important issues in providing quality of service (QoS). However, since failure of home agent (HA) or mobile anchor point (MAP) causes service interruption, the hierarchical mobile IPv6 (HMIPv6) has only weak survivability. In this paper, we propose robust hierarchical mobile IPv6 (RH-MIPv6), which provides fault tolerance and robustness in mobile networks. In RH-MIPv6, a mobile node (MN) registers primary (P-RCoA) and secondary (S-RCoA) regional care of addresses to two different MAPs (primary and secondary) simultaneously. We develop a mechanism to enable the mobile node or correspondent node (CN) to detect the failure of primary MAP and change their attachment from the primary to secondary MAP. By this recovery procedure, it is possible to reduce the failure recovery time. Analytical evaluation indicates that RH-MIPv6 has faster recovery time than HMIPv6 and we also show through simulation as like analytical result. Consequently, RH-MIPv6 shows about 60% faster recovery time compared with HMIPv6.",2003,0,
2282,2283,SEU data and fault tolerance analysis of a LEON 3FT processor,Single-bit per word error protection is implemented on this LEON 3 fault tolerant processor. SEU data is reviewed and fault analysis is examined based on processor operation and operational environment.,2009,0,
2283,2284,Investigating Test Teams' Defect Detection in Function test,"In a case study, the defect detection for functional test teams is investigated. In the study it is shown that the test teams not only discover defects in the features under test that they are responsible for, but also defects in interacting components, belonging to other test teams' features. The paper presents the metrics collected and the results as such from the study, which gives insights into a complex development environment and highlights the need for coordination between test teams in function test.",2007,0,
2284,2285,WiMAX bandpass filter using hybrid microstrip Defected-Ground-Structure,"A novel and compact WiMAX microstrip bandpass filter is proposed. The filter employs a very wide bandwidth from 2 to 11 GHz under NLOS environment of the IEEE 802.16-2004 standard, and with a 3-dB fractional bandwidth of greater than 138%. The proposed hybrid WiMAX band-pass filter had return loss more than 17 dB in the passband, and also demonstrated a reject band from 12.4 GHz to more than 20 GHz at -20 dB.",2010,0,
2285,2286,Performance management via adaptive thresholds with separate control of false positive and false negative errors,"Component level performance thresholds are widely used as a basic means for performance management. As the complexity of managed systems increases, manual threshold maintenance becomes a difficult task. This may result from a) a large number of system components and their operational metrics, b) dynamically changing workloads, and c) complex dependencies between system components. To alleviate this problem, we advocate that component level thresholds should be computed, managed and optimized automatically and autonomously. To this end, we have designed and implemented a performance threshold management sub-system that automatically and dynamically computes two separate component level thresholds: one for controlling Type I errors and another for controlling Type II errors. We present the theoretical foundation for this autonomic threshold management system, describe a specific algorithm and its implementation, and evaluate it using real-life scenarios and production data sets. As our present study shows, with proper parameter tuning, our on-line dynamic solution is capable of nearly optimal performance thresholds calculation.",2009,0,
2286,2287,Xception<sup>TM</sup> - enhanced automated fault-injection environment,"Discusses Xception, an automated fault injection environment that enables accurate and flexible V&V (verification & validation) and evaluation of mission and business critical computer systems using fault injection. Xception is designed to accommodate a variety of fault injection techniques (according to a wide range of configurations of the tool) and emulate in this way different classes of faults, with particular emphasis to hardware and software faults.",2002,0,
2287,2288,Effect of tropospheric range delay corrections on differential SAR interferograms,"The tropospheric range delay is estimated from meteorological data or GPS data, and then is used to correct JERS-1 SAR differential interferograms. Topography of the concerned area is considered in mapping the delay",2001,0,
2288,2289,Error compensation of workpiece localization,"Workpiece localization has direct relations with many manufacturing automation applications. In order to gain accurate workpiece measurement by coordinate measuring machines (CMM) or on-machine measurement system, the touch trigger probe is widely adopted. In spite of the high repeatability of the touch trigger probe, there are still error sources associated with the probe. In this paper, we will focus on probe radius compensation. Several compensation methods in related papers are reviewed. In addition, a new radius compensation method is proposed in this paper. Simulation and experimental results of probe radius compensation by different methods are given. It is shown that our proposed method has the best performance both in terms of compensation accuracy and computational time. The method is also implemented in a computer aided setup (CAS) system.",2001,0,
2289,2290,Intelligent PC-based user control interface for on-line correction of robot programs,"Today the automotive sector is dominated by high variety of types and increasing product diversification. Thus, OEM manufacturers reintegrate key technologies back in the enterprise, due to competitive reason and to stabilize leadership in innovation. As an example, state-of-the-art fuel and diesel engine cylinder heads achieve higher complexity and filigrane structure, therefore careful treatment of specimen as well as net-shape machining processes with high quality output are a major requirement in this sector. A key issue is the surface fretting and finishing of engine parts subsequent following foundry procedures. An advanced factory automation system, based on an industrial robot (6-axis joint coordinate), has been developed in order to obtain the described results. On the system apply various measuring procedures, in order to recognize and remove burr formation (particularly with optical technique). A graphical interactive programming system enables simple and user friendly correction at deburring results for individual workpiece types. A possibility was given to the system user, who can easily remove deburring errors through short operator interaction, mainly input of correction values at controller system. Thereby, each authorized robot cell operator can contribute to guaranty process quality without special technical training.",2002,0,
2290,2291,Sequential correction of perspective warp in camera-based documents,"Documents captured with hand-held devices, such as digital cameras often exhibit perspective warp artifacts. These artifacts pose problems for OCR systems which at best can only handle in-plane rotation. We propose a method for recovering the planar appearance of an input document image by examining the vertical rate of change in scale of features in the document. Our method makes fewer assumptions about the document structure than do previously published algorithms.",2005,0,
2291,2292,No-reference video quality estimation based on error-concealment effectiveness,"This paper proposes a no-reference videoquality estimation method for monitoring end-user video quality. It is suitable for applications to video transmitted over IP networks. IP network conditions vary for individual users, and assuring end-user video quality is an important issue. To do this, it is necessary to monitor video quality at end-user terminals. With the proposed method, video quality is estimated on the basis of the number of macroblocks containing errors which it has not been possible to conceal. Error concealment effectiveness is evaluated using motionlevel information and luminance discontinuity at the boundaries of error regions. Simulation results show a high correlation (0.95) between the actual mean square error and the number of macroblocks in which complete error concealment has not been possible.",2007,0,
2292,2293,Vibration fault diagnosis of hydro-turbine generating unit based on rough 1-v-1 multiclass support vector machine,"The traditional vibrant fault diagnosis classifier of hydro-turbine generating unit (HGU) can't reflect the uncertain information in fault pattern recognition. To overcome the above problem a novel classifier based on rough set (RS) and 1-v-1 multiclass support vector machine (SVM) is introduced. In this method, the basic ideas of RS: upper approximation, lower approximation and boundary region are used to describe the positive region, negative region and margin of SVM. By using 1-v-1 method, the multiclass classification of SVM is realized. Then the description of upper approximation, lower approximation and boundary region of multiclass are determined. At last, the rules of classifier are acquired. The results show that the proposed classifier has high classification reliability, more concise rule, and lower requirement of memory space in operation stage, and can reflect the uncertain information of fault diagnosis.",2010,0,
2293,2294,PEEC: a channel-adaptive feedback-based error,"Reliable transmission is a challenging task over wireless LANs since wireless links are known to be susceptible to errors. Although the current IEEE802.11 standard ARQ error control protocol performs relatively well over channels with very low bit error rates (BERs), this performance deteriorates rapidly as the BER increases. This paper investigates the problem of reliable transmission in a contention free wireless LAN and introduces a packet embedded error control (PEEC) protocol, which employs packet-embedded parity symbols instead of ARQ-based retransmission for error recovery. Specifically, depending on receiver feedback, PEEC adaptively estimates channel conditions and administers the transmission of (data and parity) symbols within a packet. This enables successful recovery of both new data and old unrecovered data from prior transmissions. In addition to theoretically analyzing PEEC, the performance of the proposed scheme is extensively analyzed over real channel traces collected on 802.11b WLANs. We compare PEEC performance with the performance of the IEEE802.il standard ARQ protocol as well as contemporary protocols such as enhanced ARQ and the hybrid ARQ/FEC. Our analysis and experimental simulations show that PEEC outperforms all three competing protocols over a wide range of actual 802.11b WLAN collected traces. Finally, the design and implementation of PEEC using an adaptive low-density-parity-check (A-LDPC) decoder is presented.",2008,0,
2294,2295,Mobile agent fault tolerance for information retrieval applications: an exception handling approach,"Maintaining mobile agent availability in the presence of agent server crashes is a challenging issue since developers normally have no control over remote agent servers. A popular technique is that a mobile agent injects a replica into stable storage upon its arrival at each agent server. However, a server crash leaves the replica unavailable, for an unknown time period, until the agent server is back online. This paper uses exception handling to maintain the availability, of mobile agents in the presence of agent server crash failures. Two exception handler designs are proposed. The first exists at the agent server that created the mobile agent. The second operates at the previous agent server visited by the mobile agent. Initial performance results demonstrate that although the second design is slower it offers the smaller trip time increase in the presence of agent server crashes.",2003,0,
2295,2296,Dynamic response of distributed synchronous generators on faults in HV and MV networks,"Connection of distributed generators (DG) on existing power distribution networks requires solution of technical, economical and regulatory issues. A brief description of current status of DG in Bosnia and Herzegovina, with special attention to the technical impact of integration of small hydro power plants with synchronous generators, is given in this paper. Dynamic response of DG (namely small hydro power plants with synchronous generators) on disturbances in HV and MV networks is analyzed in detail. Simulations of DG dynamic response are performed using free, open-source MATLAB/PSAT software package on a simple test model of distribution network with DG. Several scenarios are studied. Finally, areas for further research are identified and presented.",2009,0,
2296,2297,A fuzzy error correction control system,"This paper describes a fuzzy error correction control system used to navigate a robot along an easily modifiable path in a well-structured environment. An array of Hall sensors mounted on the bottom of a robot gathers sensory information from a path of ferromagnetic disks placed on the ground. This sensory input is processed by an analog-to-digital converter and the output signals are then inputted into a fuzzy logic engine. The fuzzy engine outputs commands for the robot wheels. These commands determine the necessary angle of rotation to correct the direction of travel in order for the robot to remain on the path. The fuzzy logic controller stores prior disk information to predict a path trajectory when no path is detected. If the controller then senses a path, it anchors on it and starts following it",2001,0,
2297,2298,Sampling rate of digital fault recorders influence on fault diagnosis,"A case study of fault classification in transmission lines using artificial neural networks (ANN) is presented. The database is built from current and voltage waveform samples obtained from fault simulations with the ATP. Utility companies usually have digital fault recorders with different sampling rates, so it is important to evaluate how good the classifier is when the sampling rate changes, this is the main purpose of the paper. A routine to reduce the sampling rate with no loss of accuracy in classifying faults was implemented.",2004,0,
2298,2299,Optimal Parameter Settings for Forward Error Correction Schemes for Multimedia Streaming over Wireless Networks,"This paper investigates algorithms to determine an optimal choice of the FEC parameters (n, k) to mitigate the effects of packet loss due to buffer overflow at a wireless base station on multimedia traffic. We develop an analytic model of the considered network scenario that takes into account the traffic arrival rates, the channel loss characteristics, the size of the buffer at the wireless access point, and the influence of the FEC parameters on the packet loss. Applying the theory of recurrent linear equations, we present a new approach to establish a closed form solution of the underlying Markov model for the buffer occupancy and verify the analytical results via simulations",2005,0,
2299,2300,Phoenix: making data-intensive grid applications fault-tolerant,"A major hurdle facing data intensive grid applications is the appropriate handling of failures that occur in the grid-environment. Implementing the fault-tolerance transparently at the grid-middleware level would make different data intensive applications fault-tolerant without each having to pay a separate cost and reduce the time to grid-based solution for many scientific problems. We analyzed the failures encountered by four real-life production data intensive applications: NCSA image processing pipeline, WCER video processing pipeline, US-CMS pipeline and BMRB BLAST pipeline. Taking the result of the analysis into account, we have designed and implemented Phoenix, a transparent middleware-level fault-tolerance layer that detects failures early, classifies failures into transient and permanent and appropriately handIes the transient failures. We applied our fault-tolerance layer to a prototype of the NCSA image processing pipeline and considerably improved the failure handling and report on the insights gained in the process.",2004,0,
2300,2301,Road to the integrated protective relaying fault information system,"This paper introduces an integrated power system protective relaying fault information system. This system interconnects several vital substations to take the protective relaying and fault recorder under control, so the real-time supervision of them and integrative analysis can be carried out. It has been installed in the State Power Corporation of China and is now under test-run. At the same time, in this paper, it is proposed to integrate fault information system with other applications related to protective relaying to form a uniform platform for the operation and analysis of protective relaying. And the further development of this system in the near future is put forward.",2003,0,
2301,2302,"A toolkit for building secure, fault-tolerant virtual private networks","Dynamic coalition networks connect multiple administrative domains. The domains have a need to communicate, but have limited mutual trust. To establish communication services, these networks must be configured consistently with respect to global service requirements and security policies. The configuration must also be done in a way that respects the autonomy of the separate domains. Commercial network configuration tools do not provide sufficient functionality for this purpose. This document outlines a toolkit for solving these problems and reports on its deployment over a wide area network between Telcordia Technologies and BBN's TIC.",2003,0,
2302,2303,"Utilizing third harmonic 100% stator ground fault protection, a cogeneration experience","Third harmonic, 100% stator ground fault protection schemes are becoming economically viable for small and mid size generators used in cogeneration applications. Practical considerations must be observed in order that such schemes are successfully applied for different machines. This paper introduces an experience with the applications of 3rd harmonic schemes in cogeneration applications and depicts their advantages and limitations. For a 50 MVA generator, actual measurements of produced third harmonics are portrayed and analyzed. In light of the experience and analysis, applications of certain third harmonic scheme configurations are contemplated",2000,0,
2303,2304,Static Detection of Disassembly Errors,"Static disassembly is a crucial first step in reverse engineering executable files, and there is a considerable body of work in reverse-engineering of binaries, as well as areas such as semantics-based security analysis, that assumes that the input executable has been correctly disassembled. However, disassembly errors, e.g., arising from binary obfuscations, can render this assumption invalid. This work describes a machine-learning-based approach, using decision trees, for statically identifying possible errors in a static disassembly; such potential errors may then be examined more closely, e.g., using dynamic analyses. Experimental results using a variety of input executables indicate that our approach performs well, correctly identifying most disassembly errors with relatively few false positives.",2009,0,
2304,2305,A novel online diagnosis of brushless generator rotary rectifier fault,"The fault detection of rotary rectifier based on harmonic analysis has some deficiencies. A new fault diagnosis method is presented using fractal theory and dynamics. Firstly, the quantitative description of exciter field currentpsilas complexity and irregularity is performed by box dimension calculation. Then the exciter field current fluctuation range under noisy environments is obtained by dynamics. Finally, the fault detection and identification of rotary rectifier is implemented through fractal dimension-signal fluctuation range trajectory area. The analysis results of testing waveforms show that box dimension and dynamics can analyze the waveform characteristics of exciterpsilas field current synthetically, the diagnosis effect is remarkable. Because the arithmetic has a little working load, it is more feasible to realize online fault diagnosis.",2008,0,
2305,2306,A Hardware-Scheduler for Fault Detection in RTOS-Based Embedded Systems,"Nowadays, Real-Time Operating Systems (RTOSs) are often adopted in order to simplify the design of safety-critical applications. However, real-time embedded systems are sensitive to transient faults that can affect the system causing scheduling dysfunctions and consequently changing the correct system behavior. In this context, we propose a new hardware-based approach able to detect faults that change the tasks' execution time and/or the tasks' execution flow in embedded systems based on RTOS. To demonstrate the effectiveness and benefits of using the proposed approach, we implemented a hardware prototype named Hardware-Scheduler (Hw-S) that provides real-time monitoring of the Plasma Microprocessor's RTOS in order to detect the above mentioned types of faults. The Hw-S has been evaluated in terms of the introduced area overhead and fault detection capability.",2009,0,
2306,2307,On the throughput of multicasting with incremental forward error correction,"In this paper, we consider a multicasting model that uses incremental forward error correction (FEC). In this model, there is one sender and r<sup>n</sup> receivers. The sender uses an ideal (n,n(1-p),np) FEC code to code a group of n(1-p) data packets with additional np redundant packets so that any set of n(1-p) packets received by a receiver can be used to recover the original n(1-p) data packets. Packets to the receivers are lost independently with probability q. For this model, we prove several strong laws of large numbers for the asymptotic throughput as n  . The asymptotic throughput is characterized by the unique solution of an equation in terms of p, q, and r. These strong laws not only provide theoretical justification for several important observations made in the literature, but also provide insights that might have impact on future design of multicasting protocols.",2005,0,
2307,2308,Error resilience of intra-die and inter-die communication with 3D spidergon STNoC,"Scaling down in very deep submicron (VDSM) technologies increases the delay, power consumption of on-chip interconnects, while the reliability and yield decrease. In high performance integrated circuits wires become the performance bottleneck and we are shifting towards communication centric design paradigms. Networks-on-chip and stacked 3D integration are two emerging technologies that alleviate the performance difficulties of on-chip interconnects in nano-scale designs. In this paper we present a design-time configurable error correction scheme integrated at link-level in the 3D Spidergon STNoC on-chip communication platform. The proposed scheme detects errors and selectively corrects them on the fly, depending on the critical nature of the transmitted information, making thus the correction software controllable. Moreover, the proposed scheme can correct multiple error patterns by using interleaved single error correction codes, providing an increased level of reliability. The performance of the link and its cost in silicon and vertical wires are evaluated for various configurations.",2010,0,
2308,2309,Fault Detection and Diagnosis Based on Modeling and Estimation Methods,"This paper investigates the problem of fault detection and diagnosis in a class of nonlinear systems with modeling uncertainties. A nonlinear observer is first designed for monitoring fault. Radial basis function (RBF) neural network is used in this observer to approximate the unknown nonlinear dynamics. When a fault occurs, another RBF is triggered to capture the nonlinear characteristics of the fault function. The fault model obtained by the second neural network (NN) can be used for identifying the failure mode by comparing it with any known failure modes. Finally, a simulation example is presented to illustrate the effectiveness of the proposed scheme.",2009,0,
2309,2310,Advanced fault management as a part of Smart Grid solution,"Concept of advanced fault management as a part of the Smart Grid solution is based on full coordination of local automation, locally controlled switchgear and relay protection with maximum exploitation for minimizing fault duration and undelivered energy. Island operation is proposed as possible solution for energization of important consumers in order to maximally protect such consumers from outages. All considerations are elaborated trough examples. Also some ideas for further improvements are also suggested.",2008,0,
2310,2311,Using Defect Reports to Build Requirements Knowledge in Product Lines,"In a recent study of a product line, we found that the defect reports both (1) captured new requirements information and (2) implicated undocumented, tacit requirements information in the occurrence of the defects. We report four types of requirements knowledge revealed by software defect reports from integration and system testing for two products in this high-dependability product line. We argue that store-and-retrieve-based requirements management is insufficient to avoid recurrence of these types of defects on upcoming members of the product line. We then propose the use of two mechanisms not traditionally associated with requirements management, one formal and one informal, to improve communication of these types of requirements knowledge to developers of future products in the product line. We show how the two proposed mechanisms, namely feature models extended with assumption specifications (formal) and structured anecdotes of paradigmatic product-line defects (informal), can together improve propagation of the requirements knowledge exposed by these defects to future products in the product line.",2009,0,
2311,2312,Blocking vs. Non-Blocking Coordinated Checkpointing for Large-Scale Fault Tolerant MPI,"A long-term trend in high-performance computing is the increasing number of nodes in parallel computing platforms, which entails a higher failure probability. Fault programming environments should be used to guarantee the safe execution of critical applications. Research in fault tolerant MPI has led to the development of several fault tolerant MPI environments. Different approaches are being proposed using a variety of fault tolerant message passing protocols based on coordinated checkpointing or message logging. The most popular approach is with coordinated checkpointing. In the literature, two different concepts of coordinated checkpointing have been proposed: blocking and non-blocking. However they have never been compared quantitatively and their respective scalability remains unknown. The contribution of this paper is to provide the first comparison between these two approaches and a study of their scalability. We have implemented the two approaches within the MPICH environments and evaluate their performance using the NAS parallel benchmarks",2006,0,
2312,2313,Rate control for low delay H.264/AVC transmission over channels with burst error,"A rate control approach is proposed to deal with the low delay H.264/AVC video transmission over channels with burst errors by applying stochastic optimization technique. Based on the exponential rate-distortion and the linear variance prediction model, the one pass rate control algorithm will take into account the channel state and round trip delay, and make an immediate decision on the optimal rate allocation for the video frame. Simulation results show that for different end to end delay constraints and round trip delay, the number of lost frames is significantly reduced, and the average reconstruction peek signal to noise ratio is improved by 0.5-1.6dB, compared with the reference rate control scheme [ARA, 01]",2006,0,
2313,2314,Accurate Measurement of Bone Mineral Density Using Clinical CT Imaging With Single Energy Beam Spectral Intensity Correction,"Although dual-energy X-ray absorptiometry (DXA) offers an effective measurement of bone mineral density, it only provides a 2-D projected measurement of the bone mineral density. Clinical computed tomography (CT) imaging will have to be employed for measurement of 3-D bone mineral density. The typical dual energy process requires precise measurement of the beam spectral intensity at the 80 kVp and 120 kVp settings. However, this is not used clinically because of the extra radiation dosage and sophisticated hardware setup. We propose an accurate and fast approach to measure bone material properties with single energy scans. Beam hardening artifacts are eliminated by incorporating the polychromatic characteristics of the X-ray beam into the reconstruction process. Bone mineral measurement from single energy CT correction is compared with that of dual energy correction and the commonly used DXA. Experimental results show that single energy correction is compatible with dual energy CT correction in eliminating beam hardening artifacts and producing an accurate measurement of bone mineral density. We can then estimate Young's modulus, yield stress, yield strain and ultimate tensile stress of the bone, which are important data for patient specific therapy planning.",2010,0,
2314,2315,Error event analysis of EPR4 and ME<sup>2</sup>PR4 channels: captured head signals versus Lorentzian signal models,"Spin-stand error event analysis is conducted for EPR<sup>4</sup> and modified E<sup>2</sup>PR4 (ME<sup>2</sup>PR4) channels. The error event distributions for 16/17 random code and 16/17 QMTR code at user bit densities (UBD) of 2.5, 3.0 and 3.5 were investigated. Captured spin-stand waveforms were processed through a software channel and the error events were analyzed. The distribution of error events for spin-stand data is compared to the conventional Lorentzian model for 100% AWGN noise colored by the equalizer and for the 100% jitter noise case. Results show that neither Lorentzian model accurately predicts the spin-stand results by themselves or as a mix; however by considering the two extreme models, all the major error events are accounted for",2000,0,
2315,2316,Fault Tolerant SoC Architecture Design for JPEG2000 using Partial Reconfigurability,"In this paper, we present the design of a new architecture tolerating faults for the Image compression standard JPEG2000. The proposed fault tolerant design is based on adding a new reconfigurable core to the rest of the cores of the SoC. When a fault happens, it is tolerated using this reconfigurable core. The paper explains the hardware architecture allowing this inter-core communication toward fault tolerance. The target is to achieve a good reliability by this fault tolerance strategy and in the same time achieve the required speed allowing to the JPEG2000 to deal with video rather than still Image compression. The high speed is implemented using an optimized data organization and memories arrangement for the computation consuming blocks of the JPEG2000. The operating speed of the proposed architecture is 125 MHz for ALTERA FPGA implementation. The proposed architecture has increased the speed by a factor of 1.5, when compared to similar memory requiring architectures and decreased the memory requirement by a factor of 1.2, when compared to similar speed requiring architectures. Additionally, the proposed architecture achieves 91.45% fault coverage and it requires only 21% hardware overhead. The architecture has an optimum latency of 78.8 seconds corresponding to an optimum test sequence of n=985. The VHDL implementation of the six blocks of JPEG2000, corresponding to the full chain, has been developed and successfully validated on various types of ALTERA FPGA.",2007,0,
2316,2317,Software product improvement with inspection. A large-scale experiment on the influence of inspection processes on defect detection in software requirements documents,"In the early stages of software development, inspection of software documents is the most effective quality assurance measure to detect defects and provides timely feedback on quality to developers and managers. The paper reports on a controlled experiment that investigates the effect of defect detection techniques on software product and inspection process quality. The experiment compares defect detection effectiveness and efficiency of a general reading technique that uses checklist based reading, and a systematic reading technique, scenario based reading, for requirements documents. On the individual level, effectiveness was found to be higher for the general reading technique, while the focus of the systematic reading technique led to a higher yield of severe defects compared to the general reading technique. On a group level, which combined inspectors' contributions, the advantage of a reading technique regarding defect detection effectiveness depended on the size of the group, while the systematic reading technique generally exhibited better defect detection efficiency",2000,0,
2317,2318,History Index of Correct Computation for Fault-Tolerant Nano-Computing,"Future nanoscale devices are expected to be more fragile and sensitive to external influences than conventional CMOS-based devices. Researchers predict that it will no longer be possible to test a device and then throw it away if it is found to be defective, as every circuit is expected to have multiple hard and soft defects. Fundamentally new fault-tolerant architectures are required to produce reliable systems that will survive with manufacturing defects and transient faults. This paper introduces the History Index of Correct Computation (HICC) as a run-time reconfiguration technique for fault-tolerant nano-computing. This approach identifies reliable blocks on-the-fly by monitoring the correctness of their outputs and forwarding only good results, ignoring the results from unreliable blocks. Simulation results show that history-based TMR modules offer a better response to fault tolerance at the module level than do conventional fault-tolerant approaches when the faults are nonuniformly distributed among redundant units. A correct computation rate of 99% is achieved despite a 13% average injected fault rate, when one of the redundant units and the decision unit are fault-free as well as when both have a low injected fault rate of 0.1%. A correct computation rate of 89% is achieved when faults are nonuniformly distributed at an average fault rate of 11% and fault rate in the decision unit is 0.5%. The robustness of the history-based mechanism is shown to be better than both majority voting and a Hamming detection and correction code.",2009,0,
2318,2319,A Solution for Fault-Tolerance Based on Adaptive Replication in MonALISA,"The domains of usage of large-scale distributed systems have been extending during the past years from scientific to commercial applications. Together with the extension of the application domains, new requirements have emerged for large-scale distributed systems. Among these, fault tolerance is needed by more and more modern distributed applications, not only by the critical ones. In this paper we present a solution aiming at fault tolerant monitoring of the distributed systems within the MonALISA framework. Our approach uses replication and guarantees that all processing replicas achieve state consistency, both in the absence of failures and after failure recovery. We achieve consistency in the former case by implementing a module that ensures that the order of monitoring tuples is the same at all the replicas. To achieve consistency after failure recovery, we rely on check pointing techniques. We address the optimization problem of the replication architecture by dynamically monitoring and estimating inter-replica link throughputs and real-time replica status. We demonstrate the strengths of our solution using the MonALISA monitoring application in a distributed environment. Our tests show that the proposed approach outperforms previous solutions in terms of latency and that it uses system resources efficiently by carefully updating replicas, while keeping overhead very low.",2010,0,
2319,2320,Hierarchical error detection in a software implemented fault tolerance (SIFT) environment,"Proposes a hierarchical error detection framework for a software-implemented fault tolerance (SIFT) layer of a distributed system. A four-level error detection hierarchy is proposed in the context of Chameleon, a software environment for providing adaptive fault tolerance in an environment of commercial off-the-shelf (COTS) system components and software. The design and implementation of a software-based distributed signature monitoring scheme, which is central to the proposed four-level hierarchy, is described. Both intra-level and inter-level optimizations that minimize the overhead of detection and are capable of adapting to runtime requirements are proposed. The paper presents results from a prototype implementation of two levels of the error detection hierarchy and results of a detailed simulation of the overall environment. The results indicate a substantial increase in availability due to the detection framework and help in understanding the tradeoffs between overhead and coverage for different combinations of techniques",2000,0,
2320,2321,Assessment and implementation of NOAA NWP-based tropospheric correction model,"Tropospheric delay is one of the dominant Global Positioning System (GPS) errors, which degrades the positioning accuracy. Recent developments in tropospheric modeling rely on implementation of more accurate Numerical Weather Prediction (NWP) models. In North America one of the NWP-based tropospheric correction models is the NOAA model, which has been developed by the US National Oceanic and Atmospheric Administration (NOAA). Because of its potential to improve the GPS positioning accuracy, the NOAA tropospheric correction model became the focus of many researchers. In this paper, we analyzed the performance of the NOAA tropospheric correction model and examined its effect on precise point positioning (PPP) solution. We generated a three-year-long tropospheric zenith total delay (ZTD) data series for the NOAA, Hopfield, and the IGS final tropospheric correction product, respectively. These data sets were generated at ten IGS reference stations spanning Canada and the United States. We analyzed the NOAA ZTD data series and compared them with those of the Hopfield model. The IGS final tropospheric product was used as a reference. The analysis shows that the performance of the NOAA model is a function of both season (time of the year) and geographical location. However, its performance was superior to the Hopfield model in all cases. We further investigated the effect of implementing the NOAA model on the PPP solution convergence and accuracy, which again showed superior performance in comparison with the Hopfield model.",2009,0,
2321,2322,A new filter scheme for the filtering of fault currents,"In protection relaying schemes, the digital filter unit plays the essential roles to calculate the accurate phasor. However, the decaying DC components in fault currents always cause false operations of relay systems. This paper presents a new filter scheme which can remove such components from fault currents. Using the proposed scheme, the full-cycle DFT (FCDFT) only needs one-cycle samples to obtain an accurate fundamental phasor. The decaying DC component is removed by an iterative computation. The proposed filter scheme can help digital filters achieve accurate results rapidly. Simulations results illustrate the effectiveness of this new algorithm for distance relaying applications.",2009,0,
2322,2323,One Step More to Understand the Bug Report Duplication Problem,"According to recent work, duplicate bug reports impact negatively on software maintenance and evolution productivity due to, among other factors, the increased time spent on report analysis and validation. Therefore, a considerable amount of time is lost mainly with duplicate bug report analysis. In this sense, this work presents am exploratory study using data from bug trackers from private and open source projects, in order to understand the possible factors (i.e. software life-time, size, amount of bug reports, etc.) that cause bug report duplication and its impact on software development. This work also discusses bug report characteristics that could help identifying duplicates.",2010,0,
2323,2324,A robust sensorless fault diagnosis algorithm for low cost motor drives,"This paper presents a sensorless fault diagnosis technique for low cost AC drives. Currently, in order to achieve a reliable fault diagnosis, a high resolution speed sensor is employed to measure the frequencies of fault signature which depends on motor shaft speed. There is an increased tendency toward sensorless control of AC motor drives because of mounting problems, associated cost, etc. Therefore, the speed sensors become less common feedback tools in high performance motor control applications. The fault diagnosis becomes quite a challenging task in the absence of information from a speed sensor as highly precise motor speed estimation is needed. In this paper, a simple and efficient algorithm for fault diagnosis is proposed utilizing frequency tracking method which does not require speed sensor feedback. It is explicitly verified that the performance of the proposed algorithm is almost comparable to the cases where an accurate speed sensor is used. The algorithm is derived mathematically and its efficacy is proved experimentally using a 3-hp motor generator setup.",2010,0,
2324,2325,Design of Bipolar Odd-Even Method for Immediately Correction of Information Errors,"Packet losses and errors are common through current wireless transmission due to signal decay, barriers in-between and other environmental related reasons. Although there are many ways to correct these errors, there is no one can deal with them immediately. For the efficient use of energy to achieve the purpose of energy conservation, and use the smallest energy to implement monitoring more, rapid implementation of parallel processing, a division of cooperation can be real-time monitoring without individual inquiries. If data are sent in hardware method, safer transmission can be reached by simply modify the hardware. Adopted the principle of variable packet, each single-chip can act as transceiver at any time, automatically send or emergency call by turns. The use of searching the initial symbols and the suspension symbols of the interpretation of the way to transmit the information and the related content. Signals can be sent to the extensions by monitoring and management of the host, also can be transferred to another host through cross-regional, as well as the transmitting DATA can be mounted other messages without wasting time. By checking the connection between any two adjacent bytes, problems like signal decay can be solved. At the meantime, it can correct errors automatically, to prevent interferences, and fortify the safety and encryption of wireless transmission.",2010,0,
2325,2326,Entropy-based optimum test points selection for analog fault dictionary techniques,"An efficient method to select an optimum set of test points for dictionary techniques in analog fault diagnosis is proposed. This is done by searching for the minimum of the entropy index based on the available test points. First, the two-dimensional integer-coded dictionary is constructed whose entries are measurements associated with faults and test points. The problem of optimum test points selection is, thus, transformed to the selection of the columns that isolate the rows of the dictionary. Then, the likelihood for a column to be chosen based on the size of its ambiguity set is evaluated using the minimum entropy index of test points. Finally, the test point with the minimum entropy index is selected to construct the optimum set of test points. The proposed entropy-based method to select a local minimum set of test points is polynomial bounded in computational cost. The comparison between the proposed method and other reported test points selection methods is carried out by statistical experiments. The results indicate that the proposed method more efficiently and more accurately finds the locally optimum set of test points and is practical for large scale analog systems.",2004,0,
2326,2327,Experiences with EtheReal: a fault-tolerant real-time Ethernet switch,"We present our experiences with the implementation of a real-time Ethernet switch called EtheReal. EtheReal provides three innovations for real-time traffic over switched Ethernet networks. First, EtheReal delivers connection oriented hard bandwidth guarantees without requiring any changes to the end host operating system and network hardware/software. For ease of deployment by commercial vendors, EtheReal is implemented in software over Ethernet switches, with no special hardware requirements. QoS support is contained within two modules, switches and end-host user level libraries that expose a socket like API to real time applications. Secondly, EtheReal provides automatic fault detection and recovery mechanisms that operate within the constraints of a real-time network. Finally EtheReal supports server-side push applications with a guaranteed bandwidth link-layer multicast scheme. Performance results from the implementation show that EtheReal switches deliver bandwidth guarantees to real time-applications within 0.6% of the contracted value, even in the presence of interfering best-effort traffic between the same pair of communicating hosts.",2001,0,
2327,2328,An Approach for Analyzing Infrequent Software Faults Based on Outlier Detection,"The fault analysis is critical process in software security system. However, identifying outliers in software faults has not been well addressed. In this paper, we define WCFPOF (weighted closed frequent pattern outlier factor) to measure the complete transactions, and propose a novel approach for detecting closed frequent pattern based outliers. Through discovering and maintaining closed frequent patterns, the outlier measure of each transaction is computed to generate outliers. The outliers are the data that contain relatively less closed frequent itemsets. To describe the reasons why detected outlier transactions are infrequent, the contradictive closed frequent patterns for each outlier are figured out. Experimental results show that our algorithm has shorter time consumption and better scalability.",2009,0,
2328,2329,Computer models of internal short circuit and incipient faults in transformers,"This article presents the computer models, which simulates the internal short circuit and incipient faults in transformers. To make an accurate diagnostic decision, transformer internal winding faults must be characterized by analyzing quantities of data, which could be generated through computer simulation or field experiments. A finite element method is presented which models internal short circuit faults (H.Wang et al., 2001). Finite element analysis (FEA) techniques are useful to obtain an accurate characterization of the electromagnetic behavior of magnetic components, such as transformers. Finite analysis is applied to calculate the parameters for an equivalent circuit of the transformer with an internal short circuit fault using ANSOFT's Maxwell Software. Various short circuit and incipient fault scenarios at different degrading insulation levels of the transformer winding insulation were simulated. Graphs of the results will be presented and discussed.",2003,0,
2329,2330,A New System for Computer-Aided Preoperative Planning and Intraoperative Navigation During Corrective Jaw Surgery,"A new system for computer-aided corrective surgery of the jaws has been developed and introduced clinically. It combines three-dimensional (3-D) surgical planning with conventional dental occlusion planning. The developed software allows simulating the surgical correction on virtual 3-D models of the facial skeleton generated from computed tomography (CT) scans. Surgery planning and simulation include dynamic cephalometry, semi-automatic mirroring, interactive cutting of bone and segment repositioning. By coupling the software with a tracking system and with the help of a special registration procedure, we are able to acquire dental occlusion plans from plaster model mounts. Upon completion of the surgical plan, the setup is used to manufacture positioning splints for intraoperative guidance. The system provides further intraoperative assistance with the help of a display showing jaw positions and 3-D positioning guides updated in real time during the surgical procedure. The proposed approach offers the advantages of 3-D visualization and tracking technology without sacrificing long-proven cast-based techniques for dental occlusion evaluation. The system has been applied on one patient. Throughout this procedure, we have experienced improved assessment of pathology, increased precision, and augmented control",2007,0,
2330,2331,Adaline for fault detection in Electrical High Voltage transmission line,"The application of neural networks to power systems has been extensively reported. Neural networks based protection techniques have been proposed by a number of authors. However, almost all the studies have so far employed the back-propagation neural network structure with supervised learning. This paper presents an on line method for fault identification in Electrical High Voltage (EHV) transmission line. This approach utilizes linear adaptive neuron, which is called Adaline. The Adaline neural network is generally used for prediction and identification problems and is rarely used for power system protection. Using current signals, the Adaline process has a strong tracking capability and is fast due to its simple construction, which makes it more suitable for the implementation. Our Adaline approach is compared with a multilayer perceptron in order to see the influence of the fault resistance over the fault time detection.",2010,0,
2331,2332,Dependable Network-on-Chip Router Able to Simultaneously Tolerate Soft Errors and Crosstalk,"As the technology scales down into deep sub-micron domain, more IP cores are integrated in the same die and new communication architectures are used to meet performance and power constraints. However, the same technologic advance makes devices and interconnects more sensitive to new types of malfunctions and failures, such as crosstalk and transient faults. This paper proposes fault tolerant techniques to protect NoC routers against the occurrence of soft errors and crosstalk at the same time, with minimum area and performance overhead. Experimental results show that a cost-effective protection alternative can be achieved by the combination of error correction codes and time redundancy techniques",2006,0,
2332,2333,Raptor versus Reed Solomon forward error correction codes,"Network conditions generally cause errors on network packets. Correction of these errors is in the subject of ""forward error correction."" Forward error correction is divided into two categories: bit-level forward error correction and packet-level forward error correction. These two categories are unfamiliar. The aim of this study is to make a literature comparison of two alternative packet-level forward error correction codes: Raptor and Reed Solomon. Nowadays when packet-level error correction codes are mentioned, these two techniques are remembered. Reed Solomon FEC codes are found on the Internet and are tested with different network conditions. Raptor codes are commercial and not used broadly yet. But several new technologies (MBMS, DVB and etc.) uses Raptor. This study shows the cases, where Raptor and Reed Solomon are appropriate to use",2006,0,
2333,2334,Error probability analysis of IP Time To Live covert channels,"Communication is not necessarily made secure by the use of encryption alone. The mere existence of communication is often enough to raise suspicion and trigger investigative actions. Covert channels aim to hide the very existence of the communication. The huge amount of data and vast number of different protocols in the Internet makes it ideal as a high-bandwidth vehicle for covert communications. A number of researchers have proposed different techniques to encode covert information into the IP time to live (TTL) field. This is a noisy covert channel since the TTL field is modified between covert sender and receiver. For computing the channel capacity it is necessary to know the probability of channel errors. In this paper we derive analytical solutions for the error probabilities of the different encoding schemes. We simulate the different encoding schemes and compare the simulation results with the analytical error probabilities. Finally, we compare the performance of the different encoding schemes for an idealised error distribution and an empirical TTL error distribution obtained from real Internet traffic.",2007,0,
2334,2335,Study of process of convergence of relative method to correction,Explored process of convergence of compensator-patch.in the relative method of correction.,2004,0,
2335,2336,Generating non-uniform distributions for fault injection to emulate real network behavior in test campaigns,"Fault injection is an efficient technique to evaluate the robustness of computer systems and their fault tolerance strategies. In order to obtain accurate results from fault injection based tests, it is important to mimic real conditions during a test campaign. When testing dependability attributes of network applications the real faulty behavior of networks must be closely emulated. We show how probability distributions can be used to inject communication faults that closely resemble the behavior observed in real network environments. To demonstrate the strengths of this strategy we develop a reusable and extensible entity called FIEND, integrate it to a fault injector and use the resulting tool to run test experiments injecting non-uniform distributed faults in a network application taken as example.",2009,0,
2336,2337,Application of fuzzy neural network in optical fiber fusion defect recognition system by AS1773 protocol,"Because of having many advantages, optical fiber network is applied widely in high-tech fields. But the existence of optical fiber fusion defects will debase the quality of message transmission. A set of defect recognized system is established based on the compensatory fuzzy neural network of using wavelet and with fast algorithm in this paper. The `energy-defect' method to extract eigenvalue is used firstly, then defect classification is recognized by fuzzy neural network. The results of simulation show that the model established by making use of this algorithm has higher efficiency, and the possibility of wrap in local minimum value of the network during the training process is smaller, which can compare to approach the precision utmost steadily and classification recognize the defect precision. The experiment result can verify this system have very high accurate rate to forecast the fusion defects and satisfy the demand of the engineering application based on AS1773 protocol, which provide the technique guarantee for further realizing the optical fiber fusion quantity monitor system.",2010,0,
2337,2338,Machine current signature analysis as a way for fault detection in permanent magnet motors in elevators,This paper focuses on the experimental investigation for incipient fault detection and fault detection methods suitably adapted for use in permanent-magnet motors for direct-drive elevator systems. The proposed system diagnoses permanent-magnet motors having two types of faults such as short circuit of stator windings and bearing fault. After processing current data the classical fast Fourier transform is applied to detect characteristics under the healthy and various faulted conditions with MCSA.,2008,0,
2338,2339,Analysis of measuring errors for the visible light phase-shifting point diffraction interferometer,"In order to improve the measuring accuracy of the visible light phase-shifting point diffraction interferometer (PS/PDI) for the extreme ultraviolet lithography (EUVL) aspheric mirrors, the main measuring errors will be discussed in this paper. At first, the elementary configuration and measuring principle of the visible light phase-shifting point diffraction interferometer are introduced briefly, then the different errors which are possible to affect the measuring result are summed up, the errors include PZT phase-shifting error, detector nonlinearity error, detector quantization error, wavelength instability error and intensity instability error of the laser source, vibration error, air refractivity instability error and so on. Through detailed analysis and simulation, the magnitude of these errors can be obtained. By analysing the reasons which cause these errors and the relationship between these errors and interferometer configuration parameters, some methods are put forward to avoid or restrain these errors accordingly.",2010,0,
2339,2340,Correction of patient movement with a phase-only correlation method in a SPECT study,"Patient movement during SPECT and PET data acquisition makes serious distortions in reconstructed images. In most conventional methods a correction of this movement is conducted in the sinogram space. However, the direction of movement occurs at an angle that is parallel to collimator holes, making it is difficult to detect the movement of an object. This paper proposes a new correction method of patient movement. This method basically uses a phase-only correlation method. We applied the phase-only correlation method to both a sinogram space and image space. That is, we apply a one dimensional Fourier transform to a measured sinogram and detect an angle at which a movement occurs with the phase information of neighboring projection data. After we detect the angle at which a movement takes place, we split the sinogram into two angular regions and reconstruct images corresponding to these angular regions. We also apply a one-dimensional Fourier transform to these reconstructed images and estimate the extent of movement. Next we replace the wrong projection data with correct data that are calculated by forward projection of the image positioned correctly. By reconstructing an image with a corrected sinogram we could obtain a distortion free image. This paper showed the validity of our method with some simulations.",2010,0,
2340,2341,Statechart Features and Pre-Release Defects in Software Maintenance,Statecharts is a design notation to model reactive systems that is part of the Unified Modeling Language (UML) and it is commonly used in the automotive and telecommunication software industry. In this paper we present a study of how the use of some statechart features correlate to the number of pre-release defects in the maintenance of large systems. We discuss possible causes for these correlations and provide some advice to both UML practitioners and to designers of new visual design languages.,2007,0,
2341,2342,Active power security correction in power market using genetic algorithms,"A method for active power security correction in power market based on genetic algorithms is proposed in this paper. The aim of active power security correction in a power market is to regulate the active power output of generators properly to alleviate overloads of transmission lines with the least increase of market purchase cost. The mathematical model of the problem is given, the principle of genetic algorithms and their application procedures are explained briefly as well. Some improvements to the crossover operator, and to crossover and mutation probability of simple genetic algorithm (SGA) are presented, forming improved genetic algorithm (IGA). The application of IGA in the active power security correction in a power market is then demonstrated. The method is proved to be correct in theory and feasible in engineering practice, according to the results of active power security correction calculation for a 22-node network example.",2002,0,
2342,2343,Investigation of Fault-Tolerant Adaptive Filtering for Noisy ECG Signals,"Studies shows that electrocardiogram (ECG) computer programs perform at least equally well as human observers in ECG measurement and coding, and can replace the cardiologist in epidemiological studies and clinical trials (Kors and Herpen, 2001). However, in order to also replace the cardiologist in clinical settings, such as for out-patients, better systems are required in order to reduce ambient noise while maintaining signal sensitivity. Therefore the objective of this work was to develop an adaptive filter to remove the contaminating signal in order to better obtain and interpret the electrocardiogram (ECG) data. To achieve reliability, the real-time computing systems must be fault-tolerant. This paper proposed a fault-tolerant adaptive filter for noise cancellation of ECG signals. Comparison of the performance and reliability of non-fault-tolerant and fault-tolerant adaptive filters are performed. Experimental results showed that the fault-tolerant adaptive filter not only successfully extract the ECG signals, but also is very reliable",2007,0,
2343,2344,Formal static fault tree analysis,"Fault tree analysis (FTA) is a traditional informal reliability and safety analysis technique. FTA is basically a combinational model in which standard Boolean logic constructs, such as AND and OR gates, are used to decompose the fault events. Several dynamic constructs, such as Functional Dependency (FDEP) and Priority AND (PAND) gates, are also proposed to handle dynamic behaviors of system failure mechanisms. In this article, we focus on some paradoxes and constraints of the traditional FDEP and PAND gates, and present our static solutions to these dynamic gates. The proposed static fault tree model is formalized with Maude, an executable algebraic formal specification language. Two example fault tolerant parallel processor (FTPP) configurations are used to demonstrate our static fault tree model.",2010,0,
2344,2345,Multiagent-based monitoring-fault diagnosis-control integrated system for chemical process,"This paper presents an integrated system which has monitoring, fault diagnosis and control function for chemical processes. The system is based on multiagent technology. In this paper, three agents are made to build the integrated system. They are monitoring agent, fault diagnosis agent and control agent. They can work together in an integrated frame. The difficulty lies in solving the coordinating tasks in communication, interaction and cooperation and finally obtains a good solution. This multiagent system can supply optimal decision for the enterprise and it will be the fundamental of completed automation in future.",2005,0,
2345,2346,Analysis of Transient Stability Enhancement of LV-Connected Induction Microgenerators by Using Resistive-Type Fault Current Limiters,"In this paper an analytical method by which the transient stability of an induction machine is maintained regardless of the fault clearance times is introduced. The method can be applied in order to improve the transient stability of a large penetration of low-voltage (LV) connected microgeneration that can be directly interfaced by single-phase induction generators within domestic premises. The analysis investigates the effectiveness of using resistive-type superconducting fault current limiters (RSFCLs) as remedial measures to prevent the microgenerators from reaching their speed limits during remote faults, and hence improving their transient stability. This will prevent unnecessary disconnection of a large penetration of LV-connected microgeneration and thus avoiding the sudden appearance of hidden loads, and unbalanced voltage conditions. The minimum required value of a resistive element of RSFCL for mitigating the transient instability phenomena of LV-connected microgeneration based on the system and connected machine parameters is determined. The analytical method has been validated by conducting informative transient studies by using detailed models of a small microwind turbine with constant mechanical output interfaced directly within residential dwellings by a single-phase induction generator, a transient model of resistive superconducting fault current limiter (RSFCL), and a typical suburban distribution network with residential loads. All the models are developed in the time-domain PSCAD/EMTDC dynamic simulation.",2010,0,
2346,2347,Based on the Phonetic Spelling Correction System Research and Implementation,"Based on English phonetic spelling correction algorithm, this paper is to solve common spelling errors such as missing letters, extra letters, disordered letters, as well as phonetic spelling errors in the perspective of from the same pronunciation, similar pronunciation. Through the analysis of the causes of the spelling errors, an algorithm of combination of phonetic spelling correction, phonetic spelling regulations, edit-distance and habit-distance is put forward, thereby overall exposition about spelling correction system is proposed.",2009,0,
2347,2348,An Evolving Model of Software Bug Reports,"We model software bug reports as a topological network called reporter network. By statistical analysis we And that the reporter network displays a number of features (scale-free, small-world, and etc.) shared by other complex networks. In order to understand the origins of these features, an evolving complex network model is proposed for the first time. The experimental results show that the model is able to reproduce many of statistical properties of the reporter network. Moreover, the scaling exponents of power-law distribution are calculated analytically. The calculation results agree well with simulation results.",2009,0,
2348,2349,Image distortion correction algorithm based on lattice array coordinate projection,"In order to improve the accuracy of image detection and target recognition, this paper presents an image distortion correction algorithm based on quadrilateral fractal approach controlling points. This method uses standard lattice image to be the measuring target, determines the coordinate by combining mathematical morphology and sliding neighborhood operation, and then Adopt simple linear regression analysis for optical center and establish coordinate system, finally proposes Two-step and one-dimensional gray linear interpolation backward mapping algorithm to define the pixel intensity. An image acquisition hardware system is designed contains TMS320DM6437 and taking a building as target. Experiments with above method have shown that this algorithm can correct distortion in short time without loss edge information.",2010,0,
2349,2350,Bifurcation of frequency perturbations analysis due to faults and power swings,"The paper presents the result of faults and power swings transient measurements occurred at 400 kV Switching Station Girawali, Maharashtra State, India. It is having lines fed from Parali Thermal Power station, Super Thermal Power Station; Chandrapur, Solapur, Lonikand and indirectly connected to Koyna hydro power station. The oscillographs of phase currents and voltages with neutral parameters are measured online during transient processes of line faults and power swings to compare them with respect to the frequency perturbations. Computer simulation is performed to analyze frequency from point to point. The fluctuations and perturbations in frequency during fault stage and rotor dynamics stage are measured. An algorithm is developed to activate and block the power swing detection by measuring frequency changes. The occurrences of frequency fluctuations and perturbations are high in case of power swings as compared to faults. Logic is developed to utilize this feature for bifurcation",2005,0,
2350,2351,Error-tolerance,"Summary form only given. Because of trends in scaling, in the near future every high performance dice can contain a massive number of defects and process aggravated noise and performance problems. In an attempt to obtain useful yields, designers and test engineers need to adopt a qualitatively different approach to their work. They need to learn, enhance and deploy techniques such as fault- and defect-tolerance. For some applications, they may even apply error-tolerance, a somewhat controversial emerging paradigm. A circuit is error-tolerant (ET) with respect to an application, if (1) it contains defects that cause internal and may cause external errors, and (2) the system that incorporates this circuit produces acceptable results. In this presentation we illustrate and give quantitative bounds on several factors that shape the future of digital design. We compare and contrast defect and fault-tolerant schemes with that of error-tolerance. We discuss how yield can be optimized by appropriately selecting the granularity of spares in light of defect densities and interconnect complexity. Finally, we show that several large classes of consumer electronic applications are resilient to errors, and how error-tolerance can then be used to significantly enhance effective yield.",2004,0,
2351,2352,Gains achieved by symbol-by-symbol rate adaptation on error-constrained data throughput over fading channels,"Methods for symbol-by-symbol channel feedback and adaptation of symbol durations have been recently proposed. In this paper, we quantitatively analyze the gain in error-constrained data throughput due to such an extremely rapid adaptation of symbol durations to fast-time-varying channels. The results show that a symbol-by-symbol adaptation can achieve a throughput gain by orders of magnitude over a frame-by-frame adaptation.",2007,0,
2352,2353,A hybrid algorithm of on-line fault diagnosis,"This paper proposes a hybrid algorithm of on-line fault diagnosis, which calls the data of breakers, protections and digital recorders by layer for diagnosis. Breaker tripping information is used first to determine the power failure area that contains fault components. Where after, protection actions information is used to determine the exact fault elements. At last, digital recorder information is used to evaluate protection actions. The analysis on protection information is the main task and character of this paper. There are two steps in processing of protection information. First we find the intersection of regions of acted protections to locate the exact fault elements with rule-based reasoning technique. And then for each element in the suspicious fault components list, the posterior fault probability is calculated as its fault credence degree. Then the elements in the suspicious fault components list can be sequenced by fault credence degree. Theory of evidence is used for calculating the fault credence degree of the suspicious fault components. It is stricter and more reasonable than before. Simulation analyses of the actual systems verify the effectiveness of the algorithm.",2004,0,
2353,2354,Thermal Switching Error Versus Delay Tradeoffs in Clocked QCA Circuits,"The quantum-dot cellular automata (QCA) model offers a novel nano-domain computing architecture by mapping the intended logic onto the lowest energy configuration of a collection of QCA cells, each with two possible ground states. A four-phased clocking scheme has been suggested to keep the computations at the ground state throughout the circuit. This clocking scheme, however, induces latency or delay in the transmission of information from input to output. In this paper, we study the interplay of computing error behavior with delay or latency of computation induced by the clocking scheme. Computing errors in QCA circuits can arise due to the failure of the clocking scheme to switch portions of the circuit to the ground state with change in input. Some of these non-ground states will result in output errors and some will not. The larger the size of each clocking zone, i.e., the greater the number of cells in each zone, the more the probability of computing errors. However, larger clocking zones imply faster propagation of information from input to output, i.e., reduced delay. Current QCA simulators compute just the ground state configuration of a QCA arrangement. In this paper, we offer an efficient method to compute the N-lowest energy modes of a clocked QCA circuit. We model the QCA cell arrangement in each zone using a graph-based probabilistic model, which is then transformed into a Markov tree structure defined over subsets of QCA cells. This tree structure allows us to compute the N-lowest energy configurations in an efficient manner by local message passing. We analyze the complexity of the model and show it to be polynomial in terms of the number of cells, assuming a finite neighborhood of influence for each QCA cell, which is usually the case. The overall low-energy spectrum of multiple clocking zones is constructed by concatenating the low-energy spectra of the individual clocking zones. We demonstrate how the model can be used to study the tradeoff betwee- - n switching errors and clocking zones.",2008,0,
2354,2355,Efficiency analysis of illumination correction methods for face recognition performance,"Face recognition is an important task in the computer vision community leading to multiple applications such as building control access, video surveillance or forensics, to mention only a few. Face images are acquired in the enrollment process to form a database, which is the first stage usually performed off line. Second stage implies real-time test procedure where a new unseen face is captured and the face recognition system authorizes or does not, or recognize the identity of the person, based on similarity matching between the new acquired face image and the ones existing in the database, providing a matching score. When the acquiring conditions provided by the enrollment process highly differ by test environmental conditions, some preprocessing steps may be required. An instance of such step is given by illumination correction. The paper aims at analyzing the efficiency of five recent state-of-the-art normalization approaches in term of illumination correction and their effect on face recognition. Surprisingly, to the best of our knowledge, no systematic comparison exist in the literature to date, fact that had motivated us to carry out such analysis. We should also note that, apart from the face recognition task, other domains, such as medical imaging, could benefit from those preprocessing techniques.",2010,0,
2355,2356,Power System Fault Estimation under Non-white Noise,"In this paper, a novel fault detection and isolation filter design method for power distribution in DC electronic system is proposed. The proposed double filter method is a combination of quadratic programming (QP) and Kalman filter which has three main advantages: first, it could estimate the fault position accurately under non-white noise. Second, this is a sequential method which could be used for real-time fault estimation with limited computation. Third, the model is simple and could be used on other circuits without much effort. The performance of the proposed method is verified by numerical simulations.",2010,0,
2356,2357,Theoretical and Numerical Study of MLEM and OSEM Reconstruction Algorithms for Motion Correction in Emission Tomography,"Patient body-motion and respiratory-motion impacts the image quality of cardiac SPECT and PET perfusion images. Several algorithms exist in the literature to correct for motion within the iterative maximum-likelihood reconstruction framework. In this work, three algorithms are derived starting with Poisson statistics to correct for patient motion. The first one is a motion compensated MLEM algorithm (MC-MLEM). The next two algorithms called MGEM-1 and MGEM-2 (short for Motion Gated OSEM, 1 and 2) use the motion states as subsets, in two different ways. Experiments were performed with NCAT phantoms (with exactly known motion) as the source and attenuation distributions. Experiments were also performed on an anthropomorphic phantom and a patient study. The SIMIND Monte Carlo simulation software was used to create SPECT projection images of the NCAT phantoms. The projection images were then modified to have Poisson noise levels equivalent to that of clinical acquisition. We investigated application of these algorithms to correction of (1) a large body-motion of 2 cm in Superior-Inferior (SI) and Anterior-Posterior (AP) directions each and (2) respiratory motion of 2 cm in SI and 0.6 cm in AP. We determined the bias with respect to the NCAT phantom activity for noiseless reconstructions as well as the bias-variance for noisy reconstructions. The MGEM-1 advanced along the bias-variance curve faster than the MC-MLEM with iterations. The MGEM-1 also lowered the noiseless bias (with respect to NCAT truth) faster with iterations, compared to the MC-MLEM algorithms, as expected with subset algorithms. For the body motion correction with two motion states, after the 9th iteration the bias was close to that of MC-MLEM at iteration 17, reducing the number of iterations by a factor of 1.89. For the respiratory motion correction with 9 motion states, based on the noiseless bias, the iteration reduction factor was approximately 7. For the MGEM-2, however, bias-plot or the bias-vari- ance-plot saturated with iteration because of successive interpolation error. SPECT data was acquired simulating respiratory motion of 2 cm amplitude with an anthropomorphic phantom. A patient study acquired with body motion in a second rest was also acquired. The motion correction was applied to these acquisitions with the anthropomorphic phantom and the patient study, showing marked improvements of image quality with the estimated motion correction.",2009,0,
2357,2358,Multimodal target correction by local bone registration: A PET/CT evaluation,"PET/CT guidance for percutaneous interventions allows biopsy of suspicious metabolically active bone lesions even when no morphological correlation is delineable in the CT images. Clinical use of PET/CT guidance with conventional step-by-step technique is time consuming and complicated especially in cases in which the target lesion is not shown in the CT image. Our recently developed multimodal instrument guidance system (IGS) for PET/CT improved this situation. Nevertheless, bone biopsies even with IGS have a trade-off between precision and intervention duration which is proportional to patient and personnel exposure to radiation. As image acquisition and reconstruction of PET may take up to 10 minutes, preferably only one time consuming combined PET/CT acquisition should be needed during an intervention. In case of required additional control images in order to check for possible patient movements/deformations, or to verify the final needle position in the target, only fast CT acquisitions should be performed. However, for precise instrument guidance accounting for patient movement and/or deformation without having a control PET image, it is essential to be able to transfer the position of the target as identified in the original PET/CT to a changed situation as shown in the control CT. Therefore, we present a pipeline for faster target-position correction by isolating and registering the bone of interest, as shown in the control CT, with the CT dataset of the original PET/CT acquisition. Challenges such as the masking of the bone of interest and registration robustness in the presence of the needle and its associated metal artifacts are also addressed in this work. Our results confirmed the feasibility of clinically using this technique for target correction on PET/CT bone intervention, and motivated us to incorporate it as part of our IGS for multimodal intervention.",2010,0,
2358,2359,Estimation of channel parameters in a multipath environment via optimizing highly oscillatory error functions using a genetic algorithm,Channel estimation is of crucial importance for tomorrow's wireless mobile communication systems. This paper focuses on the solution of channel parameters estimation problem in a scenario involving multiple paths in the presence of additive white Gaussian noise. We assumed that number of paths in the multipath environment is known and the transmitted signal consists of attenuated and delayed replicas of a known transient signal. In order to determine the maximum likelihood estimates one has to solve a complicated optimization problem. Genetic algorithms (GA) are well known for their robustness in solving complex optimization problems. A GA is considered to extract channel parameters to minimize the derived error-function. The solution is based on the maximum-likelihood estimation of the channel parameters. Simulation results also demonstrate GA's robustness to channel parameters estimation errors.,2007,0,
2359,2360,A tool for automatically translating dynamic fault trees into dynamic bayesian networks,"The unreliability evaluation of a system including dependencies involving the state of components or the failure events, can be performed by modelling the system as a dynamic fault tree (DFT). The combinatorial technique used to solve standard Fault Trees is not suitable for the analysis of a DFT. The conversion into a dynamic Bayesian network (DBN) is a way to analyze a DFT. This paper presents a software tool allowing the automatic analysis of a DFT exploiting its conversion to a DBN. First, the architecture of the tool is described, together with the rules implemented in the tool, to convert dynamic gates in DBNs. Then, the tool is tested on a case of system: its DFT model and the corresponding DBN are provided and analyzed by means of the tool. The obtained unreliability results are compared with those returned by other tools, in order to verify their correctness. Moreover, the use of DBNs allows to compute further results on the model, such as diagnostic and sensitivity indices",2006,0,
2360,2361,Fault tolerant automotive systems: an overview,There is a trend in the automobile industry for an increasing number of safety-related electronic systems in vehicles that are directly responsible for active and passive vehicle safety. These applications will increase overall vehicle safety by liberating the driver from routine tasks and assisting the driver to find solutions in critical situations. Thus it is of the utmost importance to solve fault tolerance issues of the electronic systems themselves. This paper gives an overview of the strategies and structures employed in the automotive environment to assure a good degree of fault tolerance both for complete systems and for integrated circuits,2001,0,
2361,2362,Content-Adaptive Motion Compensated Frequency Selective Extrapolation for error concealment in video communication,"If digital video data is transmitted over unreliable channels such as the internet or wireless terminals, the risk of severe image distortion due to transmission errors is ubiquitous. To cope with this, error concealment can be applied on the distorted data at the receiver. In this contribution we propose a novel spatio-temporal error concealment algorithm, the Content-Adaptive Motion Compensated Frequency Selective Extrapolation. The algorithm operates in two stages, whereas at first the motion in a distorted sequence is estimated. After that, a model of the signal is generated for concealing the distortion. The novel algorithm is based on an already existent error concealment algorithm. But by adapting the model generation to the content of a sequence, the novel algorithm is able to exploit the remaining information, which is still available in the distorted sequence, more effectively compared to the original algorithm. In doing so, a visually noticeable gain of up to 0.51 dB PSNR compared to the underlying algorithm and more than 3 dB compared to other error concealment algorithms can be achieved.",2010,0,
2362,2363,Highly Parallel FPGA Emulation for LDPC Error Floor Characterization in Perpendicular Magnetic Recording Channel,"Low-density parity-check (LDPC) codes offer a promising error correction approach for high-density magnetic recording systems due to their near-Shannon limit error-correcting performance. However, evaluation of LDPC codes at the extremely low bit error rates (BER) required by hard disk drive systems, typically around 10<sup>-12</sup> to 10<sup>- 15</sup>, cannot be carried out on high-performance workstations using conventional Monte Carlo techniques in a tractable amount of time. Even field-programmable gate array (FPGA) emulation platforms take a few weeks to reach BER between 10<sup>-11</sup> and 10<sup>-12</sup>. Thus, we implemented a highly parallel FPGA processing cluster to emulate a perpendicular magnetic recording channel, which enabled us to accelerate the emulation by > 100times over the fastest reported emulation. This increased throughput enabled us to characterize the performance of LDPC code BER down to near 10<sup>-14</sup> and investigate its error floor.",2009,0,
2363,2364,Mapping software faults with web security vulnerabilities,"Web applications are typically developed with hard time constraints and are often deployed with critical software bugs, making them vulnerable to attacks. The classification and knowledge of the typical software bugs that lead to security vulnerabilities is of utmost importance. This paper presents a field study analyzing 655 security patches of six widely used web applications. Results are compared against other field studies on general software faults (i.e., faults not specifically related to security), showing that only a small subset of software fault types is related to security. Furthermore, the detailed analysis of the code of the patches has shown that web application vulnerabilities result from software bugs affecting a restricted collection of statements. A detailed analysis of the conditions/locations where each fault was observed in our field study is presented allowing future definition of realistic fault models that cause security vulnerabilities in web applications, which is the key element to design a realistic attack injector.",2008,0,
2364,2365,Incorporating Fault Tolerance with Replication on Very Large Scale Grids,"Providing fault tolerance for message passing parallel application on a distributed environment is a rule rather than an exception. A node failure can cause the whole computation to stop and has to be restarted from the beginning if no fault tolerance is available. However, introducing fault tolerance has some overhead on speedup that can be achieved. In this paper, we introduce a new technique called replication with cross-over packets for reliability and to increase fault tolerance over Very Large Scale Grids (VLSG). This technique has two pronged effect of avoiding single point of failure and single link of failure. We incorporate this new technique into the L-BSP model and show the possible speedup of parallel process. We also derive the achievable speedup for some fundamental parallel algorithms using this technique.",2007,0,
2365,2366,Eliminating harmful redundancy for testing-based fault localization using test suite reduction: an experimental study,"In the process of software maintenance, it is usually a time-consuming task to track down bugs. To reduce the cost on debugging, several approaches have been proposed to localize the fault(s) to facilitate debugging. Intuitively, testing-based fault localization (TBFL), such as dicing and TRANTULA, is quite promising as it can take the advantage of a large set of execution traces at the same time. However, redundant test cases may bias the distribution of the test suite and harm this kind of approaches. Therefore, we suggest that the test suite, which is the input of TBFL, should be reduced before used in TBFL. To evaluate whether and to what extent TBFL can benefit from test suite reduction, we performed an experimental study on two source programs. The experimental results show that, for test suites containing unevenly distributed redundant test cases, performing test suite reduction before applying TBFL may be more advantageous.",2005,0,
2366,2367,Azvasa:- Byzantine Fault Tolerant Distributed Commit with Proactive Recovery,"This paper describes Azvasa protocol: a Byzantine fault tolerant distributed commit protocol with proactive recovery for transactions running over untrusted networks. Traditional Three phase agreement protocol among coordinator replicas to tolerate Byzantine faults has been used in distributed commit. We propose two phase agreement protocol to tolerate Byzantine faults which not only reduces total time to reach agreement but also message overhead. Proactive recovery is an essential method for ensuring long term reliability of fault tolerant systems that are under continuous threats from malicious adversaries. The primary benefit of our proactive recovery scheme is faster standby node registration, service migration and reduced overhead in new membership notification to participants.",2009,0,
2367,2368,Improving Fault Tolerance in High-Precision Clock Synchronization,"The very popular Precision Time Protocol (PTP or IEEE 1588) is widely used to synchronize distributed systems with high precision. The underlying principle is a master/slave concept based on the regular exchange of synchronization messages. This paper investigates an approach to enhance PTP with fault tolerance and to overcome the transient deterioration of synchronization accuracy during a recovery from a master failure. To this end, a concept is proposed where a group of masters negotiates a fault-tolerant agreement on the system-wide time and transparently synchronizes the associated IEEE 1588 slaves. Experimental verification on the basis of an Ethernet implementation shows that the approach is feasible and indeed improves the overall synchronization accuracy in terms of fault tolerance.",2010,0,
2368,2369,Scatter estimation and motion correction in PET,"This paper discusses how to estimate scatter when performing motion correction of PET data. It concentrates on head movement, but some results are valid for non-rigid motion as well. We show that rigid motion affects scattered events differently then unscattered events. This has consequences for the scatter estimation procedure. We show with simulations, phantom measurements and patient data that our proposed method obtains fully quantitative motion-corrected PET images.",2005,0,
2369,2370,A Deterministic Methodology for Identifying Functionally Untestable Path-Delay Faults in Microprocessor Cores,"Delay testing is crucial for most microprocessors. Software-based self-test (SBST) methodologies are appealing, but devising effective test programs addressing the true functionally testable paths and assessing their actual coverage are complex tasks. In this paper, we propose a deterministic methodology, based on the analysis of the processor instruction set architecture, for determining rules arbitrating the functional testability of path-delay faults in the data path and control unit of processor cores. Moreover, the performed analysis gives guidelines for generating test programs. A case study on a widely used 8-bit microprocessor is provided.",2008,0,
2370,2371,Intelligent fault diagnosis technique based on causality diagram,"We discuss the knowledge expression, reasoning and probability computing in causality diagram, which is developed from the belief network and overcomes some shortages. The model of causality diagram used for system fault diagnosis is brought forward, and the model constructing method and reasoning algorithm are also presented. At last, an application example in the fault diagnosis of the nuclear power plant is given which shows that the method is effect.",2004,0,
2371,2372,Considering fault removal efficiency in software reliability assessment,"Software reliability growth models (SRGMs) have been developed to estimate software reliability measures such as the number of remaining faults, software failure rate, and software reliability. Issues such as imperfect debugging and the learning phenomenon of developers have been considered in these models. However, most SRGMs assume that faults detected during tests will eventually be removed. Consideration of fault removal efficiency in the existing models is limited. In practice, fault removal efficiency is usually imperfect. This paper aims to incorporate fault removal efficiency into software reliability assessment. Fault removal efficiency is a useful metric in software development practice and it helps developers to evaluate the debugging effectiveness and estimate the additional workload. In this paper, imperfect debugging is considered in the sense that new faults can be introduced into the software during debugging and the detected faults may not be removed completely. A model is proposed to integrate fault removal efficiency, failure rate, and fault introduction rate into software reliability assessment. In addition to traditional reliability measures, the proposed model can provide some useful metrics to help the development team make better decisions. Software testing data collected from real applications are utilized to illustrate the proposed model for both the descriptive and predictive power. The expected number of residual faults and software failure rate are also presented.",2003,0,
2372,2373,RAID Architecture with Correction of Corrupted Data in Faulty Disk Blocks,"Disk capacities and processor performances have increased dramatically ever since. With rising storage space the probability of failures gets higher. Reliability of storage systems is achieved by adding extra disks for redundancy, like RAID-Systems or separate backup space in general. These systems cover the case when disks fail but do not recognize corrupted data in faulty blocks. Especially new storage systems like Solid State Drives are more vulnerable to corrupted data as cells are AagingA over time. We propose to add error detection and correction of data to a RAID-system without increasing the amount of space needed to store redundancy information compared to the common implementation RAID 6. To overcome higher computation complexity the implementation uses parallel execution paths available in modern Multicore and Multiprocessor systems.",2009,0,
2373,2374,Mesh-based error-scalable video object codec for variable bandwidth multimedia communications,"The work introduces a complete chain of video object compression. The process is based on an automatic extraction of video objects from raw video. The recent MPEG-4 standard philosophy, including mesh models and wavelet-based compression are involved in the scheme. Constrained Delaunay meshes are used to represent articulated video objects in a flexible manner conveying shape and motion information. The wavelet transform is applied to residual errors for scalable and efficient compression. Results on MPEG-4 test sequences for very low bitrate video communications are encouraging.",2002,0,
2374,2375,A high-performance application protocol for fault-tolerant CAN networks,"CAN is a communication protocol largely used in automotive and industrial appliances because of the simple procedure for its parameterization and the low cost of its circuitry. The native CAN, however, does not assure the fault-tolerance level required by safety-critical appliances and somewhat sophisticated protocols have been introduced for their networking. In an attempt to keep on the advantages of CAN, several application protocols have been developed to supplement the native CAN with the aim of giving the CAN networks safety features. This paper continues such a research trend by proposing a CAN application protocol termed time-triggered bus-redundant CAN (TTBR-CAN) that outdoes the performance of the existing ones. After describing the architecture of TTBR-CAN, details are given on its implementation and experimental results are reported to demonstrate its effectiveness.",2010,0,
2375,2376,State space model based dimensional errors analysis for rigid part in multistation manufacturing processes,"Modeling of variation propagation in multistation machining process is one of the most important research fields. In this paper, a mathematic model to depict the part dimensional variation of the complex multistation manufacturing process is formulated. A linear state space dimensional propagation equation is established through kinematics analysis of the relationships among of locating parameter variation, locating datum variation, so the dimensional error accumulation and transformation within the multistation process are quantitatively described. A systematic procedure to build the model is presented, which enhances the way to determine the variation sources in complex machining systems. Finally, an industrial case of multistation machining part in one manufacturing shop is given to testify the validation and practicability of the method. The analytical model is essential to quality control and improvement for multistation systems in machining quality forecasting and design optimization.",2010,0,
2376,2377,A Dynamic Binary Translation Framework Based on Page Fault Mechanism in Linux Kernel,"Dynamic binary translation and optimization is one of the most important essential techniques for computing system virtualization. This paper proposes a new dynamic translation framework for co-designed virtual machines. It generates and handles translation requests based on page fault mechanism provided in Linux kernel. In this new framework, the translation of guest codes and the execution of translated codes can be performed on different processors in parallel. The framework support the coprocessor translating guest code pages and the host CPU executing translated pages simultaneously, thus the translator becomes more efficient. The paper also presents a qualitative analysis of the time cost in our framework on an x86-ARM co-designed dynamic binary translation system, and suggests that the performance of this framework can be further improved if shared memory between host CPU and coprocessor is used. The framework can also be used in a dynamic binary translator on multi-core platforms.",2010,0,
2377,2378,A human study of fault localization accuracy,"Localizing and repairing defects are critical software engineering activities. Not all programs and not all bugs are equally easy to debug, however. We present formal models, backed by a human study involving 65 participants (from both academia and industry) and 1830 total judgments, relating various software- and defect-related features to human accuracy at locating errors. Our study involves example code from Java textbooks, helping us to control for both readability and complexity. We find that certain types of defects are much harder for humans to locate accurately. For example, humans are over five times more accurate at locating extra statements than missing statements based on experimental observation. We also find that, independent of the type of defect involved, certain code contexts are harder to debug than others. For example, humans are over three times more accurate at finding defects in code that provides an array abstraction than in code that provides a tree abstraction. We identify and analyze code features that are predictive of human fault localization accuracy. Finally, we present a formal model of debugging accuracy based on those source code features that have a statistically significant correlation with human performance.",2010,0,
2378,2379,Minimizing the error of time difference of arrival method in mobile networks,"Estimating the position of a mobile set is of great importance in new mobile services. However, in most cases, the accuracy should be less than 100 meters. This accuracy is hard to reach especially in urban areas. The main problem is that there are a lot of obstacles like buildings between the BTS and the mobile set. Thus the time measured between BTS and the mobile set is somehow greater than the time it takes the wave to travel directly between two points. This paper introduces an optimized solution for TDOA as one of the most efficient ways for finding the location of a mobile phone. Considering the standards and limitations of both GSM and UMTS, the authors present a solution for optimizing mobile phone positioning. In this new method, the positioning is done via common TDOA methods. Moreover, the non-linear effects of the environment such as non-line of sight error (NLOS) are minimized through comparing the measured data with pre measured points to reduce the error of finding the MS location. The simulations show that compared to direct measurement, the optimized protocol, reduces the error of location method by 33%.",2005,0,
2379,2380,Comparison for the accuracy of defect fix effort estimation,"Software defects have become the dominant cause of customer outage and the study of software errors has been necessitated by the emphasis on software reliability. Yet, software defects are not well enough understood to provide a clear methodology for avoiding or recovering from them. So software defect fix efforts play the critical role in software quality assurance. In this paper, the comparison of the defect fix effort estimation by using Self-Organizing Map in neural network is presented. To estimate the defect fix effort, KC3 static defect attributes that is one of the publicly available data from NASA products is used in this paper. To implement the SOM, finding different clusters from the SOM and for the calculation of fix effort time of defect SOM Toolbox in MATLAB 7.0.1 is used.",2010,0,
2380,2381,Performance of multicode DS/CDMA with noncoherent M-ary orthogonal modulation in the presence of timing errors,"The paper derives an accurate approximation to the bit error rate (BER) of multicode DS/CDMA with noncoherent M-ary modulation in wideband fading channels, when timing errors are made at the receiver. This reflects the practical scenario where the path delays are imperfectly estimated, leading to synchronization errors between the correlation receivers and the received signals. The analysis is applicable to any type of fading distribution, and is shown to match closely the Monte Carlo system simulations, especially for small timing errors.",2004,0,
2381,2382,Assessment of Data Diversity Methods for Software Fault Tolerance Based on Mutation Analysis,"One of the main concerns in safety-critical software is to ensure sufficient reliability because proof of the absence of systematic failures has proved to be an unrealistic goal. fault-tolerance (FT) is one method for improving reliability claims. It is reasonable to assume that some software FT techniques offer more protection than others, but the relative effectiveness of different software FT schemes remains unclear. We present the principles of a method to assess the effectiveness of FT using mutation analysis. The aim of this approach is to observe the power of FT directly and use this empirical process to evolve more powerful forms of FT. We also investigate an approach to FT that integrates data diversity (DD) assertions and TA. This work is part of a longer term goal to use FT in quantitative safety arguments for safety critical systems.",2006,0,
2382,2383,On the structure and performance of a novel blind source separation based carrier phase synchronization error compensator,"In this paper we carry out a detailed performance analysis of a novel blind-source-separation (BSS) based DSP algorithm that tackles the carrier phase synchronization error problem. The results indicate that the mismatch can be effectively compensated during the normal operation as well as in the rapidly changing environments. Since the compensation is carried out before any modulation specific processing, the proposed method works with all standard modulation formats and lends itself to efficient real-time custom integrated hardware or software implementations.",2002,0,
2383,2384,AccMon: Automatically Detecting Memory-Related Bugs via Program Counter-Based Invariants,"This paper makes two contributions to architectural support for software debugging. First, it proposes a novel statistics-based, on-the-fly bug detection method called PC-based invariant detection. The idea is based on the observation that, in most programs, a given memory location is typically accessed by only a few instructions. Therefore, by capturing the invariant of the set of PCs that normally access a given variable, we can detect accesses by outlier instructions, which are often caused by memory corruption, buffer overflow, stack smashing or other memory-related bugs. Since this method is statistics-based, it can detect bugs that do not violate any programming rules and that, therefore, are likely to be missed by many existing tools. The second contribution is a novel architectural extension called the Check Look-aside Buffer (CLB). The CLB uses a Bloom filter to reduce monitoring overheads in the recently-proposed iWatcher architectural framework for software debugging. The CLB significantly reduces the overhead of PC-based invariant debugging. We demonstrate a PC-based invariant detection tool called AccMon that leverages architectural, run-time system and compiler support. Our experimental results with seven buggy applications and a total of ten bugs, show that AccMon can detect all ten bugs with few false alarms (0 for five applications and 2-8 for two applications) and with low overhead (0.24-2.88 times). Several existing tools evaluated, including Purify, CCured and value-based invariant detection tools, fail to detect some of the bugs. In addition, Purify's overhead is one order of magnitude higher than AccMon's. Finally, we show that the CLB is very effective at reducing overhead.",2004,0,
2384,2385,A Taxonomy for the Analysis of Scientific Workflow Faults,"Scientific workflows generally involve the distribution of tasks to distributed resources, which may exist in different administrative domains. The use of distributed resources in this way may lead to faults, and detecting them, identifying them and subsequently correcting them remains an important research challenge. We introduce a fault taxonomy for scientific workflows that may help in conducting a systematic analysis of faults, so that the potential faults that may arise at execution time can be corrected (recovered from). The presented taxonomy is motivated by previous work [4], but has a particular focus on workflow environments (compared to previous work which focused on Grid-based resource management) and demonstrated through its use in Weka4WS.",2010,0,
2385,2386,A Fault-Tolerant Scheme for Multicast Communication Protocols,"Since the multicast communication is the best technology to provide one to many communication, more and more service providers are using this technology to deliver the same service to multiple customers. As such, providing fault tolerance to multicast connections is gaining attention both in business and research communities because a single link or a node failure in the multicast communication delivery tree affects a large number of customers. There are some existing schemes proposed for fault recovery in the multicast communication. They either calculate a new tree without using any node from the existing tree or calculate a path from affected node/tree to the unaffected tree when a fault occurs. In either case, they need the global view of the multicast communication tree. In this paper, we propose a fault tolerant scheme in which we do not need the global view of the multicast tree. We compute the shortest path from a node to the source of the multicast tree assuming that the node's link to its parent node in the multicast tree is broken. The shortest path information is sent hop-by-hop toward the source and is stored in the routers. When the assumed broken link really breaks the recovery message is sent toward the source and the previously stored fault recovery message at each node is used to make a multicast recovery tree",2005,0,
2386,2387,Formally verified Byzantine agreement in presence of link faults,"This paper shows that deterministic consensus in synchronous distributed systems with link faults is possible, despite the impossibility result of Gray (1978). Instead of using randomization, we overcome this impossibility by moderately restricting the inconsistency that link faults may cause system-wide. Relying upon a novel hybrid fault model that provides different classes of faults for both nodes and links, we provide a formally verified proof that the m+1-round Byzantine agreement algorithm OMH (Lincoln and Rushby (1993)) requires n > 2f<sub>l</sub><sup>s</sup> + f<sub>l</sub><sup>r</sup> + f<sub>l</sub><sup>ra</sup> + 2(f<sub>a</sub> + f<sub>s</sub>) + f<sub>o</sub> + f<sub>m</sub> + m nodes for transparently masking at most f<sub>l</sub><sup>s</sup> broadcast and f<sub>l</sub><sup>r</sup> receive link faults (including at most f<sub>l</sub><sup>ra</sup> arbitrary ones) per node in each round, in addition to at most f<sub>a</sub>, f<sub>s</sub>, f<sub>o</sub>, f<sub>m</sub> arbitrary, symmetric, omission, and manifest node faults, provided that m  f<sub>a</sub> + f<sub>o</sub> + 1. Our approach to modeling link faults is justified by a number of theoretical results, which include tight lower bounds for the required number of nodes and an analysis of the assumption coverage in systems where links fail independently with some probability p.",2002,0,
2387,2388,Small animal imaging with attenuation correction using clinical SPECT/CT scanners,"Our group has previously reported on performing single photon emission tomography (SPECT) studies on rats, using a GE Hawkeye millennium SPECT-CT scanner with pinhole collimators. The main challenge to obtaining quantitative physiological information from such data is the lack of X-ray attenuation data for the small animals. In this paper we present an experimental design in which a nonspecific clinical SPECT-CT scanner is used to collect both transmission and emission scan data for small animals. Necessary hardware enhancements include construction of a new animal bed and construction of a modified emission-transmission calibration object. An emission scan of this calibration object is used to determine the study-specific geometrical parameters of the gantry. In the past, we have shown that 25 adjustable parameters are needed to describe the angle-dependent positions of the detectors and pinhole collimators in order to obtain sufficient spatial precision. A CT scan of the calibration object is used to achieve three-dimensional image registration by aligning the transmission and emission fields of view. The emission system matrix is calculated by ray tracing. The algorithm corrects for X-ray attenuation and for the system geometric response that results from the finite sizes of the pinhole aperture and the detector pixels. The system matrix was applied to the reconstruction of static and dynamic SPECT images using the ML-EM algorithm with a total-variation regularization term. In future studies, time-varying data from the first minute of acquisition will be used to extract kinetic information about how radiopharmaceuticals interact with different tissue types.",2007,0,
2388,2389,Educational visualizations of syntax error recovery,"This work is focused on the syntax error recovery visualization within the compilation process. We have observed that none of the existing tools, which display some views of the compilation, give a solution to this aspect. We present an educational tool called VAST which allows to visualize the different views of the compilation process. Besides, VAST allows to display different syntax error recovery strategies.",2010,0,
2389,2390,Fault tolerant routing and broadcasting in de Bruijn networks,"In this paper, we study fault tolerant routing and broadcasting in interconnection networks based on de Bruijn graph (dBG) for constructing large scale multiprocessors networks. Our paper presents a new approach to provide fault tolerance routing and broadcasting which haven't been investigated. The proposed approach is based on multi level discrete set concept in order to find a fault free shortest path among several paths provided. In the proposed fault tolerant broadcasting, we can achieve k (network diameter) as maximum time step to finish broadcast process and there is no overhead in the broadcast message.",2005,0,
2390,2391,Memory Address Scrambling Revealed Using Fault Attacks,"Today's trend in the smart card industry is to move from ROM+EEPROM chips to Flash-only products. Recent publications have illustrated the vulnerability of Floating Gate memories to UV and heat radiation. In this paper, we explain how, by using low cost means, such a vulnerability can be used to modify specific data within an EEPROM memory even in the presence of a given type of counter-measure. Using simple means, we devise a fault injection tool that consistently causes predictable modifications of the targeted memories' contents by flipping `1's to `0's. By mastering the location of those modifications, we illustrate how we can reverse-engineer a simple address scrambling mechanism in a white box analysis of a given EEPROM. Such an approach can be used to test the security of Floating Gate memories used in security devices like smart cards. We also explain how to prevent such attacks and we propose some counter-measures that can be either implemented on the hardware level by chip designers or on the software level in the Operating System interacting with those memories.",2010,0,
2391,2392,Decomposition of error based control system optimization,"Decomposition possibilities of closed-loop control error is investigated to find the limits of optimality in generic two-degree of freedom control systems. New relationships are introduced for the different degradation components in a general and in a Youla-parametrized control system. It is investigated, how the major components in the decomposed problem can be optimized for open-loop stable plants. The new decomposition approach introduced helps the construction of new algorithms for simultaneous robust identification and control.",2005,0,
2392,2393,Maintenance planning for fault tolerant designs used in critical applications,"In the last few years, digital systems, especially computers, have been incorporated into commercial and military aircraft flight control systems, and industrial controllers. A system used in a typical critical application is the architecture of the X-29 aircraft flight control system, based on triple modular redundancy. The control system uses three identical computers performing the same operations. The results from each computer are examined and the output from the system is formed via a majority vote of the three results. For such a system, used in critical applications, it is extremely important, besides other evaluations, to plan the maintenance logistic support, knowing the failure rates and cost of all its blocks, the system quantity and installation sites, possible stock and spare source locations, etc",2001,0,
2393,2394,Blinded Fault Resistant Exponentiation Revisited,"Cryptographic algorithm implementations are subject to specific attacks, called side channel attacks, focusing on the analysis of their power consumption or execution time or on the analysis of faulty computations. At FDTC06, Fumaroli and Vigilant presented a generic method to compute an exponentiation resistant against different side channel attacks. However, even if this algorithm does not reveal information on the secrets in case of a fault attack, it can not be used to safely implement a crypto-system involving an exponentiation. In this paper, we propose a new exponentiation method without this drawback and give a security proof of resistance to fault attacks. As an application, we propose an RSA algorithm implemented using the Chinese Remainder Theorem protected against side channel attacks. The exponentiation algorithm is also 33% faster than the previous method.",2009,0,
2394,2395,Application-level fault tolerance in the orbital thermal imaging spectrometer,"Systems that operate in extremely volatile environments, such as orbiting satellites, must be designed with a strong emphasis on fault tolerance. Rather than rely solely on the system hardware, it may be beneficial to entrust some of the fault handling to software at the application level, which can utilize semantic information and software communication channels to achieve fault tolerance with considerably less power and performance overhead. We show the implementation and evaluation of such a software-level approach, application-level fault tolerance and detection (ALFTD) into the orbital thermal imaging spectrometer (OTIS).",2004,0,
2395,2396,A better way to handle instrument error checking,"The LabWindows/CVItrade ""C"" language compiler has no built in method of handling errors that occur in functions or drivers. The usual method is to use an ""if"" statement for every function call made, but this is very tedious and generates a lot of code. Often programs simply ignore errors and keep on going and depend on an eventual test failure to prevent acceptance of the item being tested. This leads to the wrong conclusion about what really went wrong. Other languages such as C++ have ""try"", ""catch"" and ""throw"" for exception handling. This paper explores several methods of handling the problem with much less code",2005,0,
2396,2397,Studies on the Internal Fault Simulations of a High-Voltage Cable-Wound Generator,"This paper discusses the set up of a mathematical model of the powerformer, a new type of salient-pole synchronous machine, for analyzing internal phase and ground faults in stator windings. The method employs a direct-phase representation considering the cable capacitance. To effectively implement the internal fault simulation, the magnetic axis locations of fault parts are arranged appropriately. Moreover, all machine windings supposed to be sinusoidally distributed in space and the system are magnetically linear. With the above-mentioned assumptions, the current-equivalent equations, voltage-equivalent equations, and the rotor-motion equations are formed and combined to implement the fault simulations. Simulation results showing the fault currents, during a single-phase-to-ground fault, a two-phase-to-ground fault, and a phase-to-phase fault, are presented here. With the data generated by this internal fault simulation model, the protection scheme used for the powerformer can be validated and improved accordingly.",2007,0,
2397,2398,Fault-tolerant clock synchronization for embedded distributed multi-cluster systems,"When time-triggered (TT) systems are to be deployed for large embedded real-time (RT) control systems in cars and airplanes, one way to overcome bandwidth limitations and achieve complexity reduction is the organization in clusters of strongly interacting computing nodes with well-defined interfaces. In this case, clock synchronization of different cluster times supports meaningful exchange of time-related data between clusters and allows coordinated control. This paper addresses fault-tolerant clock synchronization of clusters for TT systems that are already internally synchronized. By addressing systematic and stochastic errors of cluster times differently, the influence of systematic errors is eliminated and the quality of synchronization only depends on stochastic errors. Since systematic errors of cluster times are at least an order of magnitude larger than stochastic errors for typical RT embedded control systems, the presented algorithm achieves a significant improvement to known synchronization algorithms. An implementation of the proposed clock synchronization algorithm on top of the Time-Triggered Architecture and experiments show that synchronization is achieved with accuracy values of less than one microsecond.",2003,0,
2398,2399,Synthesizing Byzantine Fault-Tolerant Grid Application Wrapper Services,"The grid is inherently unreliable due to its geographical dispersion, heterogeneity and the involvement of multiple administrative domains. The most general case of failures are so-called Byzantine failures where no assumptions about the behavior of faulty components can be made. In this paper a novel system is described that allows to diagnose and tolerate byzantine faults based on service replication. We suggest, briefly describe and compare two fail-stop and two byzantine fault tolerance algorithms. Given that many scientific larger-scale grid applications have complex outputs the comparison of replica results as needed to implement byzantine fault tolerance becomes a non-trivial task. Therefore we include an automation mechanism based on a generic description language and code generation for this particualar problem. Our approach has been implemented as extension to the Otho Toolkit, a system that synthesizes tailor-made wrapper services for a given application, grid environment and resource. An analysis of performance and overheads for three real-world applications completes our work.",2008,0,
2399,2400,Automatic detection of pronunciation errors in CAPT systems based on confidence measure,"Computer aided pronunciation training (CAPT) systems aim at listening to a learner's utterance, judging the overall pronunciation quality and pointing out any pronunciation errors in the utterance. However, the performance of the current error detection techniques can not satisfy the users' expectation. In this paper, we introduce confidence measures and anti-models into the error detection algorithm. The basic theory and the roles of confidence measures and anti-models are discussed. We then propose a new algorithm, and evaluate it on an English learning system. Experiments are conducted based on the TIMIT database and an adaptive database, which involves 40 Chinese undergraduates. The results show that confidence measures can be utilized to effectively improve the performance of the CAPT system.",2008,0,
2400,2401,Analysis and research of association pattern between network performances and faults in voip network,"The paper focuses on implementation of association pattern between the network performances and the general network faults. We use network simulation tool, OPNET, to accomplish network simulations of different network faults and different situations based on VoIP. Network performance parameters such as end-to-end delay, jitter, traffic dropped, queuing delay, link throughout, link utilization are monitored and collected. The simulation results are processed using data mining software, Weka, to discover the association pattern.",2009,0,
2401,2402,A robust model-based information system for monitoring and fault detection of large scale belt conveyor systems,"In this paper an information system is presented, which is developed to meet the requirements on fault detection and online monitoring of large scale belt conveyor systems. The core of this information system consists of a mathematical model, observer and fault detection system.",2002,0,
2402,2403,Extended character defect model for recognition of text from maps,"Topographic maps contain a small amount of text compared to other forms of printed documents. Furthermore, the text and graphical components typically intersect with one another thus making the extraction of text a very difficult task. Creating training sets with a suitable size from the actual characters in maps would therefore require the laborious processing of many maps with similar features and the manual extraction of character samples. This paper extends the types of defects represented by Baird's document image degradation model in order to create pseudo randomly generated training sets that closely mimic the various artifacts and defects encountered in characters extracted from maps. Two Hidden Markov Models are then trained and used to recognize the text. Tests performed on extracted street labels show an improvement in performance from 88.4% when only the original Baird's model is used to a character recognition rate of 93.2% when the extended defect model is used for training.",2010,0,
2403,2404,Approximation method and formulas for average unavailability of systems with latent faults,Approximation method and formulas are provided for the average per flight probability of failure for aircraft systems whose components can fail latently. The approximations are given for two and three component redundant systems. The accuracy of the approximate formulas is studied by comparing them to the numerical results from a Markov solver. It shows that the approximation is close to the exact result in ultra-high reliability systems for a large range of failure rates of the components. The formulas are useful in representing event combinations involving latent events in fault trees. They could also be used to estimate the average per flight failure probability of more complex systems by a combination of the rare event approximation and the application of the formulas to the minimal cutsets of the system failure event,2002,0,
2404,2405,Analysis and modelling of novel band stop and band pass millimeter wave filters using defected microstrip structure (DMS),"This paper presents novel millimeter wave filter structures by creation of some slots on the strip. These slots perform a serious LC resonance property in certain frequency and suppress the spurious signals. In high frequencies and high density applications, the board area is seriously limited, so using these filters; the circuit area is minimized. The proposed filters are compact and very suitable for high density MMIC circuits.",2009,0,
2405,2406,Correction of the Off-Axis Reflector Beam Squint in Passive Images of the Fourth Stokes Parameter at 91 GHz,"In this paper we analyze effects of the antenna beam squint on the fourth Stokes parameter V images taken by the fully polarimetric imager SPIRA and suggest methods to correct them. The first images of complex scenery, containing man- made and natural objects, showed unexpected features in the y-parameter with amplitudes of up to 30 K, depending on the contrast in the total intensity images. They have pronounced variation in elevation direction, whereas in azimuth (horizontal) only small changes are observed. A simple relation can be established between the measured fourth Stokes parameter and the scene brightness distributions in V and the total intensity I. It allows correction of the beam-squint effects in the V Stokes parameter image using image processing methods. Another, less general method, could be more easily applied to SPIRA images, achieving a comparable enhancement. In this way, the beam-squint effects were reduced down to the uncertainty of the instrumental polarimetric calibration.",2007,0,
2406,2407,Analytical Evaluation of Timing Offset Error in OFDM System,This paper proposes an idea of developing a model of estimating timing offset error of OFDM (Orthogonal Frequency Division Multiplexing) system without the use of additional pilots relying on inherent characteristics of the OFDM signal. An analytical expression has been derived and formulated to analyze the effect of bit error rate (BER) due to timing offset introduced by the transmission channel. This will help in determining the exact length of cyclic prefix to be added to each OFDM symbol to avoid misinterpretation by the receiver. The performances have also been evaluated under coded (convolution) and uncoded systems. The introduction of channel coding decreases this basic impairment of OFDM systems significantly. Simulated results show that the symbol error rate (SER) linearly depends on timing offset. It is expected that further works on the proposed estimated method will lead to a standard model so that at the receiving end the effect of timing offset can be eliminated totally.,2010,0,
2407,2408,Performance evaluation of CoolMOS/sup /spl trade// and SiC diode for single-phase power factor correction applications,"The low conduction loss and switching loss characteristics make CoolMOS/sup /spl trade// and SiC diode attractive for the single-phase CCM PFC converters. In this paper, based on the device level and converter level evaluation, the loss reduction capability of the CoolMOS/sup /spl trade// and SiC diode is quantified. In addition, for the first time, a successfully operating 1 kW 400 kHz single-phase CCM PFC is demonstrated by using CoolMOS/sup /spl trade// and SiC diode.",2003,0,
2408,2409,Rational Fitting of S-Parameter Frequency Samples With Maximum Absolute Error Control,"Rational fitting techniques are often used for the macromodeling of linear systems from tabulated S-parameter frequency samples. This letter proposes a modified weighting scheme for the Vector Fitting algorithm that iteratively minimizes the maximum absolute error over the frequency range of interest, rather than the least-squares error. By considering the appropriate error measure in the fitting process, it is possible to obtain more accurate broadband macromodels without increasing the number of poles. The effectiveness of the approach is illustrated by several numerical examples.",2010,0,
2409,2410,Bit error rate performance of iterative decoding in a perpendicular magnetic recording channel,"The BER performance of iterative decoding systems and PRML systems in a perpendicular magnetic recording channel is experimentally investigated. A performance evaluation system consisting of a spinstand, a recording waveform generator, a digital storage oscilloscope and a personal computer was used. PR1ML, PR2ML, EPR3ML, E<sup>2</sup>PR3ML and ME<sup>2</sup>PR4ML systems are discussed as a PRML channel. The 8/9 and 16/17 iterative decoding systems consists of the encoder, with RSC encoder serially concatenated with preceded PR channels and the iterative decoder with APP modules using Max-log-MAP algorithm. The results show that EPR3ML and E<sup>2</sup>PR3ML provide good BER performance in a perpendicular magnetic recording channel and that the iterative decoding system is also an efficient way to improve the BER performance in the channel",2001,0,
2410,2411,Applications of the Fault Decoupling Device to Improve the Operation of LV Distribution Networks,"The aim of this paper is to present the operating principle of a new resonant device, called the fault decoupling device (FDD), able to improve power quality in electrical distribution systems. In low-voltage networks, this device can be employed in order to mitigate voltage dips due to faults or large induction motor startup. Moreover, in the presence of distributed-generation (DG) units, the FDD allows one to obtain various benefits such as a reduction of the fault current in each node of the network and an increase in the voltage at the DG unit node. In order to show the performances of the FDD, analytical studies and computer simulations were carried out which took into account various working operation conditions. Finally, the prototype of the FDD as well as the preliminary experimental results are presented.",2008,0,
2411,2412,Atmospheric Correction at AERONET Locations: A New Science and Validation Data Set,"This paper describes an Aerosol Robotic Network (AERONET)-based Surface Reflectance Validation Network (ASRVN) and its data set of spectral surface bidirectional reflectance and albedo based on Moderate Resolution Imaging Spectroradiometer (MODIS) TERRA and AQUA data. The ASRVN is an operational data collection and processing system. It receives 50 times 50 km<sup>2</sup> subsets of MODIS level 1B (L1B) data from MODIS adaptive processing system and AERONET aerosol and water-vapor information. Then, it performs an atmospheric correction (AC) for about 100 AERONET sites based on accurate radiative-transfer theory with complex quality control of the input data. The ASRVN processing software consists of an L1B data gridding algorithm, a new cloud-mask (CM) algorithm based on a time-series analysis, and an AC algorithm using ancillary AERONET aerosol and water-vapor data. The AC is achieved by fitting the MODIS top-of-atmosphere measurements, accumulated for a 16-day interval, with theoretical reflectance parameterized in terms of the coefficients of the Li Sparse-Ross Thick (LSRT) model of the bidirectional reflectance factor (BRF). The ASRVN takes several steps to ensure high quality of results: 1) the filtering of opaque clouds by a CM algorithm; 2) the development of an aerosol filter to filter residual semitransparent and subpixel clouds, as well as cases with high inhomogeneity of aerosols in the processing area; 3) imposing the requirement of the consistency of the new solution with previously retrieved BRF and albedo; 4) rapid adjustment of the 16-day retrieval to the surface changes using the last day of measurements; and 5) development of a seasonal backup spectral BRF database to increase data coverage. The ASRVN provides a gapless or near-gapless coverage for the processing area. The gaps, caused by clouds, are filled most naturally with the latest solution for a given pixel. The ASRVN products include three parameters of the LSRT model (k<sup>L</sup>, k<sup- - >G</sup>, and k<sup>V</sup>), surface albedo, normalized BRF (computed for a standard viewing geometry, VZA = 0deg, SZA = 45deg), and instantaneous BRF (or one-angle BRF value derived from the last day of MODIS measurement for specific viewing geometry) for the MODIS 500-m bands 1-7. The results are produced daily at a resolution of 1 km in gridded format. We also provide a cloud mask, a quality flag, and a browse bitmap image. The ASRVN data set, including 6 years of MODIS TERRA and 1.5 years of MODIS AQUA data, is available now as a standard MODIS product (MODASRVN) which can be accessed through the Level 1 and Atmosphere Archive and Distribution System website ((http://ladsweb.nascom.nasa.gov/data/search.html).). It can be used for a wide range of applications including validation analysis and science research.",2009,0,
2412,2413,The Effectiveness of Automated Static Analysis Tools for Fault Detection and Refactoring Prediction,"Many automated static analysis (ASA) tools have been developed in recent years for detecting software anomalies. The aim of these tools is to help developers to eliminate software defects at early stages and produce more reliable software at a lower cost. Determining the effectiveness of ASA tools requires empirical evaluation. This study evaluates coding concerns reported by three ASA tools on two open source software (OSS) projects with respect to two types of modifications performed in the studied software CVS repositories: corrections of faults that caused failures, and refactoring modifications. The results show that fewer than 3% of the detected faults correspond to the coding concerns reported by the ASA tools. ASA tools were more effective in identifying refactoring modifications and corresponded to about 71% of them. More than 96% of the coding concerns were false positives that do not relate to any fault or refactoring modification.",2009,0,
2413,2414,Efficient Simulation of Structural Faults for the Reliability Evaluation at System-Level,"In recent technology nodes, reliability is considered a part of the standard design ow at all levels of embedded system design. While techniques that use only low-level models at gate- and register transfer-level offer high accuracy, they are too inefficient to consider the overall application of the embedded system. Multi-level models with high abstraction are essential to efficiently evaluate the impact of physical defects on the system. This paper provides a methodology that leverages state-of-the-art techniques for efficient fault simulation of structural faults together with transaction-level modeling. This way it is possible to accurately evaluate the impact of the faults on the entire hardware/software system. A case study of a system consisting of hardware and software for image compression and data encryption is presented and the method is compared to a standard gate/RT mixed-level approach.",2010,0,
2414,2415,An empirical study on bug assignment automation using Chinese bug data,"Bug assignment is an important step in bug life-cycle management. In large projects, this task would consume a substantial amount of human effort. To compare with the previous studies on automatic bug assignment in FOSS (free/open source software) projects, we conduct a case study on a proprietary software project in China. Our study consists of two experiments of automatic bug assignment, using Chinese text and the other non-text information of bug data respectively. Based on text data of the bug repository, the first experiment uses SVM to predict bug assignments and achieve accuracy close to that by human triagers. The second one explores the usefulness of non-text data in making such prediction. The main results from our study includes that text data are most useful data in the bug tracking system to triage bugs, and automation based on text data could effectively reduce the manual effort.",2009,0,
2415,2416,Linux Bugs: Life Cycle and Resolution Analysis,"Efforts to improve application reliability can fade if the reliability of the underlying operating system on which the application resides is not seriously considered. An important first step in improving the reliability of an operating system is to first gain insights into why and how the bugs originate, contributions of the different modules to the bugs, their distribution across severities, the different ways in which the bug may be resolved and the impact of bug severity on the resolution time. To gain this insight we conducted an extensive analysis of the publicly available bug data on the Linux kernel over a period of seven years. Our observations suggest that the Linux kernel may draw significant benefits from the continual reliability improvement efforts of its developers. These efforts, however, are disproportionately targeted towards popular configurations and hardware platforms, due to which the reliability of these configurations may be better than those that are not commonly used. Thus, a key finding of our study is that it may be prudent to restrict to using common configurations and platforms when using open source systems such as Linux in applications with stringent reliability expectations.",2008,0,
2416,2417,Assisting Students with Typical Programming Errors During a Coding Session,"Typical programming errors that introductory students make are known to stall and frustrate them during their first few lab coding sessions. Traditionally, lab instructors help with the process of correcting these errors; however, with medium or large size computer labs, the degree of interaction between students and the instructor tends to decline; leaving some students feeling unattended and subsequently perhaps uninterested. To help automate the process of finding and correcting these errors we have devised a solution that collects periodical or on-compile time code snapshots which we analyze upon unsuccessful compilation or whenever a lengthy stall is detected. The analysis is then used to provide feedback to students directly into their Integrated Development Environment (IDE), and generate useful reports that can be perused by both instructors and students later on. This article describes the design and implementation of our solution in the context of BlueJ; a common first year IDE.",2010,0,
2417,2418,Fault Diagnosis Expert System for Broadcasting Station Interface Circuit Board on VXI Bus,"A high-tech information electronic equipment of some given type is designed in order to proceed automatically fault detection and improve the efficiency and accuracy of diagnosis. This thesis, which is a part of the program, introduces the research of algorithm of fault diagnosis expert system of the broadcasting station of the equipment and algorithm realization and example proving on the hardware platform. It's quicker and more convenient to locate fault on the circuit boards on this equipment. It's proved that this expert system can solve the problems of high cost and long intervals of maintenance and keep the equipment in a stable status",2006,0,
2418,2419,FTCloud: A Component Ranking Framework for Fault-Tolerant Cloud Applications,"Cloud computing is becoming a mainstream aspect of information technology. The cloud applications are usually large-scale, complex, and include a lot of distributed components. Providing highly reliable cloud applications is a challenging and critical research problem. To attack this challenge, we propose FTCloud which is a component ranking based framework for building fault-tolerant cloud applications. FTCloud employs the component invocation structures and the invocation frequencies to identify the significant components in a cloud application. An algorithm is proposed to automatically determine optimal fault tolerance strategy for these significant components. The experimental results show that by tolerating faults of a small part of the most significant components, the reliability of cloud application can be greatly improved.",2010,0,
2419,2420,Higher-order corrections to the pi criterion using center manifold theory,"The frequency-dependent pi criterion of Bittanti et al. has been used extensively in applications to predict potential performance improvement under periodic forcing in a nonlinear system. The criterion, however, is local in nature and is limited to periodic forcing functions of small magnitude. The present work develops a method to determine higher-order corrections to the pi criterion, derived from basic results of center manifold theory. The proposed method is based on solving the center manifold PDE via recursive Taylor series. The advantage of the proposed approach is the improvement of the accuracy of the pi criterion in predicting performance under larger amplitudes. The proposed method is applied to a continuous stirred tank reactor, where the yield of the desired product must be maximized.",2002,0,
2420,2421,Assessment and elimination of errors due to electrode displacements in elliptical and square models in EIT,"This study modifies a Tikhonov regularized ""maximum a posteriori"" algorithm proposed for reconstructing both the conductivity changes and electrode positioning variations in EIT and uses this algorithm for reconstructing images of 2d elliptical and square models, instead of simple circular model used in previous works. This algorithm had been proposed By C. Gomez for compensating the errors due to electrode movements in image reconstruction. The jacobian matrix has been constructed via perturbation both conductivity and electrode positioning. The prior image matrix should incorporate some kind of augmented inter-electrode positioning correlations to impose a smoothness constraint on both the conductivity change distribution and electrode movement. For each model, conductivity change image is reconstructed in 3 cases: a) With no electrode displacement using standard algorithm b) With electrode displacement using standard algorithm c) With electrode displacement using proposed algorithm. In all models, a comparison between 3 cases has been implemented. Also, the results obtained from each model have been compared with the other models in similar cases. The results obtained in this study will be useful to investigate the ellipticity effects of organs being imaged in clinical applications. Moreover, the effects of model deviation from circular form on reconstructed images can be used in special industrial applications.",2010,0,
2421,2422,Soft Error Resilient System Design through Error Correction,"This paper presents an overview of the built-in soft error resilience (BISER) technique for correcting soft errors in latches, flip-flops and combinational logic. The BISER technique enables more than an order of magnitude reduction in chip-level error soft rate with minimal area impact, 6-10% chip-level power impact, and 1-5% performance impact (depending on whether combinational logic error correction is implemented or not). In comparison, several classical error-detection techniques introduce 40-100% power, performance and area overheads, and require significant efforts for designing and validating corresponding recovery mechanisms. Design trade-offs associated with the BISER technique and other existing soft error protection techniques are also analyzed",2006,0,
2422,2423,Frame Error Concealment Technique Using Adaptive Inter-Mode Estimation for H.264/AVC,"Since H.264/AVC achieves a high compression ratio by reducing spatio-temporal redundancy in video sequences, the payload of a single packet can often contain a whole frame encoded by H.264/AVC. Therefore, the loss of a single packet does not only cause the loss of a whole frame, but also produce error propagation into succeeding frames. To deal with this problem, in this paper, we propose a novel frame error concealment method for H.264/AVC. First, the proposed method extrapolates motion vectors from available neighboring frames onto the lost frame. Then, inter-modes of all macroblocks in the lost frame are adoptively estimated by using the extrapolated motion vectors and features of H.264/AVC. Experimental results exhibit that the proposed method outperforms the conventional methods in terms of both objective and subjective video quality.",2008,0,
2423,2424,Operating MicroGrid Energy Storage Control during Network Faults,"A MicroGrid is expected to operate both as a sub system connected to the main grid or as an islanded system. However the provision of fault currents, in an islanded MicroGrid consisting only of micro-generation interfaced with relatively low-current power electronics, is a serious system protection issue. This paper presents the novel concept of using the central energy storage system (flywheel) as the main fault current source in islanded mode. The three-phase MicroGrid test rig used at University of Manchester, and the flywheel control system are described. The importance of accurate systems modeling of the whole microgrid and energy storage unit is shown. A fault study is carried out on the test rig and in PSCAD. The flywheel inverter system is shown to contribute enough fault current for a sufficient duration to cause the system protective device to clear the fault.",2007,0,
2424,2425,Hardware Building Blocks for High Data-Rate Fault-Tolerant In-vehicle Networking,"This paper discusses the hardware implementation of high speed and fault-tolerant communication systems for in-vehicle networking. Emerging safety-critical automotive control systems, such as X-by-wire and active safety, need complex distributed algorithms. Large bunches of data have to be exchanged in real-time and with high dependability between electronic control units, sensors and actuators. According to this perspective, the FlexRay protocol, which features data-rates up to 10 Mb/s, time and event triggered transmissions, as well as scalable fault-tolerance support, was developed and it is now expected to become the future standard for in-vehicle communication. However, collision avoidance and driver assistance applications based on vision/radar systems, poses requirements on the communication systems that can hardly be covered by current and expected automotive standards. A candidate that will play a significant role in the development of safety systems which need data-rates up to hundreds of Mb/s as well as fault-tolerance seems to be the new SpaceWire protocol, whose effectiveness has already been proved in avionic and aerospace. This paper presents the design of the major hardware building blocks of the FlexRay and SpaceWire protocols.",2007,0,
2425,2426,Classification of power system faults using wavelet transforms and probabilistic neural networks,Automation of power system fault identification using information conveyed by the wavelet analysis of power system transients is proposed. The Probabilistic Neural Network (PNN) for detecting the type of fault is used. The work presented in this paper is focused on identification of simple power system faults. Wavelet Transform (WT) of the transient disturbance caused as a result of the occurrence of a fault is performed. The detail coefficient for each type of simple fault is characteristic in nature. PNN is used for distinguishing the detail coefficients and hence the faults.,2003,0,
2426,2427,A selection sort method of test cases based on severity of defects,"In this paper, a selection sort method of test cases is presented that is according to historical datum from the test cases pools, statistical analysis of test cases based on defect severities and choose the order of test cases and then measure the efficiency by the formula to find the choice of test cases in testing process in order. Finally, the test case is given overall selection method in regression testing. Examples of this method for analysis, experimental results show that the proposed method effectively reduce the number of test cases and improve the efficiency of discovery the defects in regression testing.",2010,0,
2427,2428,On-line fast motor fault diagnostics based on fuzzy neural networks,"An on-line method was developed to improve diagnostic accuracy and speed for analyzing running motors on site. On-line pre-measured data was used as the basis for constructing the membership functions used in a fuzzy neural network (FNN) as well as for network training to reduce the effects of various static factors, such as unbalanced input power and asymmetrical motor alignment, to increase accuracy. The preprocessed data and fuzzy logic were used to find the nonlinear mapping relationships between the data and the conclusions. The FNN was then constructed to carry motor fault diagnostics, which gives fast accurate diagnostics. The on-line fast motor fault diagnostics clearly indicate the fault type, location, and severity in running motors. This approach can also be extended to other applications.",2009,0,
2428,2429,Research on fault diagnosis for ship course control system,"This paper analyzes the fault information of ship's course control system, and establishes a fault diagnosis model based on fuzzy nerve network algorithms. Using the fuzzy logic processing data so as to make full use of the experience and knowledge, using the nerve network so as to avoid some problems of the complications fault tree diagnosis system, such as matching conflict, combination explosion, and infinite recursion. Also adopting the improved BP arithmetic to train the nerve network which can solve the problems of convergence speed and convergence surge. The fault diagnosis result shows this fault diagnosis system has strong robustness and generalization. That method uses model free diagnosis, so that is easy to learn by self and perfect system function constantly, and has some theories and engineering application value.",2009,0,
2429,2430,Single byte error control codes with double bit within a block error correcting capability for semiconductor memory systems,"Computer memory systems when exposed to strong electromagnetic waves or radiation are highly vulnerable to multiple random bit errors. Under this situation, we cannot apply existing SEC-DED or S<sub>b</sub>EC capable codes because they provide insufficient error control performance. This correspondence considers the situation where two random bits in a memory chip are corrupted by strong electromagnetic waves or radioactive particles and proposes two classes of codes that are capable of correcting random double bit errors occurring within a chip. The proposed codes, called Double bit within a block Error Correcting-Single byte Error Detecting ((DEC)<sub>B</sub>-S<sub>b</sub>ED) code and Double bit within a block Error Correcting-Single byte Error Correcting ((DEC)<sub>B</sub>-S<sub>b </sub>EC) code, are suitable for recent computer memory systems",2000,0,
2430,2431,On fault-sensitive feasibility analysis of real-time task sets,"In this paper, we consider the problem of checking the feasibility of a set of n aperiodic real-time tasks while provisioning for timely recovery from (at most) k transient faults. We extend the well-known processor demand approach to take into account the extra overhead that may be induced by potential recovery operations under earliest deadline first scheduling. We develop a necessary and sufficient test using dynamic programming technique. An improvement upon the previous solutions is to address and efficiently solve the case where the recovery blocks associated with faults of a given task do not have necessarily the same execution time. Further, we provide an on-line version of our algorithm that does not require a priori knowledge of release times. The on-line algorithm runs in O(mk<sup>2</sup>) time where m is the number of ready tasks. We also show how to quickly adjust the recovery-related parameters of the algorithm for the remaining part of the execution when a fault is detected.",2004,0,
2431,2432,MPEG-2 to WMV Transcoder With Adaptive Error Compensation and Dynamic Switches,"In this paper, we study the problem of video transcoding from MPEG-2 to Windows Media Video (WMV) format, together with several desired functionalities such as bit-rate reduction and spatial resolution downscaling. Based on in-depth analysis of error propagation behavior, we propose two architectures (for different typical application scenarios) that are unique in their complexity scalability and adaptive drifting error control, which in return provide a mechanism to achieve a desired tradeoff between complexity and quality. We perform extensive experiments for various design targets such as complexity, scalability, performance tradeoff, and drifting control effect. The proposed transcoding architectures can be straightforwardly applied to the MPEG-2 to MPEG-4 transcoding applications due to the significant overlap between the MPEG-4 and WMV coding technology",2006,0,
2432,2433,Identification inrush current and internal faults of transformer based on hyperbolic S-transform,"In order to solve the problem of mis-operation of transformer differential relay owing to the inrush current, internal faults and inrush current must be discriminate effectively. In this paper, a novel approach of identification between inrush current and internal faults based on hyperbolic S-transform (HST) which is a very powerful tool for nonstationary signal analysis giving the information of transient currents both in time and in frequency domains is presented. The signal is transformed to phase space by using HST and the features are detected. It is found that the time-frequency contours in case of inrush current are different from that in case of internal faults. The results obtained by using HST and discrete wavelet transform (DWT) were compared. Comparison results indicate that the time-frequency localization characteristics are more distinct in ST domain, and HST has strong capability in noise reduction. The spectral energy and standard deviation from the HST of signal are computed, classification of inrush current and internal faults is done by BP neural networks. Simulation results indicate that this technique is effective and feasible.",2009,0,
2433,2434,ECC design for fault-tolerant crossbar memories: A case study,"Crossbar memories are promising memory technologies for future data storage. Although the memories offer trillion-capacity of data storage at low cost, they are expected to suffer from high defect densities and fault rates impacting their reliability. Error correction codes (ECCs), e.g., Redundant Residue Number System (RRNS) and Reed Solomon (RS) have been proposed to improve the reliability of memory systems. Yet, the implementation of the ECCs was usually done at software level, which incurs high cost. This paper analyzes ECC design for fault-tolerant crossbar memories. Both RS and RRNS codes are implemented and experimentally compared in terms of their area overhead, speed and error correction capability. The results show that the encoder and decoder of RS requires 7.5 smaller area overhead and operates 8.4 faster as compared to RRNS. Both ECCs has fairly similar error correction capability.",2010,0,
2434,2435,Accuracy of motion correction methods for PET brain imaging,"Recently published methods for motion correction in neurological PET include the multiple acquisition frame (MAF) and LOR rebinning methods. The aim of the present work was to compare the accuracy of reconstructions obtained with these methods when multiple, arbitrary movements were applied to a Hoffman brain phantom during 3D list mode acquisition. A reflective target attached to the phantom enabled a Polaris optical motion tracking system to monitor the phantom position and orientation in the scanner coordinate frame. The motion information was used in the motion correction algorithms. The MAF method was applied to the list-mode data after sorting them into a series of dynamic frames, while the LOR rebinning method was applied directly to the list-mode data. A proportion of the list mode events had to be discarded during rebinning because the application of the corrective spatial transformation removed them from the 3D projection space. A correction for these 'lost' events was implemented as a global post-reconstruction scale factor, based on the overall fraction of lost events. Reconstructions from both motion correction methods were compared with a motion-free reference scan of the same phantom. Motion correction produced a marked improvement in image clarity and reduced errors with respect to the reference scan. LOR rebinning with global loss correction was found to be more accurate than the MAF method",2004,0,
2435,2436,Componentization of Fault Tolerance Software for Fine-Grain Adaptation,"The evolution of systems during operational lifetime is becoming a core assumption of the design. This is the case for resource constrained embedded systems. Such an evolution may be driven by environment or the execution context. The adequacy of the service delivery with respect to the current operational conditions depends on the ability to tune the software configuration accordingly. This is true for application services, but also for dependability services, in particular the fault tolerance software. This paper presents a design of fault tolerance software for its runtime adaptation. This design relies on a reflective framework and open component based software engineering (CBSE) techniques. We demonstrate in this paper the feasibility of adapting componentized fault tolerance at a meta-level of the application.",2008,0,
2436,2437,Incorporation of hard-fault-coverage in model-based testing of mixed-signal ICs,"The application of the Linear Error Mechanism Modeling Algorithm (LEMMA) to various DAC and ADC architectures has raised the issue of including hard-fault-coverage as an integral part of the algorithm. In this work, we combine defect-oriented functionality tests and specification-oriented linearity tests of a mixed-signal IC to save test time. The key development is a novel test point selection strategy which not only optimizes the INL-prediction variance of the model, but also satisfies hard-fault-coverage constraints",2000,0,
2437,2438,Virtual measuring system of the geometric error based on LabView,"The paper constructs a virtual detection system of geometrical errors with powerful virtual instrument LabView software, and realizes the simulation tests of the parts' form error, direction error, position error and run-out error. The system could test the data by different data processing methods, and get the reports of the tested data and error results.",2010,0,
2438,2439,On Reducing Circuit Malfunctions Caused by Soft Errors,Soft errors due to radiation are expected to increase in nanoelectronic circuits. Methods to reduce system failures due to soft errors include use of redundancy and making circuit elements robust such that soft errors do not upset signal values. Recent works have noted that electronic circuits have partial intrinsic immunity to soft errors since single event upsets on a large percentage of signal lines do not cause errors on circuit outputs. Using ISCAS-89 benchmark circuits we present experimental evidence that the partial immunity to single event upsets is in most cases due to redundancy in the circuits and thus immunity to soft errors may not be available in irredundant circuits. Thus goals on immunity to soft errors may not be achievable in highly optimized circuits without adding circuit redundancy and/or relaxing the requirements on system failures due to soft errors.,2008,0,
2439,2440,Quantitative nondestructive evaluation of material defect using GP-based fuzzy inference system,"This paper deals with a quantitative nondestructive evaluation in eddy current testing for steam generator tubes of nuclear power plants by using genetic programming (GP) and fuzzy inference system. Defects can be detected as a probe impedance trajectory by scanning a pancake type probe coil. An inference system is proposed for identifying the defect shape inside and/or outside tubes. GP is applied to extract and select effective features from a probe impedance trajectory. Using the extracted features a fuzzy inference system detects presence, position, and size of a defect of a test sample. The effectiveness of the proposed method is demonstrated through computer simulation studies",2000,0,
2440,2441,Agent-based real-time fault diagnosis,"Theory and applications of model-based fault diagnosis have progressed significantly in the last four decades. In addition, there has been increasing use of model-based design and testing in automotive industry to reduce design errors, perform real-time simulations for rapid prototyping, and hardware-in-the-loop testing. For vehicle diagnosis, a global diagnosis method, which collects the diagnostic information from all the subsystem electronic control units (ECUs), is not practical because of high communication requirements and time delays induced by centralized diagnosis. Consequently, an agent-based distributed diagnosis architecture is needed. In this architecture, each subsystem resident agent (embedded in the ECU) performs its own fault inference and communicate the diagnostic results to a vehicle expert agent. A vehicle expert agent performs cross-subsystem diagnosis to resolve conflicts among resident agents, and to provide an accurate vehicle-level diagnostic inference. In this paper, we propose a systematic way to design an agent-based diagnosis architecture. A hybrid model-based technique that seamlessly employs a graph-based dependency model and quantitative models for intelligent diagnosis is applied to each individual ECU. Diagnostic tests for each individual ECU are designed via model-based diagnostic techniques based on a quantitative model. The fault simulation results, in the form of a diagnostic matrix, are extracted into a dependency model for fast fault inference by a resident agent. The global diagnostic inference is performed through a vehicle expert agent that trades off computational complexity and communication load. This architecture is demonstrated on the engine air induction subsystem. The solution is generic and can be applied to a variety of distributed control systems",2005,0,
2441,2442,Application of error control codes (ECC) in ultra-low power RF transceivers,"Wireless sensor networks provide the ability to gather and communicate critical environmental, industrial or security information to enable rapid responses to potential problems. The limited embedded battery life time requires ultra low power sensing, processing and communication systems. To achieve this goal, new approaches at the device, circuit, system and network level need to be pursued (Roundy et al., 2003). Adoption of error control codes (ECC) reduces the required transmit power for reliable communication, while increasing the processing energy of the encoding and decoding operations. This paper discusses the above trade off for systems with and without standard ECC, such as convolutional and Reed Solomon codes. The comparison of the required energy per bit, based on several implemented decoders, shows that the adoption of an ECC with simple decoding structures (such as Reed Solomon) is quite energy efficient. This has specially been observed for long distances.",2005,0,
2442,2443,Adaptive bug isolation,"Statistical debugging uses lightweight instrumentation and statistical models to identify program behaviors that are strongly predictive of failure. However, most software is mostly correct; nearly all monitored behaviors are poor predictors of failure. We propose an adaptive monitoring strategy that mitigates the overhead associated with monitoring poor failure predictors. We begin by monitoring a small portion of the program, then automatically refine instrumentation over time to zero in on bugs. We formulate this approach as a search on the control-dependence graph of the program. We present and evaluate various heuristics that can be used for this search. We also discuss the construction of a binary instrumentor for incorporating the feedback loop into post-deployment monitoring. Performance measurements show that adaptive bug isolation yields an average performance overhead of 1% for a class of large applications, as opposed to 87% for realistic sampling-based instrumentation and 300% for complete binary instrumentation.",2010,0,
2443,2444,An novel loss protection scheme for H.264 video stream based on Frame Error Propagation Index,"In this paper, a novel GOP level unequal loss protection (G-ULP) scheme is proposed for robust H.264-coded video streaming over packet loss networks. This scheme use frame error propagation index (FEPI) to characterize video quality degradation caused by error propagation in different frames in a GOP when suffer from packet loss. A fast FEPI calculation method in compression domain is also proposed in this paper. By exploiting the unequal significance in different frames in a GOP, different amount of forward error correction (FEC) packets are allocated to different frames in a GOP. The optimal FEC allocation algorithm is based on the FEPI of each frame in the GOP. The simulation results show that the proposed G-ULP scheme can improve the receiver side reconstructed video quality remarkably under different channel loss patterns.",2008,0,
2444,2445,Practical Diagnostic Approach of Energy Consumption and Systematic Fault of AC System,"Due to the vast amount of energy consumption of AC (Air Conditioning) systems, the reasonable design, optimal operation and efficient management of AC systems are inevitable and necessary to the whole city energy management (CEM). To achieve these objectives, the verification of systematic characteristics, practical approaches of energy consumption and systematic fault diagnosis of AC systems should be taken as the premises and the most elementary works. Based on AC commissioning principle, a practical diagnosis approach of energy consumption and systematic fault of AC system is established. The framework and general procedures of the practical approach is presented and introduced in this paper.",2010,0,
2445,2446,Coupling Three-Dimensional Mesh Adaptation with an A Posteriori Error Estimator,"A three-dimensional unstructured mesh adaptation technique coupled to a posteriori error estimation techniques is presented. In contrast to other work [1,2] the adaptation in three dimensions is demonstrated using advanced unstructured meshing techniques to realize automatic adaptation. The applicability and usability of this complete automation are presented with a real-world example.",2005,0,
2446,2447,A CORBA design pattern to build load balancing and fault tolerant telecommunication software,"As a mature distributed object computing middleware, CORBA is being used more and more widely in many fields to build large scale distributed software system. In telecommunication system, load balance and fault tolerance are especially important because of strict requirements for high reliability and capacity. In former studies, load balance and fault tolerance are considered separately more often. This paper introduces a new CORBA design pattern, named GenericFactory pattern, and discusses how to combine load disturbance and fault tolerance effectively. Advantages of this pattern are pointed out.",2003,0,
2447,2448,Analysis of the error performance of adaptive array antennas for CDMA with noncoherent M-ary orthogonal modulation in Nakagami fading,"This letter presents an analytical model for evaluating the bit error rate (BER) of a direct sequence code division multiple access (DS-CDMA) system, with M-ary orthogonal modulation and noncoherent detection, employing an array antenna operating in a Nakagami fading environment. An expression of the signal to interference plus noise ratio (SINR) at the output of the receiver is derived, which allows the BER to be evaluated using a closed form expression. The analytical model is validated by comparing the obtained results with simulation results.",2005,0,
2448,2449,Lightweight fault tolerance in CORBA,"Although fault-tolerant implementations of CORBA have been available for several years, the standard specification of fault-tolerant CORBA (FT-CORBA) has been finalized only recently. This specification defines simple, minimal mechanisms for regular clients to deal with fault-tolerant servers, as well as a wide spectrum of services and API for implementing replicated, fault-tolerant servers. While extremely powerful, these advanced server-side mechanisms come with significant complexity both for the FT-CORBA implementor and the application developer. This paper proposes an alternative, lightweight approach to fault tolerance for applications that do not have strong requirements in terms of data consistency. This approach builds on the client-side mechanisms of FT-CORBA and takes advantage of semantic knowledge of the server objects to mediate distributed interactions in an efficient and fault-tolerant manner. Although the approach proposed in this paper is not applicable to any application, it can be deployed in existing systems to transparently increase their reliability and availability without requiring any re-engineering",2001,0,
2449,2450,A Method for Predicting Marker Tracking Error,"Many augmented reality (AR) applications use marker-based vision tracking systems to recover camera pose by detecting one or more planar landmarks. However, most of these systems do not interactively quantify the accuracy of the pose they calculate. Instead, the accuracy of these systems is either ignored, assumed to be a fixed value, or determined using error tables (constructed in an off-line ground-truthed process) along with a run-time interpolation scheme. The validity of these approaches are questionable as errors are strongly dependent on the intrinsic and extrinsic camera parameters and scene geometry. In this paper we present an algorithm for predicting the statistics of marker tracker error in real-time. Based on the scaled spherical simplex unscented transform (SSSUT), the algorithm is applied to the augmented reality toolkit plus (ARToolKitPlus). The results are validated using precision off-line photogrammetric techniques.",2007,0,
2450,2451,A Passive Fault Tolerant Control Strategy for the uncertain MIMO Aircraft Model F-18,"In this paper, we design a passive fault tolerant control strategy for an uncertain MIMO aircraft model F-18. A novel variable structure controller with sliding surface and Lyapunov function is proposed in order to eliminate the effect of certain type of pre-specified faults. The main features of the proposed control strategy are its simplicity and robustness against uncertainties and parameter variations and some pre-specified faults. Computer experiments illustrating the application of the proposed approach to the longitudinal flight control of an F-18 aircraft model are presented to show the effectiveness of the design method.",2007,0,
2451,2452,Using inspection data for defect estimation,"To control projects, managers need accurate and timely feedback on the quality of the software product being developed. I propose subjective team estimation models calculated from individual estimates and investigate the accuracy of defect estimation models based on inspection data",2000,0,
2452,2453,A Systems Approach to Fault Detection and Diagnosis for Condition-Based Maintenance,"Current methods for actuator condition monitoring on the railways have been developed as individual solutions, using simple thresholding techniques to raise alarms. These are only partly helpful to the maintainer as considerable diagnostics must still be carried out when a fault is detected. A more comprehensive approach is developed here as a general solution to diagnosis for all actuators belonging to the class known as single-throw mechanical equipment. A systems engineering approach has been used to develop the solution, by taking a basic set of requirements as a starting point and using these to form functions by collecting and evaluating methods from all parts of industry",2006,0,
2453,2454,ERSA: Error Resilient System Architecture for probabilistic applications,"There is a growing concern about the increasing vulnerability of future computing systems to errors in the underlying hardware. Traditional redundancy techniques are expensive for designing energy-efficient systems that are resilient to high error rates. We present Error Resilient System Architecture (ERSA), a low-cost robust system architecture for emerging killer probabilistic applications such as Recognition, Mining and Synthesis (RMS) applications. While resilience of such applications to errors in low-order bits of data is well-known, execution of such applications on error-prone hardware significantly degrades output quality (due to high-order bit errors and crashes). ERSA achieves high error resilience to high-order bit errors and control errors (in addition to low-order bit errors) using a judicious combination of 3 key ideas: (1) asymmetric reliability in many-core architectures, (2) error-resilient algorithms at the core of probabilistic applications, and (3) intelligent software optimizations. Error injection experiments on a multi-core ERSA hardware prototype demonstrate that, even at very high error rates of 20,000 errors/second/core or 2??10<sup>-4</sup> error/cycle/core (with errors injected in architecturally-visible registers), ERSA maintains 90% or better accuracy of output results, together with minimal impact on execution time, for probabilistic applications such as K-Means clustering, LDPC decoding and Bayesian networks. Moreover, we demonstrate the effectiveness of ERSA in tolerating high rates of static memory errors that are characteristic of emerging challenges such as Vccmin problems and erratic bit errors. Using the concept of configurable reliability, ERSA platforms may also be adapted for general-purpose applications that are less resilient to errors (but at higher costs).",2010,0,
2454,2455,Virtual fault simulation of distributed IP-based designs,"Fault simulation and testability analysis are major concerns in design flows employing intellectual-property (IP) protected virtual components. In this paper we propose a paradigm for the fault simulation of IP-based designs that enables testability analysis without requiring IP disclosure, implemented within the JavaCAD framework for distributed design. As a proof of concept, stuck-at fault simulation has been performed for combinational circuits containing virtual components",2000,0,
2455,2456,Improving the performance of speech recognition systems using fault-tolerant techniques,"In this paper, using of fault tolerant techniques are studied and experimented in speech recognition systems to make these systems robust to noise. Recognizer redundancy is implemented to utilize the strengths of several recognition methods that each one has acceptable performance in a specific condition. Duplication-with-comparison and NMR methods are experimented with majority and plurality voting on a telephony Persian speech-enabled IVR system. Results of evaluations present two promising outcomes, first, it improves the performance considerably; second, it enables us to detect the outputs with low confidence.",2008,0,
2456,2457,MPE-IFEC: An enhanced burst error protection for DVB-SH systems,"Digital video broadcasting-satellite services to handhelds (DVB-SH) is a new hybrid satellite/terrestrial system for the broadcasting of multimedia services to mobile receivers. To improve the link budget, DVB-SH uses a long interleaver to cope with land mobile satellite (LMS) channel impairments. Multi-protocol encapsulation-inter-burst forward error correction (MPE-IFEC) is an attractive alternative to the long physical interleaving option of the standard and is suited for terminal receivers with limited de-interleaving memory. In this paper, we present a tutorial overview of this powerful error-correcting technique and report new simulation results that show MPE-IFEC improves the quality of broadcast mobile television (TV) reception.",2009,0,
2457,2458,Rigorous geometric modeling and correction of QuickBird imagery,"This paper presents a quantitative evaluation of the geometric accuracy that can be achieved with QuickBird imagery using the metadata provided with DigitalGlobe's image products. We explore two geometric models: the rational function model, and a rigorous sensor model based on camera, attitude, and ephemeris data included in the product metadata. We assess the rational function model against the rigorous sensor model, showing that the two models provide comparable geometric accuracy. We then assess the absolute geometric accuracy of a sample set of QuickBird products, measuring both the systematic geometric accuracy, and demonstrating the improvements which can be achieved using a full photogrammetric block adjustment.",2003,0,
2458,2459,Haloperidol Impairs Learning and Error-related Negativity in Humans,"Humans are able to monitor their actions for behavioral conflicts and performance errors. Growing evidence suggests that the error-related negativity (ERN) of the event-related cortical brain potential (ERP) may index the functioning of this response monitoring system and that the ERN may depend on dopaminergic mechanisms. We examined the role of dopamine in ERN and behavioral indices of learning by administering either 3 mg of the dopamine antagonist (DA) haloperidol (<italic>n</italic> = 17); 25 mg of diphenhydramine (<italic>n</italic> = 16), which has a similar CNS profile but without DA properties; or placebo (<italic>n</italic> = 18) in a randomized, double-blind manner to healthy volunteers. Three hours after drug administration, participants performed a go/no-go Continuous Performance Task, the Eriksen Flanker Task, and a learning-dependent Time Estimation Task. Haloperidol significantly attenuated ERN amplitudes recorded during the flanker task, impaired learning of time intervals, and tended to cause more errors of commission, compared to placebo, which did not significantly differ from diphenhydramine. Drugs had no significant effects on the stimulus-locked P1 and N2 ERPs or on behavioral response latencies, but tended to affect post-error reaction time (RT) latencies in opposite ways (haloperidol decreased and diphenhydramine increased RTs). These findings support the hypothesis that the DA system is involved in learning and the generation of the ERN.",2004,0,
2459,2460,Developing auto recipe management system for LCD panel auto defect detecting inspection machine,"Recently, there is an intensive price competition among mass LCD panel makers. to decrease labor costs, many manufacturing processes such as auto defect detecting are getting automated. However, to maintain its optimal performance, user must keep attention to hardware settings and many recipe parameters. These efforts are supposed to be managed constantly as developing new model. In this paper, we introduce automated defect finding algorithm generally used in periodic pattern image, and suggest auto recipe management algorithms and system that help user to devote less effort for maintenance. Three algorithms are developed - auto pitch calculation, auto threshold control, auto light source intensity control. To verify the performance of auto recipe management algorithm, simulation and machine tests are executed with several color filter and TFT pattern image. Through the tests, we verified the performance of developed algorithm and got successful result.",2008,0,
2460,2461,A novel arc-suppression method for grounding fault in wind farm,"Neutral-point non-effectively grounded system is used in the wind farm with complex structure, high cost, wide distribution and higher capacitance current. This paper presents an arc-suppression method for the neutral non-effective grounding system-voltage arc-suppression method. A reactor is directly connected to the fault bus and ground. The voltage of the reactor is used to suppress the voltage of the fault phase, to make sure it become much smaller than the amplitude of electric strength recovery, undermine the arc renewed mechanism, and remove the arc grounding. Simulation result in the paper verifies the validity and advantages of the method and it is not affected by grounding current, eliminating the complex track and compensation calculation. Arc-suppression method is simple. Especially suitable for system with high capacity current,such as cables wind farms.",2010,0,
2461,2462,Effective Fault Injection Model for Variant Network Traffic,"As cyber attacks by the malicious users are increased with vulnerabilities in software, fuzz testing is emerging as an effective way to find out a security bug. Fuzz testing is mainly used in verifying the robustness of software by injecting the random or semi-valid data to areas such as network port, API and user interface. In fuzz testing of network software, the repeated transmission of packet is necessary and all network fuzz tools are depending on the recording scheme of packets for it. The characteristic causes a big overhead in the situation that network traffic is variant in doing the same task. This paper identifies four disadvantages of the general network fuzzer with the packet recording and replaying scheme. Their most expensive cost is to code a routine to handle the variant traffic of each same upcoming communication. By proposing fuzz model to inject the fault into the packet at the real-time, we address the weakness in the existing network fuzz tools. Last, we experiment the implemented tool, named RINF, against Windows RPC based service, and show that it works effectively comparing with the exiting.",2007,0,
2462,2463,Machine learning techniques for ocular errors analysis,"The conventional techniques for refractive error measurements (myopia, hypermetropia, and astigmatism) have been considered inadequate for several optometry researches. In this context, they have investigated alternative methodologies for refractive error measurement. A new strategy is the determination of refractive errors from images of the globe of the eye. A process named Hartmann-Shack can obtain these images. The HS images should be analysed in order to extract relevant information for identification of refractive errors. The present paper investigates a technique based on radial basis functions (RBFs), an artificial neural network (ANN), and on support vector machines (SVMs), which automatically performs analysis of images from the globe of the eye and identifies refractive errors. The most relevant data of these images are extracted using Gabor wavelets transform, and then these machine learning techniques carry out the image analysis",2004,0,
2463,2464,Reliability and Fault Tolerance in Trust,"The ubiquity of information systems has made correct and reliable operation of critical systems indispensable. The trustworthiness of digital systems is increasingly dependent on the trustworthiness of the software. While hardware trustworthiness is by no means a solved problem, system-wide problems are increasingly blamed on poorly tested, defective software. System trustworthiness is therefore a combination of several key software attributes: reliability, safety, security, availability, performance, fault-tolerance, and privacy. Some of these attributes can be directly measured, some cannot. For example performance and availability can be numerically measured; safety and security cannot. Further, several of these attributes may conflict, such as security and performance. Therefore to demonstrate that the software of a system can be trusted, it requires a combination of qualitative arguments concerning the level achieved for some attributes in combination with the numerical (quantitative) scores measured for others. In order to understand the trustworthiness and security of a software system, we first need to understand its reliability and fault tolerance",2006,0,
2464,2465,A New Approach to Ungrounded Fault Location in a Three-Phase Underground Distribution System using Combined Neural Networks & Wavelet Analysis,"This paper presents the results of investigations into a new fault location technique based on a new modified cable model, in the EMTP software. The simulated data is then analysed using advanced signal processing technique based on wavelet analysis to extract useful information from signals and this is then applied to the artificial neural networks (ANNs) for locating ungrounded shunt faults in a practical underground distribution system. The paper concludes by comprehensively evaluation the performance of the technique developed in the case of ungrounded short circuit faults. The results indicate that the fault location technique has an acceptable accuracy under a whole variety of different systems and fault conditions.",2006,0,
2465,2466,Evaluating the Use of Reference Run Models in Fault Injection Analysis,"Fault injection (FI) has been shown to be an effective approach to assessing the dependability of software systems. To determine the impact of faults injected during FI, a given oracle is needed. Oracles can take a variety of forms, including (i) specifications, (ii) error detection mechanisms and (iii) golden runs. Focusing on golden runs, in this paper we show that there are classes of software which a golden run based approach can not be used to analyse. Specifically, we demonstrate that a golden run based approach can not be used in the analysis of systems which employ a main control loop with an irregular period. Further, we show how a simple model, which has been refined using FI experiments, can be employed as an oracle in the analysis of such a system.",2009,0,
2466,2467,Analysis of a multi-layer fault-tolerant COTS architecture for deep space missions,"Fault-tolerant systems are traditionally divided into fault containment regions and custom logic is added to ensure the effects of a fault within a containment region would not propagate to the other regions. This technique may not be applicable in a commercial-off-the-shelf (COTS) based system. While COTS technology is attractive due to its low cost, they are not developed with the same level of rigorous fault tolerance in mind. Furthermore, COTS suppliers usually have no interest to add any overhead or sacrifice performance to implement fault tolerance for a narrow market of high reliability applications. To overcome this shortcoming, Jet Propulsion Laboratory (JPL) has developed a multi-layer fault protection methodology to achieve high reliability in COTS-based avionics systems. This methodology has been applied to the bus architecture that uses the COTS bus interface standards IEEE 1394 and I<sup>2</sup>C. The paper first gives an overview of the multi-layer fault-protection design methodology for COTS based mission-critical systems. Then the effectiveness of the methodology is analyzed in terms of coverage and cost. The results are compared to the traditional custom designed system",2000,0,
2467,2468,"""ITRS test challenges need defect based test: fact or fiction?""","An important distinction between traditional ""logical fault model"" based testing and defect based testing is the potential for the latter to better handle emerging defect types and changing circuit sensitivities in VDSM circuits. ITRS gives specific examples of emerging defects including the potential for more particle-related blocked-etch resistive opens that result from the change from a subtractive aluminum process to damascene Cu. A second example derives from aggressive scaling into the nanometer domain which increases the probability of incomplete etch and the occurrence of resistive vias.",2004,0,
2468,2469,Analog Circuit Fault Simulation Based on Saber,"Although analog circuit simulation tool like Saber software are numerous, however, it is lack of software which can simulate analog circuits affected by fault modes. Research in the fields of analog circuit fault simulation has not achieved the same degree of success as digital circuits, because of the difficulty in modeling the more complex analog behavior. This article presents a new approach to this problem by simulating the good and fault circuits in Saber and introduces the general circuit fault simulation process. In view of failure mechanism under the components, some novel approaches for fault modeling are proposed. Fault injection and simulation interface based on Saber are detailed in this paper. This method is verified by an example and the actual engineering value is indicated.",2010,0,
2469,2470,Coverage method for FPGA fault logic blocks by spares,"A fault coverage method for digital system-on-chip by means of traversal the logic block matrix to repair the FPGA components is proposed. A method enables to obtain the solution in the form of quasioptimal coverage for all faulty blocks by minimum number of spare tiles. A choice one of two traversal strategies for rows or columns of a logic block matrix on the basis of the structurization criteria, which determine a number of faulty blocks, reduced to the unit modified matrix of rows or columns is realized.",2010,0,
2470,2471,QoS-constrained Fault-tolerant Routing in MANETs based on Segment-Backup Paths,"In the context of mobile ad hoc networks (MANETs), we consider the problem of identifying (a) an optimal primary path which satisfies the required QoS constraints, and (b) a set of alternate paths that may be used in case a link or a node on the primary path fails. The alternate paths are also required to satisfy the same set of QoS constraints as is the case with the primary path. This methodology ensures two things: (a) when there is no link or node failure the traffic moves along the preferred optimal route, but (b) if there is link or node failure the traffic will be instantly re-routed along a route that continues to satisfy the same QoS constraints, although it may experience some performance degradation. In the paper, we have proposed that the traffic be re-routed along a sub-path that by-passes a segment of the primary path that contains the failed link or node. The identification of the segments is not fixed a priori but is determined based on (a) availability of alternate paths, and (b) so that QoS constraints are met. This approach ensures that if connectivity between a given pair of nodes is rich enough then for any primary path one can always find alternate paths so as to address the problem of link or node failure. This flexibility in identifying the segments can also be used to ensure that the delay in switching traffic over to an alternate path, and the resulting packet loss, are bounded. We have described a protocol to identify (a) a primary path, (b) the collections of segments, and (c) the corresponding set of alternate paths, one for each segment, each of which satisfies specified QoS constraints, and so that the delay in switching traffic over to an alternate path is bounded",2006,0,
2471,2472,A quantitative study of firewall configuration errors,"The protection that firewalls provide is only as good as the policy they are configured to implement. Analysis of real configuration data show that corporate firewalls are often enforcing rule sets that violate well established security guidelines. Firewalls are the cornerstone of corporate intranet security. Once a company acquires a firewall, a systems administrator must configure and manage it according to a security policy that meets the company's needs. Configuration is a crucial task, probably the most important factor in the security a firewall provides.",2004,0,
2472,2473,An emotional decision making help provision approach to distributed fault tolerance in MAS,"Fault is inevitable especially in MASS (multi-agent system) because of their distributed nature. This paper introduces a new approach for fault tolerance using help provision and emotional decision-making. Tasks that are split into criticality-assigned real-time subtasks according to their precedence graph are distributed among specialized agents with different skills. If a fault occurs for an agent in a way that it cannot continue its task, the agent requests help from the others with the same skill to redo or continue its task. Requested agents would help the faulty agent based on their nervousness on their own tasks compared to his task. It Is also possible for an agent to discover death of another agent, which has accepted one of his tasks, by polling. An implementation using JADE platform is presented in this paper and the results are reported.",2004,0,
2473,2474,From High Availability Systems to Fault Tolerant Production Infrastructures,"Disaster Recovery Infrastructures, which become an important part of all major IT infrastructures, are usually implemented in a stand-by mode, anticipating a disaster to justify their existence. This paper suggests the transformation of existing High Availability Standby Systems to fully Fault Tolerant Production Infrastructures in order to increase productivity, effectiveness and availability. The most important differences of the two approaches are presented and a transformation strategy that brings together technical and managerial points of view is suggested.",2007,0,
2474,2475,A Fault-Tolerant Communication Algorithm of ARINC429 Based on Hybrid Redundancy,"This paper presents a hybrid redundancy algorithm to improve the fault-tolerance of ARINC429, which synthesize hardware redundancy with dual channels in physical layer and information redundancy with Cycle Redundancy Check (CRC) in data link layer. The concept of failover delay is introduced to analyze the capability of error detection in this algorithm. Validations of this algorithm is obtained through the method called fault injection, which is proved that the algorithm presented here can reduce failover delay to 18 ms in the practical system, and prevent any undetected error with even number of error bits caused by parity check thoroughly.",2009,0,
2475,2476,Fault recovery based on checkpointing for hard real-time embedded systems,"Safety-critical embedded systems often operate in harsh environmental conditions that necessitate fault-tolerant computing techniques. Many safety-critical systems also execute real-time applications. The correctness of these systems depends not only on the logical result of computation, but also on the time at which the results are produced. The missing of task deadlines can therefore be viewed as a temporal fault. In this paper, we examine fault-recovery based on checkpointing for real-time systems. We present schedulability tests for check-pointing in real-time systems. These feasibility-of-scheduling tests provide the criteria under which checkpointing can provide fault tolerance and real-time guarantees for hard real-time embedded systems under two different fault arrival models.",2003,0,
2476,2477,Fault detection method for the subcircuits of a cascade linear circuit,"The fault detection method for the subcircuits of a cascade linear circuit is discussed. While there is any fault (either hard or soft and either single or multiple) at one subcircuit of a cascade linear circuit, it can be quickly detected by using the method proposed. While there are faults simultaneously existing at multiple subcircuits, they can generally be detected by the searching approach proposed here. The aforementioned method is the continuation and development of the unified fault detection dictionary method for linear circuits proposed previously by the authors (see ibid., vol. 46, Oct. 1999)",2000,0,
2477,2478,Probabilistic approaches to fault detection in networked discrete event systems,"In this paper, we consider distributed systems that can be modeled as finite state machines with known behavior under fault-free conditions, and we study the detection of a general class of faults that manifest themselves as permanent changes in the next-state transition functionality of the system. This scenario could arise in a variety of situations encountered in communication networks, including faults occurred due to design or implementation errors during the execution of communication protocols. In our approach, fault diagnosis is performed by an external observer/diagnoser that functions as a finite state machine and which has access to the input sequence applied to the system but has only limited access to the system state or output. In particular, we assume that the observer/diagnoser is only able to obtain partial information regarding the state of the given system at intermittent time intervals that are determined by certain synchronizing conditions between the system and the observer/diagnoser. By adopting a probabilistic framework, we analyze ways to optimally choose these synchronizing conditions and develop adaptive strategies that achieve a low probability of aliasing, i.e., a low probability that the external observer/diagnoser incorrectly declares the system as fault-free. An application of these ideas in the context of protocol testing/classification is provided as an example.",2005,0,
2478,2479,Research of air-drive AMT fault diagnostic system based on models,"In this paper, a research about air-drive AMT (automatic mechanical transmission) fault diagnostic system is made. The redundancy analysis method and action analysis method are used to detect the failures in the AMT system and a fault diagnostic system of air-drive AMT in heavy commercial vehicle is developed by model based design which is very adapt to the development of automotive control system. The fault diagnostic models of sensors and execute components in AMT system are successfully built in Matlab/Simulink. The fault diagnostic models are implemented through a RCP (rapid control prototyping) experiment based on dSPACE.",2008,0,
2479,2480,"<formula formulatype=""inline""> <img src=""/images/tex/18913.gif"" alt=""{K}""> </formula>-Space and Image-Space Combination for Motion-Induced Phase-Error Correction in Self-Navigated Multicoil Multishot DWI","Motion during diffusion encodings leads to different phase errors in different shots of multishot diffusion-weighted acquisitions. Phase error incoherence among shots results in undesired signal cancellation when data from all shots are combined. Motion-induced phase error correction for multishot diffusion-weighted imaging (DWI) has been studied extensively and there exist multiple phase error correction algorithms. A commonly used correction method is the direct phase subtraction (DPS). DPS, however, can suffer from incomplete phase error correction due to the aliasing of the phase errors in the high spatial resolution phases. Furthermore, improper sampling density compensation is also a possible issue of DPS. Recently, motion-induced phase error correction was incorporated in the conjugate gradient (CG) image reconstruction procedure to get a nonlinear phase correction method that is also applicable to parallel DWI. Although the CG method overcomes the issues of DPS, its computational requirement is high. Further, CG restricts to sensitivity encoding (SENSE) for parallel reconstruction. In this paper, a new time-efficient and flexible <i>k</i>-space and image-space combination (KICT) algorithm for rigid body motion-induced phase error correction is introduced. KICT estimates the motion-induced phase errors in image space using the self-navigated capability of the variable density spiral trajectory. The correction is then performed in <i>k</i> -space. The algorithm is shown to overcome the problem of aliased phase errors. Further, the algorithm preserves the phase of the imaging object and receiver coils in the corrected <i>k</i> -space data, which is important for parallel imaging applications. After phase error correction, any parallel reconstruction method can be used. The KICT algorithm is tested with both simulated and in vivo data with both multishot single-coil and multishot multicoil acquisitions. We show that KICT correction results in diffusion-weighted- images with higher signal-to-noise ratio (SNR) and fractional anisotropy (FA) maps with better resolved fiber tracts as compared to DPS. In peripheral-gated acquisitions, KICT is comparable to the CG method.",2009,0,
2480,2481,A new fault-tolerant technique for improving schedulability in multiprocessor real-time systems,"In real-time systems, tasks have deadlines to be met despite the presence of faults. Primary-Backup (PB) scheme is one of the most common schemes that has been employed for fault-tolerant scheduling of real-time tasks, wherein each task has two versions and the versions are scheduled on two different processors with time exclusion. There have been techniques proposed for improving schedulability of the PB-based scheduling. One of the more popular ones include Backup-Backup (BB) overloading, wherein two or more backups can share/overlap in time on a processor. In this paper we propose a new schedulability enhancing technique, called primary-backup (PB) overloading, in which the primary of a task can share/overlap in time with the backup of another task an a processor. The intuition is that, for both primary and backup of a task, the PB-overloading can assign an earlier start time than that of the BB-overloading, thereby increasing the schedulability. We conduct schedulability and reliability analysis of PB- and BB-overloading techniques through simulation and analytical studies. Our studies show that PB-overloading offers better schedulability (25% increase in the guarantee ratio) than that of BB-overloading, and offers reliability comparable to that of BB-overloading. The proposed PB-overloading is a general technique that can be employed in any static or dynamic fault-tolerant scheduling algorithm",2001,0,
2481,2482,A forward error recovery technique for real-time MPEG-2 video transport and its performance over wireless IEEE 802.11 LAN,"Real-time MPEG-2 video transport applications do not usually have the luxury of a reverse channel for recovering from any errors that might occur during communication. Degradation in quality of decoded video frames is immediately apparent in the presence of errors in headers. In this paper, we focus on protecting header information by replicating it in any free space that might be available in the defined MPEG-2 transport stream packets. We also present our implementation experience over wired ATM as well as wireless IEEE 802.11 LAN by incorporating this forward error recovery approach with a real-time MPEG-2 encoder. In our experiments, it is found that the free space available is generally more than adequate for replicating essential header information",2000,0,
2482,2483,Event-orthogonal error-insensitive multiple fault detection with cascade correlation network,"This paper presents the design of a fault detection system with cascade correlation network (CCN) for a power system. Associate fault components with the states of protective devices would form symptomatic patterns to create training data. The proposed method makes use of information from both the primary and back-up devices involving single fault, multiple faults, data communication with errors, or fault with the failed operation of relays and circuit breakers. With a sample power system, computer simulations were conducted to show the effectiveness of the proposed system.",2003,0,
2483,2484,Fine-Grained Fault Tolerance for Process Variation-Aware Caches,"Continuous scaling in CMOS fabrication process makes circuits more vulnerable to process variations, which results in variable delay, malfunctioning, and/or leaky circuits. Caches are one of the biggest victims of process variations due to their large sizes and minimal cell features. To mitigate the impacts of process variations on caches, we propose to localize the effects of process variations at a word level, not at the conventional cache set, cache way, or cache line level. Faulty words are disabled or shut down completely and accesses to those words are bypassed to a small set of word-length buffers. This technique is shown to be effective in reducing performance penalty due to process variations and in increasing the parametric yield up to 90% when subjected to the performance constraints.",2010,0,
2484,2485,Topographic Correction for ALOS PALSAR Interferometry,"L-band synthetic aperture radar (SAR) interferometry is very successful for mapping ground deformation in densely vegetated regions. However, due to its larger wavelength, the capacity to detect slow deformation over a short period of time is limited. Stacking and small baseline subset (SBAS) techniques are routinely used to produce time series of deformation and average deformation rates by reducing the contribution of topographic and atmospheric noise. For large sets of images that are presently available from C-band European Remote Sensing Satellites (ERS-1/2) and Environmental Satellite (ENVISAT), the standard stacking and SBAS algorithms are accurate. However, the same algorithms are often inaccurate when used for processing of interferograms from L-band Advanced Land Observing Satellite Phased Array type L-band SAR (ALOS PALSAR). This happens because only a limited number of interferograms is acquired and also because of large spatial baselines often correlated with the time of acquisition. In this paper two techniques are suggested that can be used for removing the residual topographic component from stacking and SBAS results, thereby increasing their accuracy.",2010,0,
2485,2486,A cascaded correction method to reduce the contamination of ionospheric frequency modulation for HF skywave radars,"From the presented simulations and others not shown here, we conclude that the proposed cascaded correction method provides an efficient way to reduce the contamination of ionospheric frequency modulation for the HF skywave radar even in a serious case where the contaminated Bragg lines in Doppler domain overlap. The idea of the cascaded correction also implies the possibility that we can achieve correction gain from integration of the advantages of different correction methods.",2009,0,
2486,2487,Knowledge Oriented Network Fault Resolution Method Based on Active Information Resource,"To reduce the loads imposed on network administrators, we have proposed AIR-NMS, which is a network management support system (NMS) based on Active Information Resource (AIR). In AIR-NMS, various information resources (e.g., state information of a network, practical knowledge of network management) are combined with software agents which have the knowledge and functions for supporting the utilization of the resources, and thus individual resources are given activities as AIRs. Through the organization and cooperation of AIRs, AIR-NMS provides the administrators with practical measures against a wide range of network faults. To make AIR-NMS fit for practical use, this paper proposes a method for achieving the effective installation and utilization of the network management knowledge needed in AIR-NMS.",2010,0,
2487,2488,COMTRADE-Based Fault Information System for TNB Substations,"This paper describes the process of extracting the fault information from common format for transient data exchange (COMTRADE) record. The COMTRADE format record was obtained from the distance relay recording device, provided by the Protection Department of Tenaga Nasional Berhad (TNB). A graphical user interface (GUI) simulation program using MATLAB was developed which implements the one-cycle cosine filter relaying algorithm to digitally filter out the unwanted signals at system's fundamental frequency from the COMTRADE record. This work aims to help the TNB protection engineer to have a faster solution in parameter extraction from protective relay recorded from substations in COMTRADE format.",2005,0,
2488,2489,Fault tolerant H<inf></inf> control for a class of nonlinear discrete-time systems: Using sum of squares optimization,"This paper studies the fault tolerant control (FTC) problem for a class of nonlinear discrete-time systems with guaranteed H<sub>infin</sub> performance objective in the presence of actuator faults. The mode of faults under consideration is typical aberration of actuator effectiveness. The novelty of this paper is that the effect of the nonlinear terms is described as an index in order to transform the FTC design problem into a semi-definite programming (SDP). The proposed optimization approach is to find zero optimum for this index. Combined with H<sub>infin</sub> performance index, the conceived multi-objective optimization problem is solved by using sum of squares method (SOS) in a reliable and efficient way. A numerical example is included to verify the applicability of this new approach for the nonlinear FTC synthesis.",2008,0,
2489,2490,A Robustness Approach for Handling Modeling Errors in Parallel-Plate Electrostatic MEMS Control,"This paper addresses the control of electrostatic parallel-plate microactuators in the presence of such modeling errors as unmodeled fringing field effect and deformations. In general, accurate descriptions of these phenomena often lead to very complicated mathematical models, while ignoring them may result in significant performance degradation. In this paper, it is shown by finite-element-method-based simulations that the capacitance due to fringing field effect and deformations can be compensated by introducing a variable serial capacitor. When a suitable robust controller is used, the full knowledge of the introduced serial capacitor is not required, but merely its boundaries of variation. Based on this model, a robust control scheme is derived using the theory of input-to-state stability combined with backstepping state feedback design. Since the full state measurement may not be available under practical operational conditions, an output feedback control scheme is developed. The stability and performance of the system using the proposed control schemes are demonstrated through both stability analysis and numerical simulation. The present approach allows the loosening of the stringent requirements on modeling accuracy without compromising the performance of control systems.",2008,0,
2490,2491,Tropospheric heterogeneities corrections in differential radar interferometry,"Differential radar interferometry (DInSAR) has been used more and more widely to monitor crustal deformations due to underground mining and oil extraction, earthquakes, volcanoes, landslides, and so on. However, tropospheric heterogeneities have been identified as one of the major errors in DInSAR, which can be up to 40 cm as derived from dual-frequency GPS measurements in the example given in this paper. Therefore, it is crucial to correct the tropospheric heterogeneities in the DInSAR results for monitoring crustal deformation. These corrections from several GPS stations in the radar imaging area can be interpolated and applied to the DInSAR results. The discussions are based on data from the Tower Colliery test site southwest Sydney, Australia.",2002,0,
2491,2492,Concepts and methods in fault-tolerant control,"Faults in automated processes will often cause undesired reactions and shut-down of a controlled plant, and the consequences could be damage to technical parts of the plant, to personnel or the environment. Fault-tolerant control combines diagnosis with control methods to handle faults in an intelligent way. The aim is to prevent that simple faults develop into serious failure and hence increase plant availability and reduce the risk of safety hazards. Fault-tolerant control merges several disciplines into a common framework to achieve these goals. The desired features are obtained through online fault diagnosis, automatic condition assessment and calculation of appropriate remedial actions to avoid certain consequences of a fault. The envelope of the possible remedial actions is very wide. Sometimes, simple re-tuning can suffice. In other cases, accommodation of the fault could be achieved by replacing a measurement from a faulty sensor by an estimate. In yet other situations, complex reconfiguration or online controller redesign is required. This paper gives an overview of recent tools to analyze and explore structure and other fundamental properties of an automated system such that any inherent redundancy in the controlled process can be fully utilized to maintain availability, even though faults may occur",2001,0,
2492,2493,Motion Estimation of Vortical Blood Flow Within the Right Atrium in a Patient with Atrial Septal Defect,"Patients with an atrial septal defect (ASD) have a left to right shunt with associated complications. Currently, various imaging modalities, including echocardiography and invasive cardiac catheterization, are utilized in the management of these patients. Cardiac magnetic resonance (CMR) imaging provides a novel and non-invasive approach for imaging patients with ASDs. A study of vortices generated within the right atrium (RA) during the diastolic phase of the cardiac cycle can provide useful information on the change in the magnitude of vorticity pre-and post-ASD closure. The motion estimation of blood applied to CMR is performed. In this study we present, a two dimensional (2D) visualization of in-vivo right atrial flow. This is constructed using flow velocities measured from the intensity shifts of turbulent blood flow regions in MRI. In particular, the flow vortices can be quantified and measured, against controls and patients with ASD, to extend medical knowledge of septal defects and their haemodynamic effects.",2007,0,
2493,2494,Combined solution for fault location in three-terminal lines based on wavelet transforms,"This work presents the study and development of a combined fault location scheme for three-terminal transmission lines using wavelet transforms (WTs). The methodology is based on the low- and high-frequency components of the transient signals originated from fault situations registered in the terminals of a system. By processing these signals and using the WT, it is possible to determine the time of travelling waves of voltages and/or currents from the fault point to the terminals, as well as estimate the fundamental frequency components. A new approach presents a reliable and accurate fault location scheme combining some different solutions. The main idea is to have a decision routine in order to select which method should be used in each situation presented to the algorithm. The combined algorithm was tested for different fault conditions by simulations using the ATP (Alternative Transients Program) software. The results obtained are promising and demonstrate a highly satisfactory degree of accuracy and reliability of the proposed method.",2010,0,
2494,2495,Switching-based fault-tolerant control for an F-16 aircraft with thrust vectoring,"Thrust vectoring technique enables aircraft to perform various maneuvers not available to conventional-engined planes. This paper presents an application of switching control concepts to fault-tolerant control design for an F-16 aircraft model augmented with thrust vectoring. Two controllers are synthesized using a switching logic, and they are switched on a fault parameter. During normal flight conditions, the F-16 aircraft relies on no vectored thrust and the elevator. The thrust vectoring nozzle is only turned on in the presence of elevator failures. Two elevator fault scenarios, lock and loss of effectiveness, are considered. Nonlinear simulation results show that the switching control can guarantee the stability and performance of the faulted system.",2009,0,
2495,2496,Distributed control and communication fault tolerance for the CKBot,"In this paper we present a method for distributed fault tolerance in a modular robotic system. We describe an implementation of this method on the CKBot system. In particular, we broadcast infrared (IR) signals to modules which collaboratively vote on a majority course of action. Various gait selections for a 7 module caterpillar and a 16 module quadruped with a faulty subset of IR receivers have been verified to demonstrate the algorithm's robustness. We conclude the paper with a discussion of modes of fault tolerances and this method's applicability to other modular robotic systems.",2009,0,
2496,2497,An approach for analyzing and correcting spelling errors for non-native Arabic learners,"Spellcheckers are widely used in many software products for identifying errors in users' writings. However, they are not designed to address spelling errors made by non-native learners of a language. As a matter of fact, spelling errors made by non-native learners are more than just misspellings. Non-native learners' errors require special handling in terms of detection and correction, especially when it comes to morphologically rich languages such as Arabic, which have few related resources. In this paper, we address common error patterns made by non-native Arabic learners and suggest a two-layer spell-checking approach, including spelling error detection and correction. The proposed error detection mechanism is applied on top of Buckwalter's Arabic morphological analyzer in order to demonstrate the capability of our approach in detecting possible spelling errors. The correction mechanism adopts a rule-based edit distance algorithm. Rules are designed in accordance with common spelling error patterns made by Arabic learners. Error correction uses a multiple filtering mechanism to propose final corrections. The approach utilizes semantic information given in exercising questions in order to achieve highly accurate detection and correction of spelling errors made by non-native Arabic learners. Finally, the proposed approach was evaluated using real test data and promising results were achieved.",2010,0,
2497,2498,Intelligent Fault Isolation of Control Valves in a Power Plant,"In this paper fault happening for control valves in a power plant is considered. The method is implemented for boiler feed water valve and is extendable to boiler fuel valve and governor valve. Fault kind and location is determined using measurement of boiler state variables. In spite of robust controller design, it may be unable to handle these faults and restricts their effects. A fuzzy compensator does fault accommodation tasks prosperously with modification set-points to the controller. Fuzzy supervisor is implemented in a way which prompts simulation time related to the current implements. Simulation results show the effectiveness of the proposed methodology",2006,0,
2498,2499,Optimizing the fault tolerance capabilities of distributed real-time systems,"Industrial real-time systems typically have to satisfy complex requirements, mapped to the task attributes, eventually guaranteed by a fixed priority scheduler in a distributed environment. These systems consist of a mix of hard and soft tasks with varying criticality, as well as associated fault tolerance requirements. Time redundancy techniques are often preferred in industrial applications and, hence, it is extremely important to devise resource efficient methodologies for scheduling real-time tasks under failure assumptions. In this paper, we propose a methodology to provide a priori guarantees in distributed real-time systems with redundancy requirements. We do so by identifying temporal feasibility windows for all task executions and re-executions, as well as allocating them on different processing nodes. We then use optimization theory to derive the optimal feasibility windows that maximize the utilization on each node, while avoiding overloads. Finally on each node, we use integer linear programming (ILP) to derive fixed priority task attributes that guarantee the task executions within the derived feasibility windows, while keeping the associated costs minimized.",2009,0,
2499,2500,Use of invariant properties to evaluate the results of fault-injection-based robustness testing of protocol implementations,"Robustness testing has as main objective to determine how a system behaves in the presence of unexpected inputs or stressful environmental conditions. An approach commonly used for that purpose is fault injection, in which faults are deliberately injected into a system to observe its behavior. One main limitation of this approach is results evaluation: a system is considered as robust if it does not crash or hang during testing. This is not enough because a system can still continue to execute, but present a wrong behavior. To overcome this limitation, we propose a passive approach for robustness testing, in which the system under test is instrumented for fault injection during runtime, as well as for monitoring its behavior. At the end, the readouts collected are analyzed to determine whether the observed behavior under faults is consistent with properties based on a finite state model of the system. We illustrate the approach using an implementation of the wireless application protocol (WAP). The approach was implemented using off-the-shelf tools; results obtained thus far are presented.",2008,0,
2500,2501,On effective use of reliability models and defect data in software development,"In software technology today, several development methodologies such as extreme programming and open source development increasingly use feedback from customer testing. This makes the customer defect data become more readily available. This paper proposes an effective use of reliability models and defect data to help managers make software release decisions by applying a strategy for selecting a suitable reliability model, which best fits the customer defect data as testing progresses. We validate the proposed approach in an empirical study using a dataset of defect reports obtained from testing of three releases of a large medical system. The paper describes detailed results of our experiments and concludes with suggested guidelines on the usage of reliability models and defect data.",2006,0,
2501,2502,Hardware Fault Tolerance implemented in software at the compiler level with special emphasis on array-variable protection,"Advanced and sophisticated microprocessor-based systems are often applied in safety or mission critical subsystems. The problem of designing radiation-tolerant devices becomes very important, especially in places such as accelerators and synchrotrons, where the results of the experiments depend on the reliability of control mechanisms. In this paper, we propose a new technique for safe and reliable computing in the presence of radiation-induced errors. In our solution, Software Implemented Hardware Fault Tolerance (SIHFT) algorithms are implemented automatically during the compilation process. This approach makes it possible to use standard optimization algorithms during the compilation. In addition, a responsibility for implementing fault tolerance is transferred to the compiler and it is transparent to the programmers. Special emphasis has been placed on the array protection algorithm.",2008,0,
2502,2503,Fault tolerant control of spacecraft in the presence of sensor bias,"We develop a stable scheme for bias estimation in the case of attitude tracking. The scheme is based on the design of nonlinear observers for unknown bias identification and state estimation. In the case of gyro bias, our nonlinear observer design, based on the quaternion dynamics, leads to an error model and adjustment laws that result in guaranteed convergence of the unknown bias estimates to their true values. We demonstrate that our scheme results in a stable overall system, and achieves highly accurate pointing in the presence of unknown sensor bias. The properties of the proposed scheme are evaluated through simulations using a generic spacecraft model",2000,0,
2503,2504,Fault diagnosis with Coloured Petri Nets using Latent Nestling Method,"This paper presents a new methodology for permanent and intermittent fault diagnosis, named Faults Latent Nestling Method (FLNM), using Coloured Petri Nets (CPNs). CPNs and FLNM method allow for an enhanced capability for synthesis and modelling in contrast to the classical phenomena of combinational state explosion when using Finite State Machine based methods.",2008,0,
2504,2505,Voltage sags pattern recognition technique for fault section identification in distribution networks,"This paper presents a method to identify a faulted section in a distribution network using voltage sags pattern characteristics. The method starts with fault analysis to establish analytical voltage sags database. When a fault occurs, the voltage sag at the monitored node is compared with the established voltage sags in the database to find all the possible faulted sections. Finally, the method applied rank reasoning analysis to prioritize all the possible faulted sections. The method has been tested on an urban distribution network feeder. The results show that the most fault sections in the tested distributed network feeder can be located by the first attempt. All remaining faulted sections can be found by the second attempt.",2009,0,
2505,2506,Concurrent error detection in wavelet lifting transforms,"Wavelet transforms, central to multiresolution signal analysis and important in the JPEG2000 image compression standard, are quite susceptible to computer-induced errors because of their pipelined structure and multirate processing requirements. Such errors emanate from computer hardware, software bugs, or radiation effects from the surrounding environment. Implementations use lifting schemes, which employ update and prediction estimation stages, and can spread a single numerical error caused by failures to many output transform coefficients without any features to warn data users. We propose an efficient method to detect the arithmetic errors using weighted sums of the wavelet coefficients at the output compared with an equivalent parity value derived from the input data. Two parity values may straddle a complete multistage transform or several values may be used, each pair covering a single stage. There is greater error-detecting capability at only a slight increase in complexity when parity pairs are interspersed between stages. With the parity weighting design scheme, a single error introduced at a lifting section can be detected. The parity computation operation is properly viewed as an inner product between weighting values and the data, motivating the use of dual space functionals related to the error gain matrices. The parity weighting values are generated by a combination of dual space functionals. An iterative procedure for evaluating the design of the parity weights has been incorporated in Matlab code and simulation results are presented.",2004,0,
2506,2507,Faulted circuit indicators and system reliability,"Faulted circuit indicators (FCIs) can be a useful tool for improving power system reliability. While FCIs alone may not prevent outages or other problems with system reliability, FCI application can help identify problem areas of the electric distribution system, as well as reduce crew patrol time in locating faulted cables, thus reducing outage duration. The use of FCIs can be linked to outage statistics such as the system average interruption duration index (SAIDI) and customer average interruption duration index (CAIDI), to track reduction in outage duration. By spending less time identifying and locating faulted cables, crew productivity can also be improved through the use of FCIs, allowing more time to be spent on productive system operation and improvement. In addition to real cost savings and increased productivity for utilities, reduced outage duration through FCI use can lead to increased customer satisfaction. For commercial and industrial customers, who can incur significant costs in lost production due to power interruptions, minimizing outage duration can lead to a reduction in customer costs resulting from outages. As innovations in rate design lead to more performance based ratemaking, FCIs can help utilities avoid paying penalties by keeping certain service quality measurements within acceptable levels. Technology advancements in recent years have led to more reliable fault indicators, along with new and unique features that help ensure the most efficient application of faulted circuit indicators for improving system reliability",2000,0,
2507,2508,Improving RFID Read Rate Reliability by a Systematic Error Detection Approach,"Reliability, security and privacy are the key concerns with RFID (radio frequency identification) adoption. While the mainstream RFID research is focused on solving the security and privacy issues, this paper focuses on addressing the reliability issues in general and detecting read rate failures in particular. We specifically consider the issue of detecting if some RFID tags are not read at all, and if the tags are not read an alarm should be activated. This is quite different from the main stream RFID reliability research which attempts to increase the read rate by developing new and powerful antennas or improving the surrounding environment. To address this issue, we present a novel solution which can detect missed readings and notify appropriate entity to take suitable action against it. The novelty of the proposed solution lies in the combined use of RFID reader along with a normal weighing machine. The concept is to compare the gross weight of the tagged items against the gross weight (of the same items) stored in a backend database. The backed database can only be accessed for those RFID tags which are properly read. If some tags are not read at all these weights would vary and hence incorrect readings could be identified. This paper provides the detailed theoretical foundation for the proposed solution. In addition we compare the proposed solution against existing solutions to demonstrate the success and potential of our solution for practical deployment of RFID in library or supermarket scenario.",2007,0,
2508,2509,A Distributed Approach to Autonomous Fault Treatment in Spread,"This paper presents the design and implementation of the distributed autonomous replication management (DARM) framework built on top of the Spread group communication system. The objective of DARM is to improve the dependability characteristics of systems through a fault treatment mechanism. Unlike many existing fault tolerance frameworks, DARM focuses on deployment and operational aspects, where the gain in terms of improved dependability is likely to be the greatest. DARM is novel in that recovery decisions are distributed to each individual group deployed in the system, eliminating the need for a centralized manager with global information about all groups. This scheme allows groups to perform fault treatment on themselves. A group leader in each group is responsible for fault treatment by means of replacing failed group members; the approach also tolerates failure of the group leader. The advantages of the distributed approach is: (i) no need to maintain globally centralized information about all groups which is costly and limits scalability, (ii) reduced infrastructure complexity, and (iii) less communication overhead. We evaluate the approach experimentally to validate its fault handling capability; the recovery performance of a system deployed in a local area network is evaluated. The results show that applications can recover to their initial system configuration in a very short period of time.",2008,0,
2509,2510,Digital correction techniques for accuracy improvement in measurements of SnO<sub>2</sub> sensor impedance,"In this paper, the performance improvement of a gas-sensing system by digital correction techniques is discussed. The considered system operates as a vectorial impedance meter and performs impedance measurements of eight sensors arranged in an array in the frequency range 10 Hz-15 MHz. The measurements of the chemical sensors' impedance is an innovative technique that allows highlighting different adsorption mechanisms taking place when the sensors are exposed to gases. Of course, impedance analyzers are commercially available, but they usually make measurements on only one device at time and they are very expensive. The proposed PC-based impedance analyzer is a versatile one and shows good performances for gas-sensing applications. A digital correction technique is used in this work to improve the impedance measurement accuracy of each channel of the gas-sensing system (eight sensors  eight channels), in order to compensate for the conditioning electronics response. The latter is evaluated in a characterization procedure. A linear black box two-port model is used to take into account crosstalk, amplitude, and phase distortions. Two different techniques to evaluate the response of the measurement system are discussed in this paper, and experimental results are presented on both the measure of reference impedances and on the measure of chemical sensors.",2004,0,
2510,2511,H.264 video communication based refined error concealment schemes,"Error resilience is an essential problem for video communications, such as digital TV broadcasting, mobile video terminals and video telephone. The latest video compression standard H.264/AVC provides more coding efficiency for a wide range of video consumer applications. Yet H.264 video streams are still vulnerable to transmission errors. In this paper, a set of error concealment techniques are proposed to provide error resilience based on new coding and network characteristics of H.264. The temporal concealment involves a method of subblock-based refined motion compensated concealment using weighting boundary match, which improves the ability to deal with high motion activity areas. The spatial concealment scheme involves an algorithm of refined directional weighted spatial interpolation, which could protect object edge integrity. Combining the above algorithms, an adaptive spatial/temporal estimation method with low complexity is presented. Transmission over typical 3GPP/3GPP2 mobile IP channels is simulated with a wide range of bit rate and BER. The refined concealment techniques provide more error robustness for video consumer electronics than those suggested in H.264 without any encoder-side modifications.",2004,0,
2511,2512,Efficient fault-prone software modules selection based on complexity metrics,"In order to improve the software reliability early, this paper proposes an efficient algorithm to select fault-prone software module. Based on software module's complexity metrics, the algorithm uses modified cascaded-correlation algorithm as neural network classifier to select the fault-prone software module. Finally, by analyzing the algorithm's application in the project MAP, the paper shows the advantage of the algorithm.",2009,0,
2512,2513,Defect-Aware High-Level Synthesis Targeted at Reconfigurable Nanofabrics,"Entering the nanometer era, a major challenge to current design methodologies and tools is how to effectively address the high defect densities projected for nanoelectronic technologies. To this end, a reconfiguration-based defect-avoidance methodology for defect-prone nanofabrics was proposed. It judiciously architects the nanofabric, using probabilistic considerations, such that a very large number of alternative implementations can be mapped into it, enabling defects to be circumvented at configuration time, in a scalable way. Building on this foundation, in this paper, a synthesis framework aimed at implementing this new design paradigm is proposed. A key novelty of the approach with respect to traditional high-level synthesis (HLS) is that, rather than carefully optimizing a single (""deterministic"") solution, the goal is to simultaneously synthesize a large family of alternative solutions, so as to meet the required probability of successful configuration, or yield, while maximizing the average performance of the family of synthesized solutions. Experimental results generated for a set of representative benchmark kernels, assuming different defect regimes and target yields, empirically show that the proposed algorithms can effectively explore the complex probabilistic design space associated with this new class of HLS problems",2007,0,
2513,2514,Research of Environment-Oriented Fault Representation for Software-Intensive Equipment,"Faults triggered by variability and inconsistency of running environment emerge in the highest frequency, and are difficult to recognize and diagnose in the software-intensive equipment. By systematically analyzing and summarizing environmental factors which can disable the equipment for work, a kind of fresh fault classification is put forward in this paper. Further, the formal representation based on frame is clearly given to each type of environmental fault in detail. Diagnosis system based on this classification is successful and effective to resolve most of faults in practice.",2010,0,
2514,2515,Spike Timing Precision and Neural Error Correction: Local Behavior,"The effects of spike timing precision and dynamical behavior on error correction in spiking neurons were investigated. Stationary dischargesphase locked, quasiperiodic, or chaoticwere induced in a simulated neuron by presenting pacemaker presynaptic spike trains across a model of a prototypical inhibitory synapse. Reduced timing precision was modeled by jittering presynaptic spike times. Aftereffects of errorsin this communication, missed presynaptic spikeswere determined by comparing postsynaptic spike times between simulations identical except for the presence or absence of errors. Results show that the effects of an error vary greatly depending on the ongoing dynamical behavior. In the case of phase lockings, a high degree of presynaptic spike timing precision can provide significantly faster error recovery. For nonlocked behaviors, isolated missed spikes can have little or no discernible aftereffects (or even serve to paradoxically reduce uncertainty in postsynaptic spike timing), regardless of presynaptic imprecision. This suggests two possible categories of error correction: high-precision locking with rapid recovery and low-precision nonlocked with error immunity.",2005,0,
2515,2516,Simulated SMOS Levels 2 and 3 Products: The Effect of Introducing ARGO Data in the Processing Chain and Its Impact on the Error Induced by the Vicinity of the Coast,"The Soil Moisture and Ocean Salinity (SMOS) Mission is the second of the European Space Agency's Living Planet Program Earth Explorer Opportunity Missions, and it is scheduled for launch in July 2009. Its objective is to provide global and frequent soil-moisture and sea-surface-salinity (SSS) maps. SMOS' single payload is the Microwave Imaging Radiometer by Aperture Synthesis (MIRAS) sensor, an L-band 2-D aperture-synthesis interferometric radiometer. For the SSS, the output products of SMOS, at Level 3, will have global coverage and an accuracy of 0.1-0.4 psu (practical salinity units) over 100 times 100-200 times 200 km<sup>2</sup> in 10-30 days. During the last few years, several studies have pointed out the necessity of combining auxiliary data with the MIRAS-measured brightness temperature to provide the required accuracy. In this paper, we propose and test two techniques to include auxiliary data in the SMOS SSS retrieval algorithm. Aiming at this, pseudo-SMOS Level-3 products have been generated according to the following steps: 1) A North Atlantic configuration of the NEMO-OPA ocean model has been run to provide consistent geophysical parameters; 2) the SMOS end-to-end processor simulator has been used to compute the brightness temperatures as measured by the MIRAS; 3) the SMOS Level-2 processor simulator has been applied to retrieve SSS values for each point and overpass; and 4) Level-2 data have been temporally and spatially averaged to synthesize Level-3 products. In order to assess the impact of the proximity to the coast at Level 3, and the effect of these techniques on it, two different zones have been simulated: the first one in open ocean and the second one in a coastal region, near the Canary Islands (Spain) where SMOS and Aquarius CAL/VAL activities are foreseen. Performance exhibits a clear improvement at Level 2 using the techniques proposed; at Level 3, a smaller effect has been recorded. Coastal proximity has been found to affect the retrieva- - l of up to 150 and 300 km from the coast, at Levels 2 and 3, respectively. Results for both scenarios are presented and discussed.",2009,0,
2516,2517,A process to reduce reproducibility error in VNA measurements,"Vector Network Analyzers have proven to be useful for characterizing the electrical properties of passive interconnects and determining their ability to transmit high speed signals. It is highly desirable to have a measurement process that is both accurate and precise. Because of the complexity of the measurement, there are many potential factors that could affect the precision of the measurement. For example, when taking probed measurements, operators typically use different methods to align the probes, which often introduce subtle variations in measurements. Additionally calibration algorithms (or procedures) may have slight differences that introduce errors as well. This paper will present a method to identify the largest sources of variation that impact the precision of the measurement. The method is based on an extension of the analysis of variance (ANOVA) so that it can be applied to complex variables.",2010,0,
2517,2518,On Bypassing Blocking Bugs during Post-Silicon Validation,"Design errors (or bugs) inadvertently escape the pre- silicon verification process. Before committing to a re-spin, it is expected that the escaped bugs have been identified during post-silicon validation. This is however hindered by the presence of blocking bugs in one erroneous module that inhibit the search for bugs in other parts of the chip that process data received from the erroneous module. In this paper we discuss how to design a novel embedded debug module that can bypass blocking bugs and aid the designer in validating the first silicon.",2008,0,
2518,2519,Acyclic circuit partitioning for path delay fault emulation,"Summary form only given. Acyclic partitioning of VLSI circuits is studied under area/delay, 1-0 size and communication constraints. In this paper, we define the path-delay-fault emulation problem which adds a new constraint, viz. path count constraint, to partitioning problem. We present two algorithms to solve the problem. The first algorithm decomposes a circuit into entirely-fanout-free cones (EFFC), and clusters them into partitions. The second one finds an intermediate partitioning solution with the partitioning algorithm ignoring path count constraint. Later, it applies the first algorithm to the partitions which violate the path count constraint. We implemented the first algorithm and measured its efficiency in terms of the number of resulting partitions, cut-cost, and time cost for ISCAS85 benchmarks.",2005,0,
2519,2520,Study on remote fault diagnosis system using multi monitoring methods on dredger,"Power machinery and key equipments are regarded as the monitoring objects. Remote fault diagnosis system based Web is proposed using multi monitoring methods in dredger, which gives a comprehensive consideration of monitoring technology about performance parameters, lubricant oil, vibration and instantaneous speed. The modern ship maintenance management mode of two modes, three levels and four methods is established and used in the selected dredgers of Changjiang Waterway Bureau. Monitoring subsystem on ship, diagnostic subsystem in lab and maintenance decision subsystem in dredgers maintenance management center are designed and realized using computer technology and data fusion technology. Remote fault diagnosis system has been realized using wireless communication technology between the dredger and the technology center. The system to be used on the dredgers has proved to be effective.",2010,0,
2520,2521,Studying Experiment Teaching in Error Theory and Data Processing with MATLAB,"In order to strengthen practice, MATLAB software is introduced in the course of error theory and data processing as the teaching assistance method. Based on its ability formidable calculating function, graph handling ability and the rich toolbox, the computer-experiment has been set up. It improves the studentspsila creative capability and stimulates learning activity, because the abstract theory becomes direct-viewing and vivid, so the teaching effect is improved.",2009,0,
2521,2522,Design about Real-time Fault Detection Information System of the Transformer Substation,"Nowadays, the automation of our countrypsilas power grid operation and management has reached a high level, by contrast, the automation of analysis and management of relay protection monitoring, system fault and protection action behavior seems relatively backward. This paper emphatically introduces the design of fault information system based on real-time detection platform aiming at 220 kV transformer substation, including design of master station and sub-station, communication realization between master station and substation and security protection of the system and briefly clarifies the functions which the system can realize according to the characteristics of power grid.",2009,0,
2522,2523,Effect of voltage stability constraints and corrective control on pricing components,"Electricity markets that have adopted nodal locational marginal pricing schemes decompose prices into three components. These components relate to generation, congestion and losses. The usual scheme of pricing follows the mandate that when the market clears, every node is assigned the same value for the generation component. And so it follows that the difference in price across any pair of nodes is because of the congestion and loss component. In this paper we examine the congestion component in the light of new constraints that will arise because of the transfer limits imposed by voltage stability across critical interfaces.",2008,0,
2523,2524,Unified Architectural Support for Soft-Error Protection or Software Bug Detection,"In this paper we propose a unified architectural support that can be used flexibly for either soft-error protection or software bug detection. Our approach is based on dynamically detecting and enforcing instruction- level invariants. A hardware table is designed to keep track of run-time invariant information. During program execution, instructions access this table and compare their produced results against the stored invariants. Any violation of the predicted invariant suggests a potential abnormal behavior, which could be a result of a soft error or a latent software bug. In case of a soft error, monitoring invariant violations provides opportunistic soft-error protection to multiple structures in processor pipelines. Our experimental results show that invariant violations detect soft errors promptly and as a result, simple pipeline squashing is able to fix most of the detected soft errors. Meanwhile, the same approach can be easily adapted for software bug detection. The proposed architectural support eliminates the substantial performance overhead associated with software-based bug-detection approaches and enables continuous monitoring of production code.",2007,0,
2524,2525,Dynamic aspects for runtime fault determination and recovery,"One of the most promising applications of aspect oriented programming (AOP) is the area of fault tolerance and recovery. In traditional programming languages, error handling code must be closely interwoven with program logic. AOP allows the programmer to take a more modular approach - error handling code can be woven into the code by expressing it as an aspect. One major impediment to handling error code in this way is that while errors are a dynamic, runtime property, most research on AOP has focused on static properties. In this paper, we propose a method for handling a variety of run-time faults as dynamic aspects. First, we separate fault handling into two different notions: fault determination, or the discovery of faults within a program, and fault recovery, or the logic used to recover from a fault. Our position is that fault determination should be expressed as dynamic aspects. We propose a system, called Rescue, that exposes underlying features of the virtual machine in order to express faults as variety of run-time constraints. We show how our methodology can be used to address several of the flaws in state of the art fault handling techniques. This includes their limitations in handling parallel and distributed faults, their obfuscated nature and their overly simplistic notion of what a ""fault"" actually may comprise",2006,0,
2525,2526,Low voltage fault attacks to AES,"This paper presents a new fault based attack on the Advanced Encryption Standard (AES) with any key length, together with its practical validation through the use of low voltage induced faults. The CPU running the attacked algorithm is the ARM926EJ-S: a 32-bit processor widely deployed in computer peripherals, telecommunication appliances and low power portable devices. We prove the practical feasibility of this attack through inducing faults in the computation of the AES algorithm running on a full fledged Linux 2.6 operating system targeted to two implementations of the ARM926EJ-S on commercial development boards.",2010,0,
2526,2527,Improving Real-time Fault Analysis and Validating Relay Operations to Prevent or Mitigate Cascading Blackouts,"This paper proposes a new strategy at the local (substation) level, aimed at preventing or mitigating the cascading blackouts that involve relay misoperations or inadequate local diagnostic support. The strategy consists of an advanced real-time tool that combines neural network based fault detection and classification (NNFDC) algorithm and synchronized sampling based fault location (SSFL) algorithm with a relay monitoring tool using event tree analysis (ETA). The fault analysis tool provides a reference for conventional distance relay with its better performance and the relay monitoring tool provides detailed local information about the disturbances. The idea of the entire strategy is to meet several NERC recommendations to prevent blackouts using wide area protection and control",2006,0,
2527,2528,HACKER: human and computer knowledge discovered event rules for telecommunications fault management,"Visualization integrated with data mining can offer 'human-assisted computer discovery' and 'computer-assisted human discovery'. Such a visual environment; reduces the time to understand complex data, thus enabling practical solutions to many real world problem to be developed far more rapidly than either humans or computers operating independently. In doing so the remarkable perceptual abilities that humans possess can be utilized, such as the capacity to recognize lanes quickly, and detect the subtlest changes in size, color, shape, movement or texture. One such complex real world problem is fault management in global telecommunication systems. These system have a large amount of built in redundancy to ensure robustness and quality of service. Unfortunately, this means that when a fault does occur, it can trigger a cascade of alarm events as individual parts of the system discover and report fallen making it difficult to locate the origin of the fault. This alarm behavior has been described as appearing to an operator as non-deterministic, yet it does result in a large data mountain that is ideal for data mining. The paper presents a visualization data mining prototype that incorporates the principles of human and computer discovery, the combination of computer-assisted human discovery with human-assisted computer discovery through a three-tier framework. The prototype is specifically designed to assist in the semi-automatic discovery of previously unknown alarm rules that can then be utilized in commercial role based component solutions, ""business rules"", which are at the heart of many of todays fault management systems.",2002,0,
2528,2529,Modeling and estimation of the spatial variation of elevation error in high resolution DEMs from stereo-image processing,The spatial variability of elevation errors in high-resolution digital elevation models (DEMs) derived from stereo-image processing is examined. Error models are developed and evaluated by examining the correlation between various DEM parameters and the magnitude of the observed DEM vertical error. DEM vertical errors were estimated using a dataset of more than 51000 points of known elevation obtained from a kinematic Global Positioning Satellite (GPS) ground survey. Elevation variability and the quality of the stereo-correlation match over small spatial scales were the dominant factors that determined the magnitude of the DEM error at any given location. The error models are strongly correlated with the magnitude of the DEM vertical error and are shown to adequately represent the full range of the observed error. The error models are used to estimate the magnitude of the vertical error for every point in the DEMs. The models are then used to predict the overall error in the DEMs. The results demonstrate that the error models can accurately quantify and predict the spatial variability of the DEM error,2001,0,
2529,2530,A Fault Tolerant Method for Residue Arithmetic Circuits,"As a result of shrinking device dimensions, the occurrence of transient errors is increasing. This causes system reliability to be reduced. Thus, fault-tolerant methods are becoming increasingly important, particularly in safety-critical applications. In this paper a novel fault-tolerant method is proposed through combining time redundancy with information redundancy to reduce hardware complexity. Residue codes are selected as the source of information redundancy and the proposed technique is compared with some well-known fault tolerant schemes considering required hardware and delay. This method can be applied to various types of arithmetic circuits. Simulations results of a multiplier circuit shows that by using quadruple residue redundancy in comparison with a simple Residue Redundancy when multiplying two 64-bit numbers, number of gates can be reduced to 90% by exposing only 9% extra delay. Therefore,this technique can effectively reduce hardware complexity and consequently leads to large savings on the ALU as a whole,while introducing only a reasonable delay.",2009,0,
2530,2531,Fault-tolerant grid services using primary-backup: feasibility and performance,"The combination of grid technology and Web services has produced an attractive platform for deploying distributed applications: grid services, as represented by the Open Grid Services Infrastructure (OGSI) and its Globus toolkit implementation. As the use of grid services grows in popularity, tolerating failures becomes increasingly important. This work addresses the problem of building a reliable and highly-available grid service by replicating the service on two or more hosts using the primary-backup approach. The primary goal is to evaluate the ease and efficiency with which this can be done, by first designing a primary-backup protocol using OGSI, and then implementing it using Globus to evaluate performance implications and tradeoffs. We compared three implementations: one that makes heavy use of the notification interface defined in OGSI, one that uses standard grid service requests instead of notification, and one that uses low-level socket primitives. The overall conclusion is that, while the performance penalty of using Globus primitives - especially notification - for replica coordination can be significant, the OGSI model is suitable for building highly-available services and it makes the task of engineering such services easier.",2004,0,
2531,2532,Simulation-based techniques for calculating fault resolution and false removal statistics,"This paper discusses the use of diagnostic simulations to generate the Fault Resolution metric for a system or equipment. Simulation-based calculations are free of some of the biases that inhere within traditional, math-based approaches. Moreover, a simulation-based evaluation of the replacement of failed items also provides a basis for the calculation of the effect of diagnostic ambiguity upon false removals-including the estimated costs that can be attributed to removals beyond those that would be expected during a product's intended lifetime",2000,0,
2532,2533,Improved scatterer property estimates from ultrasound backscatter using gate-edge correction and a Pseudo-Welch technique,"Quantitative ultrasound (QUS) techniques have been widely used to estimate the size, shape and mechanical properties of tissue microstructure for specified regions of interest (ROIs). For conventional methods, an ROI size of 4 to 5 beamwidths laterally and 15 to 20 spatial pulse lengths axially has been suggested to estimate accuracy and precision better than 10% and 5%, respectively. A new method is developed to decrease the standard deviation of the quantitative ultrasound parameter estimate in terms of effective scatterer diameter (ESD) for small ROIs. The new method yielded estimates of the ESD within 10% of actual values at an ROI size of five spatial pulse lengths axially by two beamwidths laterally, and the estimates from all the ROIs had a standard deviation of 15% of the mean value. Such accuracy and precision cannot be achieved using conventional techniques with similar ROI sizes.",2010,0,
2533,2534,Respiratory Motion Correction in 3-D PET Data With Advanced Optical Flow Algorithms,"The problem of motion is well known in positron emission tomography (PET) studies. The PET images are formed over an elongated period of time. As the patients cannot hold breath during the PET acquisition, spatial blurring and motion artifacts are the natural result. These may lead to wrong quantification of the radioactive uptake. We present a solution to this problem by respiratory-gating the PET data and correcting the PET images for motion with optical flow algorithms. The algorithm is based on the combined local and global optical flow algorithm with modifications to allow for discontinuity preservation across organ boundaries and for application to 3-D volume sets. The superiority of the algorithm over previous work is demonstrated on software phantom and real patient data.",2008,0,
2534,2535,A Service-Oriented Alarm Correlation Method for IT Service Fault Localization,"To improve the quality of IT services it is important to quickly and accurately detect and diagnose its faults. Because associations exist among entities, one fault in a single entity may cause a number of accompanying alarms beside the root alarm during fault propagation. Service-oriented alarm correlation analysis, that isolates the root causes or faults from numerous alarms, is an important method to ensure quality of service. This paper firstly provides an entity relationships model of IT infrastructure. Based on the fault propagation map depicted in the model, we then propose a service-oriented alarm correlation method. We have validated our efforts by developing a prototype and testing it in a real environment",2006,0,
2535,2536,On power and fault-tolerance optimization in FPGA physical synthesis,"Power and fault tolerance are deemed to be two orthogonal optimization objectives in FPGA synthesis, with independent attempts to develop algorithms and CAD tools to optimize each objective. In this paper, we study the relationship between these two optimizations and show empirically that there are strong ties between them. Specifically, we analyze the power and reliability optimization problems in FPGA physical synthesis (i.e., packing, placement, and routing), and show that the intrinsic structures of these two problems are very similar. Supported by the post routing results with detailed power and reliability analysis for a wide selection of benchmark circuits, we show that with minimal changes - fewer than one hundred lines of C code - an existing power-aware physical synthesis tool can be used to minimize the fault rate of a circuit under SEU faults. As a by-product of this study, we also show that one can improve the mean-time-to-failure by 100% with negligible area and delay overhead by performing fault-tolerant physical synthesis for FPGAs. The results from this study show a great potential to develop CAD systems co-optimized for power and fault-tolerance.",2010,0,
2536,2537,The system of copper strips surface defects inspection based on intelligent fusion,"Directing towards the characteristics and importance of copper strips surface inspection, this paper proposes a copper strips surface inspection system based on artificial intelligence from the standpoint of artificial intelligence. It uses computer vision to capture defects images, uses Hu invariant moments as the eigenvectors and uses BP neural network based on genetic algorithm combined with expert system to train and learn the samples. The trained weights are used to diagnose as the knowledge base and a copper surface defects inspection system based on intelligent fusion is constructed . The experimental results proved that intelligent fusion model can compliment all kinds of intelligent models and has a better performance than single model on copper strips surface inspection.",2008,0,
2537,2538,High Impedance Fault Detection Using Harmonics Energy Decision Tree Algorithm,"In this paper a new pattern recognition based algorithm is presented to detect high impedance fault in distribution networks. In this method the total energy of odd, even and in-between harmonics up to 400 Hz is calculated and is fed to a decision tree as a classifier. The proposed scheme can successfully distinguish the HIFs from normal operations in power system such as harmonic load switching, capacitors switching, and transformer energization. The results show high accuracy of the proposed method in the detection task.",2006,0,
2538,2539,Estimation of error in large area underwater photomosaics using vehicle navigation data,Creating geometrically accurate photomosaics of underwater sites using images collected from an AUV or ROV is a difficult task due to dimensional errors which grow as a function of 3D image distortion and the mosaicking process. Although photomosiacs are accurate locally their utility for accurately representing a large survey area is jeopardized by this error growth. Evaluating the error in a mosaic is the first step in creating globally accurate photomosaics of an unstructured environment with bounded error. Using vehicle navigation data and sensor offsets it is possible to estimate the error present in large area photomosaics independent of the mosaic construction method. This paper presents a study of the error sources and an estimation of the error growth across an underwater photomosaic. World coordinate locations of the individual image centers are projected into the image coordinate space of the mosaic. The spatial error is then shown as the divergence between the position of the corresponding image centers in the mosaic and the positions determined by the navigation projection. Accurate world coordinate system position estimates of the image centers are obtained from the on board navigation sensors and the EXACT acoustic navigation system. Several large area mosaics using imagery collected by the JASON ROV are shown as examples,2001,0,
2539,2540,More bang for the bug: An account of 2003's attack trends,"We can find considerable information security debris in the wake of 2003's attack trends and new security flaws. New and serious vulnerabilities were discovered, disclosed, and subsequently exploited in many ways - from simple, straightforward methods to more advanced and innovative exploitation techniques. This paper examines a handful of the more than 3,000 unique vulnerabilities and 115,000 security incidents reported in 2003 (according to CERT Coordination Center's report for quarters one through three) and do my best to predict information security woes for 2004. The author's analysis focuses on the distinguishing characteristics of the 2003 attacks trends rather than on specific vulnerabilities or a precisely defined taxonomy of security bugs.",2004,0,
2540,2541,Guaranteed error correction rate for a simple concatenated coding scheme with single-trial decoding,"We consider a concatenated coding scheme using a single inner code, a single outer code, and a fixed single-trial decoding strategy that maximizes the number of errors guaranteed to be corrected in a concatenated codeword. For this scheme, we investigate whether maximizing the guaranteed error correction rate, i.e., the number of correctable errors per transmitted symbol, necessitates pushing the code rate to zero. We show that this is not always the case for a given inner or outer code. Furthermore, to maximize the guaranteed error correction rate over all inner and outer codes of fixed dimensions and alphabets, the code rate of one (but not both) of these two codes should be pushed to zero",2000,0,
2541,2542,Formalization and automated detection of human errors,"The contribution describes a novel approach for the detection and classification of human errors in interaction with complex dynamic systems, according to Donierpsilas error taxonomy. The programmed implementation of the approach based on situation-operator-modeling (SOM) is already realized using software tools for high-level Petri nets (HPN). An experimental environment consisting of an arcade style game communicating with the HPN-software CPN Tools is used. With CPN Tools, the interaction between a human operator and the arcade game is modeled and further mapped to an automatically generated state space. Using generically formulated state space queries, the human error dasiarigiditypsila is detected.",2008,0,
2542,2543,Joint error resilient and rate control for H.264,"A joint error resilient and rate control method for H.264 over Internet channel is presented in this paper. An accurate rate model and a fast encoding mode selection for packet loss channel are used in this method. By rate-distortion based frame-layer bit allocation and slice-layer quantization parameter (QP) calculation, good error resilient and rate control results are demonstrated.",2005,0,
2543,2544,Specifications overview for counter mode of operation. Security aspects in case of faults,"In 2001, after a selection process, NIST added the counter mode of operation to be used with the advanced encryption standard (AES). In the NIST recommendation a standard incrementing function is defined for generation of the counter blocks which are encrypted for each plaintext block, IPsec Internet draft (R. Housley et al., May 2003) and ATM security specifications contain implementation specifications for counter mode standard incrementing function. In this paper we present those specifications. We analyze the probability to reveal useful information in case of faults in standard incrementing function described in NIST recommendation. The confidentiality of the mode can be compromised with the fault model presented in this paper. We recommend another solution to be used in generation of the standard incrementing function in the context of the counter mode.",2004,0,
2544,2545,On the tracking performance improvement of optical disk drive servo systems using error-based disturbance observer,"There are many control methods to guarantee the robustness of a system. Among them, the disturbance observer (DOB) has been widely used because it is easy to apply and the cost is low due to its simplicity. Generally, an output signal of the system is required to construct a DOB, but for some systems such as magnetic/optical disk drive systems, we cannot measure the position output signal, but only the position error signal (PES). In order to apply a DOB to such systems, we must use an error signal instead of an output signal. We call it the error-based disturbance observer (EDOB) system. We analyze the differences between a conventional DOB system and EDOB system, and show the effectiveness of the proposed EDOB through simulations and experiments. Also, this paper proposes criteria to enhance the robustness of an EDOB system, and reveals the disturbance rejection property of the EDOB system. Finally, we propose a new method of a double Q system to improve the track-following performance. This is also verified through experiments for a DVD 12 optical disk drive system.",2005,0,
2545,2546,Enhancing pipelined processor architectures with fast autonomous recovery of transient faults,"Recent technology trends have made radiation-induced soft errors a growing threat to the reliability of microprocessors, a problem previously only known to the aerospace industry. Therefore, the ability to handle higher soft error rates in modern processor architectures is essential in order to allow further technology scaling. This paper presents an efficient fault-tolerance method for pipeline-based processors using temporal redundancy. Instructions are executed twice at each pipeline stage, which allows the detection of transient faults. Once a fault is detected the execution is stopped immediately and recovery is implicitly performed within the pipeline stages. Due to this fast reaction the fault is contained at its origin and no expensive rollback operation is required later on.",2010,0,
2546,2547,An effective schedulability analysis for fault-tolerant hard real-time systems,"We propose worst-case response time schedulability analysis for fault-tolerant hard real-time systems which takes into account the effects of temporary faults. The major contribution of our approach is to consider the recovery of tasks running with higher priorities. This characteristic is very useful since faulty tasks certainly have a shorter period of time to meet their deadlines. Due to its flexibility and simplicity, the proposed approach provides an effective schedulability analysis, where system predictability can be fully guaranteed",2001,0,
2547,2548,Super-resolution with integrated radial distortion correction,"Super-resolution (SR) is a technique where a high resolution (HR) image is obtained from a low resolution (LR) image sequence. In SR, the camera scans scenery to form a mosaic of overlapped image frames. To achieve SR, the relative movement along the low resolution set of images is considered as first step, and later the spatial resolution through data fusion is increased. Resolution increase is important not only for a better image visualization, but also to get additional image details. In the image acquisition process, lenses in cameras induce optics distortions. In the algorithm proposed, the radial distortion is considered. From a priori camera motion information, a super-resolution algorithm is proposed. Unlike other algorithms, radial distortion elimination is incorporated. Radial distortion is notably present in low cost sensors. To validate the algorithm, a software system was developed, with real environments and resolution patterns measurements.",2005,0,
2548,2549,Location-known-exactly human-observer ROC studies of attenuation and other corrections for SPECT lung imaging,"We use receiver operating characteristic (ROC) analysis of a location-known-exactly (LKE) lesion detection task to compare the image quality of SPECT reconstruction with and without various combinations of attenuation correction (AC), scatter correction (SC) and resolution compensation (RC). Hybrid images were generated from Tc-99m labelled NeoTect clinical backgrounds into which Monte Carlo simulated solitary pulmonary nodule (SPN) lung lesions were added, then reconstructed using several strategies. Results from a human-observer study show that attenuation correction degrades SPN detection, while resolution correction improves SPN detection, even when the lesion location is known. This agrees with the results of a previous localization-response operating characteristic (LROC) study using the same images, indicating that location uncertainty is not the sole source of the changes in detection accuracy.",2006,0,
2549,2550,Voltage and Current Patterns for Fault Location in Transmission Lines,"After a severe disturbance due to an insulation failure in a transmission line, the precise fault location is a critical problem for the maintenance crew. In order to avoid further economical and social costs, fault diagnosis has to be performed as soon as possible. Fault diagnosis has been a major area of investigation among power system problems and intelligent system applications. Several approaches have been proposed for solving this problem. This paper advocates the application of neural networks for mapping the relationship between electrical signals and fault locations in transmission lines. The significance of voltages and currents is analysed using steady-state and electromagnetic transient information. Electromagnetic transient information has been extracted using fast fourier and wavelet transforms. Thus, three ways of feeding the fault location models are compared, i.e., with a steady-state input space and with two transient based input spaces, which are built by the previously mentioned transforms. The tests consider different operating and fault conditions, including different types of fault impedances, fault angles, line loading, equivalent system impedances, and fault locations.",2007,0,
2550,2551,Adaptive Interaction Fault Location Based on Combinatorial Testing,"Combinatorial testing aims to detect interaction faults, which are triggered by interaction among parameters in system, by covering some specific combinations of parametric values. Most works about combinatorial testing focus on detecting such interaction faults rather than locating them. Based on the model of interaction fault schema, in which the interaction fault is described as a minimum fault schema and several corresponding parent-schemas, we propose an iterative adaptive interaction fault location technique for combinatorial testing. In order to locate interaction faults that detected in combinatorial testing, such technique utilizes delta debugging strategy to filtrate suspicious schemas by generating and running additional test cases iteratively. The properties, which include both recall and precision, of adaptive interaction fault location techniques are also analyzed in this paper. Analytical results suggest that the high scores in both recall and precision are guaranteed. It means that such technique can provide an efficient guidance for the applications of combinatorial testing.",2010,0,
2551,2552,Mutation-based diagnostic test generation for hardware design error diagnosis,We propose the use of mutation-based error injection to guide the generation of high-quality diagnostic test patterns. A software-based fault localization technique is employed to derive a ranked candidate list of suspect statements. Experimental results for a set of Verilog designs demonstrate that a finer diagnostic resolution can be achieved by patterns generated by the proposed method.,2010,0,
2552,2553,Fault tolerant mechatronics [automotive applications],"Modern cars exhibit a variety of new functionalities concerning engine management, safety, vehicle dynamics control as well as comfort and convenience. Safety features like airbags, antilock braking systems (ABS), anti-skid systems, belt tensioners or the electronic stability program (ESP) are standard fittings of present day car models and in some cases even stipulated by legislation. These safety systems have led to an increased avoidance of accidents by actively affecting vehicle dynamics and to a mitigation of the consequences of accidents on the driver and passengers by innovative restraint systems. As a rule these systems are mechatronic systems. Mechatronic systems today derive their functionality by an interlocked interaction of mechanics, electronics and information technology. Their deployment in the safety-relevant environment requires fault tolerance. Fault tolerant mechatronics is based on redundancy, which must be supervised and tested permanently. Reliability of sensor and actuator technology is essential for future motor vehicle systems. Operability and reliability are to be achieved by suitable on-board and on-line test methods. Exemplarily this is shown for future X-by-wire applications.",2004,0,
2553,2554,Research of steep-front wave impulse voltage test effectiveness in finding internal fault of composite insulators,"In order to verify the effectiveness of steep-front impulse voltage testing in finding the internal faults of composite insulators, some insulators with faults are modeled which include conductive channel, semi-conductive airy channel and partial little air bubbles that occur separately at different places. A steep-front wave impulse voltage test (steepness of wave front is 1000-4000 kV/s) is respectively made for these faulty and normal insulators. At the same-time the internal electric field intensity and its distribution in the insulator is calculated by making use of infinite element analysis software in order to test whether breakdown has occurred. The result is consistent with the experimental one. The final result shows that steep-front wave impulse voltage testing plays a very effective part in finding severe faults of the insulator, however, tiny faults are not easy found using this method. The result of this research gives a reference to revise the steep-front wave impulse voltage test standard",2001,0,
2554,2555,An exploratory study of fault-proneness in evolving aspect-oriented programs,"This paper presents the results of an exploratory study on the fault-proneness of aspect-oriented programs. We analysed the faults collected from three evolving aspect-oriented systems, all from different application domains. The analysis develops from two different angles. Firstly, we measured the impact of the obliviousness property on the fault-proneness of the evaluated systems. The results show that 40% of reported faults were due to the lack of awareness among base code and aspects. The second analysis regarded the fault-proneness of the main aspect-oriented programming (AOP) mechanisms, namely pointcuts, advices and intertype declarations. The results indicate that these mechanisms present similar fault-proneness when we consider both the overall system and concern-specific implementations. Our findings are reinforced by means of statistical tests. In general, this result contradicts the common intuition stating that the use of pointcut languages is the main source of faults in AOP.",2010,0,
2555,2556,Comparative Study of Fault-Proneness Filtering with PMD,"Fault-prone module detection is important for assurance of software quality. We have proposed a novel approach for detecting fault-prone modules using spam filtering technique, named Fault-proneness filtering. In order to show the effectiveness of fault-proneness filtering, we conducted comparative study with a static code analysis tool, PMD. In the study, Fault-proneness filtering obtains higher F<sub>1</sub> than PMD.",2008,0,
2556,2557,Fault detection effectiveness of spathic test data,"This paper presents an approach for generating test data for unit-level, and possibly integration-level, testing based on sampling over intervals of the input probability distribution, i.e., one that has been divided or layered according to criteria. Our approach is termed ""spathic"" as it selects random values felt to be most likely or least likely to occur from a segmented input probability distribution. Also, it allows the layers to be further segmented if additional test data is required later in the test cycle. The spathic approach finds a middle ground between the more difficult to achieve adequacy criteria and random test data generation, and requires less effort on the part of the tester. It can be viewed as guided random testing, with the tester specifying some information about expected input. The spathic test data generation approach can be used to augment ""intelligent"" manual unit-level testing. An initial case study suggests that spathic test sets defect more faults than random test data sets, and achieve higher levels of statement and branch coverage.",2002,0,
2557,2558,"The development of GM(1,1) error toolbox","In the prediction research, the main purpose is to minimize the prediction error; however, the goals cannot be fulfilled completely. Even we choose GM(1,1) model, we also need to minimize the prediction error. Hence, in this paper, we first focus on the influence parameter alpha in GM(1,1) model, then, analyze the characteristics of alpha step by step. Second, we give up the alpha = 0.5 method, and use numerical method to find the prediction error corresponding with alpha value and plot the figure of the function of error. Finally, after the mathematics model has been presented; we also develop a toolbox, which based on C language to assist us to implement our approach. Consequently, we conclude that the value of alpha is adaptive in the interval of [0,1] in GM(1,1) model.",2007,0,
2558,2559,The ATPG Conflict-Driven Scheme for High Transition Fault Coverage and Low Test Cost,"This paper presents two new conflict-driven techniques for improving transition fault coverage using multiple scan chains. These techniques are based on a novel test application scheme, in order to break the functional dependency of broadside testing. The two new techniques analyze the ATPG conflicts in broadside test generation, and try to control the flip-flops with most influence on the fault coverage.The conflict-driven method selects some flip-flops that work in the enhanced mode and distributes them into different chains. In the multiple scan chain architecture, to avoid too many scan-in pins, some chains are driven by the same scan-in pin to construct a tree-based architecture. Based on the architecture, the new test application scheme allows some flip-flops working in the enhanced mode, while most of others working in the traditional broadside mode. With the efficient conflict-driven selection method, fault coverage is improved greatly, which can also reduce test application time and compress test data volume. Experimental results show that fault coverage based on the proposed method is comparable to the enhanced scan.",2009,0,
2559,2560,Irregular pixel defects in hybrid IR-sensitive integrated circuits and their effect on results of measurements in medicine and multichannel spectrometry,"Distinctive features of effects of irregular pixel defects arisen in lineand 2D-array multielement photodetecting devices designed on the base of hybrid integrated circuits of ""sandwich"" type are investigated. The experimental results are presented for the photosensitive structures consisted of indium-arsenide and silicon chips. Possibility to solve properly IR-thermographic and spectrometric problems with use of detectors containing pixel defects is considered.",2002,0,
2560,2561,Minimization of Product Utility Estimation Errors in Recommender Result Set Evaluations,"Recommender systems are wide-spread web applications which can effectively support users in finding suitable products in a large and/or complex product domain. Although state-of-the-art systems manage to accomplish the task of finding and presenting suitable products they show big deficits in the applied model of human behavior. Time limitations, cognitive capacities, and willingness to cognitive effort bound rational decision taking which can lead to unforeseen side effects and furthermore to sub-optimal decisions. Decoy effects are cognitive phenomenons which are omni-present on result pages. State-of-the-art recommender systems are completely unaware of such effects. Due to the fact that such effects constitute one source of irrational decisions their identification and, if necessary, the neutralization of their biasing potential is extremely important. This paper introduces an approach for identifying and minimizing decoy effects on recommender result pages. To undergird the presented approach we present the results of a corresponding user study which clearly proofs the concept.",2009,0,
2561,2562,?omplete set of the special process equipment for the defect-free production of reticles,"The paper presents an integrated solution of a problem to develop a set of the equipment for the defect-free production of reticles and photomasks. The integrated approach to the equipment design allows to obtain certain advantages disclosed below. Accordingly, the paper highlights the following main issues: (-) Practical realization of these advantages in the special process equipment developed by the KBTEM-OMO enterprise of the PLANAR. (-) Advantages in the development of a complete set of the special process equipment. Without taking into account technical and chemical processes, this complete set includes three component parts: (-) Multi-beam laser pattern generator; (-) Die-to-Database reticle inspection system; (-) Laser reticle repair system.",2007,0,
2562,2563,An Image Enhancement Technique in Inspecting Visual Defects of Polarizers in TFT-LCD Industry,"This study develops an image-processing filter to enhance the visual defects such as particles, stains, and uneven intensity on polarizers in TFT-LCD industry. Each pixel in the subimage of a polarizer is initially processed to calculate its standard deviation (SD) of gray level, which is sampled by its neighbors within a window. The gray level of each pixel is re-scaled by the maximal and minimal SD values on entire subimage to determine its new gray level. Real polarizers with visual defects are tested in this study. Experimental results show that the proposed filter achieves better performance than conventional image enhancement filters do. Moreover, the proposed image enhancement scheme provides more information for potential defect detection and classification alternatives. The proposed filter is simple, straightforward, and requiring no high-resolution image. Therefore, it is practically suitable for large-polarizer manufacturers to increase inspection speed.",2009,0,
2563,2564,Modeling and Simulation of Internal Faults in a Synchronous Machines in d-q Axis Models,"This paper aims at presenting a method for modeling and simulation of internal single phase-to-ground faults in d-q axis model in stator windings of large synchronous machines. The method of partitioning the stator windings is used in analyzing the internal faults. This Partitioning method, under internal faults determines inductances of the affected windings which, is extended. In this paper, we modeled the machine in d-q axis models under specific conditions by using the obtained inductances and park transformation. The application of this method is analyzing internal single phase-to-frame faults. Finally, this method is simulated to evaluate an internal fault in stator windings.",2007,0,
2564,2565,Quality control protocol for frame-to-frame PET motion correction,"Subject motion during Position Emission Tomography (PET) brain scans can reduce image quality, and may lead to incorrect biological outcome measures, especially during analysis of dynamic data sets. This is particularly relevant when imaging with state-of-the-art scanners such as the High Resolution Research Tomograph (HRRT, Siemens Medical Solutions). Motion correction via frame-to-frame image realignment is simpler to implement and requires fewer computing resources than methods that correct for motion during data reconstruction and has been shown to significantly improve the accuracy of dynamically-derived biological variables. However, an ongoing problem is a lack of objective criteria to validate the accuracy of frame-to-frame realignment. Visual inspection of realigned images is a common method of validation but requires a significant amount of operator time and results may vary from one operator to another. This work presents a quality control protocol that automatically flags inadequate realignments based on the comparison of motion transformation matrices obtained from two independent sources: the Polaris Vicra optical tracking device and the image based realignment algorithm AIR (Automated Image Registration). A metric was computed to determine the difference between the transformations from both methods. Realignments were accepted or flagged based on the value of the metric. Since the two methods rely on independent motion assessment tools, the chance of both algorithms giving consistently wrong estimates is low. Human test cases show that the quality control protocol is capable of correctly identifying both acceptable and incorrect realigned images, thus providing an objective quality control metric. Implementation of the protocol reduces the number of images requiring visual inspection by 72% and operator time required by 50%, decreasing both operator labour and operator-dependent biases.",2009,0,
2565,2566,Defect distribution for wearable system design,"This paper describes a design process for custom wearable systems produced in an academic setting. A set of 245 wearable design defects from two distinct periods separated by six years in time is presented. These data identify aspects of the process requiring significant developer effort, which we show using an orthogonal defect classification scheme. A comparison of defect attribute distributions across the two separate design periods is given. The results show that growing electronic complexity is increasing the number of defects caused by designer error, and that more defects are being observed in earlier phases of the design process.",2002,0,
2566,2567,On-chip debugging-based fault emulation for robustness evaluation of embedded software components,"As manufacturers integrate more off-the-shelf components in embedded products, their robustness evaluation becomes more necessary. This requirement is however difficult to meet using non-intrusive evaluation methods, especially in the case of systems-on-a-chip (SoCs). Research presented in this paper investigates the use of on-chip-debugging (OCD) mechanisms to evaluate the ability of SoC-embedded software components to withstand the occurrence of external faults. These faults are emulated by corrupting the information that components are able to receive through their public interfaces. Once a fault has been injected, reaction of targeted components is studied using OCD monitoring capabilities. The ability of these capabilities to run in parallel with the rest of the SoC internal mechanisms is exploited in order to carry out previous tasks without requiring the source code of the component under study and without interfering (neither spatially nor temporally) with the system nominal execution. Results show potentials and limitations of the approach and let us define directions for future investigation.",2005,0,
2567,2568,A 160120-pixels range camera with on-pixel correlated double sampling and nonuniformity correction in 29.1m pitch,"This paper presents the design and test of a CMOS integrated circuit implementing a 160120-pixels 3D camera. The on-pixel processing allows the use of Indirect Time-Of-Flight technique for distance measurement with reset noise removal through Correlated Double Sampling and embedded fixed-pattern noise reduction, while a fast readout operation allows to stream out the pixels values at a maximum rate of 10MSample/s. The imager can operate as a fast 2D camera up to 458fps, a 3D camera up to 80fps or both. The chip has been fabricated using a standard 0.18m 1P4M 1.8V CMOS technology with MIM capacitors. The resulting pixel has a pitch of 29.1m with a fill-factor of 34% and consists of 66 transistors. Distance measurements up to 4.5m have been performed with pulsed laser light, achieving 2.5cm precision at 2m in real-time.",2010,0,
2568,2569,A Grouping-Based Strategy to Improve the Effectiveness of Fault Localization Techniques,"Fault localization is one of the most expensive activities of program debugging, which is why the recent years have witnessed the development of many different fault localization techniques. This paper proposes a grouping-based strategy that can be applied to various techniques in order to boost their fault localization effectiveness. The applicability of the strategy is assessed over - Tarantula and a radial basis function neural network-based technique; across three different sets of programs (the Siemens suite, grep and gzip). Results are suggestive that the grouping-based strategy is capable of significantly improving the fault localization effectiveness and is not limited to any particular fault localization technique. The proposed strategy does not require any additional information than what was already collected as input to the fault localization technique, and does not require the technique to be modified in any way.",2010,0,
2569,2570,An enhanced low-power high-speed Adder For Error-Tolerant application,"The occurrence of errors are inevitable in modern VLSI technology and to overcome all possible errors is an expensive task. It not only consumes a lot of power but degrades the speed performance. By adopting an emerging concept in VLSI design and test-error-tolerance (ET), we managed to develop a novel error-tolerant adder which we named the Type II (ETAII). The circuit to some extent is able to ease the strict restriction on accuracy to achieve tremendous improvements in both the power consumption and speed performance. When compared to its conventional counterparts, the proposed ETAII is able to achieve more than 60% improvement in the power-delay product (PDP). The proposed ETAII is an enhancement of our earlier design, the ETAI, which has problem adding small number inputs.",2009,0,
2570,2571,Neutralization of errors and attacks in wireless ad hoc networks,"This paper proposes and evaluates strategies to build reliable and secure wireless ad hoc networks. Our contribution is based on the notion of inner-circle consistency, where local node interaction is used to neutralize errors/attacks at the source, both preventing errors/attacks from propagating in the network and improving the fidelity of the propagated information. We achieve this goal by combining statistical (a proposed fault-tolerant cluster algorithm) and security (threshold cryptography) techniques with application-aware checks to exploit the data/computation that is partially and naturally replicated in wireless applications. We have prototyped an inner-circle framework with the ns-2 network simulator, and we use it to demonstrate the idea of inner-circle consistency in two significant wireless scenarios: (1) the neutralization of black hole attacks in AODV networks and (2) the neutralization of sensor errors in a target detection/localization application executed over a wireless sensor network.",2005,0,
2571,2572,Fault Tolerant Delay Insensitive Inter-chip Communication,"Asynchronous interconnect is a promising technology for communication systems. Delay Insensitive (DI) interconnect eliminates relative timing assumptions, offering a robust and flexible approach to on- and inter-chip communication. In the SpiNNaker system - a massively parallel computation platform -a DI system-wide communication infrastructure is employed which uses a 4-phase 3-of-6 code for on-chip communication and a 2-phase 2-of-7 code for inter-chip communication. Fault-tolerance has been evaluated by randomly injecting transient glitches into the off-chip wires. Fault simulation reveals that deadlock may occur in either the transmitter or the receiver as handshake protocols are disrupted. Various methods have been tested for reducing or eliminating deadlock, including a novel phase-insensitive 2-phase to 4-phase converter, a priority arbiter for reliable code conversion and a scheme that allows independent resetting of the transmitter and receiver to clear deadlocks. Simulation results confirm that these methods enhance the fault tolerance of the DI communication link, in particular making it significantly more resistant to deadlock.",2009,0,
2572,2573,A Classification-Based Approach to Fault-Tolerance Support in Parallel Programs,"Fault tolerance is an important requirement for long-running parallel programs. This paper presents a different approach to fault-tolerance support in message-passing parallel programs based on their structural and behavioral characteristics, commonly known as patterns. A classification of these patterns and their applicable fault-tolerance strategies is aimed to facilitate an application developer to incorporate appropriate fault-tolerance strategies to an application. Fault-tolerance strategies for two of the patterns are discussed, and one specific strategy is elaborated and analyzed. The presented strategies have been incorporated into a fault-tolerance support framework called FT-PAS. One objective of the framework is to separate the fault tolerance related details from an application developer's main objectives (separation-of-concerns). The paper presents the additional key features of the framework, and concludes with a discussion on current and future research directions.",2009,0,
2573,2574,An improved method of differential fault analysis on SMS4 key schedule,"SMS4 is a 128-bit block cipher published as the symmetric-key encryption standard of Wireless Local Area Network(WLAN) by China in 2006. By inducing faults into the key schedule, we propose an improved method of differential fault attack on the key schedule of the SMS4 cipher. The result shows that our attack can recover its secret key by introducing 4 faulty ciphertexts.",2010,0,
2574,2575,Earth fault distance computation with artificial neural network trained by neutral voltage transients,"A novel application of the neural network approach for transient based earth fault location in 20 kV radial power distribution networks is presented. The items discussed are earth fault transients, signal pre-processing, ANN training and the performance of the proposed distance estimation method. The distribution networks considered are either unearthed or resonant earthed. Neural networks trained by the harmonic content of neutral voltage transients were found to be applicable to fault distance computation in the case of very low fault resistance. The mean error in fault location was about 1 km in the field tests using staged faults, which were recorded in real power systems.",2001,0,
2575,2576,Fault classification for power distribution systems via a combined wavelet-neural approach,"This paper presents an integrated design of a fault classifier which uses a hybrid wavelet-artificial neural network (ANN) based approach. The data for the fault classifier is produced by PSCAD/EMTDC simulation program for 34.5 kV Sagmalcilar-Maltepe distribution system in Istanbul, Turkey. It is aimed to design a classifier capable of recognizing ten classes of three-phase distribution system faults. A database of line currents and line-to-ground voltages is built up including system faults at different fault inception angles and fault locations. The characteristic information over six-channel of current and voltage samples is extracted by the wavelet multiresolution analysis technique. Then, an ANN-based tool was employed for classification task. The main idea of this approach is to solve the complex fault (three-phase short-circuit) classification problem under various system and fault conditions. A self-organizing map, with Kohonen's learning algorithm and type-one learning vector quantization technique is implemented into this study. The performance of the wavelet-neural fault classifier is presented and the results are analyzed in the paper. It is shown that the technique correctly recognizes and discriminates the fault types and faulted phases with a high degree of accuracy in the simulated model distribution system.",2004,0,
2576,2577,Online monitoring and fault diagnose system of STATCOM,"Online monitoring and fault diagnose system of electrical device is one of the main research directions in the electric field. It is also one of the key technologies for settling the decision problems of reliability, safety and maintenance. STATCOM is a kind of large Flexible AC Transmission System device which is used more and more widely. As its cost and complicated degree are high, it is especially important to build an OMFDS for STATCOM. However, as it is not a hot study problem, the current relative research work is a little. This paper states the basic theory of OMFDS and emphasizes three kinds of key technology, analyzes the particular characteristics for OMFDS application on STATCOM, concludes common fault types of STATCOM, such as power capacitor fault on DC side, bridge arm shoot-through fault, power electronics switch devices fault and so on. Finally it introduces the basic configuration and construction of hardware and software of OMFDS in STATCOM.",2009,0,
2577,2578,Syntax error repair with dynamic valid length in LR-based parsers,"In this paper, we present a syntax error repair to decide spurious errors using dynamic valid length. When the compiler encounters a syntax error, it usually attempts to restart parsing to check the remainder of the input for any further errors. One common method of error repair is to repair the input by insertion or deletion or substitution. After repair, the repair method should decide whether the error is a spurious error or not. In order to decide this, Conventional methods adopt a fixed valid length. However it is insufficient. In our method, valid length isnpsilat fixed on ahead. Our method tries all candidates for an error and decides that the one which has longest length is not a spurious error. Since the effectivity of our method was shown, the benchmark program was executed by our method and the conventional method using a fixed valid length. Compared with the conventional method, the proposed method can reduce approximately 90% of non-correcting errors and increase 23.1% of correcting errors.",2008,0,
2578,2579,Fault management for networks with link state routing protocols,"For network fault management, we present a new technique that is based on on-line monitoring of networks with link state routing protocols, such as OSPF (open shortest path first) and integrated IS-IS. Our approach employs an agent that monitors the on-line information of the network link state database, analyzes the events generated by network faults for event correlation, and detects and localizes the faults. We apply our method to a real network topology with various types of network faults. Experimental results show that our approach can detect and localize the faults in a timely manner, yet without disrupting normal network operations.",2004,0,
2579,2580,The design of general-purpose automatic testing and fault diagnosis system based on VXI bus,"According to the principles of generalization, modularization, and standardization, we have designed a general testing system for large-scale and complicated electronic equipment based on VXI bus. It introduces the design principle and the structure of the hardware and the software of the general testing system in the paper. It adopts Bayesian networks representation method to represent the uncertainty information in the system. The system has important meaning to improve the test and diagnostic capability for the electromechanical device.",2009,0,
2580,2581,A Checkpointing Technique for Rollback Error Recovery in Embedded Systems,"In this paper, a general checkpointing technique for rollback error recovery for embedded systems is proposed and evaluated. This technique is independent of used processor and employs the most important feature in control flow error detection mechanisms to simplify checkpoint selection and to minimize the overall code overhead. In this way, during the implementation of a control flow checking mechanism, the checkpoints are added to the program. To evaluate the checkpointing technique, a pre-processor is implemented that selects and adds the checkpoints to three workload programs running in an 8051 microcontroller-based system. The evaluation is based on 3000 experiments for each checkpoint.",2006,0,
2581,2582,Incomplete test vectors fail to detect obscure VoIP software errors,"Most ITU-T (i.e., international telecommunication union standardization sector) standards provide precise specifications for the proper operating behaviors of the systems they specify. However, such specifications are inappropriate for some standards such as the standards for audio coders used in VoIP. For such standards, ITU-T commonly supplies a set of input test data with corresponding correct output results. In this paper, we focus on the G.729 audio-coder algorithm. We use a version of G.729 code that can produce the bit-exact desired output for the given set of input test data to show that there can still be errors in the code even though the output matches the output in the ITU-T specification. We demonstrate that the given test vectors are not comprehensive enough to detect some of the obscure errors that can exist in the software. Therefore, we cannot rely solely on the given test vectors to test and validate our code.",2005,0,
2582,2583,Vision-based end-effector position error compensation,This paper describes a computationally efficient algorithm that provides the ability to accurately place an arm end-effector on a target designated in an image using low speed feedback from a fixed stereo camera. The algorithm is robust to visual occlusion of the end-effector and does not require high fidelity calibration of either the arm or stereo camera. The algorithm works by maintaining an error vector between the locations of a fiducial on the arm's end-effector as predicted by a kinematic model of the arm and detected and triangulated by a stereo camera pair. It then uses this error vector to compensate for errors in the kinematic model and servo to the target designated in the stereo camera pair,2006,0,
2583,2584,"Low-cost, fault-tolerance applications","This article describes an approach for designing various low-cost, fault-tolerant uniprocessor applications using a multiprocessor. The proposed software-based fault-tolerant model is an economic and effective solution towards tolerating transients and intermittent faults that may occur during the run time of a multiprocessor application system. In this article, the proposed technique adopts the strategy of defensive programming based on time redundancy. This article focuses on protecting an application from faults by running multiple copies of the application on a shared-memory multiprocessor. It saves the costs of developing various independent versions of an application program. This is a significant step towards designing a reliable system at a low cost.",2005,0,
2584,2585,Programming of PIC Micro-Controller for Power Factor Correction,"In recent years, the power quality of the AC system has become a great concern due to the rapidly increased numbers of electronic equipment, power electronics and high voltage power system. In order to reduce harmonic contamination in power lines and improve transmission efficiency, power factor correction research became a hot topic. Many control methods for the power factor correction (PFC) were proposed. This paper describes the design and development of a three-phase power factor corrector using PIC (programmable interface circuit) micro-controlling chip. This involves sensing and measuring the power factor value from the load using PIC and sensors, then using proper algorithm to determine and trigger sufficient switching capacitors in order to compensate excessive reactive components, thus withdraw PF near to unity; as a result acquires higher efficiency and better quality AC output. Various power factor correction methods will also be discussed upon their applications in a specific sections",2007,0,
2585,2586,Inter-frame error concealment using graph cut technique for video transmission,"Due to channel noise and congestion, video data packets can be lost during transmission in error-prone networks, which severely affects the quality of received video sequences. The conventional inter-frame error concealment (EC) methods estimate a motion vector (MV) for a corrupted block or reconstruct the corrupted pixel values using spatial and temporal weighted interpolation, which may result in boundary discontinuity and blurring artifacts of the reconstructed region. In this paper, we reconstruct corrupted macroblock (MB) by predicting sub-partitions and synthesizing the corrupted MB to reduce boundary discontinuity and avoid blurring artifacts. First, we select the optimal MV for each neighboring boundary using minimum side match distortion from a candidate MV set, and then we calculate the optimal cut path between the overlapping regions to synthesize the corrupted MB. The simulation results show that our proposed method is able to achieve significantly higher PSNR as well as better visual quality than using the H.264/AVC reference software.",2010,0,
2586,2587,Using Extended Letter-to-Sound Rules to Detect Pronunciation Errors Made by Chinese Learner of English,"In this paper, we use extended letter-to-sound rules for automatic mispronunciation detection, aiming at checking pronunciation errors made by Chinese learners of English. The knowledge-based approach is used to generate extended pronunciation lexicon and incorporated into the HMM-based mispronunciation detection system. The pronunciation errors lead to misunderstanding of a word are expected to be identified. The TIMIT text prompts are used to collect data from Chinese university students, and the test set includes a total of 1900 sentences. Experiments show that the F-measure is about 0.86 at word level and about 0.91 at phone level. The system shows a high degree of accuracy in classifying correct and erroneous pronunciation.",2010,0,
2587,2588,Two-point Multi-Section Nonuniformity Correction Algorithm of Infrared Image And Implementation of Its Simulation Platform,Infrared Focal Plane Array often suffer from problems of non-uniformity that limit its overall performance. This paper describes the traditional two-point correction algorithm and a new two-point multi-section algorithm and its simulation platform. The result of simulation indicate that this algorithm owns the features of low operation quantity and high precision and strong practicability.,2006,0,
2588,2589,Generic Fault Tolerant Software Architecture Reasoning and Customization,"This paper proposes a novel heterogeneous software architecture GFTSA (Generic Fault Tolerant Software Architecture) which can guide the development of safety critical distributed systems. GFTSA incorporates an idealized fault tolerant component concept, and coordinated error recovery mechanism in the early system design phase. It can be reused in the high level model design of specific safety critical distributed systems with reliability requirements. To provide precise common idioms & patterns for the system designers, formal language Object-Z is used to specify GFTSA. Formal proofs based on Object-Z reasoning rules are constructed to demonstrate that the proposed GFTSA model can preserve significant fault tolerant properties. The inheritance & instantiation mechanisms of Object-Z can contribute to the customization of the GFTSA formal model. By analyzing the customization process, we also present a template of GFTSA, expressed in x-frames using the XVCL (XML-based Variant Configuration Language) methodology to make the customization process more direct & automatic. We use an LDAS (Line Direction Agreement System) case study to illustrate that GFTSA can guide the development of specific safety critical distributed systems",2006,0,
2589,2590,An improved approach to the simulation of Single-Line-to-Ground faults in transmission networks,"These paper presents a mathematical model for studying the single-line-to-ground fault in transmission networks, giving a personal contribution for a methodical and accurate analysis of complex systems, by providing an evolution of the Acircuital methodA able to solve the case of a system in which more than one line supply the fault.",2009,0,
2590,2591,Introducing residual errors in accuracy assessment for remotely sensed change detection,"Accuracy assessment for map comparison is commonly found in urban planning research, especially for detecting error in remotely sensed imagery data. It is to compare two sources of spatial information. In analyzing such information quantitatively, the two datasets are summarized in a confusion matrix, which is represented in a form of percentage of predicted value against its actual data (ground truth). The common acceptable percentage is eighty percent and above. In this paper, we present a new way of accuracy assessment by introducing an additional value called residual error (or predicted error). The residual error is the percentage of error exists when two sources of major errors called mis-classification and mis-location are integrated. Such residual error is incorporated into the assessment so that the results are more accurate and comprehensive. As a case study, we calculate the residual errors of five independent image classifications from six different datasets. Therefore, the accuracy assessment is performed with more details that include not only the confusion matrix, but also the residual errors. In this way, the results of the change detection process can help in doing further analysis for urban growth and land development, particularly for town area.",2009,0,
2591,2592,Research of fault current limiter for 500kV power grid,"Configuring large power supply in high load density networks and interconnections between power grids will cause enormous short-circuit current. If the short-circuit current rating of equipment in power system is exceeded, the equipment must be replaced, which is a very cost- and/or time-intensive procedure. A fault current limiter offers a choice in such case. First, this paper attempts to give an overview of fault current limiting measures for medium and high voltage applications. Then several representative fault current limiters including developing and developed measures are examined with an emphasis upon the thyristor protected series capacitor (TPSC) based short-circuit current limiter (SCCL). The SCCL would be the only technical proposal which can be used in high voltage system for its proven technology, low cost, high reliability, powerful performance and easy realization in high voltage system. The technology is used in the fault current limiter demonstration project in East China power grid. First, the main circuit topology of the fault current limiter demonstration project in East China power grid was introduced, together with the parameter specification of main-circuit equipments such as capacitor, reactor, damping circuit, thyristor, spark gap, MOV, and bypass circuit breaker. Then the measures and strategies of the current limiting reactor fast insertion and the over-voltage control and protection upon the series capacitors were introduced. After that the equivalent system of the fault current limiter demonstration project in East China SOOkV power grid was established using PSCAD/EMTDC, with which, an approach to identify fault signal fast based on the line current slope, and the coordination between the slope value and the instantaneous value of line current were studied in details by the aid of PSCAD/EMTDC simulation results. The optimal coordination and corresponding time setting value were given. FCL is series connected to the transmission line a- - nd should have influence upon protective relays to some extent. The effect of automatic circuit re-closer on FCL and the co-ordination between FCL control and protection system and protective relays must be researched. The influence of automatic reclosing on FCL and the coordination between FCL protection and line protection were performed by PSCAD/EMTDC simulation, and a simulative analysis-based rational solution was given. To demonstrate the validity of PSCAD/EMTDC simulation results, the real time digital simulation (RTDS) experiment of the control and protection system for FCL demonstration project in East China SOOkV power grid was done. The paper presents the RTDS results which consist of experimental scheme, experimental models, simulation results, and analysis of fault signal identification method and influence of fault current limiter on protective relaying above. Results of RTDS experiment can partially justify the operation of the FCL control and protection system.",2010,0,
2592,2593,Closed-Form Symbol Error Probabilities of Distributed Orthogonal Space-Time Block Codes,"Orthogonal space-time block codes allow utilising the diversity provided by multiple-input-multiple-output communication channels, thereby decreasing the error probability for a given communication rate. The contribution of this paper is the derivation of closed-form expressions of the symbol error probability of distributed codes deployed over Nakagami flat fading channels with different channel gains and fading parameters",2006,0,
2593,2594,Evaluating Alpha-induced soft errors in embedded microprocessors,"This paper presents the results of alpha single event upsets tests of an embedded 8051 microprocessor. Cross sections for the different memory resources (i.e., internal registers, code RAM, and user memory) are reported as well as the error rate for different codes implemented as test benchmarks. Test results are then discussed to find the contribution of each available resource to the overall device error rate.",2009,0,
2594,2595,Automating defects simulation and fault modeling for SRAMs,"The continues improvement in manufacturing process density for very deep sub micron technologies constantly leads to new classes of defects in memory devices. Exploring the effect of fabrication defects in future technologies, and identifying new classes of realistic functional fault models with their corresponding test sequences, is a time consuming task up to now mainly performed by hand. This paper proposes a new approach to automate this procedure. The proposed method exploits the capabilities of evolutionary algorithms to automatically identify faulty behaviors into defective memories and to define the corresponding fault models and relevant test sequences. Target defects are modeled at the electrical level in order to optimize the results to the specific technology and memory architecture.",2008,0,
2595,2596,Robust Adaptive Tracking Using Mixed Normalized/Unnormalized Estimation Errors,"Parameter adjustment mechanism has an important role to obtain the smooth and fast responses in adaptive control systems. Using the normalized estimation error can improve the robustness properties of the adaptive system despite the perturbations, whereas by which the admissible tracking error and fast convergence may not be obtained necessarily. This paper concerns with the design of a parameter adjustment mechanism ensures that robust, fast and smooth convergence is obtained despite the disturbances and parameter variations. The algorithm is developed based on a variable normalizing gain to guarantee the convergence and then improved by combining with an unnormalized estimation approach to meet all the desired specifications. The proposed algorithm is then applied to model reference adaptive control (MRAC) scheme to ensure that robust tracking is obtained despite the perturbations. Simulation results show the capability of the proposed algorithm compared to the pure normalized or unnormalized approaches.",2007,0,
2596,2597,Region-based stage construction protocol for fault tolerant execution of mobile agent,"Fault tolerance is essential to the development of reliable mobile agent systems in order to guarantee continuous execution of mobile agents. For this purpose, previous work has proposed fault tolerant protocols for mobile agent execution based on stage construction. However, when previous protocols are applied to a multiregion mobile agent computing environment, the overhead of work such as monitoring, election, voting and agreement is increased. We propose a region-based stage construction (RBSC) protocol for fault tolerant execution of mobile agents in a multiregion mobile agent computing environment. The RBSC protocol uses new concepts of quasiparticipant and substage in order to put together some places located in different regions within a stage in the same region. Therefore, the RBSC protocol decreases the overhead of stage works. Consequently, the RBSC protocol decreases the total execution time of mobile agents.",2004,0,
2597,2598,Design and implementation of a pluggable fault tolerant CORBA infrastructure,"In this paper we present the design and implementation of a Pluggable Fault Tolerant CORBA Infrastructure that provides fault tolerance for CORBA applications by utilizing the pluggable protocols framework that is available for most CORBA ORBS. Our approach does not require modification to the CORBA ORB, and requires only minimal modifications to the application. Moreover; it avoids the difficulty of retrieving and assigning the ORB state, by incorporating the fault tolerance mechanisms into the ORB. The Pluggable Fault Tolerant CORBA Infrastructure achieves performance that is similar to, or better than, that of other Fault Tolerant CORBA systems, while providing strong replica consistency.",2002,0,
2598,2599,A Motion-Compensated Error Concealment Scheme for H.264 Video Transmission,"For an entropy-coded video sequence, a transmission error in a codeword will not only affect the underlying codeword but may also affect subsequent codewords, resulting in a great degradation of received video frames. In this study, a motion-compensated error concealment scheme is proposed for H.264 video transmission. For H.264 inter-coded P frames, the predicted motion vector (PMV) for each corrupted block is first determined by the spatially neighboring motion vectors (MVs) around the corrupted block and the temporally motion-projected overlapping MVs in the previous frame. With the PMV being the central search point, three rood search patterns are developed for motioncompensated error concealment of small-, medium-, and large-motion corrupted blocks. Then error concealment refinement using Lagrange interpolation is performed on all the initially concealed blocks. Finally, the improved ELA (edge based line average) algorithm [11] is employed to refine each concealed block in P frames.",2006,0,
2599,2600,Fault analysis of current-controlled PWM-inverter fed induction-motor drives,"In this paper, the fault-tolerance capability of IM-drive is studied. The discussion on the fault-tolerance of IM drives in the literature has mostly been on the conceptual level without any detailed analysis. Most of studies are only achieved experimentally. This paper provides an analytical tool to quickly analyze and predict the performance under fault conditions. Also, most of the presented results were machine specific and not general enough to be applicable as an evaluation tool. So, this paper will present a generalized method for predicting the post-fault performance of IM-drives after identifying the various faults that can occur. The fault analysis for IM in the motoring mode will be presented in this paper. The paper includes an analysis for different classifications of drive faults. The faults in an IM-drive -that will be studied- can be broadly classified as: machine fault, (i.e., one of stator windings is open or short, multiple phase open or short, bearings, and rotor bar is broken) and inverter-converter faults (i.e., phase switch open or short, multiple phase fault, and DC-link voltage drop). Briefly, a general-purpose software package for variety of IM-drive faults -is introduced. This package is very important in IM-fault diagnosis and detection using artificial intelligent techniques, wavelet and signal processing.",2003,0,
2600,2601,Detection of CMOS defects under variable processing conditions,"Transient signal analysis is a digital device testing method that is based on the analysis of voltage transients at multiple test points. In this paper, the power supply transient signals from simulation experiments on an 8-bit multiplier are analyzed at multiple test points in both the time and frequency domain. Linear regression analysis is used to separate and identify the signal variations introduced by defects and those introduced by process variation. Defects were introduced into the simulation model by adding material (shorts) or removing material (opens) from the layout. 246 circuit models were created and 1440 simulations performed on defect-free, bridging defective and open defective circuits in which process variation was modeled by varying circuit and transistor parameters within a range of 25% of the nominal parameters. The results of the analysis show that it is possible to distinguish between defect-free and defective devices with injected process variation",2000,0,
2601,2602,Combining a spread spectrum technique with error-correction code to design an immune stegosystem,"Steganography is the art and science of hiding that a communication is taken place. It embeds the secret file (text, audio or image) in other carrier file. Text in image steganography is considered in this work. The proposed stegosystem uses Spread Spectrum technique which is applied in spatial domain together with error correction coding. These are used to increase the security and robustness of the system. Random location selection within the cover image pixels is also proposed in the work. Improvement has been achieved in robustness on the expense of reducing the capacity of hiding. The imperceptibility of the stego image is assessed by using peak signal-to-noise ratio (PSNR) measure. Attacks in the form of lossy compression and additive noise are considered. The performance of the proposed system has shown good immunity to moderate levels of channel noise and lossy compression ratios.",2008,0,
2602,2603,State Estimation and Fast Fault Detection For Ship Electrical Systems,"In this paper, the advantages of state estimation on ship power systems are demonstrated and extended to perform fast fault detection. For more-electric ships it becomes critical to monitor the system and respond to faults within milliseconds to limit damage from the fault and transfer to an intact supply. A compact ship allows syncronization of sampled real time voltage and current data from electrical sensors to compute the phase angle and voltage magnitude at every bus on the system. On a ship power system the data samples can be collected every 0.5 milliseconds by the central computer. Bad data analysis, smoothed values for the operating condition, and differential current sensing are performed within milliseconds. An example of state estimation is done for an LPD17 assault ship. Other examples of using phase data and fault detection are given in the paper. Algorithms for remedial action may be activated by the results of the state estimation and fault detection.",2007,0,
2603,2604,Fault Diagnosis Expert System Based on Integration of Fault-Tree and Neural Network,"The traditional fault diagnosis expert system is dependent on knowledge acquisition of the experts. Knowledge acquisition is recognized as the ""bottleneck"" problem of expert system. In addition, there are also some limitations of adaptive capacity, learning ability and real-time. And artificial neural network with good fault-tolerance and associative memory function, as well as very strong self-adaptive, self-learning ability, just can make up for the limitations of traditional expert system. This paper will construct a new expert system with the artificial neural network into and fault tree. Besides fault tree and neural network, this article mainly introduces the system model of fault diagnosis of the fire control computer and sensor subsystem, the method and process of fault diagnosis. In this expert system, we use the object-oriented production rule to represent the knowledge, which solves the bottleneck problem of the diagnostic knowledge acquisition effectively. The inferential process begins with the abnormal event and finally finds all of the possible faults and the faulty component. For some possible faulty components, which have large number of fault samples, the neural network model can be used to diagnose. The training network of fault samples employs the BP neural network. Finally, simulation training results show that the fault diagnosis expert system based on the combination of fault tree and neural network is rational and effective in fault diagnosis of the fire control system, realizes perfectly the combination of new knowledge and old one, and can grasp the state of systems dynamically.",2009,0,
2604,2605,Interactive Software and Hardware Faults Diagnosis Based on Negative Selection Algorithm,"Both hardware and software of computer systems are subject to faults. However, traditional methods, ignoring the relationship between software fault and hardware fault, are ineffective to diagnose complex faults between software and hardware. On the basis of defining the interactive effect to describe the process of the interactive software and hardware fault, this paper present a new matrix-oriented negative selection algorithm to detect faults. Furthermore, the row vector distance and matrix distance are constructed to measure elements between the self set and detector set. The experiment on a temperature control system indicates that the proposed algorithm has good fault detection ability, and the method is applicable to diagnose interactive software and hardware faults with small samples.",2008,0,
2605,2606,Upset-like fault injection in VHDL descriptions: A method and preliminary results,Investigates an approach allowing one to evaluate the consequences of single event upset phenomena for the reliable operation of processors. The method is based on the simulation of bit flips using a modified version of a high-level circuit description. Preliminary results illustrate the potential of this new strategy,2001,0,
2606,2607,Methodology for Reliability Evaluation of N-Version Programming Software Fault Tolerance System,"Software reliability can be improved by tolerating software faults, such as using N-version programming technique. Reliability evaluation is focused on the modeling and analysis techniques for fault prediction purpose. In this paper, a straightforward analysis method for evaluating reliability of software system established by N-version programming is proposed. The dependent failure parameters are assumed as random variables instead of constant. A case study is presented of the analysis of failure data from two software projects; the effectiveness of proposed evaluation methodology is demonstrated.",2008,0,
2607,2608,The application of neural networks and Clarke-Concordia transformation in fault location on distribution power systems,"This paper presents a new approach to fault location on distribution power lines. This approach uses an artificial neural network based learning algorithm and Clarke-Concordia transformation. The /spl alpha/,/spl beta/,0 components of line currents resulting from the Clarke-Concordia transformation are used to detect all types of fault. The neural network is trained to map the nonlinear relationship existing in fault location equations. The proposed approach is able to identify and locate all different types of faults (single line to ground, double line to ground, line-to-line and three-phase short-circuit). This approach is subdivided into several main steps: Data acquisition, corresponding on three-phase current signals; Mathematical treatment by the Clarke-Concordia transformation; Fault identification, obtained by the analysis of fault and pre-fault data; Fault location artificial neural network based learning algorithm. The fault position is presented as the output of the neural network on which, as the input, it was considered the eigenvalue of matrix representing transformed line current. Results are presented which shows the effectiveness of the proposed algorithm for a correct fault location on distribution power system networks.",2002,0,
2608,2609,The Impact of Coupling on the Fault-Proneness of Aspect-Oriented Programs: An Empirical Study,"Coupling in software applications is often used as an indicator of external quality attributes such as fault-proneness. In fact, the correlation of coupling metrics and faults in object oriented programs has been widely studied. However, there is very limited knowledge about which coupling properties in aspect-oriented programming (AOP) are effective indicators of faults in modules. Existing coupling metrics do not take into account the specificities of AOP mechanisms. As a result, these metrics are unlikely to provide optimal predictions of pivotal quality attributes such as fault-proneness. This impacts further by restraining the assessments of AOP empirical studies. To address these issues, this paper presents an empirical study to evaluate the impact of coupling sourced from AOP-specific mechanisms. We utilise a novel set of coupling metrics to predict fault occurrences in aspect-oriented programs. We also compare these new metrics against previously proposed metrics for AOP. More specifically, we analyse faults from several releases of three AspectJ applications and perform statistical analyses to reveal the effectiveness of these metrics when predicting faults. Our study shows that a particular set of fine-grained directed coupling metrics have the potential to help create better fault prediction models for AO programs.",2010,0,
2609,2610,PD-SOI MOSFETs: interface effect on point defects and doping profiles,"In this work, the influence of the Silicon/Buried Oxide interface (Si/BOX) on the electrical characteristics of silicon-on-insulator (SOI) MOSFETs is investigated by means of numerical simulations. Considering the State-of-Art dopant diffusion models and the recombining effect of Si/BOX interface on point defect, process simulations were performed to investigate the two-dimensional diffusion behaviour of the dopant impurities. The impact of the Si/BOX is investigated by analyzing the standard electrical characteristics of CMOS devices. Finally, a new electrical characterization methodology is detailed to better analyze dopants lateral diffusion profiles.",2009,0,
2610,2611,Research of Real-time Forecast Technology in Fault of Missile Guidance System Based on Grey Model and Data Fusion,"To solve fault forecast in missile guidance system, a new fault forecast method was presented, in which the grey system and multiple-sensor data fusion were used. Grey Model (GM) forecast is invalid when the data sequence is zero-mean random process, to overcome the drawback, present an improved GM method. The simulation results show the fault forecast method has better performance in missile guidance systems.",2007,0,
2611,2612,Re-configuration of task in flight critical system  Error detection and control,"The paper presents the error detection and control for control metrics of the re-configuration algorithm in an embedded avionics application with extensive checks and validation. This is being carried out in real-time for decision-making. The success of the re-configurable algorithm is based on the integrity of the data from multiple sources. Hence, the integrity checks of these sources need to be controlled and maintained. Integrity checks as part of error detection and control mechanism is implemented using the Hamming code with error detection and error handling capabilities. The paper presents the experimental simulation studies in both Xilinx platform and VxWorks with target. The control parameters used in the re-configuration algorithm is treated with phase conditions of flight, data sampling and averaging before it is being applied for decision-making process. The integrity and error control/detection is quite critical particularly for the validation of control parameters used for re-configuration in the algorithm and hence the error detection and control scheme is designed and simulated using the Xilinx FPGA platform. The paper presents the algorithm in brief, data sampling techniques based on multiple threshold, identification of phases in flight, error detection/control mechanisms for data integrity and validity. The experimental and simulation studies related to the above areas are detailed with results.",2009,0,
2612,2613,Multiple disease (fault) diagnosis with applications to the QMR-DT problem,"In this paper, we present three classes of computationally efficient algorithms that can handle cases with hundreds of positive findings in QMR-DT(Quick Medical Reference, Decision-Theoretic) Network. These include Lagrangian Relaxation Algorithm (LRA), Primal Heuristic Algorithm (PHA), and Approximate Belief Revision Algorithm (ABR). These algorithms solve the QMR-DT problem by finding the most likely set of diseases given the findings. Extensive computational experiments have shown that LRA obtains the best solutions among the three algorithms proposed within a relatively small processing time. We also show that the Variational Probabilistic Inference method is a special case of our LRA. The solutions are generic and have application to multiple fault diagnosis in complex industrial systems.",2003,0,
2613,2614,Development of a dynamic routing system for a fault tolerant solid state mass memory,This paper describes a fault tolerant Solid State Mass Memory (SSMM) for satellite applications. Definition of requirements plays an important role in the architectural solutions selected for the data storing system. The interconnection system proposed is based on a crossbar switch with half duplex links. All the connections have a complete flow control and the path of switched packets is dynamically reconfigurable. A controller tests functionality of each component and handles dynamic data allocation in the memory modules. Memory modules grant data integrity: an original tool developed by the authors derives data coding parameters,2001,0,
2614,2615,Error resilient macroblock rate control for H.264/AVC video coding,"In this paper, an error resilient rate control scheme for the H.264/AVC standard is proposed. This scheme differs from traditional rate control schemes in that macroblock mode decisions are not made only to minimize their rate-distortion cost, but also take into account that the bitstream will have to be transmitted through an error-prone network. Since channel errors will probably occur, error propagation due to predictive coding should be mitigated by adequate Intra coding refreshes. The proposed scheme works by comparing the rate-distortion cost of coding a macroblock in Intra and Inter modes: if the cost of Intra coding is only slightly larger than the cost of Inter coding, the coding mode is changed to Intra, thus reducing error propagation. Additionally, cyclic Intra refresh is also applied to guarantee that all macroblocks are eventually refreshed. The proposed scheme outperforms the H.264/AVC reference software, for typical test sequences, for error-free transmission and several packet loss rates.",2008,0,
2615,2616,Interactive fault localization techniques in a spreadsheet environment,"End-user programmers develop more software than any other group of programmers, using software authoring devices such as multimedia simulation builders, e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been only a little research on finding ways to help these programmers with the dependability of the software they create. We have been working to address this problem in several ways, one of which includes supporting end-user debugging activities through interactive fault localization techniques. This paper investigates fault localization techniques in the spreadsheet domain, the most common type of end-user programming environment. We investigate a technique previously described in the research literature and two new techniques. We present the results of an empirical study to examine the impact of two individual factors on the effectiveness of fault localization techniques. Our results reveal several insights into the contributions such techniques can make to the end-user debugging process and highlight key issues of interest to researchers and practitioners who may design and evaluate future fault localization techniques.",2006,0,
2616,2617,Corrective action with power converter for faulty multiple fuel cells generator used in transportation,"This paper deals with the corrective action with a power converter for a 100kW multiple fuel cells (FC) generator under fault and used for vehicle propulsion, or high power onboard electrical assistance. The objective is to permit, through the power converter and its control strategy, a soft shut-down of a FC stack in fault and guarantee a continuous of operation at a reduced power, acceptable by the specifications. The power converter should also realize the power management during the degraded working situation. Two power system architectures are studied and compared by numerical simulation.",2010,0,
2617,2618,Forward error correction techniques suitable for the utilization in the PLC technology,"Todaypsilas PLC systems are mostly limited in transmission speeds to several kilobits per seconds. Such speeds are certainly not sufficient for running popular services like a broadband Internet, a teleworking or the Video on Demand. This fact is reducing a chance of the PLC technologypsilas deployment in a wide range. One way how to overcome this problem could be in introduction of optimal errors correcting techniques into the PLC technology. In this contribution, different types of coding techniques are discussed, including RS, Turbo or Convolutional codes. Theoretical analyses are focused on their positive and negative features visible in case of their deployment in the PLC transmission environment. Special attention is pointed to their robustness against noises, which is analyzed also practically in form of the simulations. Analyses and simulations are done for individual codes as well as combination of codes.",2008,0,
2618,2619,SIFU!-a didactic stuck-at fault simulator,"This paper presents a didactic simulator for stuck-at (sa) faults on logic circuits. The tool has a set of features that helps to understand the concepts of single and multiple stuck-at faults, being these faults testable or not, and how to generate test vectors in order to test the detectable fault subset. An interface was developed to allow the edition of a circuit, the injection of faults and the fault simulation. The tool performs two simulations concurrently, one for the original circuit and another for the faulty circuit considering the injected faults. When the two simulations differ, for a given input vector, the tool shows the error (detection of the fault) graphically.",2003,0,
2619,2620,Error density metrics for business process model,"In this paper, metrics for business process model (BPM), are proposed, which are capable to measure the usability and effectiveness of BPMs. The proposed model is adapting error density metrics to BPMs by considering the similarities between the conceptual characteristics of BPMs and software products. We applied seven software metrics for evaluating quality of business processes/ process models. Results show that our metrics help the organization to improve their process, as weighted measurements are indicators for unexpected situations/behaviour for business processes.",2009,0,
2620,2621,On the Automatic Generation of Test Programs for Path-Delay Faults in Microprocessor Cores,"Delay testing is mandatory for guaranteeing the correct behavior of today's high-performance microprocessors. Several methodologies have been proposed to tackle this issue resorting to additional hardware or to software self test techniques. Software techniques are particularly promising as they resort to Assembly programs in normal mode of operation, without requiring circuit modifications; however, the problem of generating effective and efficient test programs for path- delay fault detection is still open. This paper presents an innovative approach for the generation of path-delay self-test programs for microprocessors, based on an evolutionary algorithm and on ad-hoc software simulation/hardware emulation heuristic techniques. Experimental results show how the proposed methodology allows generating suitable test programs in reasonable times.",2007,0,
2621,2622,An extended CORBA event service with support for load balancing and fault-tolerance,"Previously, the Object Management Group (OMG) has published a standard for a common object service, called the event service, to support decoupled and asynchronous communication between distributed CORBA object components. However, the service, albeit flexible, still suffers from a number of limitations. Among others, it has poor scalability, and it is not totally reliable. In view of these drawbacks, we propose a generic framework which extends the event service with built-in support for load balancing (both static and dynamic) and fault tolerance. These functions are achieved transparently without the intervention of the application objects",2000,0,
2622,2623,Performance analysis of a fault-tolerant distributed-shared memory protocol on the SOME-bus multiprocessor architecture,"Interconnection networks allowing multiple simultaneous broadcasts are becoming feasible, mostly due to advances in fiber-optics and VLSI technology. Distributed-shared-memory implementations on such networks promise high performance even for applications with small granularity. This paper summarizes the architecture of one such implementation, the simultaneous optical multiprocessor exchange bus, and examines the performance of an augmented DSM protocol which provides fault tolerance by exploiting the natural DSM replication of data in order to maintain a recovery memory in each processing node. Theoretical and simulation results show that the additional data replication necessary to create fault-tolerant DSM causes no reduction in system performance during normal operation and eliminates most of the overhead at checkpoint creation. Data blocks which are duplicated to maintain the recovery memory may be utilized by the regular DSM protocol, reducing network traffic, and increasing the processor utilization significantly.",2003,0,
2623,2624,DSP implementation of predictive control strategy for power factor correction (PFC),"A predictive algorithm for digital control PFC is presented in this paper. Based on this algorithm, all of the duty cycles required to achieve unity power factor in one half line period are calculated in advance by the DSP. A boost converter controlled by these precalculated duty cycles can achieve sinusoidal current waveform. Input voltage feed-forward compensation makes the output voltage insensitive to the input voltage variation and guarantees sinusoidal input current even if the input voltage is distorted. A prototype of boost PFC controlled by a DSP evaluation board was setup to implement the proposed predictive control strategy. Test results show that the proposed predictive strategy for PFC achieves unity power factor.",2004,0,
2624,2625,FPGA-based fault injection for microprocessor systems,"In this paper we propose an approach to speed-up fault injection campaigns for the evaluation of dependability properties of processor-based systems. The approach exploits FPGA devices for system emulation, and new techniques are described, allowing emulating the effects of faults and to observe faulty behavior. The proposed approach combines the speed of hardware-based techniques, and the flexibility of simulation-based techniques. Experimental results are provided showing that speed-up figures up to 3 orders of magnitude with respect to state-of-the-art simulation-based techniques can be achieved",2001,0,
2625,2626,Design and simulation of Optical Frequency Domain reflectometer for short distance fault detection in optical fibers and integrated optical devices using ptolemy-II,"Optical Frequency Domain measurements are much more accurate and efficient as compared to that in time domain for short distances of fibers or integrated optical devices. In the present investigation, the traditional Optical Time Domain Reflectometry (OTDR) technique has been combined, which is used for analysis of fibers of large length, with frequency analysis of the time domain signal to develop a system that detects faults in optical IC's or fibers of very small lengths in a non-destructive fashion. The drawback of the OTDR is that it can be used only for cables of long distances because when used in short fibers the total time taken between the injected laser beam and the response will be negligible. To circumvent this problem we use the frequency domain analysis of the time domain data by applying Fourier transform.",2009,0,
2626,2627,Attention-based adaptive intra refresh for error-prone video transmission,"Error-resilient video coding plays an important role in video transmission over error-prone networks. Intra coding, which can suppress error propagation very well in a simple way, is often employed in error-resilient video coding in some special ways and named as intra refresh. However, most of the existing intra refresh methods do not make good use of subjective human vision property. In this article a novel intra refresh method is presented, taking the fact that people usually intend to pay much more attention to a certain particular area in a video frame (attention area) than to other areas (nonattention area) in the same frame. In this method, blocks in the attention area of each video frame have a higher priority to be refreshed as intra mode than the blocks outside the attention area. Meanwhile, an attention-based end-to-end rate distortion model with reference restriction for both inter coding and intra coding is developed to obtain a better subjective quality. Experimental results in H.264/AVC-based video coding demonstrate that, when compared at the same bit rate and the same packet loss condition, the proposed method can provide a much better subjective feeling than existing intra refresh methods",2007,0,
2627,2628,"On campus beta site: architecture designs, operational experience, and top product defects","Testing network products in a beta site to reduce the possibility of customer found defects is a critical phase before marketing. We design and deploy an innovative beta site on the campus of National Chiao Tung University, Hsinchu, Taiwan. It can be used for developers to test and debug products, while maintaining network quality for network users. To satisfy the needs of developers, we set up environments and mechanisms, such as a variety of test zones for multiple types of products or systems under test (SUTs), remote control, degrees of traffic volume, and traffic profiling. For network users, we set up mechanisms of failure detection, notification, and recovery. The beta site network users are all volunteers. Test results show that beta site testing is good for finding stability and compatibility defects. The period starting from the beginning of a test until the next defect is found is called the time to fail (TTF). We call it converged if the TTF exceeds four weeks, and the convergence ratio is the percentage of SUTs that reach convergence. We find that the TTF increases with longer test duration, meaning that product quality improves through beta site testing. However, the convergence ratios are only 7 and 20 percent for test durations of one month and one year, respectively, meaning that few products operate faultlessly for a long duration. The convergence ratios also indicate that it takes much more time to enhance product quality to be converged. Therefore, if considering both marketing timing and product quality, one month is our suggested minimum TD for low-end and shortlife- cycle products. However, we recommend one year as the minimum TD for high-end and long-life-cycle products.",2010,0,
2628,2629,The Simplex Reference Model: Limiting Fault-Propagation Due to Unreliable Components in Cyber-Physical System Architectures,"Cyber-physical systems are networked, component-based, real-time systems that control and monitor the physical world. We need software architectures that limit fault-propagation across unreliable components. This paper introduces our simplex reference model which is distinguished by: a plant being controlled in an external context, a machine performing the control, a domain model that estimates the plant state, and the safety requirements that must be met. The simplex reference model assists with constructing CPS architectures which limit fault-propagation. We present a representative case study to highlight the ideas behind the model and our particular decomposition.",2007,0,
2629,2630,Synchronization and fault detection in autonomous robots,"In this study, we show a group of robots can synchronize based on firefly-inspired flashing behavior and how dead robots can be detected by other robots. The algorithm is completely distributed. Each robot flashes by lighting up its on-board LEDs and neighboring robots are driven to flash in synchrony. Since robots that are suffering catastrophic failures do not flash periodically, they can be detected by operational robots. On a real multi-robot system of 10 autonomous robots, we show how the group can correctly detect multiple faults, and that when given (simulated) repair capabilities, the group can survive a relatively high rate of failure.",2008,0,
2630,2631,Influence of spatial wavefront characteristics of laser radiation on quality intracavity modal correction,"The laser systems supplied by an adaptive optical system allow obtaining output beam with high quality spatial-power characteristics. The efficiency of functioning of adaptive optical system phase conjugation with modal correcting by the bimorph deformable mirror of the resonator was investigated. The form of a mirror surface was modeled by Zernic polynom, and wavefront was modeled by the spline-interpolation of the limited set of random numbers. In the decision of the task of formation of operation action on a deformed mirror various methods of optimization were used. The influence of a standard deviation and relative entropy both of phases and angles of the slopes of the wavefront on quality of correction was examined. The quality of correction was estimated on static and dynamic errors of wavefront reconstruction. Moreover connection among spatial characteristics of a radiation beam was estimated",2005,0,
2631,2632,An Investigation into the Functional Form of the Size-Defect Relationship for Software Modules,"The importance of the relationship between size and defect proneness of software modules is well recognized. Understanding the nature of that relationship can facilitate various development decisions related to prioritization of quality assurance activities. Overall, the previous research only drew a general conclusion that there was a monotonically increasing relationship between module size and defect proneness. In this study, we analyzed class-level size and defect data in order to increase our understanding of this crucial relationship. In order to obtain validated and more generalizable results, we studied four large-scale object-oriented products, Mozilla, Cn3d, JBoss, and Eclipse. Our results consistently revealed a significant effect of size on defect proneness; however, contrary to common intuition, the size-defect relationship took a logarithmic form, indicating that smaller classes were proportionally more problematic than larger classes. Therefore, practitioners should consider giving higher priority to smaller modules when planning focused quality assurance activities with limited resources. For example, in Mozilla and Eclipse, an inspection strategy investing 80% of available resources on 100-LOC classes and the rest on 1,000-LOC classes would be more than twice as cost effective as the opposite strategy. These results should be immediately useful to guide focused quality assurance activities in large-scale software projects.",2009,0,
2632,2633,Development and Implementation of a Novel Fault Diagnostic and Protection Technique for IPM Motor Drives,"This paper presents the practical implementation of a novel fault diagnostic and protection scheme for the interior permanent-magnet (IPM) synchronous motors using wavelet packet transform (WPT) and artificial neural network. In the proposed technique, the line currents of different faulted and normal conditions of the IPM motor are preprocessed by the WPT. The second level WPT coefficients of line currents are used as inputs of a three-layer feedforward neural network. The proposed protection technique is successfully simulated and experimentally tested on the line-fed and inverter-fed IPM motors. The Texas Instrument 32-bit floating-point digital signal processor TMS320C31 is used for the real-time implementation of the proposed protection algorithm. The offline and online test results of both line-fed and inverter-fed IPM motors are given. These test results showed satisfactory performances of the proposed diagnostic and protection technique in terms of speed, accuracy, and reliability.",2009,0,
2633,2634,A background removing method of MR images and its application in the intensity nonuniformity correction methods,"Intensity nonuniformity correction is a necessary preprocessing method in MR images segmentation. Most of intensity nonuniformity correction methods need to remove the background of the images. In this paper, we propose a segmentation method based on the region growing to remove the background as a instead of the threshold method which is wildly used in the intensity nonuniformity correction methods. We tested this method on a lot of MR images and applied it to the N3 method. The experiments showed that it performances better than the threshold method.",2008,0,
2634,2635,Design and analysis of a reduced phase error digital carrier recovery architecture for high-order quadrature amplitude modulation signals,"With increasing order of quadrature amplitude modulation (QAM), the bandwidth efficiency is improved in digital communication. However, in practice, the modulation order is limited, since conventional digital carrier recovery (CR) algorithms give rise to unacceptable phase error. The authors present an efficient software-aided technique for phase error reduction in CR for high-order QAM, based on the simple and well-known fourth power CR loop. Analytical and simulation results indicate that the new technique has several attractive features such as approximate of invariance of phase error improvement over modulation order and low hardware complexity for modulation orders as high as 256-QAM. Experimental results for 64 and 256-QAM illustrate phase error variance of less than -110-dBc/Hz at the frequency offset of 10-kHz, that is, 30-dB reduction of phase error variance or 3-dB increase in system processing gain compared to the conventional fourth power CR loop. This allows a significant improvement of bandwidth efficiency by increasing the modulation order, at the cost of slight complexity overhead.",2010,0,
2635,2636,Short Paper: Data Mining-based Fault Prediction and Detection on the Grid,"This paper describes a novel approach to fault detection and prediction on the grid based on data mining techniques. Data mining techniques are here applied as a mean to effectively process the significant amount of captured data from grid sites, services, workflows and activities. The paper provides a first approach of proposed techniques in terms of its ability of utilizing relevant information and the fault tolerance requirements. Such approach is one intelligent, distributed framework of fault detection and prediction for anomaly and failed activity by using resource- and workflow-based information. We use fault predictions to improve the performance of the workflow execution by avoiding potential faults of activities",2006,0,
2636,2637,Real-time position error detecting in nanomanipulation using Kalman filter,"The main roadblock to atomic force microscope (AFM) based nanomanipulation is lack of real time visual feedback. Although the model based visual feedback can partly solve this problem, due to the complication of nano environment, it is difficult to accurately describe the behavior of nano-objects with a model. The modeling error will lead to an inaccurate feedback and a failed manipulation. In this paper, a Kalman filter is developed to real time detect this modeling error. During manipulation, the residual between the estimated behavior and the visual display behavior is real time updated. The residual's Mahalanobis distance is calculated and compared with an threshold to determine whether there is a position error. Once the threshold is exceeded, an alarm signal will be triggered to tell the system there is a position error. Furthermore, the position error can be on-line corrected by local scan method. With the assistance of Kalman filter and local scan, the position error not only can be real-time detected, but also can be online corrected. The visual display keeps matching with the real manipulation result during the whole manipulation process, which significantly improve the efficiency of the AFM based nano-assembly. Experiments of manipulating nano-particles are presented to verify the effectiveness of Kalman filter and local scan method.",2007,0,
2637,2638,Adaptive fault tolerant systems: reflective design and validation,"Reflection has been used with some success, since quite a few years now, for dealing with separation of concerns and transparency of fault-tolerance mechanisms for the application. Nevertheless, it has also shown some concern regarding the control of fine-grain information such as thread control or other deep aspects of the platform. We propose here the use of a new concept, called multi-level reflection, for firstly solving these issues, but also for introducing more adaptation into fault-tolerant reflective architectures. We also discuss some essential validation issues of reflective systems, which are still a challenge for future research.",2003,0,
2638,2639,Fault tolerance for distributed process control system,"The Yokogawa Electric Corporation has been developing distributed process control systems (DCS) since 1975. As the core of plant control, the DCS has placed increasingly severe demands on reliability. This paper describes the fault tolerant technology of the Duplexed Field Control Station in DCS and the online exchange and concurrent operation of current and new CPU cards technology supporting long-term use.",2002,0,
2639,2640,A novel family of weighted average voters for fault-tolerant computer control systems,"Fault masking is a widely used strategy for increasing the safety and reliability of computer control systems. The approach uses some form of voting to arbitrate between the results of hardware or software redundant modules for masking faults. Several voting algorithms have been used in fault tolerant control systems; each has different features, which makes it more applicable to some system types than others. This paper introduces a novel family of weighted average voters suitable for redundant sensor (and other inertial measurement unit) planes, at the interface level, of control systems. It uses two tuneable parameters, each with a ready interpretation, to provide a flexible voting performance when using the voter in different applications. The weight assignment technique is transparent to the user because the impact of the degree of agreement between any voter input and the other inputs is directly reflected in the weight value assigned to that input. The voter can be tuned to behave as the well-known inexact majority voter that is generally used in safety-critical control systems at different voting planes. We evaluated the performance of four versions of the novel voter through a series of fault injection experiments, and compared the results with those of the well-known Lorczak's weighted average voter. The experimental results showed that the novel voter gives more correct outputs (1%12% higher reliability) than the Lorczak's voter in the presence of small permanent and transient errors. With large errors, lower-order versions of the novel voter give better performance than the ones with higher orders.",2003,0,
2640,2641,Pattern recognition-a technique for induction machines rotor fault detection broken bar fault,"A pattern recognition technique based on Bayes minimum error classifier is developed to detect broken rotor bar faults in induction motors at the steady state. The proposed algorithm uses only stator currents as input without the need for any other variables. First rotor speed is estimated from the stator currents, then appropriate features are extracted. The produced feature vector is normalized and fed to the trained classifier to see if motor is healthy or has broken bar faults. Only number of poles and rotor slots are needed as preknowledge information. Theoretical approach together with experimental results derived from a 3 hp AC induction motor show the strength of the proposed method. In order to cover many different motor load conditions data are obtained from 10% to 130% of the rated load for both a healthy induction motor and an induction motor with a rotor having 4 broken bars",2001,0,
2641,2642,Data mining for distribution system fault classification,"Digital relaying equipment at substations allow for large amounts of data storage that can be triggered by predetermined system conditions. Some of this information retrieved from relays at several locations in a local utility's service territory has been mined for determining trends and relationships. Data mining aims to make sense of the retrieved data by revealing meaningful relationships. This paper discusses some useful data mining techniques that are applied to data recorded by overcurrent relays at several substations. The purpose is to classify faults, verify relay settings and determine fault induced trip per substations. High accuracy is obtained.",2005,0,
2642,2643,Comparison and application of different VHDL-based fault injection techniques,"Compares different VHDL-based fault injection techniques: simulator commands, saboteurs and mutants for the validation of fault tolerant systems. Some extensions and implementation designs of these techniques have been introduced. Also, a wide set of non-usual fault models have been implemented. As an application, a fault tolerant microcomputer system has been validated. Faults have been injected using an injection tool developed by the GSTF. We have injected both transient and permanent faults on the system model, using two different workloads. We have studied the pathology of the propagated errors, measured their latencies, and calculated both detection and recovery coverages. Preliminary results show that coverages for transient faults can be obtained quite accurately with any of the three techniques. This enables the use of different abstraction level models for the same system. We have also verified significant differences in implementation and simulation cost between the studied injection techniques",2001,0,
2643,2644,Software Defects Prediction using Operating Characteristic Curves,We present a software defect prediction model using operating characteristic curves. The main idea behind our proposed technique is to use geometric insight in helping construct an efficient and fast prediction method to accurately predict the. cumulative number of failures at any given stage during the software development process. Our predictive approach uses the number of detected faults instead of the software failure-occurrence time in the testing phase. Experimental results illustrate the effectiveness and the much improved performance of the proposed method in comparison with the Bayesian prediction approaches.,2007,0,
2644,2645,Safety-reliability of distributed embedded system fault tolerant units,"In this paper we compare the relative performance of two fault tolerant mechanisms dealing with repairable and non-repairable components that have failed. The relative improvement in the reliability and safety of a system with repairable components is calculated with respect to the corresponding system where the components are not repairable. The fault tolerant systems under study correspond to a flexible arrangement of fault tolerant units (FTU's) suitable for dependable distributed embedded systems. A simple simulation-based methodology to numerically evaluate dependability functions of a wide variety of fault tolerant units is used. The method is based on simulation of stochastic Petri nets. A set of 15 FTU configurations belonging to five groups is analysed. The methodology allows a quick and accurate evaluation of dependability functions of any distributed embedded system design in terms of the type of FTU (i.e., node or application), replicas per group, replicas per FTU, with or without repair functionality, and shared replicas.",2003,0,
2645,2646,Reliable online water quality monitoring as basis for fault tolerant control,"Clean data are essential for any kind of alarm or control system. To achieve the required level of data quality in online water quality monitoring, a system for fault tolerant control was developed. A modular approach was used, in which a sensor and station management module is combined with a data validation and an event detection module. The station management module assures that all relevant data, including operational data, is available and the state of the monitoring devices is fully documented. The data validation module assures that unreliable data is detected, marked as such, and that the need for sensor maintenance is timely indicated. Finally, the event detection module marks unusual system states and triggers measures and notifications. All these modules were combined into a new software package to be used on water quality monitoring stations.",2010,0,
2646,2647,Transparent Fault Tolerance of Device Drivers for Virtual Machines,"In a consolidated server system using virtualization, physical device accesses from guest virtual machines (VMs) need to be coordinated. In this environment, a separate driver VM is usually assigned to this task to enhance reliability and to reuse existing device drivers. This driver VM needs to be highly reliable, since it handles all the I/O requests. This paper describes a mechanism to detect and recover the driver VM from faults to enhance the reliability of the whole system. The proposed mechanism is transparent in that guest VMs cannot recognize the fault and the driver VM can recover and continue its I/O operations. Our mechanism provides a progress monitoring-based fault detection that is isolated from fault contamination with low monitoring overhead. When a fault occurs, the system recovers by switching the faulted driver VM to another one. The recovery is performed without service disconnection or data loss and with negligible delay by fully exploiting the I/O structure of the virtualized system.",2010,0,
2647,2648,Correction of motion artifact in cardiac optical mapping using image registration technique,"Myocardial contraction causes motion artifact in cardiac optical recording. Mechanical and chemical methods have been used, both with significant limitations, to reduce motion artifact in optical mapping. We propose an image registration approach using mutual information between image frames to solve this problem. The algorithm was tested with optical mapping data from isolated, perfused rabbit hearts. Both affine and nonrigid registration methods reduced motion artifact as measured by a reduction in excessive positive and negative deflection in the optical potential traces after the registration process. Such an approach could be further developed for real-time, in vivo electrophysiological measurement.",2002,0,
2648,2649,New attenuation correction for the HRRT using transmission scatter correction and total variation regularization,In the standard software for the Siemens HRRT PET scanner the most commonly used segmentation in the A-map reconstruction for human brain scans is MAP-TR. Problems with bias in the lower cerebellum and pons in HRRT brain images have been reported. The main source of the problem is poor bone / soft tissue segmentation in these regions and the lack of scatter correction in the A-map reconstruction. In this paper we describe and validate the new TXTV segmentation method (included in the HRRTU 1.0 and 1.1 user software) aimed at solving the bias problem.,2009,0,
2649,2650,Detecting packet-dropping faults in mobile ad-hoc networks,"Mobile ad-hoc networks are inherently prone to security attacks, with node mobility being the primary cause in allowing security breaches. This makes the network susceptible to Byzantyne faults with packets getting misrouted, corrupted or dropped. In this paper we propose solutions using an unobtrusive monitoring technique using the ""detection manager"" to locate malicious or faulty nodes that misroute, corrupt or drop packets. The unobtrusive monitoring technique is similar to an intrusion detection system that monitors system activity logs to determine if the system is under attack. This technique uses information from different network layers to detect malicious nodes. The detection manager we are developing for mobile ad-hoc networks stores several rules for responding to different situations. Any single node in the network can use unobtrusive monitoring without relying on the cooperation of other nodes, which makes unobtrusive monitoring easy to implement and deploy. Simulations of mobile ad-hoc networks that contain malicious nodes indicate that unobtrusive monitoring has a high detection effectiveness with low false positive rate.",2003,0,
2650,2651,Effects of phase-locked-loop circuit on a self-commutated BTB system under line faults,"A self-commutated BTB (back-to-back) system is an AC-DC-AC power flow controller between two utility grids, and PLL (phase-locked-loop) circuits are typically used for detecting their phase information. The performance of the BTB system during line faults is strongly affected by the dynamic behavior of the PLL circuit in terms of the AC-current fluctuation, the DC-voltage fluctuation, and the DC magnetic flux deviation in the converter-transformers. However, no paper or article has been discussed explicitly on their mutual relationship. The aim of this paper is to establish a design procedure of the PLL circuit which is suitable for the BTB system. This paper also deals with the DC magnetic flux deviation in the converter-transformers under double-line-to-ground (DLG) faults.",2008,0,
2651,2652,A new protection algorithm for EHV transmission line based on singularity detection of fault transient voltage,A new non-unit voltage protection algorithm for extra high voltage (EHV) transmission lines is presented in this paper. The singularity of fault transient voltage signals in one terminal is utilized to discriminate clearly between internal and external faults. Lipschitz exponent (LE) is obtained from wavelet modulus maximum to detect singularity of signals. A typical 500 kV EHV transmission system has been simulated by ATP to evaluate the scheme. The simulation results show that this scheme is capable of providing correct responses under various system configurations and fault conditions,2005,0,
2652,2653,Increasing power efficiency in transmitter diversity systems under error performance constraints,"Motivated by combinatorial optimization theory, we propose an algorithmic power allocation method that minimizes the total transmitting power in transmitter diversity systems, provided that the instantaneous Bit-Error-Rate (BER) is not greater than a predetermined value. This method applies to many practical applications where the power transmitted by each antenna is constrained. We also provide closed-form expressions for the average total transmitted power for the case of two transmitting antennas operating in Rayleigh fading, and the average number of active antennas at the transmitter assuming Nakagami-m fading channels. Simulations and numerical results show that, compared to the conventional equi-power scheme, the proposed model offers a considerable reduction in the total transmitting power and the average number of active antennas, without loss in error performance.",2008,0,
2653,2654,Characteristics of fault-tolerant photodiode and photogate active pixel sensor (APS),"A fault-tolerant APS has been designed by splitting the APS pixel into two halves operating in parallel, where the photo sensing element has been divided in two and the readout transistors have been duplicated while maintaining a common row select transistor. This split design allows for a self correcting pixel scheme such that if one half of the pixel is faulty, the other half can be used to recover the entire output signal. The fault tolerant APS design has been implemented in a 0.18 m CMOS process for both a photodiode based and photogate based APS. Test results show that the fault tolerant pixels behave as expected where a non-faulty pixel behaves normally, and a half faulty pixel, where one half is either stuck low or high, produces roughly half the sensitivity. Preliminary results indicate that the sensitivity of a redundant pixel is approximately three times that of a traditional pixel for the photodiode APS and approximately twice that for the photogate APS.",2004,0,
2654,2655,Studies of the computational error in the probabilistic eigenvalue analysis,"Several models for probabilistic eigenvalue analysis are comparatively studied in computational precision in this paper. The focus is on the computational errors of eigenvalue expectation and variance, as well as the high cumulants. Based on particular testing systems, the study provides a useful error comparison for further application.",2004,0,
2655,2656,Locating fault using voltage sags profile for underground distribution system,This paper presents an alternative fault location algorithm to estimate short-circuit faults location in electrical distribution networks using only voltage sags data. The proposed algorithm uses voltage sags profile as a means to locate fault. The possible fault locations is estimated by incorporating the measured voltage sags magnitude and its corresponding phase angle into an equation of voltage sag as a function of fault distance. A ranking procedure is also introduced to rank possible fault locations due the same electrical distance. The uncertainty of fault resistance is also considered in this algorithm. The performance of the technique is presented by testing it using an actual underground distribution network. The simulation results indicated a possibility of practical implementation.,2010,0,
2656,2657,Application of weighted least squares to OSL vector error correction,"The use of least squares has been beneficial in providing more accurate one-port calibrations. Commercially available ECAL units have taken advantage of this method. A more generalized application of a least squares method, the weighted least squares method provides benefit when the models for the calibration standards are not all trusted equally. The use of the weighted least squares provides a method of discounting the effect of calibration standards as the model accuracy degrades instead of abruptly dropping the use of the standard outside of a specified frequency range avoiding discontinuities in subsequent measurements. A detailed view a 1.85 m calibration kit demonstrates the improved results possible using a weighted least squares method. The paper includes the description of a proximity function that enhances the results when the response of two or more standards begin to cluster. Additional enhancements due to databased models over traditional polynomial models is also presented.",2003,0,
2657,2658,Introspection-Based Fault Tolerance for COTS-Based High-Capability Computation in Space,"Future missions of deep space exploration face the challenge of designing, building,and operating progressively more capable autonomous spacecraft and planetary rovers. Given the communication latencies and bandwidth limitations for such missions, the need for increased autonomy becomes mandatory, along with the requirement for enhanced on-board computational capabilities while in deep space or time-critical situations. This will result in dramatic changes in the way missions will be conducted and supported by on-board computing systems. Specifically, the traditional approach of relying exclusively on radiation-hardened hardware and modular redundancy will not be able to deliver the required computational power. As a consequence, such systems are expected to include high-capability low-power components based on emerging Commercial-Off-The-Shelf (COTS) multi-core technology. This paper describes the design of a generic framework for introspection that supports runtime monitoring and analysis of program execution as well as a feedback-oriented recovery from faults. One of the first applications of this framework will be to provide flexible software fault tolerance matched to the requirements and properties of applications by exploiting knowledge that is either contained in an application knowledge base, provided by users, or automatically derived from specifications. A prototype implementation is currently in progress at the Jet Propulsion Laboratory, California Institute of Technology, targeting a cluster of Cell Broadband Engines.",2008,0,
2658,2659,Supervisory control of software systems for fault mitigation,This paper develops a novel technique of discrete-event supervisory control for fault mitigation in software applications. It models the interactions between a software application and a computer operating system (OS) as a deterministic finite state automation. The supervisor restricts the language of the OS to correct deviations such as CPU exceptions for the controlled execution of software applications. Feasibility of this supervisory control concept is demonstrated on process execution under the Red Hat Linux 7.2 operating system. Two supervisory control policies are implemented as proof of the concept.,2003,0,
2659,2660,Supporting Reconfigurable Fault Tolerance on Application Servers,"Dynamic reconfiguration support in application servers is a solution to meet the demands for flexible and adaptive component-based applications. However, when an application is reconfigured, its fault-tolerant mechanism should be reconfigured either. This is one of the crucial problems we have to solve before a fault-tolerant application is dynamically reconfigured at runtime. This paper proposes a fault-tolerant sandbox to support the reconfigurable fault-tolerant mechanisms on application servers. We present how the sandbox integrates multiple error detection and recovery mechanisms, and how to reconfigure these mechanisms at runtime, especially for coordinated recovery mechanisms. We implement a prototype and perform a set of controlled experiments to demonstrate the sandboxpsilas capabilities.",2009,0,
2660,2661,Locating Phase-to-Ground Short-Circuit Faults on Radial Distribution Lines,"This paper proposes a new single phase-to-ground short-circuit fault location algorithm for overhead three-phase radial distribution lines with single-ended measurements using the sinusoidal steady-state analysis method. By using this approach, two sinusoidal signals with different frequencies are first injected to the faulted line. By measuring the voltages and currents at the sending end and solving some nonlinear distributed-parameter equations, the distances and resistances of all possible fault candidates can be determined. A feature extraction method is derived to distinguish the actual fault from other pseudofault candidates. A fault locator based on the proposed approach is designed and implemented for a real-world problem. Physical model experiments and the field tests on radial distribution lines are presented to validate the proposed fault location approach",2007,0,
2661,2662,Modeling Fault Tolerant Services in Service-Oriented Architecture,"Nowadays, using of Service-Oriented Architectures (SOA) is spreading as a flexible architecture for developing dynamic enterprise systems.In this paper we investigate fault tolerance mechanisms for modeling services in service-oriented architecture. We propose a metamodel (formalized by a type graph) with graph rules for monitoring services and their communications to detect existing faults.",2009,0,
2662,2663,Using Process-Level Redundancy to Exploit Multiple Cores for Transient Fault Tolerance,"Transient faults are emerging as a critical concern in the reliability of general-purpose microprocessors. As architectural trends point towards multi-threaded multi-core designs, there is substantial interest in adapting such parallel hardware resources for transient fault tolerance. This paper proposes a software-based multi-core alternative for transient fault tolerance using process-level redundancy (PLR). PLR creates a set of redundant processes per application process and systematically compares the processes to guarantee correct execution. Redundancy at the process level allows the operating system to freely schedule the processes across all available hardware resources. PLR's software-centric approach to transient fault tolerance shifts the focus from ensuring correct hardware execution to ensuring correct software execution. As a result, PLR ignores many benign faults that do not propagate to affect program correctness. A real PLR prototype for running single-threaded applications is presented and evaluated for fault coverage and performance. On a 4-way SMP machine, PLR provides improved performance over existing software transient fault tolerance techniques with 16.9% overhead for fault detection on a set of optimized SPEC2000 binaries.",2007,0,
2663,2664,An algorithm for diagnostic fault simulation,"In diagnostic testing faults detectable by test vectors are partitioned into groups. This partitioning is such that a fault is distinguishable from faults in all other groups, but is indistinguishable from those in its own group. Diagnostic fault coverage (DC) is defined as the number of fault groups divided by the total number of faults. We present a new diagnostic fault simulation algorithm that determines the DC of given test vectors and produces a fault dictionary. For each vector, we begin with detected fault list at each primary output obtained from a convetional fault simulator. For the vector being simulated each fault is assigned a detection index that uniquely specifies its detection status at all primary outputs. Fault list is then partitioned. Faults with different detection index are distinguished by the simulated vector and are kept in separate groups. Any fault in a group by itself is dropped from further simulation with subsequent vectors for which its detection index remains unknown (X). After simulation of each vector, the cumulative DC is obtained by counting the fault groups. Fault dictionary syndrome for a fault is the array of its detection indexes.",2010,0,
2664,2665,Time-resolved scanning of integrated circuits with a pulsed laser: application to transient fault injection in an ADC,"This paper presents an experimental system for integrated circuits testing with a pulsed laser beam. The system is fully automated and simultaneously provides interesting spatial and temporal resolutions for various applications like fault injection, radiation sensitivity evaluation, or default localization. In the presented application, the system is used to visualize signal propagation in an 8 bit half-flash ADC.",2003,0,
2665,2666,Framework for testing the fault-tolerance of systems including OS and network aspects,"This paper presents an extensible framework for testing the behavior of networked machines running the Linux operating system in the presence of faults. The framework allows to inject a variety of faults, such as faults in the computing core or peripheral devices of a machine or faults in the network connecting the machines. The system under test as well as the fault- and workload run on this system are configurable. The core of the framework is a User Mode Linux, which runs on top of a real world Linux machine as a single process and simulates a single machine. A second process paired with each virtual machine is used for fault injection. The framework will be supported by utility programs to automate testing and evaluate test results",2001,0,
2666,2667,Automatic verification of fault tolerance using model checking,"Model checking is a technique that can make a verification for finite state systems absolutely automatic. We propose a method for automatic verification of fault-tolerant systems using this technique. Unlike other related work, which is tailored to specific systems, we are aimed at providing a general approach to verification of fault tolerance. The main obstacle in model checking is state explosion. To avoid the problem, we design this method so that it can use SMV, a symbolic model checking tool. Symbolic model checking can overcome the problem by expressing the state space and the transition relation by Boolean functions. Assuming that a system to be verified is specified by guarded commands, we define a modeling language suited for describing guarded command programs and propose a translation method from the modeling language to the input language of SMV. We show the results of applying the proposed method to various examples to demonstrate the usefulness",2001,0,
2667,2668,Fault Current Contribution From Synchronous Machine and Inverter Based Distributed Generators,"There are advantages of installing distributed generation (DG) in distribution systems: for example, improving reliability, mitigating voltage sags, unloading subtransmission and transmission system, and sometimes utilizing renewables. All of these factors have resulted in an increase in the use of DGs. However, the increase of fault currents in power systems is a consequence of the appearance of new generation sources. Some operating and planning limitations may be imposed by the resulting fault currents. This paper discusses a model of inverter based DGs which can be used to analyze the dynamic performance of power systems in the presence of DGs. In a style similar to protective relaying analysis, three-dimensional plots are used to depict the behavior of system reactance (X) and resistance (R) versus time. These plots depict operating parameters in relation to zones of protection, and this information is useful for the coordination of protection systems in the presence of DG",2007,0,
2668,2669,Fault Tolerant Approaches for Distributed Real-time and Embedded Systems,"Fault tolerance (FT) is a crucial design consideration for mission-critical distributed real-time and embedded (DRE) systems, which combine the real-time characteristics of embedded platforms with the dynamic characteristics of distributed platforms. Traditional FT approaches do not address features that are common in DRE systems, such as scale, heterogeneity, real-time requirements, and other characteristics. Most previous R&D efforts in FT have focused on client-server object systems, whereas DRE systems are increasingly based on component-oriented architectures, which support more complex interaction patterns, such as peer-to-peer. This paper describes our current applied R&D efforts to develop FT technology for DRE systems. First, we describe three enhanced FT techniques that support the needs of DRE systems: a transparent approach to mixed-mode communication, auto-configuration of dynamic systems, and duplicate management for peer-to-peer interactions. Second, we describe an integrated FT capability for a real-world component-based DRE system that uses off-the-shelf FT middleware integrated with our enhanced FT techniques. We present experimental results that show that our integrated FT capability meets the DRE system's real-time performance requirements for both the responsiveness of failure recovery and the minimal amount of overhead introduced into the fault-free case.",2007,0,
2669,2670,Design of LDPC decoders for improved low error rate performance: quantization and algorithm choices,"Many classes of high-performance low-density parity-check (LDPC) codes are based on parity check matrices composed of permutation submatrices. We describe the design of a parallel-serial decoder architecture that can be used to map any LDPC code with such a structure to a hardware emulation platform. High-throughput emulation allows for the exploration of the low bit-error rate (BER) region and provides statistics of the error traces, which illuminate the causes of the error floors of the (2048, 1723) Reed-Solomon based LDPC (RS-LDPC) code and the (2209, 1978) array-based LDPC code. Two classes of error events are observed: oscillatory behavior and convergence to a class of non-codewords, termed absorbing sets. The influence of absorbing sets can be exacerbated by message quantization and decoder implementation. In particular, quantization and the log-tanh function approximation in sum-product decoders strongly affect which absorbing sets dominate in the errorfloor region. We show that conventional sum-product decoder implementations of the (2209, 1978) array-based LDPC code allow low-weight absorbing sets to have a strong effect, and, as a result, elevate the error floor. Dually-quantized sum-product decoders and approximate sum-product decoders alleviate the effects of low-weight absorbing sets, thereby lowering the error floor.",2009,0,
2670,2671,Performance analysis of HTS fault current limiter combined with a ZnO varistor,"The superconducting technology nowadays is an innovation in the field of the electrical power supply. Using HTS in fault current limiter (FCL) represents a new category of electrical equipment and a novel configuration of electrical network. In fact the high temperature superconductivity (HTS) make a relatively sharp transition to a highly resistive state when the critical current density is exceeded, and this effect has suggested their use for resistive fault current limiters. FCL is an important element in order to reduce system impedance, which permits an increase of power transmission. Furthermore, it allows additional meshing of a power system, which increases the power availability. The most significant features of the SCFCLs requested from the power system operating conditions are a limiting impedance, a trigger current level and a recovery time. In this paper, a model of HTS FCL using ZnO varistor is proposed. The effectiveness of this model is investigated through results of simulation in MATLAB/Simulink software. In addition it is illustrated the limiting feature of HTS FCL and protected role of ZnO varistor",2005,0,
2671,2672,Common Software-Aging-Related Faults in Fault-Tolerant Systems,"In recent years, remarkable attention has been focused on software aging phenomena, in which the performance of software systems degrades with time. Fault-tolerant software systems which provide high assurance may suffer from such phenomena. Based on the common software-aging-related faults in fault-tolerant systems, a behavior model of a double-version fault-tolerant software system is established using Markov reward model. The performance of the system such as expected service rate in steady state is evaluated and the sensitivity analysis of some parameters is performed.",2008,0,
2672,2673,Automatic image processing filter generation for visual defects classification system,"The visual inspection system is used in various production systems. The Visual Inspection System is used to maintain the quality of products. But, there are some defects which are not detected with enough reliability on conventional systems. To meet with the problems, the automatic generation system of best image processing filters which extract the proper characteristics of images for that kind of defects has been introduced to improve recognition rate. The system is designed to generate two kinds of filters to detect the defects with vague edge and widely distributed defect images using neural network method and co-occurrence histogram images. Experiments shows the generated filters get better recognition rate.",2009,0,
2673,2674,Unequal Error Protection for Robust Streaming of Scalable Video Over Packet Lossy Networks,"Efficient bit stream adaptation and resilience to packet losses are two critical requirements in scalable video coding for transmission over packet-lossy networks. Various scalable layers have highly distinct importance, measured by their contribution to the overall video quality. This distinction is especially more significant in the scalable H.264/advanced video coding (AVC) video, due to the employed prediction hierarchy and the drift propagation when quality refinements are missing. Therefore, efficient bit stream adaptation and unequal protection of these layers are of special interest in the scalable H.264/AVC video. This paper proposes an algorithm to accurately estimate the overall distortion of decoder reconstructed frames due to enhancement layer truncation, drift/error propagation, and error concealment in the scalable H.264/AVC video. The method recursively computes the total decoder expected distortion at the picture-level for each layer in the prediction hierarchy. This ensures low computational cost since it bypasses highly complex pixel-level motion compensation operations. Simulation results show an accurate distortion estimation at various channel loss rates. The estimate is further integrated into a cross-layer optimization framework for optimized bit extraction and content-aware channel rate allocation. Experimental results demonstrate that precise distortion estimation enables our proposed transmission system to achieve a significantly higher average video peak signal-to-noise ratio compared to a conventional content independent system.",2010,0,
2674,2675,An Improved Fault-Tolerant Model for Channel Assignment in Cellular Networks,"Since the natural resources of electromagnetic spectrum are strictly administrated, the channel assignment problem (CAP) has been an important issue for mobile computing. Another important concept in mobile computing is handoff, which is also called handover. It occurs when a mobile host moves from the coverage area of one base station to the adjacent one while still involved in communication. A new channel is immediately to be assigned to continue the call. Sometimes, there aren't more channels in this cell, it is irritating for a mobile user to break the connection. Therefore, it is desirable that the channel assignment algorithm be an improved fault tolerant. Thus, even if in the presence there are insufficient channels available in the cell, it can still continue communicating with its mobile hosts. In this paper, we design an improved fault-tolerant model for channel assignment in mobile computing. Besides considering co-channel interfence, our model considers handoff by using the reserved channel technique, borrowing/lending and locking technique. We also provide results of the performance evaluation of our algorithm.",2010,0,
2675,2676,Data mining-based fault detect and diagnosis for the video amplifier circuit,"According to the principle of fault detect and diagnosis, this paper presents a new technology of data mining to deal with a lot of data got from fault detection and diagnosis. A method using database technology to solve fault detection and diagnosis is developed.",2003,0,
2676,2677,A New Model-Based Technique for the Diagnosis of Rotor Faults in RFOC Induction Motor Drives,"This paper proposes a new model-based diagnostic technique, which is the so-called virtual current technique (VCT), for the diagnosis of rotor faults in direct rotor field oriented controlled (DRFOC) induction motor drives. By measuring the oscillations at twice the slip frequency found in the rotor flux of the machine, and by conjugating this information with the knowledge of some motor parameters, as well as the parameters of the flux and current controllers, it is possible to generate a virtual magnetizing current which, after normalization, allows the detection and quantification of the extension of the fault. The proposed method allows one to overcome the major difficulties usually found in the diagnosis of rotor faults in closed-loop drives by providing information about the condition of the machine in a way that is independent of the working conditions of the drive such as the load level, reference speed, and bandwidth of the control loops. Although the VCT was primarily developed for traction drives used in railway applications, it can be incorporated in any DRFOC drive at almost no additional cost. Several simulation results, obtained with different types of DRFOC drives, as well as experimental results obtained in the laboratory, demonstrate the effectiveness of this new diagnostic approach.",2008,0,
2677,2678,Partial simulation-driven ATPG for detection and diagnosis of faults in analog circuits,"In this paper, we propose a novel fault-oriented test generation methodology for detection and isolation of faults in analog circuits. Given the description of the circuit-under-test, the proposed test generator computes the optimal transient test stimuli in order to detect and isolate a given set of faults. It also computes the optimal set of test nodes to probe at, and the time instants to make measurements. The test generation program accommodates the effects introduced by component tolerances and measurement inaccuracy, and can be tailored to fit the signal generation capabilities of a hardware tester. Experimental results show that the proposed technique can be applied to generate transient tests for both linear and non-linear analog circuits of moderate complexity in reasonably less CPU time. This will significantly impact the test development costs for an analog circuit and will decrease the time-to-market of a product. Finally, the short duration and the easy-to-apply feature of the test stimuli will lead to significant reduction in production test costs.",2000,0,
2678,2679,Automatic generation of diagnostic expert systems from fault trees,"When a fault tolerant computer-based system fails, diagnosis and repair must be performed to bring the system back to an operational state. The use of fault tolerance design implies that several components or subsystems may have failed, and that perhaps many of these faults have been tolerated before the system actually succumbed to failure. Diagnosis procedures are then needed to determine the most likely source of failure and to guide repair actions. Expert systems are often used to guide diagnostics, but the derivation of an expert system requires knowledge (i.e., a conceptual model) of failure symptoms. In this paper, we consider the problem of diagnosing a system for which there may be little experience, given that it might be a one-of-a-kind system or because access to the system may be limited. We conjecture that the same fault tree model used to help aid in the design and analysis of the system can provide the conceptual model of system component interactions needed in order to define a diagnostic process. We explore the use of a fault tree model (along with the probabilities of failure for the basic events) along with partial knowledge of the state of the system (i.e., the system has failed, and perhaps some components are known to be operational or failed) to produce a diagnostic aid.",2003,0,
2679,2680,Aliasing Error Reduction Based Fast VBSME Algorithm,"Mathematical analysis reveals that high frequency signal components are the main issues that make MRF algorithm essential. Moreover, the aliasing problem of subsampling algorithm also comes from high frequency signal components. So based on these mathematical investigations, two fast VBSME algorithms are proposed in this paper, namely roberts cross edge detector based subsampling method and motion vector based MRF early termination algorithm. Experiments show that strong correlation exists among the motion vectors of those blocks belonging to the same macroblock. Through exploiting this feature, a dynamically adjustment of the search ranges of integer motion estimation is proposed in this paper. Combing our proposed algorithms with UMHS almost saves 96%-98% Integer Motion Estimation (IME) time compared to the exhaustive search algorithm the induced coding quality loss is less than 0.8% bitrate increase or 0.04 db PSNR decline on average.",2008,0,
2680,2681,Design of Power Transformer Fault Diagnosis Model Based on Support Vector Machine,Support vector machines (SVM) is a machine-learning algorithm based on statistical learning theory. The method for power transformer fault diagnosis based on SVM is proposed in this paper. The principle and algorithm of this method are introduced. Through a finite learning sample the relation is established between the transformer fault signature and the quantity of its dissolved gas. A faults classifier is constructed by using the dissolved gas data of the fault transformer. The testing results show that this method can successfully be applied to the diagnosis of gear faults.,2009,0,
2681,2682,FTC-Charm++: an in-memory checkpoint-based fault tolerant runtime for Charm++ and MPI,"As high performance clusters continue to grow in size, the mean time between failures shrinks. Thus, the issues of fault tolerance and reliability are becoming one of the challenging factors for application scalability. The traditional disk-based method of dealing with faults is to checkpoint the state of the entire application periodically to reliable storage and restart from the recent checkpoint. The recovery of the application from faults involves (often manually) restarting applications on all processors and having it read the data from disks on all processors. The restart can therefore take minutes after it has been initiated. Such a strategy requires that the failed processor can be replaced so that the number of processors at checkpoint-time and recovery-time are the same. We present FTC-Charms ++, a fault-tolerant runtime based on a scheme for fast and scalable in-memory checkpoint and restart. At restart, when there is no extra processor, the program can continue to run on the remaining processors while minimizing the performance penalty due to losing processors. The method is useful for applications whose memory footprint is small at the checkpoint state, while a variation of this scheme - in-disk checkpoint/restart can be applied to applications with large memory footprint. The scheme does not require any individual component to be fault-free. We have implemented this scheme for Charms++ and AMPI (an adaptive version of MPl). This work describes the scheme and shows performance data on a cluster using 128 processors.",2004,0,
2682,2683,Automated FEM mesh optimization for nonlinear problems based on error estimation [IC packaging applications],"Applying the fundamental work of Zienkiewicz and Boroomand, this paper introduces a methodology for automated mesh optimization based on estimates of the discretization error. The objective of this effort has been the use of the methodology with a commercial FEM code (ANSYS<sup>TM</sup>) and its application to thermal stress analyses of flip chip modules (FC) and chip size packages (CSP) with solder joints, which means the presence of nonlinear material models (plasticity and creep).",2004,0,
2683,2684,A Strategy for fault tolerant control in Networked Control Systems in the presence of Medium Access Constraints,"This paper deals with the problem of fault-tolerant control of a Network Control System (NCS) for the case in which the sensors, actuators and controller are interconnected via various Medium Access Control protocols which define the access scheduling and collision arbitration policies in the network and employing the so-called periodic communication sequence. A new procedure for controlling a system over a network using the concept of an NCS-Information-Packet is described which comprises an augmented vector consisting of control moves and fault flags. The size of this packet is used to define a Completely Fault Tolerant NCS. The fault-tolerant behaviour and control performance of this scheme is illustrated through the use of a process model and controller. The plant is controlled over a network using Model-based Predictive Control and implemented via MATLAB and LABVIEW software.",2007,0,
2684,2685,On-load tap changer diagnosis - an off-line method for detecting degradation and defects: Part 1,"An advanced procedure for off-line power transformer diagnosis has been presented in this paper. Several different diagnostic measurements can be made using only one device. Examples and case studies were discussed to show the variety of defects and degradation mechanisms that can be detected using the procedure. In particular, it was found that the most common OLTC defects can be detected. The measurements are very sensitive to degradation due to long-term aging and to OLTC maintenance errors. A large population of OLTCs was tested, and a substantial number showed contact degradation. Finally, several areas that require careful attention were discussed, i.e., test current amplitude, circuit resistance, secondary short circuiting, and winding configuration.",2010,0,
2685,2686,Error-resilience in multimedia applications over ad-hoc networks,Ad-hoc networking has been of increasing interest in recent years. It encapsulates the ultimate notion of ubiquitous communications with the absence of reliance on any existing network infrastructure. This paper presents a concept for robust operation of multimedia applications over such networks. Error resilient communication is achieved by using a new error detection and concealment technique that exploits information from the decoded image data itself as well as using information from the underlying network. This approach unifies information from both traditional computer science and signal processing domains. A layered architecture framework for the implementation of the proposed system is also described,2001,0,
2686,2687,Towards model based prediction of human error rates in interactive systems,"Growing use of computers in safety-critical systems increases the need for Human Computer interfaces (HCIs) to be both smarter-to detect human errors-and better designed-to reduce likelihood of errors. We are developing methods for determining the likelihood of operator errors which combine current theory on the psychological causes of human errors with formal methods for modelling human-computer interaction. We present the models of the HCI and operator in an air-traffic control (ATC) system simulation, and discuss the role of these in the prediction of human error rates",2001,0,
2687,2688,Fault identification and reconfigurable control for bimodal piecewise affine systems,"This paper addresses the design of a fault detection and reconfigurable control structure for bimodal piecewise affine (PWA) systems. The PWA bimodal system will be designed to verify input-to-state stability (ISS) in closed loop. The proposed methodology is divided into two parts. First, a Luenberger-based observer structure is proposed to solve the fault detection and identification (FDI) problem for bimodal PWA systems. The unknown value of the fault parameter is estimated by an observer equation, which is derived using a Lyapunov-based methodology. Then, the ISS property is proved for the observer. Second, a fault-tolerant state feedback controller is synthesized for the PWA model. The controller is designed to deal with partial loss of control authority identified by the observer. The ISS property is also proved for the controller. Finally, the ISS property for the interconnection of the controller and the observer-based fault identification mechanism is studied. The design procedure is formulated as a set of linear matrix inequalities (LMIs), which can be solved efficiently using available software packages.",2009,0,
2688,2689,Lightweight fault-localization using multiple coverage types,"Lightweight fault-localization techniques use program coverage to isolate the parts of the code that are most suspicious of being faulty. In this paper, we present the results of a study of three types of program coverage-statements, branches, and data dependencies-to compare their effectiveness in localizing faults. The study shows that no single coverage type performs best for all faults-different kinds of faults are best localized by different coverage types. Based on these results, we present a new coverage-based approach to fault localization that leverages the unique qualities of each coverage type by combining them. Because data dependencies are noticeably more expensive to monitor than branches, we also investigate the effects of replacing data-dependence coverage with an approximation inferred from branch coverage. Our empirical results show that (1) the cost of fault localization using combinations of coverage is less than using any individual coverage type and closer to the best case (without knowing in advance which kinds of faults are present), and (2) using inferred data-dependence coverage retains most of the benefits of combinations.",2009,0,
2689,2690,Research on obstacles of knowledge sharing of transformation of science and technology achievements by fault tree analysis,"Knowledge sharing (KS) is important means to improve the rate of transformation of sci-tech achievements (TSA). The obstacles of KS of TSA are mainly from sharing subject obstacles, sharing environment obstacles and shared knowledge itself obstacles. Structure importance, probability importance and critical importance are analyzed based on expert investigation by fault tree analysis (FTA) to make a conclusion that main obstacles of KS of TSA are unreasonable organization structure and lack of technical platform.",2008,0,
2690,2691,Evaluation of Software-Implemented Fault-Tolerance (SIFT) Approach in Gracefully Degradable Multi-Computer Systems,"This paper presents an analytical method for evaluating the reliability improvement for any size of multi-computer system based on Software-Implemented Fault-Tolerance (SIFT). The method is based on the equivalent failure rate Gamma, the single node failure rate lambda, the number of nodes in the system, N, the repair rate mu, the fault coverage factor c, the reconfiguration rate delta, and the percentage of blocking faults b<sub>1</sub> and b<sub>2</sub>. The impact of these parameters on the reliability improvement has been evaluated for a gracefully degradable multi-computer system using our proposed analytical technique based on Markov chains. To validate our approach, we used the SIFT method which implements error detection at the node level, combined with a fast reconfiguration algorithm for avoiding faulty nodes. It is worth noting that the proposed method is applicable to any multi-computer systems' topology. The evaluation work presented in this paper focuses on the combination of analytical and experimental approaches, and more precisely on Markov chains. The SIFT method has been successfully implemented for a multi-computer system, nCube. The time overhead (reconfiguration & recomputation time) incurred by the injected fault, and the fault coverage factor c, are experimentally evaluated by means of a parallel version of the Software Object-Oriented Fault-Injection Tool (nSOFIT). The implemented SIFT approach can be used for real-time applications, when the time constraints should be met despite failures in the gracefully degradable multi-computer system",2006,0,
2691,2692,Thermal behaviour of a three-phase induction motor fed by a fault tolerant voltage source inverter,"This paper presents the results of an investigation regarding the thermal behaviour of a three-phase induction motor, when supplied by a reconfigured three-phase voltage source inverter with fault tolerant capabilities. For this purpose, a fault tolerant operating strategy based on the connection of the faulty inverter leg to the DC link middle point was considered. The experimental obtained results show that, as far as the motor thermal characteristics are concerned, it is not necessary to reinforce the motor insulation properties since it is already prepared for such an operation",2005,0,
2692,2693,An approach of fault detection based on multi-mode,"Conventional multi-scale principal component analysis (MSPCA) only detects fault, but it canpsilat detect fault types. For these problems, a method of fault detection based on multi-mode that incorporates MSPCA into adaptive resonance (ART) neural network is presented. Firstly, this method presents a wavelet transform for samples data, and principal component analysis can be used to analyze data at each scale. Then ART is used to classify reconstruction data. It can detect fault effectively, and ART2 can classify fault using wavelet denoising easily, it separates the fault successfully in the system. At last, it develops multi-mode fault detection in autocorrelation system application through computer simulation experiment. The theory and simulation experiments shows that this method is of wide application prospect.",2008,0,
2693,2694,Ringing out fault tolerance. A new ring network for superior low-cost dependability,"Dependability properties of bi-directional and braided rings are well recognized in improving communication availability. However, current ring-based topologies have no mechanisms for extreme integrity and have not been considered for emerging high-dependability markets where cost is a significant driver, such as the automotive ""by-wire"" applications. This paper introduces a braided-ring architecture with superior guardian functionality and complete Byzantine fault tolerance while simultaneously reducing cost. This paper reviews anticipated requirements for high-dependability low-cost applications and emphasizes the need for regular safe testing of core coverage functions. The paper describes the ring's main mechanisms for achieving integrity and availability levels similar to SAFEbus but at low automotive costs. The paper also presents a mechanism to achieve self-stabilizing TDMA-based communication and design methods for fault-tolerant protocols on a network of simplex nodes. The paper also introduces a new self-checking pair concept that leverages braided-ring properties. This novel message-based self-checking-pair concept allows high-integrity source data at extremely low cost.",2005,0,
2694,2695,Irregular Puncturing for Convolutional Codes and the Application to Unequal Error Protection,"In this paper, convolutional codes are studied for puncturing with irregular puncturing periods. Irregular puncturing can generate punctured codes with more available rates and better bit-error-rate performance compared with the conventional scheme with a single puncturing period. For the application to unequal error protection, a new multiplexing scheme is also proposed for rate-compatible punctured convolutional (RCPC) codes which can guarantee smooth transition between rates without extra overheads. Finally, families of good RCPC codes with irregular puncturing tables are given by a computer search",2006,0,
2695,2696,"Analysis of transmit diversity schemes: impact of fade distribution, spatial correlation and channel estimation errors","In this paper, we analyze the average symbol error rate performance of arbitrary two dimensional signal constellations in conjunction with both open-loop and close-loop transmit diversity schemes in a generalized fading environment (including Rayleigh, Nakagami-m, Rician and mixed multipath fading environments). The mathematical framework can treat the case of non-identical fading parameters and dissimilar mean signal strengths across the diversity paths. We also present an analysis of the impact of spatial correlation on the performance of various transmit diversity schemes over Nakagami multipath fading channels. The impact of imperfect channel estimates (Gaussian errors) on various transmit diversity schemes in Rayleigh fading environment is also studied via analytical as well as simulation techniques.",2003,0,
2696,2697,Current Sensor Fault-Tolerant Control for WECS With DFIG,"The performances of wind energy conversion systems (WECS) heavily depend on the accurate current sensing. A sudden failure in one of the current sensors decreases the system performances. Moreover, if a fault is not detected and handled quickly, its effect leads to system disconnection. Hence, to reduce the failure rate and to prevent unscheduled shutdown, a real-time fault detection, isolation, and compensation scheme could be adopted. This paper introduces a new field-programmable-gate-array (FPGA)-based grid-side-converter current sensor fault-tolerant control for WECS with doubly fed induction generator. The proposed current sensor fault detection is achieved by a predictive model. ldquoFPGA-in-the-looprdquo and experimental results validate the effectiveness and satisfactory performances of the proposed method.",2009,0,
2697,2698,Verifying formal specifications using fault tree analysis,"Specification before implementation has been suggested as a sensible approach to software evolution. The quality of this approach may be improved by using formal specification. However, to serve as a trustable foundation for implementation and to help reduce the cost of program testing, the formal specification must be ensured to be satisfiable, consistent, complete and accurate in recording the user requirements. In this paper, we first define these four concepts and then introduce a technique for verifying formal specifications that combines fault-tree analysis with static analysis and testing techniques",2000,0,
2698,2699,Error Reduction on Automatic Segmentation in Microarray Image,"DNA Microarray hybridization is a popular high throughput technique in academic as well as in industrial genomics research. The Microarray image is considered as an important tool and powerful technology for large-scale gene sequence and gene expression analysis. There are many methods to analyze the Microarray image by automatic segmentation or gridding spot. These methods always have the same problem of noise and tilt in spot array. It is difficult to process strong noise image in automation. In this paper, we can reduce the error of the edge detection which is influenced by noise and tilt spot array. We propose an automatic segmentation method with some techniques from video segmentation to process the Microarray image. By the proposed method, we can reduce the automatic spot segmentation errors and get more exact spot position. Our method has the advantages of low computation and easy implementation. Eventually, we compare the result with ScanAlyze tool since ScanAlyze tool extracts spot position and edge by artificial interface. We obtain the 1.43% average differential value of spots analysis ratio in result with ScanAlyze.",2007,0,
2699,2700,Adapting to dynamic registration errors using level of error (LOE) filtering,"We describe our initial work on generating augmented reality (AR) displays in the face of dynamically changing errors in the pose (position and orientation) of both the user and objects in the world. Dealing with this problem is particularly important in mobile AR environments, where the tracking accuracy of the user's head can change frequently and dramatically as she moves between areas with radically different tracking systems, such as in and out of buildings. We introduce the notion of level of error filtering, analogous to level of detail culling in 3D graphics systems, to help programmers build interfaces that automatically adapt to changing registration errors",2000,0,
2700,2701,Parameterization of a model-based 3D whole-body PET scatter correction,"Parameterization of a fast implementation of the Ollinger model-based 3D scatter correction method for PET has been evaluated using measured phantom data from a GE PET Advance<sup>TM</sup>. The Ollinger method explicitly estimates the 3D single-scatter distribution using measured emission and transmission data and then estimates the multiple-scatter as a convolution of the single-scatter. The main algorithm difference from that implemented by Ollinger (1996) is that the scatter correction does not explicitly compute scatter for azimuthal angles; rather, it determines 2D scatter estimates for data within 2D ""super-slices"" using as input data from the 3D direct-plane (non-oblique) slices. These axial super-slice data are composed of data within a parameterized distance from the center of the super-slice. Such a model-based method can be parameterized, choice of which may significantly change the behavior of the algorithm. Parameters studied in this work included transaxial image downsampling, number of detectors to calculate scatter to, multiples kernel width and magnitude, number and thickness of super-slices and number of iterations. Measured phantom data included imaging of the NEMA NU-2001 image quality phantom, the IQ phantom with 2 cm extra water-equivalent tissue strapped around its circumference and an attenuation phantom (20 cm uniform cylinder with bone, water and air inserts) with two 8 cm diameter water-filled non-radioactive arms placed by it's side. For the IQ phantom data, a subset of NEMA NU-2001 measures were used to determine the contrast-to-noise, lung residual bias and background variability. For the attenuation phantom, ROIs were drawn on the nonradioactive compartments and on the background. These ROIs were analyzed for inter and intra-slice variation, background bias and compartment-to-background ratio. Results: In most cases, the algorithm was most sensitive to multiple-scatter parameterization and least sensitive to transaxial downsampling. The algorithm showed convergence by the second iteration for the metrics used in this study. Also, the range of the magnitude of change in the metrics analyzed was small over all changes in parameterization. Further work to extend these results to other more realistic phantom and clinical dataset- s is warranted.",2001,0,
2701,2702,Detecting attacks that exploit application-logic errors through application-level auditing,"Host security is achieved by securing both the operating system kernel and the privileged applications that run on top of it. Application-level bugs are more frequent than kernel-level bugs, and, therefore, applications are often the means to compromise the security of a system. Detecting these attacks can be difficult, especially in the case of attacks that exploit application-logic errors. These attacks seldom exhibit characterizing patterns as in the case of buffer overflows and format string attacks. In addition, the data used by intrusion detection systems is either too low-level, as in the case of system calls, or incomplete, as in the case of syslog entries. This paper presents a technique to enforce nonbypassable, application-level auditing that does not require the recompilation of legacy systems. The technique is implemented as a kernel-level component, a privileged daemon, and an offline language tool. The technique uses binary rewriting to instrument applications so that meaningful and complete audit information can be extracted. This information is then matched against application-specific signatures to detect attacks that exploit application-logic errors. The technique has been successfully applied to detect attacks against widely-deployed applications, including the Apache Web server and the OpenSSH server.",2004,0,
2702,2703,Investigation of a fault tolerant and high performance motor drive for critical applications,"This paper evaluates a fault-tolerant electric motor drive with redundancy for a potential all-electric aircraft (AEA), and investigates to increase the redundancy against partial or complete motor failures. A flexible motor configuration is proposed to increase the redundancy against motor failures. The paper highlights the importance of effectively simulating the system on computer, and the way in which the ideas and techniques used to simulate the system have been adapted to the physical prototype. The computer modelling section of the paper outlines the steps involved with simulating the complete system using an object orientated programming language, LabVIEW, and some simulation results are provided",2001,0,
2703,2704,Timely use of the CAN protocol in critical hard real-time systems with faults,The presence of network errors such as electrical interference affects the timing properties of a CAN (Controller Area Network) bus. In hard real-time systems it is often better to not receive a message than to receive it too late. Aborting late messages is a form of real-time error confinement which prevents late messages affecting the timeliness of other messages and processes. This can be used to help guarantee hard real-time performance in a distributed system using CAN in the presence of unbounded network errors,2001,0,
2704,2705,Enhancing the Success Rate of Primary Version While Guaranteeing Fault-Tolerant Capability for Real-Time Systems,"Primary/alternate version technique is a cost-effective means which trades the quality of computation results for promptness to tolerate the software faults. Generally speaking, this method requires that each real-time periodic task has two versions: primary and alternate. The primary version provides a result that is in some sense more desirable, but it may be subject to timing failure due to its complexity. On the contrary, the alternate version simply affords an acceptable service, but it could guarantee the timeliness owing to its simplicity. The kernel algorithm proposed in this paper employs the off-line backwards-RM scheme to pre-allocate time intervals to the alternate version and the on-line RM scheme to dispatch the primary version. Simulation results show that kernel algorithm provides higher success rate of primary version.",2009,0,
2705,2706,Content-Adaptive Interpolation for Spatial Error Concealment,"When transmitting encoded images over a communication channel, the reconstructed image quality can be substantially degraded by channel errors. This paper presents a spatial error concealment algorithm that utilizes variance of surrounding pixels, and then classifies each error block (EB) into two categories: uniform block and edge block. For uniform block, nearest border prior spatial interpolation is adopted to restore missing pixels. We use the Wiener interpolation algorithm and special interpolation sequence for edge block. Experimental results indicate the proposed algorithm can attain well restored quality of intra-frames both subjectively and objectively. Meanwhile, the computational cost of the proposed algorithm can be significantly reduced, compared to Lipsilas method. And the restored quality is almost the same, sometimes even much better.",2009,0,
2706,2707,Evaluation of fault-tolerant distributed Web systems,"Replication of information among multiple servers is necessary to service requests for Web application such as Internet banking. A dispatcher in distributed Web systems distributes client requests among Web application servers and multiple dispatchers are also needed for fault-tolerant Web services. In this paper, we describe issues related to building fault-tolerant distributed Web systems. We evaluate the performance of fault-tolerant distributed Web systems based on replication. Our evaluation is conducted on LVS (Linux Virtual Server) and the Apache Web server using the request generator, LoadCube. We show some performance measurements for the systems.",2005,0,
2707,2708,Computational system to detect defects in mounted and bare PCB Based on connectivity and image correlation,"In this paper we present an image analysis system for printed circuit board (PCB) automated inspection. In the last years PCB manufacturing industry has been advanced in inspection automation systems, especially to solve smaller tolerance requirements. A PCB consists in a circuit and electronic components assembled in a surface. There are three main process involved in its manufacture, where the inspection is necessary. The main process consists in the printing itself. Another important procedure is the components placement over the PCB surface. And the third is the components soldering. In the proposed inspection system we consider the board printing and components placements defects. We first compare a PCB standard image with a PCB image, using a simple subtraction algorithm that can highlight the main problem-regions. Then we used connection analysis in the printed circuit to find fatal and potential errors, like breaks, circuit shorts, missing components. Besides, using digital image correlation techniques, the system detects component errors, like absence, change, and wrong position. In other to develop this methodology in real PCB, we propose to magnify the problem-regions and start to find the errors in a set of PCB sections, which are smaller than the main PCB image.",2008,0,
2708,2709,Unequal Error Protection Schema for Wireless H.264 Video Transmission Based on Perceived Motion Energy Model,"Unequal error protection on video transmission is widely used to combat with bit errors in the wireless channel. However, current UEP schemas are based on heuristic approaches and taking no account of the characteristics of human visual system. In this paper, a novel unequal error protection schema for wireless H.264 video transmission based on a modified perceived motion energy (PME) model is presented. According to the sensitive characteristics to video motions of human eyes, the proposed modified PME model taking account to the encoding features of H.264/AVC standard to analyze and model the motions in H.264/AVC encoded video. Based on this model, the video bitstream is divided into several quality layers and unequal error protection is designed to protect the layered bitstream for the transmission over wireless channels. Experiment results show that higher video transmission quality is obtained.",2008,0,
2709,2710,Fault tolerance analysis of Extended Pruned Vertically Stacked Optical Banyan networks with link failures and variable group size,"Vertically stacked optical banyan (VSOB) networks are attractive for serving as optical switching systems due to the good properties of banyan network structures (such as the small depth and self-routing capability), and it is expected that using the VSOB structure will lead to a better fault-tolerant capability because it is composed of multiple identical copies of banyan networks. In the Extended Pruned Vertically Stacked Optical Banyan (EP-VSOB) network, the number of pruned planes has always been considered as (N), and a few extra planes (regular banyan) has been added with this pruned planes. In this paper, we present the results of blocking analysis of EP-VSOB network incorporating link-failures in which the number of pruned planes can be 2<sup>x</sup>, where 0    log<sub>2</sub> N, in addition to the variable extra planes. This generalization helps us trade-off between different constraints and performance metrics. Our simulation results show that for some given performance requirements (e.g. cost, speed or blocking probability), we can choose a network that has lower switch count compared to N - plane pruned crosstalk-free optical banyan networks. Our result also reveals a fact that by accepting small link failure probability, the blocking behavior of EP-VSOB network is very similar to that of a fault-free one, which demonstrates our expectation of good fault-tolerant property of VSOB networks. Simulation results also show that the blocking probability does not always increase with the increase of link-failures; blocking probability decreases for certain range of link-failures, and then increases again.",2010,0,
2710,2711,Questioning Human Error Probabilities in Railways,"Human errors are regarded as one of the main causes for railway accidents these days. In spite of this fact, the consideration of human error probabilities in quantified risk analyses has been very rudimentary up to now. A lack of comprehensive data and analyses in literature lead to the use of estimations and values from other industries. This paper discusses the transferability of human error probabilities for railways and identifies problems in handling methods and values. A model of working systems is used to demonstrate the particularities of railway work places and to derive a structure for performance shaping factors that influence the human error probability. A holistic approach is proposed to support the determination of appropriate human error probabilities for railways.",2008,0,
2711,2712,Study on Computer-Aided Fault Tree Construction for Geological Disasters,"Using the object-oriented plat software OEC, an expert system for aiding construction of fault trees for geological disasters is developed. The friendly user interface is provided by the system. This system also affords user some other functions, such as knowledge base maintenance, following up the scent and help function. By means of the computer expert system for aiding fault tree construction, the common people can construct fault tree for geological disasters at the level of an expert, and analyzes the cause of the accidents subsequently. The principle, process of the construction and important functions of computer-aided fault tree construction were discussed in this paper.",2009,0,
2712,2713,On a filter bank correction scheme for mitigating mismatch distortions in time-interleaved converter systems,"This paper presents a multirate filterbank architecture that mitigates the distortion caused by non-ideal samplers in a time-interleaved system. Closed form fractional delay filters with finite impulse response (FIR) and infinite impulse response (IIR) type are employed to model the behaviour of the non-ideal converters. Based on a polyphase description of the system, the reconstruction filters are derived for the IIR case, which can be regarded as a generalization of the FIR design scheme. Furthermore, the achieved performance of filter banks for various fractional delay filters is compared. To investigate the numerical robustness, the impact of limited coefficient lengths on different figures-of-merit was explored. Finally, the reconstruction of a non-uniformly sequence was used as an example to verify the reconstruction scheme.",2009,0,
2713,2714,Macroblock-based retransmission for error-resilient video streaming,"This paper revisits the problem of source-channel coding for error-resilient video streaming. We propose a new method to enable adaptive redundancy in the bitstream: fine-grain retransmission. Redundancy decisions are made per macroblock (MB), which are locally adaptive and of low overhead, as opposed to coarse packet-level redundancy (e.g. forward error correction). In this scheme, the encoder jointly optimizes the coding mode and redundancy per MB. A corresponding algorithm is presented for exploiting this redundancy at the decoder. The proposed method is general in nature, and can be implemented on top of any (hybrid) video codec. An example implementation is provided, which uses the redundant slice mechanism of H.264 (JM 13.2 reference software). Simulation results show significant performance gains over conventional error-resilient coding techniques.",2008,0,
2714,2715,Soft Error Tolerant Carry-Select Adders Implemented into Altera FPGAs,"The drastic shrink in transistor dimensions is making circuits more susceptible to radiation-induced soft errors. While single-event upsets are beginning to be a concern for electronic systems fabricated with nanometer CMOS technology at the sea level, single-event transients (SETs) are also expected to be a serious problem for the upcoming technologies. Thanks to the high logic density and fast turnaround time, FPGAs are currently the main fabric used to implement electronic systems. However, to provide high logic density FPGA devices are also fabricated with state-of-the-art CMOS technology and thus are also susceptible to soft errors. This paper presents a novel technique to protect carry-select adders against SETs. Such technique is based on triple module redundancy (TMR) and explores the inherent duplication existing in carry-select adders to reduce resource overhead.",2007,0,
2715,2716,"Flexible, Any-Time Fault Tree Analysis with Component Logic Models","This article presents a novel approach to facilitating fault tree analysis during the development of software-controlled systems. Based on a component-oriented system model, it combines second-order probabilistic analysis and automatically generated default failure models with a level-of-detail concept to ensure early and continuous analysability of system failure behaviour with optimal effort, even in the presence of incomplete information and dissimilar levels of detail in different parts of an evolving system model. The viability and validity of the method are demonstrated by means of an experiment.",2010,0,
2716,2717,Bandwidth Mismatch Correction for a Two-Channel Time-Interleaved A/D Converter,"Mismatches between sample-and-hold (S/H) circuits in a time-interleaved analog-to-digital data converter (ADC) cause undesirable distortions in the output spectrum. To reduce these undesired spectral components, we introduce a hybrid filter-bank model of a two channel time-interleaved ADC. The model allows the development of a digital domain correction technique that removes the first-order effects of S/H mismatches. A single FIR correction filter is required, and simulations demonstrate the effectiveness of the proposed correction method",2007,0,
2717,2718,Error recovery for interactive video transmission over the Internet,"Real-time interactive video transmission in the current Internet has mediocre quality because of high packet loss rates. Loss of packets in a video frame manifests itself not only in the reduced quality of that frame but also in the propagation of that distortion to successive frames. This error propagation problem is inherent in any motion compensation-based video codec. In this paper, we present a new error recovery scheme, called recovery from error spread using continuous updates (RESCU), that effectively alleviates error propagation in the transmission of interactive video. The main benefit of the RESCU scheme is that it allows more time for transport-level recovery such as retransmission and forward error correction to succeed while effectively masking out delays in recovering lost packets without introducing any playout delays, thus making it suitable for interactive video communication. Through simulation and real Internet experiments, we study the effectiveness and limitations of our proposed techniques and compare their performance to that of existing video error recovery techniques including H.263+ (NEWPRED). The study indicates that RESCU is effective in alleviating the error spread problem and can sustain much better video quality with less bit overhead than existing video error recovery techniques under various network environments.",2000,0,
2718,2719,Application of time series analysis to fault management in MANETs,"Traditional network management systems are not usually able to differentiate between mobility and other causes of communication degradation in wireless mobile ad hoc networks. A fault management system needs the ability to not only detect changes in performance but also reason about their possible causes and how to fix the problems. We propose a system called TimeSAFE (Time Series Analyzer Front End) that performs time series analysis as a front end input to a central fault management system. We show how such an analysis can help to distinguish between motion, obstacles, and interference as possible causes of changes in SINR (Signal to Interference and Noise Ratio). We propose three different methods of time series analysis and also methods for dynamic order selection and dynamic window selection. We describe our implementation of TimeSAFE and provide some preliminary results of detecting changes in SINR and identifying their causes.",2010,0,
2719,2720,Optimal Design of Nearfield Wideband Beamformers Robust Against Errors in Microphone Array Characteristics,"Nearfield wideband beamformers for microphone arrays have wide applications, such as hands-free telephony, hearing aids, and speech input devices to computers. The existing design approaches for nearfield wideband beamformers are highly sensitive to errors in microphone array characteristics, i.e., microphone gain, phase, and position errors, as well as sound speed errors. In this paper, a robust design approach for nearfield wideband beamformers for microphone arrays is proposed. The robust nearfield wideband beamformers are designed based on the minimax criterion with the worst case performance optimization. The design problems can be formulated as second-order cone programming and be solved efficiently using the well-established polynomial time interior-point methods. Several interesting properties of the robust nearfield wideband beamformers are derived. Numerical examples are given to demonstrate the efficacy of the proposed beamformers in the presence of errors in microphone array characteristics.",2007,0,
2720,2721,Gate level fault diagnosis in scan-based BIST,"A gate level, automated fault diagnosis scheme is proposed for scan-based BIST designs. The proposed scheme utilizes both fault capturing scan chain information and failing test vector information and enables location identification of single stuck-at faults to a neighborhood of a few gates through set operations on small pass/fail dictionaries. The proposed scheme is applicable to multiple stuck-at faults and bridging faults as well. The practical applicability of the suggested ideas is confirmed through numerous experimental runs on all three fault models",2002,0,
2721,2722,Error analysis and a new arithmetic study on computing horizontal temperature gradient in marine GIS,"There are three problems using current GIS software to calculate the horizontal temperature gradient of fishing grid. This work analyzed the errors caused by these problems, and presented a kind of new GIS algorithm of calculating the horizontal temperature gradient of fishing grid, and also presented multisection & single point change surface interpolation method for interpolating grid point temperature based on SST isoline. The results by our algorithms are better than results by ArcGIS.",2004,0,
2722,2723,Robust Fault Diagnosis for Atmospheric Reentry Vehicles: A Case Study,"This paper deals with the design of robust model-based fault detection and isolation (FDI) systems for atmospheric reentry vehicles. This work draws expertise from actions undertaken within a project at the European level, which develops a collaborative effort between the University of Bordeaux, the European Space Agency, and European Aeronautic Defence and Space Company Astrium on innovative and robust strategies for reusable launch vehicles (RLVs) autonomy. Using an <i>H</i><sub></sub>/<i>H</i><sub>-</sub> setting, a robust residual-based scheme is developed to diagnose faults on the vehicle wing-flap actuators. This design stage is followed by an original and specific diagnosis-oriented analysis phase based on the calculation of the generalized structured singular value. The latter provides a necessary and sufficient condition for robustness and FDI fault sensitivity over the whole vehicle flight trajectory. A key feature of the proposed approach is that the coupling between the in-plane and out-of-plane vehicle motions, as well as the effects that faults could have on the guidance, navigation, and control performances, are explicitly taken into account within the design procedure. The faulty situations are selected by a prior trimmability analysis to determine those for which the remaining healthy control effectors are able to maintain the vehicle around its center of gravity. Finally, some performance indicators including detection time, required onboard computational effort, and CPU time consumption are assessed and discussed. Simulation results are based on a nonlinear benchmark of the HL-20 vehicle under realistic operational conditions during the autolanding phase. The Monte Carlo results are quite encouraging, illustrating clearly the effectiveness of the proposed technique and suggesting that this solution could be considered as a viable candidate for future RLV programs.",2010,0,
2723,2724,Fast motion estimation based on adaptive search range adjustment and matching error prediction,"This work presents fast motion estimation (ME) by using both an adaptive search range adjustment and a matching error prediction. The basic idea of the proposed scheme is based on adjusting a given search range adaptively and predicting a block matching errors effectively. The adaptive search range adjustment is first performed by analyzing the contents of a scene. Next, the total block matching error is estimated by using partial errors of sub-sampled blocks to eliminate invalid blocks earlier for ME. In order to evaluate the proposed scheme, several baseline approaches are described and compared. The experimental results show that the proposed algorithm can reduce the computational cost more than 81% for ME at the cost of 0.01 dB image quality degradation versus the conventional PDE algorithm1. The main contributions of the proposed approach are that 1) it can reduce the computational cost considerably; 2) it can be applied to conventional PDE algorithms without significant changes; and 3) it can be a useful tool for fast ME in the consumer electronics-related field.",2009,0,
2724,2725,Joint Power Control and FEC Unequal Error Protection for Scalable H.264 Video Transmission over Wireless Fading Channels,"H.264/AVC scalable video coding (SVC) is an upto-date video compression standard. This paper deals with the issue of transmitting H.264 scalable video bitstreams over wireless fading channels. The contribution is twofold: Firstly, to exploit the importance of prioritized video packets in different temporal layer, quality layer and group of pictures (GOP), a simple and accurate performance metric, namely, layer-GOP-weighted expected zone of error propagation (LGW-EZEP) model is proposed. Secondly, a joint power control and forward error correction (FEC) unequal error protection (UEP) scheme is proposed to transmit the video streams over orthogonal frequency division multiplexing (OFDM) systems efficiently and robustly. Meanwhile, a new iterative algorithm is given to solve the joint optimization problem. Compared to other independent power control or FEC UEP schemes, the combined protecting scheme demonstrates stronger robustness and flexibility via various fading channels.",2009,0,
2725,2726,A fault diagnosis system for heat pumps,"During the operation of heat pumps, faults like heat exchanger fouling, component failure, or refrigerant leakage reduce the system performance. In order to recognize these faults early, a fault diagnosis system has been developed and verified on a test bench. The parameters of a heat pump model are identified sequentially and classified during operation. For this classification, several `hard' and `soft' clustering methods have been investigated, while fuzzy inference systems or neural networks are created automatically by newly developed software. Choosing a simple black-box model structure, the number of sensors can be minimized, whereas a more advanced grey-box model yields better classification results",2001,0,
2726,2727,Correction of myocardial artifact due to limited spatial resolution in PET,"A kind of artifact in myocardial positron emission tomography (PET) is the count loss due to the limited spatial resolution of PET, which is also named as partial volume effect (PVE). A deconvolution method was developed to correct for the artifact, in which the counts were evaluated as a parameter in the convolution. One-dimensional (1D) and two-dimensional (2D) models were used to create the convolution equation. Counts, background, myocardial wall position and thickness were parameters of the equation. The method needs non-linear fitting by an iterative process. Computer simulated myocardial images, PET image of phantoms and patient's myocardial PET images were used for evaluating the method. For simulated images, the corrected recovery coefficient is between 0.97 and 1.10 for 2D model and is near 0.9 for 1D model. 2D model can be applied for myocardium as thin as 0.25 times of the system spatial resolution while 1D model is only for 1.5 times of it. For phantom images, the recovery coefficient of 2D is near 1.0 for different thickness of the myocardium. The iterating process converges for a wide range of different size of myocardium and noise levels. 2D model allows correction for PVE exactly even for very thin myocardium.",2005,0,
2727,2728,Robust fault detection using interval LPV models,"In this paper, robust fault detection using a passive robust approach interval based on generating an adaptive threshold for interval linear parameter varying (LPV) models is proposed. This approach allows to consider parameters and associated uncertainty intervals of such model dependence on the operating point. Additionally, a parameter estimation algorithm for interval LPV models is presented. Finally, an piece of a sewer system has been used to test the validity of the proposed approach.",2007,0,
2728,2729,Scalable fault-tolerant network design for Ethernet-based wide area process control network systems,"Providing fault-tolerant Ethernet capability on the large control network systems becomes very important issue. In this paper, we present two efficient scalable fault-tolerant network architecture designs: the ""FTE protocol-independent multi-domain approach"" and the ""layer 2 switch/router-based multi-domain approach"", which can efficiently integrate the layer-2-based FTE protocol and the existing standard router fault-tolerant protocols such as Virtual Router Redundancy Protocol (VRRP), Hot Standby Router Protocol (HSRP), and Open Shortest Path First (OSPF). The network designs take into consideration minimizing the control overheads in supporting large network systems, meeting the detection and recovery time requirements of the application regardless of the network size, using COTS redundancy protocols without overall network performance degradation, and justifying the solution cost. The feasibility and performance of our designs are demonstrated through experiment and analysis.",2001,0,
2729,2730,A case for fault tolerance and performance enhancement using chip multi-processors,"This paper makes a case for using multi-core processors to simultaneously achieve transient-fault tolerance and performance enhancement. Our approach is extended from a recent latency-tolerance proposal, dual-core execution (DCE). In DCE, a program is executed twice in two processors, named the front and back processors. The front processor pre-processes instructions in a very fast yet highly accurate way and the back processor re-executes the instruction stream retired from the front processor. The front processor runs faster as it has no correctness constraints whereas its results, including timely prefetching and prompt branch misprediction resolution, help the back processor make faster progress. In this paper, we propose to entrust the speculative results of the front processor and use them to check the un-speculative results of the back processor. A discrepancy, either due to a transient fault or a mispeculation, is then handled with the existing mispeculation recovery mechanism. In this way, both transient-fault tolerance and performance improvement can be delivered simultaneously with little hardware overhead",2006,0,
2730,2731,"Improving the Performance of Fault-Aware Scheduling Policies for Desktop Grids (Be Lazy, Be Cool)","Desktop Grids have proved to be a suitable platform for the execution of Bag-of-Tasks applications but, being characterized by a high resource volatility, require the availability of scheduling techniques able to effectively deal with resource failures and/or unplanned periods of unavailability. Fault-aware scheduling, proposed in [2], can be considered a promising approach, yielding to both performance improvements for Bag-of-Task-Applications and increased utilization for Desktop Grids. The best fault-aware scheduling strategy available at the moment uses on-line scheduling, that is it starts a task as soon as a machine becomes available. In this paper we present a machine selection policy based on the idea that sometimes is better to wait for another machine rather than greedily exploit an immediately available one. An extensive simulation study, carried on for a variety of realistic Desktop Grid configurations and Bag-of-Task workloads, has revealed that the new scheduling strategy further improve application performance and machine utilization with respect to the best fault- aware scheduling strategy among those proposed in [2].",2007,0,
2731,2732,Stability and performance of the stochastic fault tolerant control systems,"In this paper, the stability and performance of the fault tolerant control system (FTCS) are studied. The analysis is based on a stochastic framework of integrated FTCS, in which the system component failure and the fault detection and isolation (FDI) scheme are characterized by two Markovian parameters. In addition, the model uncertainties and noise/disturbance are treated in the same framework. The sufficient conditions for stochastic stability and the system performance using a stochastic integral quadratic constraint are developed. A simulation study on an example system is performed with illustrative results obtained.",2003,0,
2732,2733,A new approach to find fault locations on distribution feeder circuits - Part II,"This paper focuses on results of that initial project, and introduce another R&D fault location project.",2008,0,
2733,2734,A novel approach to minimizing the risks of soft errors in mobile and ubiquitous systems,"A novel approach to minimizing the risks of soft errors at modeling level of mobile and ubiquitous systems is outlined. From a pure dependability viewpoint, critical components, whose failure is likely to impact on system functionality, attract more attention of protection/prevention mechanisms (against soft errors) than others do. Tolerating soft errors can be much improved if critical components can be identified at an early design phase and measures are taken to lower their criticalities at that stage. This improvement is achieved by presenting a criticality ranking (among the components) formed by combining a prediction of soft errors, consequences of them, and a propagation of failures at system modeling phase; and pointing out the ways to apply changes in the model to minimize the risks of degradation of desired functionalities. Case study results are given to illustrate and validate the approach.",2009,0,
2734,2735,Fast detector of symmetrical fault during power swing for distance relay,"Distance relay should be blocked during power swing to ensure the reliability, but still should trip as soon as possible after an internal fault occurs during power swing. It was very difficult to detect the symmetrical fault reliably and fast during power swing with complex power swing conditions and fault conditions considered. This paper presents a new fast detector of symmetrical fault during power swing. Based on the sudden reduction of absolute value of the change rate of power swing centre voltage (PSCV), the presented detector can detect the symmetrical fault reliably and sensitively in two cycles. This detector is easy to set and immune to the swing period, fault arc, fault location and power angle. EMTP simulations and real-time digital simulator system (RTDS) tests prove the presented detector is fast, sensible and reliable.",2005,0,
2735,2736,Monitoring and Diagnosis of External Faults in Three Phase Induction Motors Using Artificial Neural Network,"This paper addresses the possibility of integration of an external motor faults (e.g., phase failure, unbalanced voltage, locked rotor, undervoltage, overvoltage, phase sequence reversal of supply voltage, mechanical overload) monitoring and diagnostic technique into batch simulation with a digital protection set by using an artificial neural network (ANN) for a three-phase induction motor. The proposed set-up has been simulated using""Matlab/Simulink"" Software and tested for external motor faults. The simulated results clearly show that well-trained neural networks can precisely of early fault detection, diagnosis of external faults induction motor, also validating the proposed setup as a simple, reliable and effective protection for the three-phase induction motor fault identification scheme using an artificial neural network (ANN).",2007,0,
2736,2737,Dissolved gas analysis technique for incipient fault diagnosis in power transformers: A bibliographic survey,This article presents a bibliographic survey over the last 40 years on the research and development and on the procedures for evaluating faults by dissolved gas analysis of power transformers.,2010,0,
2737,2738,Formal Fault Tolerant Architecture,"This paper shows the need of development by refinement: from most abstract specification to the implementation, in order to ensure 1) the traceability of the needs and requirements, 2) a good management of the development and 3) a reliable and fault-tolerant design of systems. We propose a formal architecture of models and methods for critical requirements and fault-tolerance. System complexity increases and the choices of their implementation are numerous. So the architecture verification achieves a prominent role in the system design cycle. Fault detecting at this early level decreases the time and costs of correction. We show how a formal method, B method, may be used to write the abstract specification of a system then to product correct-by-construction architecture through many steps of formal refinement. During these steps, a fault scenario is injected with a suitable introspective reaction by the system. All refinement steps, including the introspective correction, should be proven to be correct and satisfy the initial specification of the system. At the lower levels, design is separated between hardware and software communities. But even at these levels many design traces could be captured to prove not only the consistency of each design unit but the coherence between the different sub-parts: software, digital or other technologies",2010,0,
2738,2739,Methodology and Tools Developed for Validation of COTS-based Fault-Tolerant Spacecraft Supercomputers,"Commercial off-the-shelf (COTS) electronic components are attractive for space applications. However, fault-tolerant architectures are required to cope with the Single Event Effect sensitivity of these components. CNES has developed a methodology, and the related validation tools, by injecting faults into these fault- tolerant architectures for validation purposes. The methodology is a hybrid one, combining deterministic and random fault injection phases. The main tools used are a boundary scan fault injector, made from an off-the-shelf JTAG tool, and software to analyse and process data obtained from the fault injection tests. This paper highlights the experience feedback relating to both the design and use of these tools, which were implemented to validate fault-tolerant architectures developed by CNES. Although this development has been done in the framework of the space domain, the methodology and tools are applicable for any fault-tolerant systems.",2007,0,
2739,2740,Fault restoration and spare capacity allocation with QoS constraints for MPLS networks,This paper investigates distributed fault restoration techniques for multiprotocol label switching (MPLS) to automatically reroute label switched paths in the event of link or router failures while maintaining quality of service (QoS) requirements. Protocols for path and partial path restoration are evaluated. A backup route selection algorithm based on optimization of equivalent bandwidth is formulated and demonstrated for an example network,2000,0,
2740,2741,Isolated singularity points recognition of hydroelectric generators fault signals based on CWT,"In order to extract fault feature information from low frequency vibrating transient signal of the main shaft of hydro-generator, an elective method is put forward. With bi-orthogonal spline wavelet, the method is designed to locate the isolated singularity points and estimate their singularity degree by applying WTMM (wavelet transformation maximal module) and MRA (multi-resolving analysis) of continuous wavelet transform (CWT) and least square algorithm and taking account of the length, intensity and Lipschitz exponent of WTMM lines. Result shows that the method has perfect effect.",2004,0,
2741,2742,A fault-tolerant directory-based cache coherence protocol for CMP architectures,"Current technology trends of increased scale of integration are pushing CMOS technology into the deep-submicron domain, enabling the creation of chips with a significantly greater number of transistors but also more prone to transient failures. Hence, computer architects will have to consider reliability as a prime concern for future chip-multiprocessor designs (CMPs). Since the interconnection network of future CMPs will use a significant portion of the chip real state, it will be especially affected by transient failures. We propose to deal with this kind of failures at the level of the cache coherence protocol instead of ensuring the reliability of the network itself. Particularly, we have extended a directory-based cache coherence protocol to ensure correct program semantics even in presence of transient failures in the interconnection network. Additionally, we show that our proposal has virtually no impact on execution time with respect to a non fault-tolerant protocol, and just entails modest hardware and network traffic overhead.",2008,0,
2742,2743,Phase distance relaying algorithm for unbalanced inter-phase faults,"This study presents a new fault impedance estimation algorithm for inter-phase faults for the purpose of phase distance relaying of extra high voltage (EHV) and ultra high voltage (UHV) transmission lines. The principle is based on the assumption that the fault arc path is predominantly resistive, and the phase angles of the unmeasured fault point voltage and fault arc path current are equal. This is used to construct the fault impedance estimation equation, which naturally prevents the effects of fault arc path resistance, load current, power swing and load encroachment. PSCAD (simulation software) simulation shows the validity of the proposed algorithm.",2010,0,
2743,2744,Study of online fault diagnosis for distributed substation based on Petri nets,"According to the distributed substation protection configuration and the principle of fault-clearance, a new model of fault diagnosis based on Petri net is proposed in this paper. In Petri net diagnosis model, all kinds of fault have a specific token which make it easily and clearly to find the fault location and understand the sequence of the fault events. The diagnostic is implemented by solving some matrix equations, which has a fast computational speed and a definite diagnostic result. The approach proposed is particular suitable for substation fault online diagnosis. Specially, in this model, the differential protection of the transformer and the bus bars are concerned as well as over current protection.",2010,0,
2744,2745,Plant-wide mass balance using extended support vector regression based data reconciliation and gross error detection,"In any modern petrochemical plant, the plant-wide mass data rendering the real conditions of manufacturing is the key to the operation managements such as production planning, production scheduling and performance analysis. Because of the characteristic of data reconciliation and gross error detection, it is quite suitable to address plant-wide mass balance problem using data reconciliation and gross error detection techniques. In this paper, an extended support vector regression approach for data reconciliation and gross error detection is proposed to achieve plant-wide mass balance, which can simultaneously detect and estimate measurement errors and missing mass movement information. The simulation results demonstrate that the proposed approach is effective and accurate.",2010,0,
2745,2746,Propagation model for estimating vor bearing error in the presence of windturbines  Hybridation of parabolic equation with physical optics,"Windturbines near VOR ground station can yield significant bearing errors in the azimuth estimation. We propose a model that combines the parabolic equation and the physical optics approximation to predict these errors. It accounts for a possible hilly terrain, and a generic model of windturbines that includes dielectric blades. All the hypotheses made in the model are carefully justified by means of numerical simulations. In a realistic test case, this model is employed to compute the error caused by a complete windfarm located on a hilly terrain within acceptable computation time.",2010,0,
2746,2747,Fault Detection of Backlash Phenomenon in Mechatronic System with Parameter Uncertainties Using Bond Graph Approach,"This paper presents a method for fault detection and residuals evaluation, applied on electromechanical test bench system in the presence of backlash phenomenon and parameter uncertainties. The analytical redundancy relations (ARRs) are generated using uncertain bond graph model, in linear fractional transformation form (LFT). Through the presented method, one can distinguish between backlash default and system parameter variation. Simulation tests presented in this paper show the influence of dead zone magnitude and system parameters variations on residuals evaluation",2006,0,
2747,2748,A Test Pattern Generation Method Based on Fault Injection for Logic Elements of FPGA,"In this paper, we present a test pattern generation method based on fault injection for logic elements of FPGAs (Field Programmable Gate Arrays). This method is able to perform fault diagnosis for stuck-at-0 and stuck-at-1 faults, which can locate logic resource faults in the logic elements of FPGA. We use EP2C8Q208C8N's LE (Logic Element) of Altera as the object to generate the test pattern, work out the test circuit and synthesis them by Quartus II. Finally, the test circuit is injected with stuck-at-0 and stuck-at-1 faults and the test patterns are generated by using SPICE.",2010,0,
2748,2749,Correction Model of Pressure Sensor Based on Support Vector Machine,"The temperature and voltage fluctuation characteristics of pressure sensor was analyzed and found that the sensor output is nonlinear and easy to be affected by temperature and voltage fluctuation over a wide measuring range, a correction model of pressure sensor based on Support Vector Machine was presented. The approximate ability of the SVM to any nonlinear function was utilized to drill the correction model. so as to enable it to be setup at different temperatures and voltage fluctuation, thus allowing the sensor output can be in a nonlinear mapping relation to the voltage values the sensor actually sensed. The experimental results showed that the max comes down from 22.2% for 0.64%; the model can not only eliminate the influence of temperature fluctuation and voltage fluctuation but obtain the expected linear output from the output terminal of correction model.",2009,0,
2749,2750,An accurate method for correction of head movement in PET,"A method is presented to correct positron emission tomography (PET) data for head motion during data acquisition. The method is based on simultaneous acquisition of PET data in list mode and monitoring of the patient's head movements with a motion tracking system. According to the measured head motion, the line of response (LOR) of each single detected PET event is spatially transformed, resulting in a spatially fully corrected data set. The basic algorithm for spatial transformation of LORs is based on a number of assumptions which can lead to spatial artifacts and quantitative inaccuracies in the resulting images. These deficiencies are discussed, demonstrated and methods for improvement are presented. Using different kinds of phantoms the validity and accuracy of the correction method is tested and its applicability to human studies is demonstrated as well.",2004,0,
2750,2751,Adaptation of neural network and application of digital ultrasonic image processing for the pattern recognition of defects in semiconductor,"In this study, the classification of artificial defects in semiconductor devices are performed by using pattern recognition technology. For this target, a pattern recognition algorithm including user made software was developed and the total procedure including image processing and self-organizing map was treated by a backpropagation neural network, where image processing was composed of ultrasonic image acquisition, equalization filtering, binary processing and edge detection. Image processing and self-organizing map were compared as preprocessing methods for the reduction of dimensionality as input data into multi-layer perceptron or backpropagation neural networks. Also, the pattern recognition technique has been applied to classify two kinds of semiconductor defects: cracks and delamination. According to these results, it was found that the self-organizing map provided recognition rates of 83.4% and 75.7% for delamination and cracks, respectively, while BP provided 100% recognition rates for the results",2001,0,
2751,2752,Optimal Periodic Testing of Intermittent Faults In Embedded Pipelined Processor Applications,"Today's nanometer technology trends have a very negative impact on the reliability of semiconductor products. Intermittent faults constitute the largest part of reliability failures that are manifested in the field during the semiconductor product operation. Since software-based self-test (SBST) has been proposed as an effective strategy for on-line testing of processors integrated in non-safety critical low-cost embedded system applications, optimal test period specification is becoming increasingly challenging. In this paper we first introduce a reliability analysis for optimal periodic testing of intermittent faults that minimizes the test cost incurred based on a two-state Markov model for the probabilistic modeling of intermittent faults. Then, we present for the first time an enhanced SBST strategy for on-line testing of complex pipelined embedded processors. Finally, we demonstrate the effectiveness of the proposed optimal periodic SBST strategy by applying it to a fully-pipelined RISC embedded processor and providing experimental results",2006,0,
2752,2753,Fault Tolerance Management for a Hierarchical GridRPC Middleware,"The GridRPC model is well suited for high performance computing on grids thanks to efficiently solving most of the issues raised by geographically and administratively split resources. Because of large scale, long range networks and heterogeneity, Grids are extremely prone to failures. GridRPC middleware are usually managing failures by using 1) TCP or other link network layer provided failure detector, 2) automatic checkpoints of sequential jobs and 3) a centralized stable agent to perform scheduling. Most recent developments have provided some new mechanisms like the optimal Chandra & Toueg & Aguillera failure detector, most numerical libraries now providing their own optimized checkpoint routine and distributed scheduling GridRPC architectures. In this paper we aim at adapting to these novelties by providing the first implementation and evaluation in a grid system of the optimal fault detector, a novel and simple checkpoint API allowing to manage both service provided checkpoint and automatic checkpoint (even for parallel services) and a scheduling hierarchy recovery algorithm tolerating several simultaneous failures. All those mechanisms are implemented and evaluated on a real grid in the DIET middleware.",2008,0,
2753,2754,Scatter and cross-talk corrections in simultaneous Tc-99m/I-123 brain SPECT using constrained factor analysis and artificial neural networks,"Simultaneous imaging of Tc-99m and I-123 would have a high clinical potential in the assessment of brain perfusion (Tc-99m) and neurotransmission (I-123) but is hindered by cross-talk between the two radionuclides. Monte Carlo simulations of 15 different dual-isotope studies were performed using a digital brain phantom. Several physiologic Tc-99m and I-123 uptake patterns were modeled in the brain structures. Two methods were considered to correct for cross-talk from both scattered and unscattered photons: constrained spectral factor analysis (SFA) and artificial neural networks (ANN). The accuracy and precision of reconstructed pixel values within several brain structures were compared to those obtained with an energy windowing method (WSA). In I-123 images, mean bias was close to 10% in all structures for SFA and ANN and between 14% (in the caudate nucleus) and 25% (in the cerebellum) for WSA. Tc-99m activity was overestimated by 35% in the cortex and 53% in the caudate nucleus with WSA, but by less than 9% in all structures with SFA and ANN. SFA and ANN performed well even in the presence of high-energy I-123 photons. The accuracy was greatly improved by incorporating the contamination into the SFA model or in the learning phase for ANN. SFA and ANN are promising approaches to correct for cross-talk in simultaneous Tc-99m/I-123 SPECT",2000,0,
2754,2755,Grey Clustering Analysis Based Classifier for Steam Turbine-Generator Fault Diagnosis,"This paper proposes a method for steam turbine-generator fault diagnosis using grey clustering analysis (GCA). According to the field records, diagnostic information can be provided to monitor mechanical condition by the spectrum of the vibration signal. Frequency-based features are computed by fast Fourier transformation (FFT), the frequency ranges are <0.4f, 1f, 2f, 3f, and >3f. The maximum and minimum values of power spectrum indicate mechanical vibration fault at a particular frequency, and frequency patterns are applied to diagnose faults. For numerical tests with practical filed records, test results were conducted to show the proposed method demonstrates computational efficiency and high accuracy.",2007,0,
2755,2756,Fault Location in Distribution Systems by Means of a Statistical Model,"The enhancement of power distribution system reliability requires a great investment but not all the utilities are in a position to assume it. Therefore, any strategy that allows the improvement of reliability should be reflected directly in the decrease of the duration and frequency of interruptions. In this paper an alternative solution to the problems of continuity associated to fault location is presented. A methodology of statistical nature is proposed using mixture of distributions. With this approach a statistical model is obtained from the extraction of characteristic patterns of the signals registered by measurement equipments, along with the parameters and own topology of the network during an event. The purpose of this methodology is to offer an economic alternative of easy implementation for the development of strategies oriented to improve the reliability from the decrease in the times of attention and recovery of the system",2006,0,
2756,2757,On Rigorous Design and Implementation of Fault Tolerant Ambient Systems,"Developing fault tolerant ambient systems requires many challenging factors to be considered due to the nature of such systems, which tend to contain a lot of mobile elements that change their behavior depending on the surrounding environment, as well as the possibility of their disconnection and reconnection. It is therefore necessary to construct the critical parts of fault tolerant ambient systems in a rigorous manner. This can be achieved by deploying formal approach at the design stage, coupled with sound framework and support at the implementation stage. In this paper, we briefly describe a middleware that we developed to provide system structuring through the concepts of roles, agents, locations and scopes, making it easier for the developers to achieve fault tolerance. We then outline our experience in developing an ambient lecture system using the combination of formal approach and our middleware",2007,0,
2757,2758,Trajectory zero phase error tracking control using comparing coefficients method,This paper presents the studies on trajectory zero phase error tracking control without factorisation of zeros polynomial where the controller parameters are determined using comparing coefficients methods. The controller was applied to two types of third-order non-minimum phase plant. The first plant was having a zero outside and far from the unity circle. Another plant was having a zero outside and near to the unity circle. Simulation and experimental results will be presented to discuss its tracking performance.,2009,0,
2758,2759,Condition monitoring and fault diagnosis of electrical motors-a review,"Recently, research has picked up a fervent pace in the area of fault diagnosis of electrical machines. The manufacturers and users of these drives are now keen to include diagnostic features in the software to improve salability and reliability. Apart from locating specific harmonic components in the line current (popularly known as motor current signature analysis), other signals, such as speed, torque, noise, vibration etc., are also explored for their frequency contents. Sometimes, altogether different techniques, such as thermal measurements, chemical analysis, etc., are also employed to find out the nature and the degree of the fault. In addition, human involvement in the actual fault detection decision making is slowly being replaced by automated tools, such as expert systems, neural networks, fuzzy-logic-based systems; to name a few. It is indeed evident that this area is vast in scope. Hence, keeping in mind the need for future research, a review paper describing different types of faults and the signatures they generate and their diagnostics' schemes will not be entirely out of place. In particular, such a review helps to avoid repetition of past work and gives a bird's eye view to a new researcher in this area.",2005,0,
2759,2760,Design and Construction of a Magnetic Fault Current Limiter,A fault current limiter using permanent magnets has been designed and its performance simulated using a two-dimensional dimensional time-stepping finite-element method incorporating a model of hysteresis for hard magnetic materials.,2006,0,
2760,2761,A Safety Analysis Method Using Fault Tree Analysis and Petri Nets,"In this paper, we describe a safety analysis method that utilizes two models, namely, Petri nets to model the behavioral aspects of a system, and fault tree analysis to model failure and hence unacceptable behaviors of a system. Using petri nets and fault tree analysis, we should be able to perform both forward and backward reachability analyses that are related to acceptable and unacceptable behaviors of a system. To show the feasibility of our proposed method, a case study, railroad crossing system, has been conducted.",2009,0,
2761,2762,Fault-tolerant router with built-in self-test/self-diagnosis and fault-isolation circuits for 2D-mesh based chip multiprocessor systems,"A fault-tolerant router design (20-path router) is proposed to reduce the impacts of faulty routers for 2D-mesh based chip multiprocessor systems. In our experiments, the OCNs using 20PRs can reduce 75.65% ~ 85.01% unreachable packets and 7.78% ~ 26.59% latency in comparison with the OCNs using generic XY routers.",2009,0,
2762,2763,Fault Detection and Isolation in Aircraft Systems Using Stochastic Nonlinear Modelling of Flight Data Dependencies,"This paper introduces a fault detection and isolation (FDI) scheme for aircraft systems based on the modelling of the relationships among flight variables. The modelling is performed by means of pooled nonlinear autoregressive with exogenous (NARX) excitation representations. During the system's operation in healthy mode, these relationships are valid. Hence, a scheme using statistical hypothesis testing is designed to detect changes in these relationships as a result of fault occurrence. The FDI scheme's performance and robustness are assessed with flights conducted under various external flight conditions (turbulence)",2006,0,
2763,2764,Predicting Eclipse Bug Lifetimes,"In non-trivial software development projects planning and allocation of resources is an important and difficult task. Estimation of work time to fix a bug is commonly used to support this process. This research explores the viability of using data mining tools to predict the time to fix a bug given only the basic information known at the beginning of a bug's lifetime. To address this question, a historical portion of the Eclipse Bugzilla database is used for modeling and predicting bug lifetimes. A bug history transformation process is described and several data mining models are built and tested. Interesting behaviours derived from the models are documented. The models can correctly predict up to 34.9% of the bugs into a discretized log scaled lifetime class.",2007,0,
2764,2765,An Efficient Test Pattern Selection Method for Improving Defect Coverage with Reduced Test Data Volume and Test Application Time,"Testing using n-detection test sets, in which a fault is detected by n (n > 1) input patterns, is being increasingly advocated to increase defect coverage. However, the data volume for an n-detection test set is often too large, resulting in high testing time and tester memory requirements. Test set selection is necessary to ensure that the most effective patterns are chosen from large test sets in a high-volume production testing environment. Test selection is also useful in a time-constrained wafer-sort environment. The authors use a probabilistic fault model and the theory of output deviations for test set selection - the metric of output deviation is used to rank candidate test patterns without resorting to fault grading. To demonstrate the quality of the selected patterns, experimental results were presented for resistive bridging faults and non-feedback zero-resistance bridging faults in the ISCAS benchmark circuits. Our results show that for the same test length, patterns selected on the basis of output deviations are more effective than patterns selected using several other methods",2006,0,
2765,2766,A Novel Robust Video Transmission Scheme for Error Resilient Transcoding,"For video transmission over wireless or highly congested networks, video transcoding is typically used to reduce the rate and change the format of the originally encoded video source to match network conditions and terminal capabilities. Error resilient video transcoding can insert error resilient tools in the compressed video to enhance error resilience of the video over wireless channels by increasing bit rate. This paper proposed a novel error resilient video transcoding and streaming transmission scheme specially designed for Mpeg2 to H.264 transcoded video to portable devices in wireless channel. Based on the rate-distortion models developed in this paper, an optimal transcoding marcoblock mode selection and bit allocation scheme is proposed. In order to improve the video quality in the presence of transmission error, this paper also investigated how to allocate redundant pictures more efficiently according to the content characteristics of the primary pictures. The experiment results demonstrated that the proposed transcoding scheme can enhance speed and improve the decode image quality, get better error resilience.",2009,0,
2766,2767,PageChaser: A Tool for the Automatic Correction of Broken Web Links,"PageChaser is a system that monitors links between Web pages and searches for the new locations of moved Web pages when it finds broken links. The problem of searching for moved pages is different from typical information retrieval problems. First, it is impossible to identify the final destination until the page is actually moved, so the index-server approach is not necessarily effective. Secondly, there is a large bias about where the new address is likely to be and crawler-based solutions can be effectively implemented, avoiding the need to search the entire Web. PageChaser incorporates a comprehensive set of heuristics, some of which are novel, in a single unified framework. This paper explains the underlying ideas behind the design and development of PageChaser.",2008,0,
2767,2768,End-to-end defect modeling,"In this context, computer models can help us predict outcomes and anticipate with confidence. We can now use cause-effect modeling to drive software quality, moving our organization toward higher maturity levels. Despite missing good software quality models, many software projects successfully deliver software on time and with acceptable quality. Although researchers have devoted much attention to analyzing software projects' failures, we also need to understand why some are successful - within budget, of high quality, and on time-despite numerous challenges. Restricting software quality to defects, decisions made in successful projects must be based on some understanding of cause-effect relationships that drive defects at each stage of the process. To manage software quality by data, we need a model describing which factors drive defect introduction and removal in the life cycle, and how they do it. Once properly built and validated, a defect model enables successful anticipation. This is why it's important that the model include all variables influencing the process response to some degree.",2004,0,
2768,2769,A Variable Printer Model in Tone-Dependent Error Diffusion Halftone,"Halftone is a technique used for binary devices such printers to simulate the continuous tone image. But the quality of halftone prints produced by printers can be limited by dot gain and dot-placement errors. In this paper, we propose a variable printer model which can be combined in tone-dependent error diffusion (TDED). First we analyses the characteristic of the test printer, and based on the experimental data we propose a variable inkjet printer model. With this printer model, TDED algorithm shows better quality than traditional.",2008,0,
2769,2770,Fault-Tolerant Indirect Adaptive Neurocontrol for a Static Synchronous Series Compensator in a Power Network With Missing Sensor Measurements,"Identification and control of nonlinear systems depend on the availability and quality of sensor measurements. Measurements can be corrupted or interrupted due to sensor failure, broken or bad connections, bad communication, or malfunction of some hardware or software (referred to as missing sensor measurements in this paper). This paper proposes a novel fault-tolerant indirect adaptive neurocontroller (FTIANC) for controlling a static synchronous series compensator (SSSC), which is connected to a power network. The FTIANC consists of a sensor evaluation and (missing sensor) restoration scheme (SERS), a radial basis function neuroidentifier (RBFNI), and a radial basis function neurocontroller (RBFNC). The SERS provides a set of fault-tolerant measurements to the RBFNI and RBFNC. The resulting FTIANC is able to provide fault-tolerant effective control to the SSSC when some crucial time-varying sensor measurements are not available. Simulation studies are carried out on a single machine infinite bus (SMIB) as well as on the IEEE 10-machine 39-bus power system, for the SSSC equipped with conventional PI controllers (CONVC) and the FTIANC without any missing sensors, as well as for the FTIANC with multiple missing sensors. Results show that the transient performances of the proposed FTIANC with and without missing sensors are both superior to the CONVC used by the SSSC (without any missing sensors) over a wide range of system operating conditions. The proposed fault-tolerant control is readily applicable to other plant models in power systems.",2008,0,
2770,2771,Transfer and Error Rate Measurement in the Lon Works Power Line Communication Systems,"This paper is focused on analyzing the transfer and the error rate measurement using network variables in LonWorks systems. The measurement was done by using the LonWorks power line modem mini evaluation kit (Mini EVK) feed by a standard RS-232 interface of the personal computer and the special software which was developed for this aim The Mini EVK power line modem contains the PL3150 circuit, which is designed for intelligent buildings control and home automation. The results of a measurement of the transfer and the error rate are presented.",2007,0,
2771,2772,Goal trees and fault trees for root cause analysis,"Typical enterprise applications are built upon different platforms, operate in a heterogeneous, distributed environment, and utilize different technologies, such as middleware, databases and Web services. Diagnosing the root causes of problems in such systems is difficult in part due to the number of possible configuration and tuning parameters. Today a variety of tools are used to aid operators of enterprise applications identify root causes. For example, a user input validation tool detects and prevents Website intrusions or a log analysis tool identifies malfunctioning components. Searching for the root causes of such failures in a myriad of functional and non-functional requirements poses significant challenges-not only for users, but also for experienced operators when monitoring, auditing, and diagnosing systems. We propose the notion of a guide map-a set of goal trees and fault trees-to aid users in the process of choosing (supported by high level goal trees) and applying (supported by low level fault trees) suitable diagnostic tools. In this paper we discuss two case studies to illustrate how the guide map aids users to apply two home grown diagnostic tools.",2008,0,
2772,2773,Evaluation of H.264/AVC error resilience in HD IPTV applications,"The delivery of High Definition Television (HDTV) over IP networks, namely the HD IPTV, has emerged as one of the major distribution and access techniques for broadband multimedia services. IPTV adopts H.264/AVC as its coding standard due to its high video compression efficiency as well as powerful error resilience features. This paper presents studies on some of these features applied to HD IPTV applications. A test system is deployed to simulate the delivery of HD video over a DSL based IPTV network. Effects of error resilience of slicing and Instantaneous Decoding Refreshing (IDR) features on video quality are examined in both channel non-impaired and channel impaired with burst noise circumstances. Based on the acquired results, optimal slice size was obtained for HD video transmission over an impaired channel. The quality of experience related to the IDR interval in combating error propagation was also characterized.",2010,0,
2773,2774,Improving Fault Tolerance by Virtualization and Software Rejuvenation,"The phenomenon that the state of software degrades with time is known as software aging. The primary method to fight aging is software rejuvenation. This paper presents new ways of effective software rejuvenation using virtualization for addressing software aging. This new approach is meant to be the less disruptive as possible for the running service and to get a zero downtime in most of the cases. We construct the state transition models to describe the behaviors of virtualized and non- virtualized application server. We map through the rejuvenation actions to this transition model with stochastic process and express availability, downtime and downtime costs in terms of the parameters in our models. Our results show that virtualization and software rejuvenation can be used to prolong the availability of the services.",2008,0,
2774,2775,Sensitivity of Real-Time Operating Systems to Transient Faults: A case study for MicroC kernel,"This paper explores sensitivity of RTOS kernels in safety-critical systems. We characterize and analyze the consequences of transient faults on key components of the MicroC kernel, a popular RTOS. We specifically focus on its task scheduling and context switching modules. Classes of fault syndromes specific to safety-critical real-time systems are identified. Results reported in this paper demonstrate that 34% of faults led to scheduling dysfunctions. In addition 17% of faults results in system crashes. This represents an important fraction of faults that cannot be ignored during the design phase of safety-critical applications running under an RTOS.",2005,0,
2775,2776,Real-word spelling correction using Google Web 1T n-gram with backoff,We present a method for correcting real-word spelling errors using the Google Web 1T n-gram data set and a normalized and modified version of the longest common subsequence (LCS) string matching algorithm. Our method is focused mainly on how to improve the correction recall (the fraction of errors corrected) while keeping the correction precision (the fraction of suggestions that are correct) as high as possible. Evaluation results on a standard data set show that our method performs very well.,2009,0,
2776,2777,Architectural Design of CA-Based Double Byte Error Correcting Codec,"Cellular Automata (CA) is a novel approach for designing byte error-correcting codes. The regular, modular and cascaded structure of CA can be economically built with VLSI technology. In this correspondence, a modular architecture of CA based (32, 28) byte error correcting encoder and decoder has been proposed. The design is capable of locating and correcting all double byte errors. CA-based implementation of the proposed decoding scheme provides a simple cost effective solution compared to the existing decoding scheme for the Reed-Solomon (RS) decoder, having double error correcting capability.",2008,0,
2777,2778,A fault tolerant topology control in wireless sensor networks,"Summary form only given. Topology control of wireless sensor networks (WSNs) is a key design challenge in terms of extending the lifetime of the network. The paper presents a fault tolerant topology control by adding necessary redundant nodes to the network's simple communication backbone, which results a higher vertex connectivity degree. It provides not only fault tolerance for unreliable node failure, but also support for upper level protocols. The paper also identifies several factors and synchronization methods which may affect the redundant node selection. A simulation study shows the improvement of network lifetime with a desired vertex connectivity degree.",2005,0,
2778,2779,Unequal error protection for ROI coded images over fading channels,"Region of interest (ROI) coding is a feature supported by the Joint Photographic Experts Group 2000 (JPEG 2000) image compression standard and allows particular regions of interest within an image to be compressed at a higher quality than the rest of the image. In this paper, unequal error protection (UEP) is proposed for ROI coded JPEG 2000 images as a technique for providing increased resilience against the effects of transmission errors over a wireless communications channel. The hierarchical nature of an ROI coded JPEG 2000 code-stream lends itself to the use of UEP whereby the important bits of the code-stream are protected with a strong code while the less important bits are protected with a weaker code. Simulation results obtained using symbol-by-symbol maximum a posteriori probability (MAP) decoding demonstrate that the use of UEP offers significant gains in terms of the peak signal to noise ratio (PSNR) and the percentage of readable files. Moreover the use of ROI-based UEP leads to reduced computational complexity at the receiver.",2005,0,
2779,2780,Study on Integration Diagnosis System for Automobile Faults and Its Key Technologies,"With the rapid development of automobile electronics level, the mechatronics of automobile product becomes more and more obviously. When we try our best to improve function of automobile by electronic controlled system, the difficulty of failure diagnosis is increasing too. In order to improve the maintenance quality and efficiency, automobile manufacturer and maintenance server have established the integration diagnosis system on the basis of on-board diagnosis and off-board diagnosis. There are two types of automobile fault diagnosis integration system, primary integration system which composed of detection technology and expert system; higher integration which composed of detection technology, expert system and network communication. These two systems may be composed of the parameters measuring module, data fusion module, and fault diagnosis module, information obtaining module and network communication module. Based on the analysis of module functions, the reason and basis to some key technologies such as measuring apparatus or equipment communication standards, the available of fault diagnosis expert system, the multifunction inferring methods and fault diagnosis knowledge obtaining by network are presented in this paper.",2008,0,
2780,2781,Predicting Re-opened Bugs: A Case Study on the Eclipse Project,"Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on the Eclipse project. We structure our study along 4 dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed on), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). Our case study on the Eclipse Platform 3.0 project shows that the comment and description text, the time it took to fix the bug, and the component the bug was found in are the most important factors in determining whether a bug will be re-opened. Based on these dimensions we create decision trees that predict whether a bug will be re-opened after its closure. Using a combination of our dimensions, we can build explainable prediction models that can achieve 62.9% precision and 84.5% recall when predicting whether a bug will be re-opened.",2010,0,
2781,2782,Complexity issues in automated synthesis of failsafe fault-tolerance,"We focus on the problem of synthesizing failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. However, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of synthesizing failsafe fault-tolerant distributed programs from their fault-intolerant version is NP-complete in the state space of the program. We also identify a class of specifications, monotonic specifications, and a class of programs, monotonic programs, for which the synthesis of failsafe fault-tolerance can be done in polynomial time (in program state space). As an illustration, we show that the monotonicity restrictions are met for commonly encountered problems, such as Byzantine agreement, distributed consensus, and atomic commitment. Furthermore, we evaluate the role of these restrictions in the complexity of synthesizing failsafe fault-tolerance. Specifically, we prove that if only one of these conditions is satisfied, the synthesis of failsafe fault-tolerance is still NP-complete. Finally, we demonstrate the application of monotonicity property in enhancing the fault-tolerance of (distributed) nonmasking fault-tolerant programs to masking.",2005,0,
2782,2783,On the Accuracy of Spectrum-based Fault Localization,"Spectrum-based fault localization shortens the test- diagnose-repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. However, as no model of the system is taken into account, its diagnostic accuracy is inherently limited. Using the Siemens Set benchmark, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near- optimal diagnostic accuracy (exonerating about 80% of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. The influence of the number of test cases is of primary importance for continuous (embedded) processing applications, where only limited observation horizons can be maintained.",2007,0,
2783,2784,Research on influences of sampling errors on performances of three-level PWM rectifier,"In order to solve the problem that the sampling errors depress the control performances of pulse width modulation (PWM) rectifier, this paper focuses on a three-level PWM rectifier with voltage oriented control (VOC) strategy, introduces the structure of sampling process, and analyzes the influences of sampling errors on the control performances quantificationally. A software optimization method aimed for restraining the persistent and random sampling errors is then proposed. Simulation and experimental results verified the validity of the analyses and the feasibility of the proposed method. The performances of the rectifier were remarkably improved by using the proposed optimization method.",2009,0,
2784,2785,Toward accurate modeling of the IEEE 802.11e EDCA under finite load and error-prone channel,"In this paper we study the performance of IEEE 802.11e enhanced distributed channel access (EDCA) priority schemes under finite load and error-prone channel. We introduce a multi-dimensional Markov Chain model that includes all the mandatory differentiation mechanisms of the standard: QoS parameters, CW<sub>MIN</sub>, CW<sub>MAX</sub> arbitration inter-frame space (AIFS), and the virtual collision handler. The model faithfully represents the functionality of the EDCA access mechanisms, including lesser known details of the standard such as the management of the backoff counter which is technically different from the one used in the legacy DCF. We study the priority schemes under both finite load and saturation conditions. Our analysis also takes into consideration channel conditions.",2008,0,
2785,2786,Transformer Fault Analysis Using Event Oscillography,"Transformer differential protection operates on Kirchhoff's well-known law that states, ""the sum of currents entering and leaving a point is zero"". Although Kirchhoff's law is well understood, the implementation of the law in transformer differential protection involves many practical considerations such as current transformer (CT) polarity, phase-angle correction, zero-sequence removal, and CT grounding. Still, even correctly implemented transformer differential protection misoperates occasionally, resulting from conditions such as CT saturation during heavy through faults. Whereas electromechanical and electronic relays provide no or very little fault information, numerical relays provide an abundance of information. However, the analyst must still select the correct fault information from this abundance of information to perform useful fault analysis. This paper demonstrates how to begin analysis of such events by using real-life oscillographic data and going through a step-by-step analysis of the relay algorithm using a mathematical relay model. Relay engineers can use this paper as a reference for analyzing transformer oscillography in a systematic and logical manner",2007,0,
2786,2787,A novel diagnostic method for single power switch open-circuit faults in voltage-fed PWM motor drives,"Nowadays, variable speed AC drives have become a standard in many industrial applications. Hence, due to the widespread adoption of advanced power control devices, fault diagnosis of power converters, in particular the voltage source inverter in variable speed AC drives, is becoming more and more important. Although there are different fault types that may occur in these converters, in this paper, only single power switch open-circuit failures are considered in a vector controlled permanent magnet synchronous motor drive. A new real-time algorithm that allows the detection and localization of the faulty device using just the motor phase currents is proposed. Several results under different operating conditions are presented, proving the method effectiveness, low detection time and robustness against false alarms.",2010,0,
2787,2788,"""A Bug's Life"" Visualizing a Bug Database","Visualization has long been accepted as a viable means to comprehend large amounts of information. Especially in the context of software evolution a well-designed visualization is crucial to be able to cope with the sheer data that needs to be analyzed. Many approaches have been investigated to visualize evolving systems, but most of them focus on structural data and are useful to answer questions about the structural evolution of a system. In this paper we consider an often neglected type of information, namely the one provided by bug tracking systems, which store data about the problems that various people, from developers to end users, detected and reported. We first briefly introduce the context by reporting on the particularities of the present data, and then propose two visualizations to render bugs as first-level entities.",2007,0,
2788,2789,Razor: circuit-level correction of timing errors for low-power operation,Dynamic voltage scaling is one of the more effective and widely used methods for power-aware computing. We present a DVS approach that uses dynamic detection and correction of circuit timing errors to tune processor supply voltage and eliminate the need for voltage margins,2004,0,
2789,2790,Fault-tolerant and reliable computation in cloud computing,"Cloud computing, with its great potentials in low cost and on-demand services, is a promising computing platform for both commercial and non-commercial computation clients. In this work, we investigate the security perspective of scientific computation in cloud computing. We investigate a cloud selection strategy to decompose the matrix multiplication problem into several tasks which will be submitted to different clouds. In particular, we propose techniques to improve the fault-tolerance and reliability of a rather general scientific computation: matrix multiplication. Through our techniques, we demonstrate that fault-tolerance and reliability against faulty and even malicious clouds in cloud computing can be achieved.",2010,0,
2790,2791,Research on Multi-Sensor Information Fusion for the Detection of Surface Defects in Copper Strip,"Based on the defects detection on the surface of the copper strips, this paper firstly studies how to enhance system stability with the multi-sensors information fusion method. This method combines infrared, visible light and laser sensors to deal with defects detection, utilizes fuzzy logic and neural network to carry on the sensor's management, and uses wavelet transformation in image fusion. Experimental results show that this method can effectively detect surface defects in copper strips. Furthermore, it enhances the accuracy of recognizing and classifying, and makes the overall system more automatic and intelligent.",2009,0,
2791,2792,Actuator fault compensation for a helicopter model,"A fault-tolerant system is the one that can continue its operation without significant impact on performance in the presence of hardware and/or software errors. In this paper, the design of a fault-tolerant flight controller to control UH-60 helicopter is investigated. A 9th-order state space representation of the helicopter model operating at the forward mode with 80 knots is presented; then a fault-tolerant optimal feedback controller is designed and tested.",2003,0,
2792,2793,State monitoring and fault diagnosis of the PWM converter using the magnetic field near the inductor components,"This paper proposes a new fault diagnostic method for the PWM converter. A loop magnetic near field probe is used to detect the magnetic field near the inductor components in the PWM converters, and the measured waveform is utilized as the diagnostic criterion. The features of the waveform are extracted by the Fast Fourier Transform, and the interested low and high order harmonic components are used to classify the states of the converters. The low order harmonic components are classified by the Back Propagation Neural Network and the high order harmonic components are classified by the simple mathematical method. Finally, by compromising both the two results, the detailed diagnostic conclusions are obtained.",2010,0,
2793,2794,Router group monitoring: making traffic trajectory error detection more efficient,"Detecting errors in traffic trajectories (i.e., packet forwarding paths) is important to operational networks. Several different traffic monitoring algorithms such as Trajectory Sampling, PSAMP, and Fatih can be used for traffic trajectory error detection. However, a straight-forward application of these algorithms will incur the overhead of simultaneously monitoring all network interfaces in a network for the packets of interest. In this paper, we propose a novel technique called router group monitoring to improve the efficiency of trajectory error detection by only monitoring the periphery interfaces of a set of selected router groups. We analyze a large number of real network topologies and show that effective router groups with high trajectory error detection rates exist in all cases. However, for router group monitoring to be practical, those effective router groups must be identified efficiently. To this end, we develop an analytical model for quickly and accurately estimating the detection rates of different router groups. Based on this model, we propose an algorithm to select a set of router groups that can achieve complete error detection and low monitoring overhead. Finally, we show that the router group monitoring technique can significantly improve the efficiency of trajectory error detection based on Trajectory Sampling or Fatih.",2010,0,
2794,2795,The importance of life cycle modeling to defect detection and prevention,"In many low mature organizations dynamic testing is often the only defect detection method applied. Thus, defects are detected rather late in the development process. High rework and testing effort, typically under time pressure, lead to unpredictable delivery dates and uncertain product quality. This paper presents several methods for early defect detection and prevention that have been in existence for quite some time, although not all of them are common practice. However, to use these methods operationally and scale them to a particular project or environment, they have to be positioned appropriately in the life cycle, especially in complex projects. Modeling the development life cycle, that is the construction of a project-specific life cycle, is an indispensable first step to recognize possible defect injection points throughout the development project and to optimize the application of the available methods for defect detection and prevention. This paper discusses the importance of life cycle modeling for defect detection and prevention and presents a set of concrete, proven methods that can be used to optimize defect detection and prevention. In particular, software inspections, static code analysis, defect measurement and defect causal analysis are discussed. These methods allow early, low cost detection of defects, preventing them from propagating to later development stages and preventing the occurrence of similar defects in future projects.",2002,0,
2795,2796,The effect of 3D building reconstruction errors on propagation prediction using geospatial data in cyberspace,"When the 3D building structures visualized in Google Earth are reconstructed using photogrametric method, errors or inaccuracies will occur to the building vertices and the building heights. In this paper the statistics of these errors are discussed and the effect of these errors on the propagation prediction results is examined in detail. It is found that our reconstruction method introduces distance errors to the building vertices and height errors to the building heights. These errors are less than 0.5 meters in 95% of the cases. The vertex error will cause an average mean error of -0.2 dB and an average standard deviation of 5.1 dB to the predicted path gains compared to the reference case. And the height error, in the cases investigated in this paper, is very small and can be ignored. These results match the observations in the literature for different propagation environments. The 3D reconstruction method is then shown to be of satisfactory accuracy in terms of propagation prediction.",2009,0,
2796,2797,Robust and extreme unequal error protection scheme for the transmission of scalable data over OFDM systems,Wireless applications are subject to the end-to-end quality of service (QoS) requirements. This paper presents a new resources allocation algorithm that allows to transmit scalable multimedia data over a frequency selective channel with partial channel knowledge. The available resources are subject to payload and QoS constraints and the algorithm aims at maximizing the transmission robustness to channel estimation errors. The impact of this technique is evaluated for a MPEG-4 audio application.,2008,0,
2797,2798,Ventilator Fault Diagnosis Based on Fuzzy Theory,"Fault diagnosis has been the research hotspot in the industry fields. It has a practical significance to discuss the effective fault diagnosis methods. Aiming at the fuzzy and random features of the occurrence probabilities, this paper presents a hybrid method that combines the fault tree with fuzzy set theory.In this approach, fuzzy aggregation and defuzzification are adopted and this method is used in ventilator fault diagnosis. The research shows that this method is feasible and effective and can be applied to the other rotating machinery fault diagnosis.",2009,0,
2798,2799,A novel RF phase error Built-in-Self-Test for GSM,"This paper discusses a novel RF Built-in-Self-Test (RF-BiST) targeting to replace the traditionally expensive and time-consuming RF parametric phase error test on a GSM/EDGE Digital Radio Processor (DRP) radio transceiver. The verification of the RF BiST in a production environment and a comparison of the internal BiST vs. the current test in are presented, which validates the RF BiST as an accepted test method for determining the phase error of GSM devices. The results illustrate that there are great opportunities in reduction of test time and costs by moving to the internal digital method of BiST for testing RF/analog IC products.",2008,0,
2799,2800,A hardware immune system for benchmark state machine error detection,A novel error detection mechanism is demonstrated for integration into a hardware fault tolerant system. Inspiration is taken from principles of immunology to create a hardware immune system that runs in real-time hardware and continuously monitors a finite state machine architecture for errors. The work is demonstrated through immunisation of the ISCAS'89 benchmark state machine data set,2002,0,
2800,2801,Faults Diagnosis by Parameter Identification of the Squirrel Cage Induction Machine,"The authors present the faults diagnosis by parameter identification of the squirrel-cage rotor induction machine using real data. The model of electric parameter identification of the induction machine from the input-output observations of the stator is elaborated.To experimentally verify this approach, the tests are carried out on four squirrel-cage rotor induction machines especially constructed for the purpose of the diagnosis. All the model parameters of the squirrel-cage rotor induction machine are identified by least-squares method. Experimental results show good agreement and confirm the possibility the detection and localization of the faults.",2007,0,
2801,2802,WYSIWIB: A declarative approach to finding API protocols and bugs in Linux code,"Eliminating OS bugs is essential to ensuring the reliability of infrastructures ranging from embedded systems to servers. Several tools based on static analysis have been proposed for finding bugs in OS code. They have, however, emphasized scalability over usability, making it difficult to focus the tools on specific kinds of bugs and to relate the results to patterns in the source code. We propose a declarative approach to bug finding in Linux OS code using a control-flow based program search engine. Our approach is WYSIWIB (What You See Is Where It Bugs), since the programmer expresses specifications for bug finding using a syntax close to that of ordinary C code. The key advantage of our approach is that search specifications can be easily tailored, to eliminate false positives or catch more bugs. We present three case studies that have allowed us to find hundreds of potential bugs.",2009,0,
2802,2803,Distributed error handling and HRI,"The implementations of a distributed, autonomous error handler (EH) and a human-robot interface (HRI) are presented. The interface is combined with the EH to allow a human operator to see that a failure has occurred on a robot and whether or not it has been served by the EH. An experiment was run to test how well the EH and the interface work together, as well as the usefulness of the EH. The results were inconclusive, although the EH and interface worked together successfully.",2004,0,
2803,2804,Recursive prediction error identification and scaling of non-linear systems with midpoint numerical integration,"A new recursive prediction error algorithm (RPEM) based on a non-linear ordinary differential equation (ODE) model of black-box state space form is presented. The selected model is discretised by a midpoint integration algorithm and compared to an Euler forward algorithm. When the algorithm is applied, scaling of the sampling time is used to improve performance further. This affects the state vector, the parameter vector and the Hessian. This impact is analysed and described in three Theorems. Numerical examples are provided to verify the theoretical results obtained.",2010,0,
2804,2805,Stress test for disturb faults in non-volatile memories,Non-volatile memories are susceptible to special type of faults known as program disturb faults. Testing for such faults requires the application of stress tests which have long application time to distinguish faulty cells from non-faulty cells. In this paper we present a new sensing scheme that can be used with stress tests to allow for efficient detection of faulty cells based on the notion of margin reads. We demonstrate the efficiency of the margin-read approach for distinguishing between faulty and fault-free cells using electrical simulations.,2003,0,
2805,2806,Structural method of fault location in a LAN segment,"The structural method of fault location in a LAN segment has been proposed. It combines a method of many-valued fault table analysis using vectors of elementary probes with a method of structural fault location by reachability matrix, where lines of the matrix are used instead of fault table lines. Such an approach allows reduction of the area of suspected faults and fault location time. Experimental results are valid and they correspond to real behavior of LAN with defined conditions.",2003,0,
2806,2807,Notice of Retraction<BR>Fault diagnosis in cracked rotor based on fractal box counting dimension,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The vibration of large rotating machinery rotor system with a free-cracked shaft and cracked Shafts were simulated by the experiments on rotor experiment platform. According to the fractal characteristics of vibration signal about cracked rotor and free-cracked rotor shaft, fractal box counting dimensions of test data were calculated. The orbit of free-cracked shaft shows as an elliptic, and the corresponding fractal box counting dimension is small. The orbit of cracked shaft becomes more complex and its corresponding fractal box counting dimension is larger than that of free-cracked shafts. Vertical crack has less impact on the vibration of rotor while horizontal cracks have obvious impact. Therefore, the fractal box counting dimension can be used to fault diagnosis of cracked rotor.",2010,0,
2807,2808,The digital and high-precision error detection of complex freeform surface,"The exploitation and modification of automobile die attach great importance to the automobile exploitation. It should be realized in the process of modifying mould-obtaining 3D model by reverse engineering-testing errors-confirming model. This thesis applies the method of ATOS Optical Scanner and CMM (Three-Coordinate Measuring Machine) to obtain the complete high-precision point cloud data because of the complicated free form surface of automobile model. It is difficult to have error evaluation because the CAD model and the point cloud data are impossible to make automatic alignment. Therefore, on the basis of the surface reconstruction of reverse engineering software Imageware and the comparative functions between surface and point cloud data, we propose a new perspective of detecting digital error, i.e. the contour adjustment of surface automatically by the way of golden section method. Under the condition of satisfying the least square method, this method, which can make anastomosis for the actual and theoretical surface, reduces and even eliminates the position error caused by the inconsistency between the actual reference and CAD model reference, laying a foundation to measure shape error by the way of direct alignment method and reducing time as well as improving the error detection precision. Being simplistic, effective and practical, this method has been successfully utilized in the female die design of interior plastic trimming for a certain type of Shanghai Volkswagen car, having great reference value in terms of the automatization, flexibility and digitalization of products on-line detection.",2008,0,
2808,2809,On the Characterization and Optimization of On-Chip Cache Reliability against Soft Errors,"Soft errors induced by energetic particle strikes in on-chip cache memories have become an increasing challenge in designing new generation reliable microprocessors. Previous efforts have exploited information redundancy via parity/ECC codings or cacheline duplication for information integrity in on-chip cache memories. Due to various performance, area/size, and energy constraints in various target systems, many existing unoptimized protection schemes may eventually prove significantly inadequate and ineffective. In this paper, we propose a new framework for conducting comprehensive studies and characterization on the reliability behavior of cache memories, in order to provide insight into cache vulnerability to soft errors as well as design guidance to architects for highly efficient reliable on-chip cache memory design. Our work is based on the development of new lifetime models for data and tag arrays residing in both the data and instruction caches. Those models facilitate the characterization of cache vulnerability of stored items at various lifetime phases. We then exemplify this design methodology by proposing reliability schemes targeting at specific vulnerable phases. Benchmarking is carried out to showcase the effectiveness of our approach.",2009,0,
2809,2810,Information system of Relay protection for fault record analysis using WEB technology,"The paper describes the information system of the Relay protection department of the transmission system operator HEP, Area of Rijeka used for the remote access to relay protection equipment and the archival of data fault records captured in the power system. The system described facilitates analysis of power system event and relay protection operation using WEB technology.",2008,0,
2810,2811,A single DC reactor type fault current limiting interrupter for three-phase power system,"The authors propose a single DC reactor type fault current limiting interrupter (FCLI) for a three-phase power system. The device uses a single high temperature superconducting (HTS) coil that operates in conjunction with a modified half control bridge composed of thyristors and diodes connected to a transformer's secondary windings. One variety is an automatic interrupter, which automatically blocks fault current through the application of DC bias current to the bridge. Another is a gate interrupter, which does the same thing by locking the thyristor's gate pulses. The authors examine the results of various simulations on a new device that both limits and interrupts fault current in a three-phase power system",2001,0,
2811,2812,Automated Generation of Similar Paths for Localizing Program Faults,"Localizing a program fault accurately in debugging is complex and time-consuming. In the process of fault diagnosing, identifying or generating the successful test paths as similar as possible to the failed test is core for the effectiveness of faults localization. A method for calculating the similarity between two test paths based on analyzing difference between program control-flow is defined, and a novel algorithm based on DD-graph for generating similar path set directly from a failed test is proposed. It is experimentally proved that the proposed algorithm can generate a similar path set for a failed path and can help to localize the program faults.",2009,0,
2812,2813,Fault tolerance protocols for parallel programs based on tasks replication,"In this paper we propose a fault-tolerant mechanism for parallel programs based on task replication. We use a sequential discrete-event simulator of a distributed system subject to failures to compare a semi-active approach and a passive approach of the protocol. In our model, each time a task of a given parallel program is allocated, a copy of it is stored in a second processor, called the buddy processor. If the original processor fails, the copies of the tasks at the buddy processor will be processed, providing fault tolerance. Some performance measures, such as program execution times and processor utilization factors, are given for the different versions of the mechanism. The performance has been studied as a function of processor degradation, and program and system sizes",2000,0,
2813,2814,The effects of quantity and location of video playback errors on the average end-users experience,"The end-user experience of a given platform or product with respect to video playback is an increasingly important aspect for engineers and product planners to understand. However, testing real world situations in an objective and repeatable fashion is complex. In an initial study, the video Gross Error Detector (GED) which is a tool that provides a quick and cost-efficient way to evaluate video playback, was mapped to end-user's perception of video smoothness. Video errors such as dropped, repeated or out of sequence frames were evaluated in varying quantities to understand the impact on the average en-user's perception of playback. A second study, discussed here, was done to understand the differences in the quantity and location of these playback errors. These end-user perception studies provide useful data so the video GED can be used to monitor errors in video for quality control, benchmark video processing and algorithms, and can be rooted into the video processing system to optimize algorithms and limit settings.",2008,0,
2814,2815,Redundancy classification for fault tolerant computer design,"The paper discusses the principles of redundancy classification for the design of fault tolerant computer systems. The basic functions of classification: definitive, characteristic and predictive are presented. It is shown that the proposed classification of redundancy possesses substantial predictive power. The proposed classification is suitable for the analysis of roles of hardware and software to achieve system fault tolerance",2001,0,
2815,2816,Detect Related Bugs from Source Code Using Bug Information,"Open source projects often maintain open bug repositories during development and maintenance, and the reporters often point out straightly or implicitly the reasons why bugs occur when they submit them. The comments about a bug are very valuable for developers to locate and fix the bug. Meanwhile, it is very common in large software for programmers to override or overload some methods according to the same logic. If one method causes a bug, it is obvious that other overridden or overloaded methods maybe cause related or similar bugs. In this paper, we propose and implement a tool Rebug-Detector, which detects related bugs using bug information and code features. Firstly, it extracts bug features from bug information in bug repositories; secondly, it locates bug methods from source code, and then extracts code features of bug methods; thirdly, it calculates similarities between each overridden or overloaded method and bug methods; lastly, it determines which method maybe causes potential related or similar bugs. We evaluate Rebug-Detector on an open source project: Apache Lucene-Java. Our tool totally detects 61 related bugs, including 21 real bugs and 10 suspected bugs, and it costs us about 15.5 minutes. The results show that bug features and code features extracted by our tool are useful to find real bugs in existing projects.",2010,0,
2816,2817,Computational issues in fault detection filter design,We discuss computational issues encountered in the design of residual generators for dynamic inversion based fault detection filters. The two main computational problems in determining a proper and stable residual generator are the computation of an appropriate left-inverse of the fault-system and the computation of coprime factorizations with proper and stable factors. We discuss numerically reliable approaches for both of these computations relying on matrix pencil approaches and recursive pole assignment techniques for descriptor systems. The proposed computational approach to design fault detection filters is completely general and can easily handle even unstable and/or improper systems.,2002,0,
2817,2818,Computers detecting inferior printing quality and errors,"As observed, the described approach to computer-aided error detection in printed matter proves itself to be exact and reliable. Our prototype printing error detection computer program has also undergone extreme testing in an industrial environment. The problem of excessive detection duration will have to be solved by an optimization of the registration and interpolation procedures. When considering the coarse image registration, a solution in the frequency domain seems appealing, where the translational and rotational information can be extracted from the phase and amplitude spectra, respectively. Fine image registration must be optimized even more. One of the ways we are presently researching tries to minimize the quantity of data included in the computation, at each iteration. We can do this by using images with lower resolution or taking into registration only an image's segment. Afterwards, the obtained registration parameters would be applied to the original-size images. For the time being, a second optimization attempt with partial images seems the most promising.",2007,0,
2818,2819,Phoenix: Detecting and Recovering from Permanent Processor Design Bugs with Programmable Hardware,"Although processor design verification consumes ever-increasing resources, many design defects still slip into production silicon. In a few cases, such bugs have caused expensive chip recalls. To truly improve productivity, hardware bugs should be handled like system software ones, with vendors periodically releasing patches to fix hardware in the field. Based on an analysis of serious design defects in current AMD, Intel, IBM, and Motorola processors, this paper proposes and evaluates Phoenix - novel field-programmable on-chip hardware that detects and recovers from design defects. Phoenix taps key logic signals and, based on downloaded defect signatures, combines the signals into conditions that flag defects. On defect detection, Phoenix flushes the pipeline and either retries or invokes a customized recovery handler. Phoenix induces negligible slowdown, while adding only 0.05% area and 0.48% wire overheads. Phoenix detects all the serious defects that are triggered by concurrent control signals. Moreover, it recovers from most of them, and simplifies recovery for the rest. Finally, we present an algorithm to automatically size Phoenix for new processors",2006,0,
2819,2820,A robust and fault-tolerant distributed intrusion detection system,"Since it is impossible to predict and identify all the vulnerabilities of a network, and penetration into a system by malicious intruders cannot always be prevented, intrusion detection systems (IDSs) are essential entities for ensuring the security of a networked system. To be effective in carrying out their functions, the IDSs need to be accurate, adaptive, and extensible. Given these stringent requirements and the high level of vulnerabilities of the current days' networks, the design of an IDS has become a very challenging task. Although, an extensive research has been done on intrusion detection in a distributed environment, distributed IDSs suffer from a number of drawbacks e.g., high rates of false positives, low detection efficiency etc. In this paper, the design of a distributed IDS is proposed that consists of a group of autonomous and cooperating agents. In addition to its ability to detect attacks, the system is capable of identifying and isolating compromised nodes in the network thereby introducing fault-tolerance in its operations. The experiments conducted on the system have shown that it has high detection efficiency and low false positives compared to some of the currently existing systems.",2010,0,
2820,2821,Laser Spectrum Measurement and Correction Based on Virtual Instrument Techniques,"According to the requirements of laser beams' spectrum measurement and correction, the laser spectrum measurement constructed with WDS4A and WDS4C raster and based on virtual instruments (Vis) technique are provided. The detailed methods and applied programs based on techniques of Graphical programming for instrumentations platform Lab VIEW are also introduced. After the description to the principles and the construction of the laser spectrum measure system, the policies to software and modules of the system are carefully discussed, including software structure and system driver configuration, raster control module, spectrum energy measure module, and spectrum calibration module. Meanwhile, the details to realize the laser spectrum correction methods and the raster control for the test are delivered. At last, measure data of the CO<sub>2</sub> laser spectrums based on VI techniques in practical implement of different environment are given. Practical examples indicate that, by the use of VI technique, the precision is excelled to plusmn0.015 mum as well as the automation levels have been improved.",2007,0,
2821,2822,Study on Monitoring and Fault Diagnosis for Ignition System of Engines,"The test technology for ignition system of automobile engine has been introduced, and the model of distributed monitoring and diagnosis system has been proposed in this paper. The whole structure of this monitoring and diagnosis system has been introduced. Moreover, method for pertinent knowledge acquisition, analysis and organization has been discussed. Finally, typical applications for distributor centrifugal organization model and ignition coil model by using the monitoring and fault diagnosis system are provided.",2009,0,
2822,2823,Comparative performance evaluation of software-based fault-tolerant routing algorithms in adaptively-routed tori,"Fault-tolerance and network routing have been among the most widely studied topics in the research of parallel processing and computer networking. A fault- tolerant routing algorithm should guarantee the delivery of messages in the presence of faulty components. In this paper, we present a comparative performance study of nine prominent fault-tolerant routings in 2D wormhole-switched tori. These networks carry the software-based routing scheme which has been suggested as an instance of a fault-tolerant method widely used in the literature to achieve high adaptivity and support inter-processor communications in parallel computer networks due to its ability to preserve both communication performance and fault-tolerant demands in such systems. The performance measures studied are the throughput, average message latency, power, and average usage of virtual channels per node. Results obtained through simulation suggest two classes of presented routing schemes as high performance candidates in most faulty networks.",2008,0,
2823,2824,Transient fault emulation of hardened circuits in FPGA platforms,"Very deep submicron and nanometer technologies are emphasizing soft errors as an important issue in the challenges of modem electronic systems. Hardened circuits are currently required in many applications where fault tolerance (FT) was not a requirement in the very near past. Together with the generation of tools and methods for hardening circuits, new ways of validating the FT are needed. These solutions must be cost effective and provide a help not only in measuring the robustness of the circuit but also in locating the weak areas and in proposing hardening solutions. FPGA emulation of SEU effects is gaining attention in order to speed up the fault tolerance evaluation. In this work a system is proposed for the evaluation of fault tolerance with respect to SEU effects by emulation in platform FPGAs. In this system, most of the modules of a typical fault injection environment are embedded in the FPGA. Therefore, the time required for the FT validation has been optimised with respect to existing approaches.",2004,0,
2824,2825,Zerotree pattern coding of motion picture residues for error-resilient transmission of video sequences,"This paper describes a compression scheme for difference-image residues in video coding. Structured spatial patterns are used to map residue pixel values into a quadtree structure, which is then coded in significance order with the SPIHT algorithm. Thus the wavelet coefficient values of standard zerotree coding are replaced by untransformed (but carefully positioned) residue pixel values. The new zerotree pattern coding method compresses as well as zerotree wavelet coding and much better than DCT coding (as in MPEG) over error-free channels. Over noisy channels, zerotree pattern coding provides built-in error resilience, allowing transmission of residue data without error control overhead. A simple postprocessing technique provides additional error concealment.",2000,0,
2825,2826,Redundant graph to improve fault diagnosis in a Gas Turbine,"This paper deals with fault diagnosis issues for a Gas Turbine, GT, of a Combined Cycle Power Plant, CCPP. To analyze under which conditions faults in the turbogenerator can be detected and isolated, structural properties of the model are used. The structure redundancy is studied by graph tools considering the standard available measurements. A non-linear dynamic model given by 37 algebraic and differential equations is considered to identify the required redundancy degrees for diverse fault scenarios without numerical values. As result 10 relations are obtained which detect faults in all units of the turbine except one: the thermodynamic gas path. Moreover, using the redundant graph concept it is suggested to add a sensor to increase the redundance and consequently to have detectability of the mechanical faults in the gas path. This is the main contribution of the work. The implementation of redundant relations with specific simulated data of a GT validates this statement.",2010,0,
2826,2827,Comparison between backpropagation and RPROP algorithms applied to fault classification in transmission lines,"The computed results from implemented artificial intelligence algorithms, used to identify and classify faults in transmission lines, are discussed in this paper. The proposed methodology uses sampled data of voltage and current waveforms obtained from analog channels of digital fault recorders (DFRs) installed in the field to monitor transmission lines. The performances of resilient propagation (RPROP) and backpropagation algorithms, implemented in batch mode, are addressed for single, double and three-phase fault types.",2004,0,
2827,2828,Entropy-driven parity-tree selection for low-overhead concurrent error detection in finite state machines,"This paper presents discuss the problem of parity-tree selection for performing concurrent error detection (CED) with low overhead in finite state machines (FSMs). We first develop a nonintrusive CED method based on compaction of the state/output bits of an FSM via parity trees and comparison to the correct responses, which are generated through additional on-chip parity prediction hardware. Similar to off-line test-response-compaction practices, this method minimizes the number of parity trees required for performing lossless compaction. However, while a few parity trees are typically sufficient, the area and the power consumption of the corresponding parity predictor is not always in proportion with the number of implemented functions. Therefore, parity-tree-selection methods that minimize the overhead of the parity predictor, rather than the number of parity trees, are required. Towards this end, we then extend our method into a systematic search that exploits the correlation between the area and the power consumption of a function and its entropy, in order to select parity trees that minimize the incurred overhead. Experimental results on benchmark circuits demonstrate that this solution achieves significant reduction in area and power consumption over the basic method that simply minimizes the number of parity trees.",2006,0,
2828,2829,A Compression Error and Optimize Compression Algorithm for Vector Data,"Vector data compression plays an important role in the research areas such as terrain environment simulation, cartography generalization, GIS and digital entertainment, etc.... It can increase the storage capacity of mobile devices and improve the transmission efficiency of vector data on network. In this study, a new compression error is proposed by analyzing the existing compression error of vector data. For single-entity and multi-entity vector data, a compression method based on dynamic programming is given. Especially to multi-entity vector data compression, a method is used which combines compression ratio and compression error to weighted average distribute the compression nodes. Experimental results show that this compression method can better reflect the behavior of a vector data and has higher compression efficiency.",2009,0,
2829,2830,Conflict driven scan chain configuration for high transition fault coverage and low test power,"Two conflict-driven schemes and a new scan architecture based on them are presented to improve fault coverage of transition fault. They make full use of the advantages of broadside, skewed-load and enhanced scan testing, and eliminate the disadvantages of them, such as low coverage, fast global scan enable signal and hardware overhead. Test power is also a challenge for delay testing, so our method tries to reduce the test power at the same time. By the analysis of the functional dependency between test vectors in broadside testing and the shift dependency between vectors in the skewed-load testing, some scan cells are selected to operate in the enhanced scan and skewed-load scan mode, while others operate in traditional broadside mode. In the architecture, scan cells with common successors are divided into one chain. With the efficient conflict driven selection methods and partition of scan cells, fault coverage can be improved greatly and test power can be reduced, without sacrificing the test time and test data. Experimental results show that the fault coverage of the proposed method can reach the level of enhanced scan design.",2009,0,
2830,2831,A General Framework for Symbol Error Probability Analysis of Wireless Systems and Its Application in Amplify-and-Forward Multihop Relaying,"New exact single-integral expressions for the evaluation of the average error probability of a wireless communication system are derived for a variety of modulation schemes in terms of the moment-generating function (MGF) of the reciprocal of the instantaneous received signal-to-noise ratio (SNR). The expressions obtained form a framework for performance evaluation of wireless communication systems for which the well-known MGF-based performance analysis method cannot be used, that is, systems for which the MGF of the instantaneous received SNR is not known or cannot be derived in closed-form. Using the framework obtained, the error probability performance in general fading of an amplify-and-forward (AF) multihop relaying system with both variable-gain and fixed-gain relays is then evaluated. In particular, a new expression for the MGF of the reciprocal of the instantaneous received SNR of an AF multihop system with fixed-gain relays is derived. Numerical examples show precise agreement between simulation results and theoretical results.",2010,0,
2831,2832,Autonomous fault recovery technology for continuous service in Distributed VoD system,"In the video on demand (VoD) service, users request heterogeneous video quality based on their preference and usage environment which are dynamically changing. On the other hand, service providers request to provide variety service with minimum total storage volume in the system because the volume of the video data is too huge. In addition, considering the characteristics of the application, services must be distributed without stopping playback or deteriorating video quality. However, conventional VoD system constructed on redundant content servers and centralized management cannot satisfy these requirements. Autonomous VoD system (AVoDS) is proposed to meet these requirements. This system is based on the faded information field (FIF) architecture in which each node collaborate with the other nodes for service provision and utilization supported by the mobile agent. In this system, the layered streaming video data are employed and each data are distributed on different service layers. Therefore the system can provide service with adaptability and minimum total storage volume. In this paper, implementation of the prototype of AVoDS is introduced and autonomous fault recovery technology is proposed for the continuous service provision. The fault of the node is autonomously detected by the connected nodes and the fault recovery processes for each user are widely distributed to other nodes in the system. The effectiveness of the proposed technology is proved through simulation",2007,0,
2832,2833,Periodic errors elimination in CVCF PWM DC/AC converter systems: repetitive control approach,A plug-in digital repetitive learning (RC) controller is proposed to eliminate periodic tracking errors in constant-voltage constant-frequency (CVCF) pulse-width modulated (PWM) DC/AC converter systems. The design of the RC controller is systematically developed and the stability analysis of the overall system is discussed. The periodic errors are forced toward zero asymptotically and the total harmonics distortion (THD) of the output voltage is substantially reduced under parameter uncertainties and load disturbances. Simulation and experimental results are provided to illustrate the validity of the proposed scheme,2000,0,
2833,2834,The Optimal Morlet Wavelet and Its Application on Mechanical Fault Detection,"De-noising and extraction of the weak signal are very important to mechanical fault detection in which case signals often have very low signal-to-noise ratio (SNR). In this paper, a denoising method based on the optimal Morlet wavelet is applied to feature extraction for mechanical vibration signals. The wavelet shape parameters are optimized based on kurtosis maximization criteria. The effectiveness of the proposed technique on the extraction of impulsive features of mechanical fault signals has been proved by practical experiments.",2009,0,
2834,2835,A versatile high speed bit error rate testing scheme,"The quality of a digital communication interface can be characterized by its bit error rate (BER) performance. To ensure the quality of the manufactured interface, it is critical to quickly and precisely test its BER behavior. Traditionally, BER is evaluated using software simulations, which are very time-consuming. Though there are some standalone BER test products, they are expensive and none of them includes channel emulators, which are essential to testing BER under the presence of noise. To overcome these problems, we present a versatile scheme for BER testing in FPGAs. This scheme consists of two intellectual property (IP) cores: the BER tester (BERT) core and the additive white Gaussian noise (AWGN) generator core. We demonstrate through case studies that the proposed solution exhibits advantages in speed and cost over existing solutions.",2004,0,
2835,2836,A Novel Test Application Scheme for High Transition Fault Coverage and Low Test Cost,"This paper presents a new method for improving transition fault coverage in hybrid scan testing. It is based on a novel test application scheme, in order to break the functional dependence of broadside testing. The new technique analyzes the automatic test pattern generation conflicts in broadside test generation and skewed-load test generation, and tries to control the flip-flops with the most influence on fault coverage. The conflict-driven selection method selects some flip-flops that work in the enhanced scan mode or skewed-load scan mode. And the conflict-driven reordering method distributes the selected flip-flops into different chains. In the multiple scan chain architecture, to avoid too many scan-in pins, some chains are driven by the same scan-in pin to construct a tree-based architecture. Based on the architecture, the new test application scheme allows some flip-flops to work in enhanced scan or skewed-load mode, while most of others to work in the traditional broadside scan mode. With the efficient conflict-driven selection and reordering schemes, fault coverage is improved greatly, which can also reduce test application time and test data volume. Experimental results show that fault coverage based on the proposed method is comparable that of enhanced scan.",2010,0,
2836,2837,Study on fault-diagnosis models of different neural networks and ensemble,"Different diagnosis models, including multiplayer perceptron (MLP), radial basis function (RBF) and two types of support vector machines (SVMs), were designed, analyzed and compared based on the fault diagnosis of an analogue circuit instance. The experimental results show SVM model is of higher classification rate than MLP and RBF models, while MLP model has better ability to deal with uncertain signals. Considering different models correspond to different strategies, we combine four models of MLP, RBF and two SVMs to combine a diagnosis ensemble, which can achieve more accurate results than any individual model in the ensemble. The ensemble technique can provide a theoretical basis for further study on the fault diagnosis of analogue circuits.",2010,0,
2837,2838,Corrective maintenance maturity model (CM<sup>3</sup>): maintainer's education and training,"What is the point of improving maintenance processes if the most important asset, people, is not properly utilised? Knowledge of the product(s) maintained, maintenance processes and communications skills is very important for achieving quality software and for improving maintenance and development processes. We present CM<sup>3</sup>: Maintainer's Education and Training-a maturity model for educating and training maintenance engineers. This model is the result of a comparative study of two industrial processes utilised at ABB, and of process models such as IEEE 1219, ISO/IEC 12207, CMM, People CMM, and TickIT",2001,0,
2838,2839,Hierarchical Aggregation and Intelligent Monitoring and Control in Fault-Tolerant Wireless Sensor Networks,"The primary idea behind deploying sensor networks is to utilize the distributed sensing capability provided by tiny, low powered, and low cost devices. Multiple sensing devices can be used cooperatively and collaboratively to capture events or monitor space more effectively than a single sensing device. The realm of applications envisioned for sensor networks is diverse including military, aerospace, industrial, commercial, environmental, and health monitoring. Typical examples include: traffic monitoring of vehicles, cross-border infiltration detection and assessment, military reconnaissance and surveillance, target tracking, habitat monitoring, and structure monitoring, to name a few. Most of the applications envisioned with sensor networks demand highly reliable, accurate, and fault-tolerant data acquisition process. In this paper, we focus on innovative approaches to deal with multivariable, multispace problem domains (data integrity, energy-efficiency, and fault-tolerant framework) in wireless sensor networks and present novel ideas that have practical implementation in developing power-aware software components for designing robust networks of sensing devices.",2007,0,
2839,2840,Fault diagnosis of electronic systems using intelligent techniques: a review,"In an increasingly competitive marketplace system complexity continues to grow, but time-to-market and lifecycle are reducing. The purpose of fault diagnosis is the isolation of faults on defective systems, a task requiring a high skill set. This has driven the need for automated diagnostic tools. Over the last two decades, automated diagnosis has been an active research area, but the industrial acceptance of these techniques, particularly in cost-sensitive areas, has not been high. This paper reviews this research, primarily covering rule-based, model-based, and case-based approaches and applications. Future research directions are finally examined, with a concentration on issues, which may lead to a greater acceptance of automated diagnosis",2001,0,
2840,2841,Normalization of illumination conditions for ground based hyperspectral measurements using dual field of view spectroradiometers and BRDF corrections,"BRDF effects present in dual field-of-view spectroscopy datasets were investigated. A data-driven normalization procedure was developed by decomposing the target BRDF into a target specific Lambertian component and a bi-directional component characterizing a group of similar targets,. The normalization method was used to convert reflectance factors obtained under cloud obscured conditions into clear sky conditions. An evaluation on four targets measured under different illumination conditions suggests that the normalization can reduce relative reflectance errors between 400 and 1800 nm from 15% to less than 5% even under full cloud obscuration. At higher wavelengths a decreased signal-to-noise ratio increases the error level.",2009,0,
2841,2842,A high-performance fault-tolerant software framework for memory on commodity GPUs,"As GPUs are increasingly used to accelerate HPC applications by allowing more flexibility and programmability, their fault tolerance is becoming much more important than before when they were used only for graphics. The current generation of GPUs, however, does not have standard error detection and correction capabilities, such as SEC-DED ECC for DRAM, which is almost always exercised in HPC servers. We present a high-performance software framework to enhance commodity off-the-shelf GPUs with DRAM fault tolerance. It combines data coding for detecting bit-flip errors and checkpointing for recovering computations when such errors are detected. We analyze performance of data coding in GPUs and present optimizations geared toward memory-intensive GPU applications. We present performance studies of the prototype implementation of the framework and show that the proposed framework can be realized with negligible overheads in compute intensive applications such as N-body problem and matrix multiplication, and as low as 35% in a highly-efficient memory intensive 3-D FFT kernel.",2010,0,
2842,2843,A Framework for Analyzing Correlative Software and Hardware Faults,"Both software and hardware of computer systems are subject to faults, however, traditional approaches, ignoring the relationship between software faults and hardware faults, are unavailable for analyzing complex software and hardware faults. This paper proposes a systematic framework to analyze correlative software and hardware fault. It includes two associated processes, module level analysis and code level location, and can be used to achieve fault modules and locate fault reasons. The framework has been integrated into the maintenance system for software-intensive system, and provides an effective and feasible method to deal with complex faults between software and hardware.",2008,0,
2843,2844,Topology error identification for the NEPTUNE power system using an artificial neural network,"The goal of the North Eastern Pacific Time-Series Undersea Networked Experiment (NEPTUNE) is to construct a cabled observatory on the floor of the Pacific Ocean, encompassing the Juan de Fuca Tectonic Plate. The power system associated with the proposed observatory is unlike conventional terrestrial power systems in many ways due to the unique operating conditions of cabled observatories. The unique operating conditions of the system require hardware and software applications that are not found in terrestrial power systems. This paper builds upon earlier work and describes a method for topology error identification in the NEPTUNE system that utilizes an artificial neural network (ANN) to determine single contingency topology errors.",2004,0,
2844,2845,Hardware-software covalidation: fault models and test generation,The increasing use of hardware-software systems in cost-critical and life-critical applications has led to heightened significance of design correctness of these systems. This paper presents a summary of research in hardware-software covalidation winch involves the verification of design correctness using simulation-based techniques. This paper focuses on the test generation process for hardware-software systems as well as the fault models and fault coverage analysis techniques which support test generation,2001,0,
2845,2846,"Current Sensor Fault Detection, Identification, and Reconfiguration for Doubly Fed Induction Generators","This work presents current sensor fault detection, identification and reconfiguration for a voltage oriented controlled doubly fed induction generator. The focus of this analysis is on the identification of the faulty sensor, and the actual reconfiguration. It is proposed to temporary switch from closed loop into open loop control to decouple the drive from faulty sensor readings. During a short period of open loop operation, the fault is identified. Then replacement signals from observers are used to reconfigure the drive and re-enter closed loop control. Measurement results are included to prove that the proposed concept leads to good results.",2007,0,
2846,2847,Susceptibility of commodity systems and software to memory soft errors,"It is widely understood that most system downtime is accounted for by programming errors and administration time. However, a growing body of work has indicated an increasing cause of downtime may stem from transient errors in computer system hardware due to external factors, such as cosmic rays. This work indicates that moving to denser semiconductor technologies at lower voltages has the potential to increase these transient errors. In this paper, we investigate the susceptibility of commodity operating systems and applications on commodity PC processors to these soft-errors and we introduce ideas regarding the improved recovery from these transient errors in software. Our results indicate that, for the Linux kernel and a Java virtual machine running sample workloads, many errors are not activated, mostly due to overwriting. In addition, given current and upcoming microprocessor support, our results indicate that those errors activated, which would normally lead to system reboot, need not be fatal to the system if software knowledge is used for simple software recovery. Together, they indicate the benefits of simple memory soft error recovery handling in commodity processors and software.",2004,0,
2847,2848,A Fault Tree Analysis Based Software System Reliability Allocation Using Genetic Algorithm Optimization,"Software fault tree analysis is first adopted to establish the lower bound data (LBD) of individual modules in a software system. Due to the fact that both the internal relations within the system and the practical requirements enforced on every functional modules is formulated while analyzing the software fault, the assigned LBD using FTA is more reasonable compared with those using traditional AHP. Then the LBD are utilized in establishing the nonlinear programming model for the software utility oriented module reliability allocation optimization. In the end, the general frame of the simple genetic algorithm is implemented and linear programming prototype corresponding to the problem is simulated as a special case. Since the promoted algorithm has incorporated the merit of determining the module reliability LBD using software fault tree analysis with the global searching ability of genetic algorithm, the assigned reliability data to respective modules can ensure reliably running as well as enhancing utility to full extent of the software system.",2009,0,
2848,2849,Incorporation of security and fault tolerance mechanisms into real-time component-based distributed computing systems,"The volume and size of real-time (RT) distributed computing (DC) applications are now growing faster than in the last century. The mixture of application tasks running on such systems is growing as well as the shared use of computing and communication resources for multiple applications including RT and non-RT applications. The increase in use of shared resources accompanies with it the need for effective security enforcement. More specifically, the needs are to prevent unauthorized users: (1) from accessing protected information; and (2) from disturbing bona-fide users in getting services from server components. Such disturbances are also called denial-of-service attacks",2001,0,
2849,2850,Magnet flux ing control of interior PM machine drives for improved response to short-circuit faults,"This paper proposes a control method to the magnet flux in an interior permanent magnet (IPM) motor following short-circuit type faults in either the inverter drive or motor stator windings. Phase control is employed to implement the flux ing control method so that it is possible to take advantage of a zero sequence current in order to minimize the current in the shorted phase. It is shown that phase control results in a smaller induced current than employing a synchronous frame dq0 current regulator. The induced torque is also less than employing a purposely commanded symmetrical short-circuit in response to a short-circuit type fault. In the paper, the complete magnet flux ing control algorithm is derived with reference to the proposed phase current control method. The impact of controlling the zero sequence on the resulting phase currents is presented. Both simulation and experimental results are presented verifying operation of the proposed methods.",2004,0,
2850,2851,Induction Motor Electrical Fault Diagnosis Using Voltage Spectrum of an Auxiliary Winding - Part II,"In this paper, a new method for induction motor fault diagnosis is presented. It is based on the so-called voltage spectrum of an auxiliary small winding inserted between two of the stator phases. An expression of the inserted inductance voltage is presented. After that, discrete Fourier transform analyzer is required for converting the voltage signal from the time domain to the frequency domain. Simulations results curried out for defected and non defected motor show the effectiveness of the proposed method.",2008,0,
2851,2852,One Double Levels Error Resilient Scheme of Joint Source and Channel Coding,"In this paper, we will consider the joint source and channel coding problem and propose a double-level error resilient scheme of joint source and channel coding. Compared to other joint source and channel coding schemes, we insert one coordination structure named error resilient entropy coding between source coding and channel coding to achieve additional error resilient capability. Using arithmetic coding based on forbidden symbol and standard LDPC code, we prove that, in coding redundancy and computational complexity, our scheme outperforms either separate source and channel coding scheme or joint source and channel coding scheme with synchronization words.",2009,0,
2852,2853,Automated post-fault diagnosis of power system disturbances,"In order to automate the analysis of SCADA and digital fault recorder (DFR) data for a transmission network operator in the UK, the authors have developed an industrial strength multi-agent system entitled protection engineering diagnostic agents (PEDA). The PEDA system integrates a number of legacy intelligent systems for analyzing power system data as autonomous intelligent agents. The integration achieved through multi-agent systems technology enhances the diagnostic support offered to engineers by focusing the analysis on the most pertinent DFR data based on the results of the analysis of SCADA. Since November 2004 the PEDA system has been operating online at a UK utility. In this paper the authors focus on the underlying intelligent system techniques, i.e. rule-based expert systems, model-based reasoning and state-of-the-art multi-agent system technology, that PEDA employs and the lessons learnt through its deployment and online use",2006,0,
2853,2854,Performance Analysis of Error Control Codes for Wireless Sensor Networks,"In wireless sensor networks, the data transmitted from the sensor nodes are vulnerable to corruption by errors induced by noisy channels and other factors. Hence it is necessary to provide a proper error control scheme to reduce the bit error rate (BER). Due to the stringent energy constraint in sensor networks, it is vital to use energy efficient error control scheme. In this paper, we focus our study on the performance analysis of various error control codes in terms of their BER performance and power consumption on different platforms. In detail, error control codes with different constraints are implemented and simulated using VHDL. Implementation on FPGA and ASIC design is carried out and the energy consumption is measured. The error control performance of these codes is evaluated in terms of bit error rate (BER) by transmitting randomly generated data through a Gaussian channel. Based on the study and comparison of the three different error control codes, we identify that binary-BCH codes with ASIC implementation are best suitable for wireless sensor networks",2007,0,
2854,2855,Proximity correction of IC layouts using scanner fingerprints,The availability of a precise physical description of the imaging system that was used to expose an OPC calibration tests pattern is now possible. This data is available from scanner manufacturers of the tool as built and also by scanner self-metrology in the Fab at any time. This information reduces significant uncertainty when regressing a model used for OPC and allows the creation of more accurate models with better predictability. This paper explores the considerations necessary for best leveraging this data into the OPC model creation flow.,2007,0,
2855,2856,ADAPTATION - Algorithms to Adaptive Fault Monitoring and their implementation on CORBA,This paper presents ADAPTATION - Algorithms to Adaptive Fault Monitoring for asynchronous distributed systems and their implementation on CORBA. Our algorithms vary the timeouts based on a recent history of last elapsed times of the monitoring messages. The aim of the proposed algorithms is to provide a better response time to crashes and a minimum discrepancy between a suspection due to the network overload and due to the real process crash. The proposed approach extends the Fault Tolerant CORBA OMG specification with the push model and the definition of pull and push ADAPTATION fault monitors. Some ADAPTATION experiments on ACE+TAO were made to observe their behavior on changing network workloads,2001,0,
2856,2857,Fast soft error rate computing technique based on state probability propagating,"Fast soft error sensitivity characterization technique is essential for the soft error tolerance optimization of modern VLSI circuits. In this paper, an efficient soft error evaluation technique based on syntax analysis and state probability propagating technique is developed, which can automatically analyze the soft error rate of combinational logic circuits and the combinational part of sequential circuits in Verilog synthesized netlist within a few seconds. We implemented the idea in a software tool called HSECT-ANLY, which use Verilog syntax analysis to automate the soft error rate evaluation procedure and state propagating technique to speed up the analyzation process. By using HSECT-ANLY, experiments are carried out on some ISCAS'85 and ISCAS'89 benchmark circuits implemented with TSMC 0.18 mum technology and results are obtained. The result comparison with the traditional test vector propagating technique shows that the introduced method is much faster (2-3 magnitudes speeding up) with some accuracy losses, and be very suitable for the reliability optimization as the sub-algorithm of the optimization algorithms such as genetic algorithms to evaluate the fitness (soft error rate) rapidly.",2009,0,
2857,2858,Integrating fault-tolerant feature into TOPAS parallel programming environment for distributed systems,"In this paper, TOPAS-a new parallel programming environment for distributed systems-is presented. TOPAS automatically analyzes data dependence among tasks and synchronizes data, which reduces the time needed for parallel program developments. TOPAS also provides supports for scheduling, dynamic load balancing and fault tolerance. Experiments show simplicity and efficiency of parallel programming in TOPAS environment with fault-tolerant integration, which provides graceful performance degradation and quick reconfiguration time for application recovery.",2002,0,
2858,2859,QoS-Aware Fault Tolerance in Grid Computing through Topology-Aware Replica Placement,Quality of service (QoS)-aware fault tolerance is defined as the capability of overcoming both hardware and software failures while maintaining communications QoS guarantees. An integrated fault tolerant scheme combining service replication and path restoration has the potential of providing QoS-aware fault tolerance while maximizing the percentage of recovered connections and minimizing the required service replicas. Previous studies focused on evaluating the optimal performance achievable by the integrated fault tolerant schemes through a mixed integer linear programming (MILP) model. This study concentrate on developing heuristics to place service replicas with topology awareness. The aim of this study is to evaluate whether topology-aware heuristics can approximate MILP optimal solutions,2006,0,
2859,2860,Automatic generation of instruction sequences targeting hard-to-detect structural faults in a processor,"Testing a processor in native mode by executing instructions from cache has been shown to be very effective in discovering defective chips. In previous work, we showed an efficient technique for generating instruction sequences targeting specific faults. We generated tests using traditional techniques at the module level and then mapped them to instruction sequences using novel methods. However, in that technique, the propagation of module test responses to primary outputs was not automated. In this paper, we present the algorithm and experimental results for a technique which automates the functional propagation of module level test responses. This technique models the propagation requirement as a Boolean difference problem and uses a bounded model checking engine to perform the instruction mapping. We use a register transfer level (RT-Level) abstraction which makes it possible to express Boolean difference as a succinct linear time logic (LTL) formula that can be passed to a bounded model checking engine. This technique fully automates the process of mapping module level test sequences to instruction sequences",2006,0,
2860,2861,Broken bar fault detection in induction motors based on modified winding Function,"In this paper, a new turn function for skewed rotor bars based on winding Function approach is presented. This approach has been used for simulating the machine behavior under healthy and broken rotor conditions. Proposed method is applied for rotor bars fault detection based on an advanced Park's vectors approach.",2010,0,
2861,2862,Panel statement: why progress in (composite) fault tolerant real-time systems has been slow (-er than expected... & what can we do about it?),"The pervasiveness of computers in our current IT driven society (transportation, e-commerce, e-transactions, communication, process control), also implies our growing dependency on their ""correct"" functionality. In many a case, the real value of the systems and also our usage of these systems comes, in part, based on the dependency (real or perceived) we are consequently willing to put into the provisioning of the services i.e., the implicit or explicit assurance of trust we put for sustained delivery of desired services. Some systems are considered as safety-critical (flight/reactor control etc), though others are accorded varied degrees of criticality. Nevertheless, our expectancy extends to obtaining the proper services when the system is fault-free and especially when it encounters perturbations (design or operational), e.g., electromagnetic interference or a lightning strike for an aircraft. Consequently, it is important to qualitatively and quantitatively associate some measures of trust in the system's ability to ""actually"" deliver us the desired services in the presence of faults. This is often termed as ""dependability"" measures for a system with a plethora of fault-tolerance (FT) strategies to help achieve desired levels of dependability. As before, dependability entails the sustained delivery of services, be they service-critical or cost-critical, regardless of the perturbations encountered during their operation.",2004,0,
2862,2863,Improve the robot calibration accuracy using a dynamic online fuzzy error mapping system,"Traditional robot calibration implements model and modeless methods. The compensation of position error in modeless method is to move the end-effector of robot to the target position in the workspace, and to find the position error of that target position by using a bilinear interpolation method based on the neighboring 4-point's errors around the target position. A camera or other measurement devices can be utilized to find or measure this position error, and compensate this error with the interpolation result. This paper provides a novel fuzzy interpolation method to improve the compensation accuracy obtained by using a bilinear interpolation method. A dynamic online fuzzy inference system is implemented to meet the needs of fast real-time control system and calibration environment. The simulated results show that the compensation accuracy can be greatly improved by using this fuzzy interpolation method compared with the bilinear interpolation method.",2004,0,
2863,2864,Multisensor track-to-track association for tracks with dependent errors,"The problem of track-to-track association has been considered until recently in the literature only for pairwise associations. In view of the extensive recent interest in multisensor data fusion, the need to associate simultaneously multiple tracks has arisen. This is due primarily to bandwidth constraints in real systems, where it is not feasible to transmit detailed measurement information to a fusion center but, in many cases, only local tracks. As it has been known in the literature, tracks of the same target obtained from independent sensors are still dependent due to the common process noise. This paper derives the likelihood function for the track-to-track association problem from multiple sources, which forms the basis for the cost function used in a multidimensional assignment algorithm that can solve such a large scale problem where many sensors track many targets. While a recent work derived the likelihood function under the assumption that the track errors are independent, the present paper incorporates the (unavoidable) dependence of these errors.",2004,0,
2864,2865,A Software Fault Tree Metric,"Analysis of software fault trees exposes hardware and software failure events that lead to unsafe system states, and provides insight on improving safety throughout each phase of the software lifecycle. Software product lines have emerged as an effort to achieve reuse, enhance quality, and reduce development costs of safety-critical systems. Safety-critical product lines amplify the need for improved analysis techniques and metrics for evaluating safety-critical systems since design flaws can be carried forward though product line generations. This paper presents a key node safety metric for measuring the inherent safety modeled by software fault trees. Definitions related to fault tree structure that impact the metric's composition are provided, and the mathematical basis for the metric is examined. The metric is applied to an embedded control system as well as to a collection of software fault tree product lines that include mutations expected to improve or degrade the safety of the system. The effectiveness of the metric is analyzed, and observations made during the experiments are discussed",2006,0,
2865,2866,Tensor reduction error analysis  Applications to video compression and classification,"Tensor based dimensionality reduction has recently been extensively studied for computer vision applications. To our knowledge, however, there exist no rigorous error analysis on these methods. Here we provide the first error analysis of these methods and provide error bound results similar to Eckart-Young Theorem which plays critical role in the development and application of singular value decomposition (SVD). Beside performance guarantee, these error bounds are useful for subspace size determination according to the required video/image reconstruction error. Furthermore, video surveillance/retrieval, 3D/4D medical image analysis, and other computer vision applications require particular reduction in spatio-temporal space, but not along data index dimension. This motivates a D-1 tensor reduction. Standard method such as high order SVD (HOSVD) compress data in all index dimensions and thus can not perform the classification and pattern recognition tasks. We provide algorithm and error bound analysis of the D-1 factorization for spatio-temporal data dimensionality. Experiments on video sequences demonstrate our approach outperforms the previous dimensionality deduction methods for spatio temporal data.",2008,0,
2866,2867,Fast algorithm for computing the roots of error locator polynomials up to degree 11 in Reed-Solomon decoders,"The central problem in the implementation of a Reed-Solomon code is finding the roots of the error locator polynomial. In 1967, Berlekamp et al. found an algorithm for finding the roots of an affine polynomial in GF(2<sup>m</sup>) that can be used to solve this problem. In this paper, it is shown that this Berlekamp-Rumsey-Solomon (1967) algorithm, together with the Chien (1964) search method, makes possible a fast decoding algorithm in the standard-basis representation that is naturally suitable in a software implementation. Finally, simulation results for this fast algorithm are given",2001,0,
2867,2868,Variance error quantifications that are exact for finite model order,"This paper is concerned with the frequency domain quantification of noise induced errors in dynamic system estimates. Preceding seminal work on this problem provides general expressions that are approximations whose accuracy increases with observed data length and model order. In the interests of improved accuracy, this paper provides new expressions whose accuracy depends only on data length. They are therefore 'exact' for arbitrarily small true model order and apply to the general cases of output-error and box-Jenkins model structures.",2003,0,
2868,2869,Improving Classification Efficiency of Orthogonal Defect Classification via a Bayesian Network Approach,"Orthogonal defect classification (ODC) is a kind of defect analysis method invented by IBM. ODC classifies software defects by eight orthogonal attributes. By analyzing these attributes' distribution and increasing trend the software process information could be obtained. It has been used widely in many companies and organizations. In this paper, we focus on the ODC records collected in a company, and research to use these data to provide guidance in actual defect management to improve the efficiency of the classification. We study the relationships of these attributes and give a Bayesian network model, then with the help of the ODC records we got, a Bayesian network for ODC is presented. It shows great help in actual work for both the developers and the testers.",2009,0,
2869,2870,Frame error model in rural Wi-Fi networks,"Commonly used frame loss models for simulations over Wi-Fi channels assume a simple double regression model with threshold. This model is widely accepted, but few measurements are available in the literature that try to validate it. As far as we know, none of them is based on field trials at the frame level. We present a series of measurements for relating transmission distance and packet loss on a Wi-Fi network in rural areas and propose a model that relates distance with packet loss probability. We show that a simple double regression propagation model like the one used in the ns-2 simulator can miss important transmission impairments that are apparent even at short transmitter-receiver distances. Measurements also show that packet loss at the frame level is a Bernoullian process for time spans of few seconds. We relate the packet loss probability to the received signal level using standard models for additive white Gaussian noise channels. The resulting model is much more similar to the measured channels than the simple models where all packets are received when the distance is below a given threshold and all are lost when the threshold is exceeded.",2007,0,
2870,2871,Model-Based Development of Fault-Tolerant Embedded Software,"Model based development has become the state of the art in software engineering. Unfortunately there are only few model-based tools available for the design of fault- tolerant embedded software: while there exist many different code generators for application code, the generation of system aspects like process management, communication in a distributed system and fault-tolerance mechanisms is very complex due to the heterogeneity of the embedded systems. We think that the design of an all-embracing code generator, that supports a priori all platforms (the combination of hardware, operating system and programming language) is impossible. Rather it is necessary to concentrate on a code generator architecture that allows an easy extension of the code generation ability. In this paper we present one possible solution: generating the code on the basis of templates, that solve different recurring aspects of safety-critical embedded software. By the use of a technique similar to preprocessor macros, these templates can be implemented in an application independent fashion. The code generator can then adapt these templates to the application by extracting the necessary information out of the model provided by the application developer. A first realization of this approach is also mentioned in this paper.",2006,0,
2871,2872,Layering Model and Fault Diagnosis Algorithm for Internet Services,"Challenges of Internet service fault management are analyzed in this paper, and a layering model is recommended. Bipartite graph is chosen to be the fault propagation model (FPM) for each layer. A window-based fault diagnosis algorithm MAlg (multi-window algorithm) is proposed for the bipartite FPM. MAlg takes into account the correlation of adjacent time windows. As a result it can reduce the impact of improper time window setting. Simulation results prove the validity and efficiency of MAlg",2006,0,
2872,2873,Research on Engine Fault Diagnosis and Realization of Intelligent Analysis System,"The paper analyses the main types of engine failures, studies the some methods of fault diagnosis for engines and presents the ideas of time eigenvalues, frequency eigenvalues, wavelet eigenvalues and RBF(BP network based on radial basis functions) eigenvalues. According to the investigative results, an intelligent analysis system based on TMS320VC5402 is designed. The particular hardware and software design based on the DSP device is present in the paper. The analysis system is high in speed, low in power consume and small in size to be portable. It is fit for on-time supervising and analyzing",2006,0,
2873,2874,Incorporating varying test costs and fault severities into test case prioritization,"Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.",2001,0,
2874,2875,Implementing research for IEC 61850-based fault analysis system,"With the maturing of the IEC 61850, utilities are beginning to implement substation automation systems (SAS) that are based on this new international standard. This paper describes such an implementing research for power fault analysis system on east china electric power group in China. In particular, it presents the idea of applying object-oriented methodology to architecture design and providing an open interface of IEC 61850 in the substation layer. Based on the idea and technique, some benefits are brought.",2004,0,
2875,2876,Enhancement of Bug Tracking Tools; the Debugger,"In this paper test documentation and effort estimations have been investigated as well as Bug Tracking Tools. Four different existing Bug Tracking Tools have been compared with each other along with their features and drawbacks. Then a new one, the Debugger has been proposed. Testing is one of the most important tasks in developing any software. According to the experts, time and money play the crucial role in testing process, if someone wants to reduce the cost of developing a new project, he should take care of testing. In order to do this, documenting the test results could help the developer to classify them in different categories such as different process models and different types of errors in each developing life cycle phase, then by having these classified results, it would be easy to estimate the future test cases in order to reduce the cost of testing phase and eventually developing cost in similar upcoming projects.",2010,0,
2876,2877,Directional aspects of balance corrections in man,This article attempts to highlight the new insights into balance control that have been gained by using multidirectional perturbations and to demonstrate how this new focus enables a better understanding of how the central nervous system (CNS) malfunctions in patients with balance disorders. Multidirectional perturbations have proven to be a valuable tool to better understand dynamic postural control in normal and balance-deficient populations. The advantages gained by using multidirectional perturbations to exert joint displacement profiles at different levels and in different directions has allowed greater insight into how passive joint characteristics and active muscle synergies are triggered and shaped by peripheral and central sensory systems to elicit directionally specific postural responses to avoid a fall.,2003,0,
2877,2878,All digital ADC with linearity correction and temperature compensation,"While digital circuits benefit from high-density digital CMOS technology, the design of analog and mixed signal blocks in the same technology is a higher challenge at each new technology node. AD converters are an example of such blocks. A possible solution is the implementation of ADCs in digital technology using logic gates as a voltage controlled oscillator. However, the limited linearity and temperature sensitivity are known issues. In this paper, a linearity correction technique that is also able to compensate for temperature effects is used. Results indicate the feasibility of the approach.",2010,0,
2878,2879,Coping with multiple Q-V solutions of the WLS state estimator induced by shunt-parameter errors,"The paper proposes a new iterative algorithm able to cope with multiple Q-V solutions of the WLS state estimator due to shunt-reactance errors. For such errors, it is shown in a 735/230-kV Hydro-Quebec subsystem that the conventional Gauss-Newton iterative algorithm converges to a strongly biased Q-V solution that is not detected as such by the residual statistical tests. By contrast, under no bad measurements, the new iterative algorithm converges to a solution foreseen by the dispatcher via the inclusion of additive state voltage weights in the gain matrix",2004,0,
2879,2880,Fractal study on fault system of Carboniferous in Junggar Basin based on GIS,"Fault system is a significant evidence of tectonic movement during crust tectonic evolution and may play an more important role in oil-gas accumulation process than other tectonic types in sedimentary basin. Carboniferous surface faults in Junggar Basin developed well and varied in size and distribution. There are about 200 faults in Carboniferous, and 187 of them are thrust faults. Chaos-fractals theories have been widely investigated and great progress has been made in the past three decades. One of the important conception-fractal dimension had become a powerful tool for describing non-linearity dynamical system characteristic. The clustered objects in nature are often fractal and fault system distribution in space is inhomogeneous, always occurs in groups, so we can describe spatial distribution of faults from the point of fractal dimension. Fractal dimension of fault system is a comprehensive factor associated with fault number, size, combination modes and dynamics mechanism, so it can evaluate the complexity of fault system quantitatively. The relationship between fault system and oil-gas accumulation is a focus and difficulty problem in petroleum geology, and fractal dimension is a new tool for describing fault distribution and predicting potential areas of hydrocarbon resources. Geographic Information System (GIS) is a kind of technological system collecting, storing, managing, computing, analyzing, displaying and describing the geospatial information supported by computer software and hardware. In the last 15-20 years, GIS have been increasingly used to address a wide variety of geoscience problems. Weights-of-evidence models use the theory of conditional probability to quantify spatial association between fractal dimension and oil-gas accumulation. The weights of evidence are combined with the prior probability of occurrence of oil-gas accumulation using Bayes'rule in a loglinear form under an assumption of conditional independence of the dimension maps t- - o derive posterior probability of occurrence of oil-gas accumulation. In this paper, we first vectorize the fault system in Carboniferous of Junggar Basin in GIS software and store it as polyline layer in Geodatabase of GIS to manage and analyze, then calculate the fractal dimension of three types which are box dimension, information dimension and cumulative length dimension using spatial functions of GIS, in the last use weights-of-evidence model to calculate the correlation coefficients in GIS environment between oil-gas accumulation and three types of fractal dimension in order to quantity the importance of fault system.",2010,0,
2880,2881,An FPGA Based Travelling-Wave Fault Location System,"This paper presents an FPGA based fault recorder capable of recording fault transient signals on MV distribution systems for both single and double ended fault location schemes. The proposed platform consists of a Xilinx Spartan-3 FPGA device; Texas Instruments 125 MSPS, 14 bit ADCs; Cypress 167 MHz 2 Mx 18 bits QDR memory; a high speed USB 2.0 link; and a GPS unit to provide accurate timestamps. The proposed system is capable of recording three current phases and three voltage phases simultaneously with the ability to set a pre gain for both current and voltage measurements independently.",2007,0,
2881,2882,A New Class of Highly Fault Tolerant Erasure Code for the Disk Array,"We present a new class of erasure codes of size ntimesn (n is a prime number) called T-code, a new family of simple, highly fault tolerant XOR-based erasure codes for storage systems (with fault tolerance up to 15). T-code is not maximum distance separable (MDS), but has many other advantages, such as high fault tolerance, simple computability, and high efficiency of coding and decoding. Because of its superior quantity over many other erasure codes for the storage system, this new coding technology is more suited in RAID or dRAID systems.",2008,0,
2882,2883,Fault-Based Test Case Generation for Component Connectors,"The complex interactions appearing in service-oriented computing make coordination a key concern in service-oriented systems. In this paper, we present a fault-based method to generate test cases for component connectors from specifications. For connectors, faults are caused by possible errors during the development process, such as wrongly used channels, missing or redundant subcircuits, or circuits with wrongly constructed topology. We give test cases and connectors a unifying formal semantics by using the notion of design, and generate test cases by solving constraints obtained from the specification and faulty connectors. A prototype symbolic test case generator serves to demonstrate the automatizing of the approach.",2009,0,
2883,2884,Prototype-based minimum error classifier for handwritten digits recognition,The paper describes an application of the prototype-based minimum error classifier (PBMEC) to the offline recognition of handwritten digits. The PBMEC uses a set of prototypes to represent each digit along with an L<sub></sub>-norm of distances as the decoding scheme. Optimization of the system is based on the minimum classification error (MCE) criterion. We introduce a new clustering criterion adapted to the PBMEC structure that minimizes an L<sub></sub>-norm-based distortion measure. The new clustering algorithm can generate a smaller number of prototypes than the standard k-means with no loss in accuracy. It is also shown that the PBMEC trained with MCE can achieve over 42% improvement from the baseline k-means process and requires only 28 Kb storage to match the performance of a 1.46 Mb sized k-NN classifier.,2004,0,
2884,2885,Application-level fault tolerance in real-time embedded systems,"Critical real-time embedded systems need to make use of fault tolerance techniques to cope with operation time errors, either in hardware or software. Fault tolerance is usually applied by means of redundancy and diversity. Redundant hardware implies the establishment of a distributed system executing a set of fault tolerance strategies by software, and may also employ some form of diversity, by using different variants or versions for the same processing. This work proposes and evaluates a fault tolerance framework for supporting the development of dependable applications. This framework is build upon basic operating system services and middleware communications and brings flexible and transparent support for application threads. A case study involving radar filtering is described and the framework advantages and drawbacks are discussed.",2008,0,
2885,2886,A fault model for fault injection analysis of dynamic UML specifications,"Verification and validation (V&V) tasks, as applied to software specifications, enable early detection of analysis and design flaws prior to implementation. Several fault injection techniques for software V&V are proposed at the code level. In this paper, we address V&V analysis methods based on fault injection at the software specification level. We present a fault model and a fault injection process for UML dynamic specifications. We use a case study based on a cardiac pacemaker for illustrating the developed approach.",2001,0,
2886,2887,Towards the application of classification techniques to test and identify faults in multimedia systems,"The advances in computer and graphic technologies have led to the popular use of multimedia for information exchange. However, multimedia systems are difficult to test. A major reason is that these systems generally exhibit fuzziness in their temporal behaviors. The fuzziness is caused by the existence of non-deterministic factors in their runtime environments, such as system load and network traffic. It complicates the analysis of test results. The problem is aggravated when a test involves the synchronization of different multimedia streams as well as variations in system loading. We conduct an empirical study on the testing and fault-identification of multimedia systems by treating the issue as a classification problem. Typical classification techniques, including Bayesian networks, k-nearest neighbor, and neural networks, are experimented with the use of X-Smiles, an open source multimedia authoring tool supporting the Synchronized Multimedia Integration Language (SMIL). The encouraging result of our study, which is based only on five attributes, shows that our proposal can achieve an accuracy of 57.6 to 79.2% in identifying the types of fault in environments where common cause variations are present. A further improvement of 7.6% is obtained via normalization.",2004,0,
2887,2888,A Developed Dynamic Environment Fault Injection Tool for Component Security Testing,"Developers using third party software components need to test them to satisfy quality requirements. In this paper, according to the characteristics of component security test, we present a new tool called GCDEFI (generic component dynamic environment fault injection). GCDEFI adopt environment fault injection based on API interception technology. Faults can be injected by GCDEFI without the source code of target applications under assessment, nor does the injection process involve interruption. To evaluate our tool, we conduct several environment fault injection testing experiments. The results show that our tool is stable and effective.",2009,0,
2888,2889,A novel fault-detection technique of high-impedance arcing faults in transmission lines using the wavelet transform,"This paper describes a novel fault-detection technique of high-impedance faults (HIFs) in high-voltage transmission lines using the wavelet transform. The wavelet transform (WT) has been successfully applied in many fields. The technique is based on using the absolute sum value of coefficients in multiresolution signal decomposition (MSD) based on the discrete wavelet transform (DWT). A fault indicator and fault criteria are then used to detect the HIF in the transmission line. In order to discriminate between HIF and nonfault transient phenomena, such as capacitor and line switching and arc furnace loads, the concept of duration time (i.e., the transient time period), is presented. On the basis of extensive investigations, optimal mother wavelets for the detection of HIF are chosen. It is shown that the technique developed is robust to fault type, fault inception angle, fault resistance, and fault location. The paper demonstrates a new concept and methodology in HIF in transmission lines. The performance of the proposed technique is tested under a variety of fault conditions on a typical 154-kV Korean transmission-line system.",2002,0,
2889,2890,Motion correction in PET brain studies,"Brain studies recorded with positron emission tomography (PET) may last between a few minutes and some hours. Head motion during long scans lead not only to blurred images, but may also seriously disturb the kinetic analysis of the metabolic information contained in the PET data. With the increase of scanner resolution this problem becomes more and more important. In the last years some methods to correct for motion have been proposed. Here we describe these approaches, especially those examined and established in our PET laboratory. We also report our first experiences regarding the accuracy of motion correction and the consequences related to the linearized calculation of metabolic images.",2005,0,
2890,2891,Diagnose Multiple Stuck-at Scan Chain Faults,"Prior effect-cause based chain diagnosis algorithms suffer from accuracy and performance problems when multiple stuck-at faults exist on the same scan chain. In this paper, we propose new chain diagnosis algorithms based on dominant fault pair to enhance diagnosis accuracy and efficiency. Several heuristic techniques are proposed, which include (1) double candidate range calculation, (2) dynamic learning and (3) two- dimensional space linear search. The experimental results illustrate the effectiveness and efficiency of the proposed chain diagnosis algorithms.",2008,0,
2891,2892,The effect of nonlinear signal transformations on bias errors in elastography,"We have reported several artifacts in elastography (1991). These include mechanical artifacts, such as stress concentration, and signal processing artifacts, such as zebras, which are caused by bias errors incurred during the estimation of the peak of correlation functions using a curve-fitting method. We investigate the bias errors and show that bias errors in curve-fitting methods are substantially increased because of nonlinear operations on the echo signals that reduce other errors. We also show that, for typical sampling rates, the bias errors can be ignored in the absence of these nonlinear operations.",2000,0,
2892,2893,Key Picture Error Concealment Using Residual Motion-Copy in Scalable Video Coding,"The current JSVM software only supports Frame- Copy and Motion-Copy Error Concealment (EC) methods for the concealment of key picture loss in Group of Picture (GOP). In the proposed method, we exploit motion retrieval information to conceal key frame in scalable video coding. The auxiliary motion vectors of only key picture are interleaved into the bit stream of previous key picture. To control bit-rate increased due to interleaving of supplement information, residual difference is calculated. This encoded residual difference is used to calculate the predictor so that concealment of each macroblock in a key picture will be performed. Consequently, the increase in bit-rate due to supplement information is compensated with increase in PSNR gain with correlated increase in GOP size. Moreover, we investigate the impact of key picture distortion propagation loss of hierarchical prediction in SVC. The proposed method provides drift-free substantial improvement without significant complexity.",2009,0,
2893,2894,SDG-based hazop and fault diagnosis analysis to the inversion of synthetic ammonia,"This paper presents some practical applications of signed directed graphs (SDGs) to computeraided hazard and operability study (HAZOP) and fault diagnosis, based on an analysis of the SDG theory. The SDG is modeled for the inversion of synthetic ammonia, which is highly dangerous in process industry, and HAZOP and fault diagnosis based on the SDG model are presented. A new reasoning method, whereby inverse inference is combined with forward inference, is presented to implement SDG fault diagnosis based on a breadth-first algorithm with consistency rules. Compared with conventional inference engines, this new method can better avoid qualitative spuriousness and combination explosion, and can deal with unobservable nodes in SDGs more effectively. Experimental results show the validity and advantages of the new SDG method.",2007,0,
2894,2895,IP core logic fault test simulation environment,"A low-level logic fault test simulation environment for embedded systems directed specifically towards application-specific integrated circuits (ASICs) and intellectual property (IP) cores is proposed in the paper. The developed simulation environment emulates a typical builtin self-testing (BIST) architecture with automatic test pattern generator (ATPG) that sends its outputs to a circuit (core) under test (CUT) and the output streams from the CUT are fed into an output response analyzer (ORA). The paper delineates the development of the test architecture, test application and fault injection including the relevance of the logic fault simulator.in great details. Some results on simulation on specific IP cores designed using combinations from ISCAS 85 combinational and ISCAS 89 sequential benchmark circuits are provided as well for evaluation.",2010,0,
2895,2896,Generating Minimal Fault Detecting Test Suites for Boolean Expressions,"New coverage criteria for Boolean expressions are regularly introduced with two goals: to detect specific classes of realistic faults and to produce as small as possible test suites. In this paper we investigate whether an approach targeting specific fault classes using several reduction policies can achieve that less test cases are generated than by previously introduced testing criteria. In our approach, the problem of finding fault detecting test cases can be formalized as a logical satisfiability problem, which can be efficiently solved by a SAT algorithm. We compare this approach with respect to the well-known MUMCUT and Minimal-MUMCUT strategies by applying it to a series of case studies commonly used as benchmarks, and show that it can reduce the number of test cases further than Minimal-MUMCUT.",2010,0,
2896,2897,Motor fault detection using Elman neural network with genetic algorithm-aided training,"Fault detection methods are crucial in acquiring safe and reliable operation in motor drive systems. Remarkable maintenance costs can also be saved by applying advanced detection techniques to find potential failures. However, conventional motor fault detection approaches often have to work with explicit motor models. In addition, most of them are deterministic or non-adaptive, and therefore cannot be used in time-varying cases. We propose an Elman neural network-based motor fault detection scheme to overcome these difficulties. The Elman neural network has the unique time series prediction capability because of its memory nodes as well as local recurrent connections. Motor faults are detected from changes in the expectation of the feature signal prediction error. A genetic algorithm (GA)-aided training strategy for the Elman neural network is further introduced to improve the approximation accuracy and achieve better detection performance. Computer simulations of a practical automobile transmission gear with an artificial fault are carried out to verify the effectiveness of our method. Encouraging fault detection results have been obtained without any prior information of the gear model",2000,0,
2897,2898,Bit Error Rate Analysis of jamming for OFDM systems,"The bit error rate (BER) analysis of various jamming techniques for orthogonal frequency-division multiplexing (OFDM) systems is given in both analytical form and software simulation results. Specifically, the BER performance of barrage noise jamming (BNJ), partial band jamming (PBJ) and multitone jamming (MTJ) in time-correlated Rayleigh fading channel with additive white gaussian noise (AWGN) has been investigated. In addition, two novel jamming methods - optimal-fraction PBJ and optimal-fraction MTJ for OFDM systems are proposed with detailed theoretical analysis. Simulation results validate the analytical results. It is shown that under the A WGN channel without fading, the optimal-fraction MTJ always gives the best jamming effect among all the jamming techniques given in this paper, while in Rayleigh fading channel the optimal-fraction MTJ can achieve acceptable performance. Both analysis and simulation indicate that the proposed optimal-fraction MTJ can be used to obtain improved jamming effect under various channel conditions with low complexity for OFDM systems.",2007,0,
2898,2899,An Agent-Based Migration Transparency and Fault Tolerance in Computational Grid,"A Grid is a large-scale, geographically distributed hardware and software infrastructure for flexible, secure, and coordinated sharing of vast amounts of heterogeneous resources within large, dynamic and distributed communities of users belonging to virtual organizations, to enable solving of complex scientific problems. From the perspective of one computer, such network partitioning may appear as a failure to other computers. These types of failures may lead to major impact on whole application which is executing on Grid for many days. Thus, failures in the grid computing environment can be solved to some extent by performing migration of application through node agents. As executing applications in nodes have contiguous service features, it becomes important to handle and mask fault and migrate the current job to another grid node without stopping the on-going processes. In this paper, we use agents to provide communication between grid nodes and handle failure tolerant techniques on grid. The agent is dynamically reallocated to Grid nodes though a transparent migration mechanism, as a way to provide fault tolerance in computational grids.",2009,0,
2899,2900,Online parameter estimation issues for the NASA IFCS F-15 fault tolerant systems,"This paper focuses on specific issues relative to real-time online estimation of aircraft aerodynamic parameters at nominal and post-actuator failure flight conditions. A specific parameter identification (PID) method, based on Fourier transform, has been applied to an approximated mathematical model of the NASA IFCS F-15 aircraft. In this effort different options relative to the application of this PID method are evaluated and compared. Particularly, the direct evaluation of stability and control derivatives versus the estimation of the coefficients of the state space system matrices evaluation is considered. Furthermore, the options of considering individual control surfaces (left and right) versus total surfaces as inputs to the PID process are discussed. Finally, since the PID method relies on the use of derivative terms, the option of using time domain derivatives versus frequency domain derivatives is also evaluated. Results are presented in terms of the accuracy and reliability of the estimates of selected stability and control derivatives.",2002,0,
2900,2901,Digitally controlled three-phase power factor correction circuit with partially resonant circuit,"This paper presents the three-phase PFC (power factor correction) circuit with partially resonant circuit using a DSP (digital signal processor). The power supply systems for telecommunication and data communication systems have been able to get the high power factor and low input current harmonic distortion factor using the PFC circuit. However, the PFC circuit requires many components and many adjusting processes especially in case of the three-phase input PFC. In this paper, the advantages of the digital control with DSP are shown and compared with the analog control. With the digital control using DSP, better regulation of the output voltage, fewer components and reducing adjusting process are achieved. Also the partially resonant circuit which is effective to the PFC circuit is also shown. In the partially resonant circuit, both the main switches and the resonant switches switch under soft switching conditions. Over 97% efficiency of the PFC circuit is realized in the 2.5 kW power supply system.",2003,0,
2901,2902,A numerical optimization-based methodology for application robustification: Transforming applications for error tolerance,"There have been several attempts at correcting process variation induced errors by identifying and masking these errors at the circuit and architecture level. These approaches take up valuable die area and power on the chip. As an alternative, we explore the feasibility of an approach that allows these errors to occur freely, and handle them in software, at the algorithmic level. In this paper, we present a general approach to converting applications into an error tolerant form by recasting these applications as numerical optimization problems, which can then be solved reliably via stochastic optimization. We evaluate the potential robustness and energy benefits of the proposed approach using an FPGA-based framework that emulates timing errors in the floating point unit (FPU) of a Leon3 processor. We show that stochastic versions of applications have the potential to produce good quality outputs in the face of timing errors under certain assumptions. We also show that good quality results are possible for both intrinsically robust algorithms as well as fragile applications under these assumptions.",2010,0,
2902,2903,Optimized routing for fault management in optical burst-switched WDM networks,"Optical burst switching (OBS) is a promising technique for supporting high-capacity, bursty data traffic over optical wavelength-division-multiplexed (WDM) networks. An optical link failure may result in a huge amount of data (and revenue) loss, and it has been an important survivability concern in optical networks. In this paper, we study the fault- management issues related with a link failure in an OBS network. We propose to use pre-planned global rerouting to balance network load and to reroute bursts after a link fails. We apply optimization techniques to pre-plan explicit backup routes for failure scenarios. Our objective is to achieve optimal load balancing both before and after a failure such that the network state can still remain stable with minimum burst-loss probability when a failure occurs. We apply the pre-planned normal and backup routing tables to an OBS network, and study the network performance after a failure occur using illustrative numerical examples. The results show that the average burst-loss probability can be significantly reduced by 60% - from an average of 0.10 to 0.04 (when the normalized link load is less than 0.5) using globally-rerouted backup routes, when compared with the scheme without global rerouting. We also observe that the burst-loss probability is reduced by 43% - from an average of 0.07 to 0.04 (when the link load is less than 0.5) if the rerouting is done using optimization techniques, when compared with shortest-path routing.for Fault Management",2007,0,
2903,2904,Predicting error floors of structured LDPC codes: deterministic bounds and estimates,"The error-correcting performance of low-density parity check (LDPC) codes, when decoded using practical iterative decoding algorithms, is known to be close to Shannon limits for codes with suitably large blocklengths. A substantial limitation to the use of finite-length LDPC codes is the presence of an error floor in the low frame error rate (FER) region. This paper develops a deterministic method of predicting error floors, based on high signal-to-noise ratio (SNR) asymptotics, applied to absorbing sets within structured LDPC codes. The approach is illustrated using a class of array-based LDPC codes, taken as exemplars of high-performance structured LDPC codes. The results are in very good agreement with a stochastic method based on importance sampling which, in turn, matches the hardware-based experimental results. The importance sampling scheme uses a mean-shifted version of the original Gaussian density, appropriately centered between a codeword and a dominant absorbing set, to produce an unbiased estimator of the FER with substantial computational savings over a standard Monte Carlo estimator. Our deterministic estimates are guaranteed to be a lower bound to the error probability in the high SNR regime, and extend the prediction of the error probability to as low as 10<sup>-30</sup>. By adopting a channel-independent viewpoint, the usefulness of these results is demonstrated for both the standard Gaussian channel and a channel with mixture noise.",2009,0,
2904,2905,Fault tolerance technology for autonomous decentralized database systems,"The Autonomous Decentralized Database System (ADDS) has been proposed in the background of e-business in respect to the dynamic and heterogeneous requirements of the users. With the rapid development of information technology, different companies in the field of e-business are supposed to cooperate in order to cope with the continuous changing demands of services in a dynamic market. In a diversified environment of service provision and service access, the ADDS provides flexibility to integrate heterogeneous and autonomous systems while assuring timeliness and high availability. A loosely-consistency management technology confers autonomy to each site for updating while maintaining the consistency of the whole system. Moreover, a background coordination technology, by utilizing a mobile agent, has been devised to permit the sites to coordinate and cooperate with each other while conferring the online property. The use of mobile agent, however, is critical and requires reliability with regard to mobile agent failures that may lead to bad response times and hence the availability of the system may lost. A fault tolerance technology is proposed in order that the system autonomously detect and recover the fault of the mobile agent due to a failure in a transmission link, site or bug in the software. The effectiveness of the proposition is shown by simulation.",2003,0,
2905,2906,An Empirical Comparison of Fault-Prone Module Detection Approaches: Complexity Metrics and Text Feature Metrics,"In order to assure the quality of software product, early detection of fault-prone products is necessary. Fault-prone module detection is one of the major and traditional area of software engineering. However, comparative study using the fair environment rarely conducted so far because there is little data publicly available. This paper tries to conduct a comparative study of fault-prone module detection approaches.",2010,0,
2906,2907,A Fault-Tolerant Legion Authentication System,"Protecting resources from unauthorized access are one of the most premier requirements of any distributed environment. Legion uses an Authentication Object that represents the user to system. AuthenticationObject is responsible for authenticating user and issuance of certificate, which is used by resource object to check validity of request. AuthenticationObject contains the password and private key of the user. User will access AuthenticationObject by using legion object identifier (LOID) containing public key of the user. If the AuthenticationObject gets deleted then there is no way to recover or regenerate th is AuthenticationObject because private key cannot be recreated. Thus, current system is not fault-tolerant and reliable. Therefore, user needs to be recreated. This paper proposes a fault-tolerant, reliable and more dynamic authentication mechanism",2006,0,
2907,2908,Bathythermograph error analysis and reduction (BEAR),"Many oceanographic and tactical studies require high-fidelity sonar predictions, which require accurate portrayals of the ocean's temperature structure. The Naval Oceanographic Office (NAVOCEANO) merges various oceanographic data into 'first-guess' temperature fields. The Modular Ocean Data Assimilation System (MODAS) then assimilates more timely bathythermograph (BT) data to create temperature nowcasts and companion uncertainty fields. Cost constraints require minimization of at-sea measurements. The Sensor Placement for Optimal Temperature Sampling (SPOTS) algorithm determines the best placements of limited numbers of BTs to provide the most accurate temperature fields at the lowest possible cost. SPOTS hypothesizes that placements which minimize uncertainty will also tend to minimize error. The bathythermograph error analysis and reduction (BEAR) algorithm objectively determines covariance distances for use in MODAS assimilation routines and performs quality control (QC) on BTs to validate the SPOTS hypothesis. BEAR models physical errors that shift the temperature uniformly (factory mis-calibration, poor storage conditions, instrument abuse), shift the depth uniformly (starting lag and wave height), and expand or contract the gradients uniformly (inaccurate rate of fall). BEAR simulates the profiles that would result by backing out every possible combination of these physical errors. Deep-water temperature profiles sharply constrain BT QC, because they tend to be spatially and temporally stationary. An error of 0.4 degrees may correspond to fifty standard deviations from the mean. Unsurprisingly, the deeper half of the variances usually accounts for 95-99% of the total. Satellite-derived Sea-Surface Temperature (MCSST) data provide a second constraint, because their uncertainties are low (compared to BTs). BEAR computes the sum of variance over all depths for each simulated error-combination against these surface and climatological constraints. The minimum va- - riance sum is usually chosen to represent the most likely error combination, which is then applied to the BT to correct it, without at any point tampering with the temperature structure. Generally the one with the lowest variance is selected. However, if more than one BT is available from the same local region and within a reasonable time period, cross-correlation can be used to align the structural features (e.g., thermocline or mixed-layer depth) and select the best member of each variance cluster",2005,0,
2908,2909,Enhancement of Fault Injection Techniques Based on the Modification of VHDL Code,"Deep submicrometer devices are expected to be increasingly sensitive to physical faults. For this reason, fault-tolerance mechanisms are more and more required in VLSI circuits. So, validating their dependability is a prior concern in the design process. Fault injection techniques based on the use of hardware description languages offer important advantages with regard to other techniques. First, as this type of techniques can be applied during the design phase of the system, they permit reducing the time-to-market. Second, they present high controllability and reachability. Among the different techniques, those based on the use of saboteurs and mutants are especially attractive due to their high fault modeling capability. However, implementing automatically these techniques in a fault injection tool is difficult. Especially complex are the insertion of saboteurs and the generation of mutants. In this paper, we present new proposals to implement saboteurs and mutants for models in VHDL which are easy-to-automate, and whose philosophy can be generalized to other hardware description languages.",2008,0,
2909,2910,Tolerance of performance degrading faults for effective yield improvement,"To provide a new avenue for improving yield for nano-scale fabrication processes, we introduce a new notion: performance degrading faults (pdef). A fault is said to be a pdef if it cannot cause a functional error at system outputs but may result in system performance degradation. In a processor, a fault is a pdef if it causes no error in the execution of user programs but may reduce performance, e.g., decrease the number of instructions executed per cycle. By identifying faulty chips that contain pdef's that degrade performance within some limits and binning these chips based on the their resulting instruction throughput, effective yield can be improved in a radically new manner that is completely different from the current practice of performance binning on clock frequency. To illustrate the potential benefits of this notion, we analyze the faults in the branch prediction unit of a processor. Experimental results show that every stuck-at fault in this unit is a pdef. Furthermore, 97% of these faults induce almost no performance degradation.",2009,0,
2910,2911,Fault location using wavelet energy spectrum analysis of traveling waves,"Power grid faults generate traveling wave signals at the fault point. The signals transmit to both ends of the faulted transmission line, and to the whole power grid. The traveling wave signals have many components with different frequencies and all the components have fault characteristics. The signals can be employed in locating accurately the fault, and the location method cannot be influenced by current transformer saturation and low frequency oscillation. The frequency band component with energy concentrated in the detected traveling wave is extracted by wavelet energy spectrum analysis. The arrival time of the component is recorded with wavelet analysis in the time domain. The propagation velocity of the component is calculated by the last recorded traveling wave arrived time at both ends of the tested transmission line, which is generated by an outside disturbance. The fault location scheme is simulated with ATP software. Results show that the accuracy of the method is little affected by fault positions, fault types and grounding resistances. The fault location error is less than 100 m.",2007,0,
2911,2912,An improved error concealment algorithm for intra-frames in H.264/AVC,The highly error-prone nature of wireless environments and limited computational power of mobile devices necessitates the implementation of robust yet simple error concealment in H.264/AVC. We propose a new and effective error concealment algorithm for intra-coded frames that utilizes the temporal redundancy in a wireless video bitstream. The proposed concealment method supports both raster scan and FMO type slices. Performance evaluations show that our approach achieves significant improvement over existing methods in both PSNR and subjective picture quality.,2005,0,
2912,2913,Bayesian Network Based Fault Section Estimation in Power Systems,"In this paper, a novel method for fault section estimation in power systems based on Bayesian network is presented. The main contributions of this paper include the following two aspects. One is that the fault diagnosis models based on Bayesian network are proposed, which are converted from the logic relationship among section fault, protective relay operation and circuit breaker trip. This method is very simple, but can perfectly treat with the uncertain information existing in power system fault diagnosis. Another is that the method is developed for creating every section's diagnosis network automatically, thus the fault diagnosis can be fulfilled in a very short time for large-scale power system and can be implemented online. Diagnostic results of instance show that the proposed method is efficient and correct, and is very suitable for complex fault diagnosis problems, especially for the multiple-section fault cases and for the cases where protective relays and circuit breakers malfunction",2006,0,
2913,2914,Performance analysis of permanent magnet synchronous motor drives under inverter fault conditions,"This paper presents a comparative study regarding the performance of a permanent magnet synchronous motor drive, under normal and faulty operating conditions. Two different failure types in the inverter are considered: single power switch and single phase open-circuit faults. In order to compare the drive performance under these three operating conditions, global results are presented concerning the analysis of some key parameters like motor efficiency, power factor, electromagnetic torque and currents RMS and total harmonic distortion values.",2008,0,
2914,2915,Performance analysis of BPSK and QPSK using error correcting code through AWGN,"This paper highlight the performance analysis of BPSK and QPSK using error correcting code. To calculate the bit error rate, different types of error correcting code were used through an Additive White Gaussian Noise (AWGN) channel. Bose-Chaudhuri-Hocquenghem (BCH), Cyclic code and hamming code were used as the encoder/decoder technique. Basically, the performance was determined in term of bit rate error (BER) and signal energy to noise power density ratio (Eb/No). Both BPSK and QPSK were also being compared in the symbol error capability known as t in which expected that the performance is graded in response to the increasing of value of t. All simulations were done using MATLAB R2007b software. In general BCH codes demonstrate better performance than Hamming code and Cyclic code for both BPSK and QPSK.",2010,0,
2915,2916,A versatile structure of S31-GGA-casc switched-current memory cell with complex suppression of memorizing errors,"This contribution describes the circuit structure of high precision S31-GGA-casc switched-current (SI) memory cell providing complex suppression of memorizing errors. The low-frequency relative current error caused by charge-injection and input/output conductance ratio is typically 50 ppm. Moreover, the DC current offset of this cell type is lower than 0.2 /spl mu/A within the signal range of 350 /spl mu/A. At this point, excellent DC stability was achieved by a new solution of current biasing circuitry. The cell provides optimized input/output impedance ratio which can further minimize the errors arising in a SI system. All the results were proven by circuit simulation and chip measurement. In the field of circuit testing, a new concept of high-precision current-sensing circuit was invented and it is also being described in this paper.",2003,0,
2916,2917,Distributed construction of a fault-tolerant network from a tree,"We present an algorithm by which nodes arranged in a tree, with each node initially knowing only its parent and children, can construct a fault-tolerant communication structure (an expander graph) among themselves in a distributed and scalable way. The tree overlayed with this logical expander is a useful structure for distributed applications that require the intrinsic ""treeness"" from the topology but cannot afford any obstruction in communication due to failures. At the core of our construction is a novel distributed mechanism that samples nodes uniformly at random from the tree. In the event of node joins, node departures or node failures, the expander maintains its own fault tolerance and permits the reformation of the tree. We present simulation results to quantify the convergence of our algorithm to a fault tolerant network having both good vertex connectivity and expansion properties.",2005,0,
2917,2918,Video text detection and localization based on localized generalization error model,"Texts in videos provide plenteous information for video analysis such as video indexing, understanding and retrieval. We propose a neural network based method detecting text in the video frames in this work. The proposed method consists of three major steps: feature extraction, text region detection and candidate region refinement. Firstly, we extract texture features from four edge maps yielded from the target video frame. Secondly, a Radial Basis Function Neural Network (RBFNN) optimized by the Localized Generalization Error Model (L-GEM) is applied to detect text candidates. Finally, a false detection of text is applied to fine tune the result. Experimental results demonstrate that the proposed method is efficient for different font-colors, font-sizes and language in complex background.",2010,0,
2918,2919,A multi-agent based fault tolerance system for distributed multimedia object oriented environment: MAFTS,"This paper presents the design and implementation of the MAFTS (a multi-agent based fault-tolerance system), which is running on distributed multimedia object oriented environment. DOORAE (distributed object oriented collaboration environment) is a good example of the foundation technology for a computer-based multimedia collaborative work that allows development of required application by combining many agents composed of units of functional module when user wishes to develop a new application field. MAFTS has been designed and implemented in DOORAE environment. It is a multi-agent system that is implemented with object oriented concept. The main idea is to detect an error by using polling method. This system detects an error by polling periodically processes with relation to sessions. And, it is to classify the type of errors automatically by using learning rules. The characteristic of this system is to use the same method to get back again it as it creates a session.",2005,0,
2919,2920,Maximum Throughput Obtaining of IEEE 802.15.3 TDMA Mechanism under Error-Prone Channel,"IEEE 802.15.3 efficiently uses time division multiple accesses (TDMA) to support the quality of service (QoS) for multimedia traffic or the transfer of multi-megabyte data for music and image files. In the TDMA mechanism for an allocated channel time (channel time allocation, CTA) and known bit error rate of the channel, the throughput can be maximized by dynamically adjusting the frame size. In this paper a throughput model under non-ideal channel condition was formulated, and then the adaptive frame size can be calculated from the model. In addition a feasible implementation of this adaptive scheme is presented. The mathematical analysis and simulation results demonstrate the effectiveness of our adaptive scheme.",2009,0,
2920,2921,Analysis of software quality cost modelings industrial applicability with focus on defect estimation,The majority of software quality cost models is by design capable of describing costs retrospectively but relies on defect estimation in order to provide a cost forecast. We identify two major approaches to defect estimation and evaluate them in a large scale industrial software development project with special focus on applicability in quality cost models. Our studies show that neither static models based on code metrics nor dynamic software reliability growth models are suitable for an industrial application.,2008,0,
2921,2922,Dynamic Security Assessment to protect systems after severe fault situations,"In the last 10 years the number of severe fault situations and black-outs world wide is increasing. The classical static security assessment is used to monitor the system situation after contingencies, but is not able to take into account the complex dynamic behaviour of an electrical system together with the control of generators and grid equipment like switched capacitors or FACTS together with the protection reaction in unforeseeable situations after severe system faults. The paper describes a modern dynamic security assessment (DSA) system which allows handling predefined dynamic contingencies in real-time and intelligent proceeding and evaluation. The base of the system is the system simulation tool PSStrade NETOMAC, which can simulate the dynamic behaviour of large electrical systems including control and protection. A contingency builder allows the user to define the interesting contingency scenarios like outage of grid elements, generators or combination of outages or system faults like short circuits, etc. The events can be calculated in real time which means, that about 200 contingency cases can be handled in about 10 minutes, depending on the system size. The DSA-system analyses the events using an intelligent and flexible criteria editor which gives the opportunity to select criteria for critical system time behaviour. These criteria allow to observe how critical a system reacts checking under voltages, frequency, angle differences in the grid, overcurrents, machine angle, etc. The information about severe cases is available in a protocol for easy recalculation of critical cases in details with more parameters checked and monitored. The results can be used to monitor the overall situation of a system periodically. The automatic monitoring of critical events is under construction.",2006,0,
2922,2923,On Fault Isolation by Functional and Hardware Redundancy,"The aim of the work is to exploit some aspects of the functional and hardware redundancy in fault detection and isolation tasks using back-propagation neural networks as functional approximation devices to be used as residuals generators which will evaluated by means of rule based strategies. Implementation procedure is carried out with the facilities supplied by a FOUNDATION<sup>TM</sup> Fieldbus compliant tool, which manage databases, neural network structures and training algorithms under mentioned standard.",2006,0,
2923,2924,Making an SCI fabric dynamically fault tolerant,"In this paper we present a method for dynamic fault tolerant routing for SCI networks implemented on Dolphin Interconnect Solutions hardware. By dynamic fault tolerance, we mean that the interconnection network reroutes affected packets around a fault, while the rest of the network is fully functional. To the best of our knowledge this is the first reported case of dynamic fault tolerant routing available on commercial off the shelf interconnection network technology without duplicating hardware resources. The development is focused around a 2-D torus topology, and is compatible with the existing hardware, and software stack. We look into the existing mechanisms for routing in SCI. We describe how to make the nodes that detect the faulty component do routing decisions, and what changes are needed in the existing routing to enable support for local rerouting. The new routing algorithm is tested on clusters with real hardware. Our tests show that distributed databases like MySQL can run uninterruptedly while the network reacts to faults. The solution is now part of Dolphin Interconnect Solutions SCI driver, and hardware development to further decrease the reaction time is underway.",2008,0,
2924,2925,Respiratory-motion errors in quantitative myocardial perfusion with PET/CT,"Respiratory motion is known to cause errors in whole-body oncologic and static cardiac imaging with PET/CT. These errors are caused by the difference in acquisition times of the PET and CT data sets leading to inconsistencies and hence artifacts when the CT scan is used for attenuation correction (CTAC). The purpose of this study was to use computer simulations to investigate how quantitative imaging of myocardial perfusion with dynamic Rb82 PET/CT may be affected by respiratory motion. The NCAT anthropomorphic computer phantom was used to generate uniform-activity images at each of 10 respiratory phases and 17 dynamic frames. PET projection data for each of these 170 images were generated using the SimSET Monte Carlo simulator. The GE Discovery LS PET/CT was modeled and 400 M photon histories were tracked for each simulation. Images were reconstructed using OSEM and 4 different approaches to CTAC: phase-matched CTAC, correction with a single-phase CT (end- inspiration, end-expiration, and mid-inspiration), correction with a CT averaged over the respiratory cycle, and correction with a CT that was a voxel-by-voxel maximum over the respiratory cycle. The dynamic image sets were then processed using software developed in-house for the kinetic analysis of myocardial perfusion data. Images of Kl (blood-flow) were converted to polar maps and compared point-by-point and by segment using 17-segment regional analysis. Comparing all results to those of the phase-matched correction, we found that a single-phase correction had mean segmental errors as high as 20% (end- inspiration) with mid-inspiration correction providing the least error at 6%. An average CTAC had errors as high as 12% in the mid-inferior wall. The max-CTAC approach produced errors as high as 21%. Also of note, though, was that the phase-matched polar map was not uniform, as expected, with a visible decrease in blood-flow in the inferior wall. This deficit is potentially caused by motion-blurring leading to i- nterference from the activity in the stomach and liver through the model fitting of the spill-over correction term. We conclude that respiratory motion can lead to errors in quantitative estimates of blood-flow obtained from dynamic Rb82 PET perfusion studies. A CT map acquired at the mid-respiratory phase provides an accurate CTAC correction, but is not practical to acquire. An average CTAC provides the most accurate, practical solution to the respiratory-motion problem, however, it still produced segmental errors as large as 12% in this simulation. Errors in the inferior wall of the heart were observed with all corrections and may be related to motion-blurring of activity from extra-cardiac organs into the heart. The source of the inferior-wall deficits requires further investigation.",2007,0,
2925,2926,Fault current limiter allocation and sizing in distribution system in presence of distributed generation,"Expose of distributed generation (DG) to the distribution network increases the fault current level. This will give rise to fault current which is normally greater than interrupt capability of breakers and fuses. The introduction of solid state fault current limiters (SSFCLs) becomes an effective way for suppressing such a high short-circuit current fault in distribution systems. In this paper, the effect of proposed SSFCL on reduction of fault current is investigated. Then genetic algorithm is employed to search for the optimal number, locations and size of proposed SSFCL. The Numerical and simulation results show the efficiency of proposed GA based FCL allocation and sizing method in terms of minimization of distribution protection system cost.",2009,0,
2926,2927,Fault tolerant XGFT network on chip for multi processor system on chip circuits,"This paper presents a fault-tolerant eXtended Generalized Fat Tree (XGFT) Network-On-Chip (NOC) implemented with a new fault-diagnosis-and-repair (FDAR) system. The FDAR system is able to locate faults and reconfigure switch nodes in such a way that the network can route packets correctly despite the faults. This paper presents how the FDAR finds the faults and reconfigures the switches. Simulation results are used for showing that faulty XGFTs could also achieve good performance, if the FDAR is used. This is possible if deterministic routing is used in faulty parts of the XGFTs and adaptive Turn-Back (TB) routing is used in faultless parts of the network for ensuring good performance and Quality-of-Service (QoS). The XGFT is also equipped with parity bit checks for detecting bit errors from the packets.",2005,0,
2927,2928,JPEG Error Analysis and Its Applications to Digital Image Forensics,"JPEG is one of the most extensively used image formats. Understanding the inherent characteristics of JPEG may play a useful role in digital image forensics. In this paper, we introduce JPEG error analysis to the study of image forensics. The main errors of JPEG include quantization, rounding, and truncation errors. Through theoretically analyzing the effects of these errors on single and double JPEG compression, we have developed three novel schemes for image forensics including identifying whether a bitmap image has previously been JPEG compressed, estimating the quantization steps of a JPEG image, and detecting the quantization table of a JPEG image. Extensive experimental results show that our new methods significantly outperform existing techniques especially for the images of small sizes. We also show that the new method can reliably detect JPEG image blocks which are as small as 8  8 pixels and compressed with quality factors as high as 98. This performance is important for analyzing and locating small tampered regions within a composite image.",2010,0,
2928,2929,A Method for Optimum Test Point Selection and Fault Diagnosis Strategy for BIT of Avionic System,"A method for optimum test point selection and the fault diagnosis strategy which is based on the fault message matrix and features of BIT is proposed. The fault message matrix is divided based on the weight of the test points The diagnosis strategy is determined using dividing the fault message matrix and the thought of detecting first and isolating next. Result shows that the optimum method is suitable for BIT to select the appropriate test points and fault diagnosis procedure. Besides, average numbers of test steps were reduced.",2009,0,
2929,2930,Semantic translation error rate for evaluating translation systems,"In this paper, we introduce a new metric which we call the semantic translation error rate, or STER, for evaluating the performance of machine translation systems. STER is based on the previously published translation error rate (TER) (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) metrics. Specifically, STER extends TER in two ways: first, by incorporating word equivalence measures (WordNet and Porter stemming) standardly used by METEOR, and second, by disallowing alignments of concept words to non-concept words (aka stop words). We show how these features make STER alignments better suited for human-driven analysis than standard TER. We also present experimental results that show that STER is better correlated to human judgments than TER. Finally, we compare STER to METEOR, and illustrate that METEOR scores computed using the STER alignments have similar statistical properties to METEOR scores computed using METEOR alignments.",2007,0,
2930,2931,The nullspace method - a unifying paradigm to fault detection,"The nullspace method is a powerful framework to solve the synthesis problem of fault detection filters in the most general setting. It is also well suited to address the least order synthesis problem. In the same time, the nullspace method represents an unifying paradigm for several methods, because popular approaches like parity space or observer based methods can be interpreted as special classes of nullspace method. The main differences among different methods lie in the numerical properties of the underlying computational algorithms.",2009,0,
2931,2932,"Simulation-based validation and defect localization for evolving, semi-formal requirements models","When requirements models are developed in an iterative and evolutionary way, requirements validation becomes a major problem. In order to detect and fix problems early, the specification should be validated as early as possible, and should also be revalidated after each evolutionary step. In this paper, we show how the ideas of continuous integration and automatic regression testing in the field of coding can be adapted for simulation-based, automatic revalidation of requirements models after each incremental step. While the basic idea is fairly obvious, we are confronted with a major obstacle: requirements models under development are incomplete and semi-formal most of the time, while classic simulation approaches require complete, formal models. We present how we can simulate incomplete, semi-formal models by interactively recording missing behavior or functionality. However, regression simulations must run automatically and do not permit interactivity. We therefore have developed a technique where the simulation engine automatically resorts to the interactively recorded behavior in those cases where it does not get enough information from the model during a regression simulation run. Finally, we demonstrate how the information gained from model evolution and regression simulation can be exploited for locating defects in the model.",2005,0,
2932,2933,Control Focused Soft Error Detection for Embedded Applications,"Advances in integrated circuits present several key challenges in system reliability as soft errors are expected to increase with successive technology generations. Computing systems must be able to continue functioning in spite of these soft errors, necessitating the development of new methods for self-healing circuits that can detect and recover from these errors. We present an area-efficient control focused soft error detector (CNFSED) capable of nonintrusively detecting soft errors within the execution of a software application without modifications to the software application or the target processor. This soft error detector achieves an error detection rate greater than 90% for control errors and 85% of unmasked errors while incurring minimal area overhead.",2010,0,
2933,2934,Optimal worst case estimation for LPV-FIR models with bounded errors,In this paper discrete time linear parameter varying (LPV) models with finite impulse response (FIR) dynamic structure are considered. Measurement errors are assumed to be bounded and in such condition the worst case parameter estimate errors are derived together with the input sequences that allow their determination. The main result of the paper shows that the optimal input design of LPV-FIR models is achieved by combining the available results on optimal input design for invariant FIR models with the results on optimal input design for static blocks,2000,0,
2934,2935,Calculating Functions of Interval Type-2 Fuzzy Numbers for Fault Current Analysis,"In this work, functions of type-2 fuzzy numbers are analyzed. For the special case of interval type-2 fuzzy numbers, the type-2 membership function of the output variable is calculated using the lower and upper membership functions of the input variables and the vertex method. This procedure is used in an application where the type-2 fuzzy fault currents of an electric distribution system are calculated. The results are shown and the advantages of the approach are discussed",2007,0,
2935,2936,Defect classification as problem classification for quality control in the software project management by DTL,"There are various reasons and causes which lead to failure of software that may come right from it's starting point of requirement analysis up to launching of product in the market. One has to do the root cause analysis of software failure so that these failures should not be reproducible. There are various problems due to which the software may give the bugs, errors, fault and ultimately the failure. Enlisting the problem, analyzing the problem after reporting is must before the fixing of the problem and going into the root cause of the problem. The classification of the problem will definitely help us to sort out the problems and will help to go to the root of problem. Once problems have been reported it can be classified by using any classification method depending upon the properties and their values. We do combine the decision tree learning with the input as the current problems. Decision Tree will be trained with trainee example with similar type of problems, their properties and values. The DTL will help us to classify the problem and ultimately give us the sorted problems to do analysis of problem. Analysis and classification of the problems will also help in the quality control of the product. We have taken defects classification as an example in this paper.",2010,0,
2936,2937,"Modeling, Detection, and Disambiguation of Sensor Faults for Aerospace Applications","Sensor faults continue to be a major hurdle for systems health management to reach its full potential. At the same time, few recorded instances of sensor faults exist. It is equally difficult to seed particular sensor faults. Therefore, research is underway to better understand the different fault modes seen in sensors and to model the faults. The fault models can then be used in simulated sensor fault scenarios to ensure that algorithms can distinguish between sensor faults and system faults. The paper illustrates the work with data collected from an electromechanical actuator in an aerospace setting, equipped with temperature, vibration, current, and position sensors. The most common sensor faults, such as bias, drift, scaling, and dropout were simulated and injected into the experimental data, with the goal of making these simulations as realistic as feasible. A neural network-based classifier was then created and tested on both experimental data and the more challenging randomized data sequences. Additional studies were also conducted to determine sensitivity of detection and disambiguation efficacy with respect to severity of fault conditions.",2009,0,
2937,2938,Error Detection via Online Checking of Cache Coherence with Token Coherence Signatures,"To provide high dependability in a multithreaded system despite hardware faults, the system must detect and correct errors in its shared memory system. Recent research has explored dynamic checking of cache coherence as a comprehensive approach to memory system error detection. However, existing coherence checkers are costly to implement, incur high interconnection network traffic overhead, and do not scale well. In this paper, we describe the token coherence signature checker (TCSC), which provides comprehensive, low-cost, scalable coherence checking by maintaining signatures that represent recent histories of coherence events at all nodes (cache and memory controllers). Periodically, these signatures are sent to a verifier to determine if an error occurred. TCSC has a small constant hardware cost per node, independent of cache and memory size and the number of nodes. TCSC's interconnect bandwidth overhead has a constant upper bound and never exceeds 7% in our experiments. TCSC has negligible impact on system performance",2007,0,
2938,2939,Fault Diagnosis Method Study on Automobile Electrical Controlled System Based on Fusing of ANN and D-S Evidence Theory,"With the improvement of automobile electric degree, more and more people begin to pay attention to the fault diagnosis method and theories of electric controlled system. The precision and accuracy of on-board diagnosis methods, which with OBDII standard and has been widely used at present need to be further improvement. So, in this paper, take the engine idling instability as the example, put forward a multi-sensor diagnosis method which fusing neural network and D-S evidence theory, this method mainly use for on-board diagnosis system datapsilas fusing process and analysis. The experimental result shows that, this method can make use of various faultspsila redundant and complementation information sufficiently, and then promote the recognition ability obviously. With electric controlled technology widely used in automobile, the performance of automobile products has been promoted largely, but these also make fault diagnosis become more difficult, traditional methods such as experience or simple instrument could not meet the flexible diagnosis demand. At present, the On-Board diagnosis with OBDII standard has been applied for electric controlled systempsilas fault diagnosis, but it could only for 70%-80%psilas fault, and the diagnosis results are mainly presented by fault code or data flow, and still need otherpsilas help, and the accuracy degree still needs further improvement. Therefore, looking for the more precious and intelligent method for electric controlled system become the key direction in automobile fault diagnosis field.",2008,0,
2939,2940,Cross-Layer Error Control Optimization in 3G LTE,"3G long-term evolution (LTE) is a recent effort taken by cellular industries to step into wireless broadband market. The key enhancements target an introduction of new all- IP architecture, enhanced link layer and radio access with OFDM modulation and multiple antenna techniques. In this study, we focus on the overhead deriving from the multilayer ARQ employed at the link and transport layers. To the aim of reducing unnecessary burden on the wireless link, we propose a cross-layer ARQ approach, called ARQ Proxy, which substitutes the transmission of TCP ACK packet with a short MAC layer request on the radio link. Packet identification is achieved through association of a hash function to the raw packet data. Performance of the ARQ Proxy is evaluated using EURAE extensions for ns2 simulator. Results demonstrate significant improvements in terms of system capacity, TCP throughput performance, and higher tolerance to transmission errors.",2007,0,
2940,2941,Improving the performance of hypervisor-based fault tolerance,"Hypervisor-based fault tolerance (HBFT), a checkpoint-recovery mechanism, is an emerging approach to sustaining mission-critical applications. Based on virtualization technology, HBFT provides an economic and transparent solution. However, the advantages currently come at the cost of substantial overhead during failure-free, especially for memory intensive applications. This paper presents an in-depth examination of HBFT and options to improve its performance. Based on the behavior of memory accesses among checkpointing epochs, we introduce two optimizations, read fault reduction and write fault prediction, for the memory tracking mechanism. These two optimizations improve the mechanism by 31.1% and 21.4% respectively for some application. Then, we present software-superpage which efficiently maps large memory regions between virtual machines (VM). By the above optimizations, HBFT is improved by a factor of 1.4 to 2.2 and it achieves a performance which is about 60% of that of the native VM.",2010,0,
2941,2942,Fault tolerance techniques for wireless ad hoc sensor networks,"Embedded sensor network is a system of nodes, each equipped with a certain amount of sensing, actuating, computation, communication, and storage resources. One of the key prerequisites for effective and efficient embedded sensor systems is development of low cost, low overhead, high resilient fault-tolerance techniques. Cost sensitivity implies that traditional double and triple redundancies are not adequate solutions for embedded sensor systems due to their high cost and high energy-consumption. We address the problem of embedded sensor network-fault-tolerance by proposing heterogeneous back-up scheme, where one type of resource is substituted with another. First we propose a broad spectrum of heterogeneous fault-tolerance techniques for sensor networks including the ones where communication and sensing are mutually backing up each other. Then, we focus our attention on two specific approaches where we back-up one type of sensors with another type of sensor. In the first, we assume faults that manifest through complete malfunctioning and in the second, we assume sensors where fault manifest through high level of error.",2002,0,
2942,2943,Delay-dependent robust fault detection for a class of nonlinear time-delay systems,"In this paper, the robust fault detection filter (RFDF) design problem is investigated for a class of nonlinear time-delay systems with unknown inputs. Firstly, a reference residual model is introduced to formulate the robust fault detection filter design problem as an H<sub>infin</sub> optimization control problem. The reference residual model designed according to the performance index is an optimal residual generator, which takes into account the robustness against disturbances and sensitivity to faults simultaneously. Then, applying robust H<sub>infin</sub> optimization control technique, the novel criteria of the robust fault detection filter for a class of nonlinear time-delay systems with unknown inputs is presented in terms of linear matrix inequality (LMI), which depends on the size of time delays. Finally, a numerical example is used to demonstrate the feasibility of our proposed method.",2008,0,
2943,2944,Cache management of dynamic source routing for fault tolerance in mobile ad hoc networks,"Mobile ad hoc networks have gained more and more research attention. They provide wireless communications without location limitations and pre-built fixed infrastructures. Because of the absence of any static support structure, ad hoc networks are prone to link failure. This has become the most serious cause of throughput degradation when using TCP over ad hoc networks. Some researchers chose dynamic source routing (DSR) as the routing protocol and showed that disabling the assignment of a route directly from cache gives better performance. We introduce an efficient cache management mechanism to increase the TCP throughput by replying with a route directly from the cache of DSR and perform cache recovery when a host failure has occurred. We use simulations to compare the performance of our algorithm with the original DSR under the link failure prone environment due to mobility. We also provide the simulation results when host failures are considered in the ad hoc networks",2001,0,
2944,2945,Unequal Error Protection (UEP) for Wavelet-Based Wireless 3D Mesh Transmission,"The recent popularity of networked graphics applications such as distributed military simulators and online games, has increased the need to transmit large 3D meshes and textures over wireless networks. To speed up large mesh transmission over low-bandwidth wireless links, we use a wavelet-based technique that aggressively compresses large meshes and enables progressive (piece-wise) transmission. Using wavelets, a server only needs to send the full connectivity information of a small base mesh along with wavelet coefficients that refine it, saving memory and bandwidth. To mitigate packet losses caused by high wireless error rates, we propose a novel forward error correction (FEC) scheme based on unequal error protection (UEP). UEP adds more error correction bits to regions of the mesh that have more details. Our work uses UEP to make wavelet-encoded meshes more resilient to wireless errors. Experimental results shows that our proposed UEP scheme is more error-resilient than no error protection (NEP) and equal error protection (EEP) as the packet loss rate increases by achieving 50% less relative errors and maintaining the decoded mesh structure. Our scheme can be integrated into future mobile devices and shall be useful in application areas such as military simulators on mobile devices.",2009,0,
2945,2946,A Network-Level Distributed Fault Injector for Experimental Validation of Dependable Distributed Systems,"The use of Java for distributed systems and high-available applications demands the validation of their fault tolerance mechanisms to avoid unexpected behavior during execution. We present an extension of FIONA (fault injector oriented to network applications), a fault injection environment to experimentally validate the dependability of distributed Java applications. The main features of this extension are: its distributed architecture, which allows centralized configuration of multiple faults scenarios, and the support for a wider fault model associated with distributed systems, which includes network partitioning. For monitoring, FIONA supports the collection of log information and includes a helper application to integrate this information in a global log for post-mortem dependability analysis. FIONA is simple to operate, and we expect it to facilitate the conduction of validation experiments by application developers and testers",2006,0,
2946,2947,Analysis of a Simple Feedback Scheme for Error Correction over a Lossy Network,"A control theoretic analysis of a simple error correction scheme for lossy packet-switched networks is presented. Based on feedback information from the error correction process in the receiver, the sender adjusts the amount of redundancy using a so called extremum-seeking controller, which do not rely on any accurate model of the network loss process. The closed-loop system is shown to converge to a limit cycle in a neighborhood of the optimal redundancy. The result are validated using packet-based simulations with data from wireless sensor network experiments.",2007,0,
2947,2948,Zero Defect Strategy for Electronic Components in Automotive Applications,Key challenges for components like ASICs and power semiconductors in automotive applications are introduced. The background for zero defects is highlighted. A vision of a strategy to achieve zero defects will be drawn,2006,0,
2948,2949,Research of industrial furnace fault diagnosis expert system,"In order to realize fast location and detection of abnormal status during running of industrial furnace, especially abnormal status of firing, this article studies and designs a fault diagnosis expert system based on fault tree theory. Firstly, formalized definition of industrial furnace fault diagnosis expert system is given in the paper, then all component elements of the expert system are analyzed and designed in detail, finally the principles and methods of design knowledge base are introduced to us by using the fault tree theory, and also reasoning algorithm is put forward, which is used to reason the fault by way of fault tree. The project practice indicates that this system knowledge model has good suitability, and what's more it is used simply and conveniently, also the outcome of fault diagnosis is reliable and steady.",2010,0,
2949,2950,Generation of compact test sets with high defect coverage,"Multi-detect (N-detect) testing suffers from the drawback that its test length grows linearly with N. We present a new method to generate compact test sets that provide high defect coverage. The proposed technique makes judicious use of a new pattern-quality metric based on the concept of output deviations. We select the most effective patterns from a large N-detect pattern repository, and guarantee a small test set as well as complete stuck-at coverage. Simulation results for benchmark circuits show that with a compact, 1-detect stuck-at test set, the proposed method provides considerably higher transition-fault coverage and coverage ramp-up compared to another recently-published method. Moreover, in all cases, the proposed method either outperforms or is as effective as the competing approach in terms of bridging-fault coverage and the surrogate BCE+ metric. In many cases, higher transition-fault coverage is obtained than much larger N-detect test sets for several values of N. Finally, our results provide the insight that, instead of using N-detect testing with as large N as possible, it is more efficient to combine the output deviations metric with multi-detect testing to get high-quality, compact test sets.",2009,0,
2950,2951,Fault tolerance design on onboard computer using COTS components,"A fault tolerance design on onboard computer (OBC) is proposed that allows commercial-off-the-shelf (COTS) devices to be incorporated into dual processing modules of onboard computer. The processing module is composed of 32-bit ARM RISC processor and other COTS devices. As well as, a set of fault handling mechanisms is implemented in the computer system. The onboard software was organized around a set of processes that communicate between each other through a routing process. The fault tolerant onboard computer has excellent data processing capability and is enough to meet the demands of the extremely tight constraints on mass, volume, power consumption and space environmental conditions",2006,0,
2951,2952,Research on fault diagnosis and forecast system of forest harvester based on CAN-bus information,"The fault diagnosis of forest harvester is updating with the application of CAN bus technology. Aiming at the CAN technology which was utilized on forest harvester currently, the complexity of the fault information and the difficulty of diagnosis, an USB-CAN intelligent interface card was designed in this paper. Based on the interface card, the software of Microsoft Visual C++6.0 is utilized to build the fault diagnosis system with BP neural network and Kalman filter. The fault diagnosis and forecast to the main systems of forest harvester were released online after incepting, filtering and removing the noise of the signal from the CAN bus. As the experiments show that Kalman filtering plays good on removal of noise from the complex fault signal, and the BP neural network trainings of the systems are effective to implement non-linear mapping from the fault phenomenon to the fault position of forest harvester.",2010,0,
2952,2953,SPHINX: A Fault-Tolerant System for Scheduling in Dynamic Grid Environments,"A grid consists of high-end computational, storage, and network resources that, while known a priori, are dynamic with respect to activity and availability. Efficient scheduling of requests to use grid resources must adapt to this dynamic environment while meeting administrative policies. In this paper, we describe a framework called SPHINX that can administrate grid policies, and schedule complex and data intensive scientific applications. We present experimental results for several scheduling strategies that effectively utilize the monitoring and job-tracking information provided by SPHINX. These results demonstrate that SPHINX can effectively schedule work across a large number of distributed clusters that are owned by multiple units in a virtual organization in a fault-tolerant way in spite of the highly dynamic nature of the grid and complex policy issues. The novelty lies in use of effective monitoring of resources and job execution tracking in making scheduling decisions and fault-tolerance - something that is missed in todays grid environments.",2005,0,
2953,2954,Beam-Based Non-Linear Optics Corrections in Colliders,"A method has been developed to measure and correct operationally the non-linear effects of the final focusing magnets in colliders, that gives access to the effects of multi-pole errors by applying closed orbit bumps, and analyzing the resulting tune and orbit shifts. This technique has been tested and used during 4 years of RHIC (the Relativistic Heavy Ion Collider at BNL) operations. I will discuss here the theoretical basis of the method, the experimental set-up, the correction results, the present understanding of the machine model, the potential and limitations of the method itself as compared with other non-linear correction techniques.",2005,0,
2954,2955,Requirement Error Abstraction and Classification: A Control Group Replicated Study,"This paper is the second in a series of empirical studies about requirement error abstraction and classification as a quality improvement approach. The Requirement error abstraction and classification method supports the developers' effort in efficiently identifying the root cause of requirements faults. By uncovering the source of faults, the developers can locate and remove additional related faults that may have been overlooked, thereby improving the quality and reliability of the resulting system. This study is a replication of an earlier study that adds a control group to address a major validity threat. The approach studied includes a process for abstracting errors from faults and provides a requirement error taxonomy for organizing those errors. A unique aspect of this work is the use of research from human cognition to improve the process. The results of the replication are presented and compared with the results from the original study. Overall, the results from this study indicate that the error abstraction and classification approach improves the effectiveness and efficiency of inspectors. The requirement error taxonomy is viewed favorably and provides useful insights into the source of faults. In addition, human cognition research is shown to be an important factor that affects the performance of the inspectors. This study also provides additional evidence to motivate further research.",2007,0,
2955,2956,Differences Between Observation and Sampling Error in Sparse Signal Reconstruction,"The field of Compressed Sensing has shown that a relatively small number of random projections provide sufficient information to accurately reconstruct sparse signals. Inspired by applications in sensor networks in which each sensor is likely to observe a noisy version of a sparse signal and subsequently add sampling error through computation and communication, we investigate how the distortion differs depending on whether noise is introduced before sampling (observation error) or after sampling (sampling error). We analyze the optimal linear estimator (for known support) and an l1 constrained linear inverse (for unknown support). In both cases, observation noise is shown to be less detrimental than sampling noise and low sampling rates. We also provide sampling bounds for a non-stochastic lA bounded noise model.",2007,0,
2956,2957,Hierarchical fault diagnosis: application to an ozone plant,"A framework for online passive fault diagnosis in hierarchical finite-state machines (HFSM) is presented and applied to an ozone generation plant. This approach takes advantage of system structure to reduce computational complexity. Here, the system model is broken into simpler substructures called D-holons. A diagnoser is constructed for each D-holon. At any given time, only a subset of the diagnosers are active, and as a result, instead of the entire model of the system, only the models of D-holons associated with active diagnosers are used for diagnosis. Furthermore, a set of sufficient conditions is provided under which the diagnosis process becomes semi-modular. The ozone generation plant under study, consisting of two units, is modeled as an HFSM. It is shown that a proper choice of sensors results in modular diagnosis (one diagnoser for each unit). Following the proposed framework, a hierarchical fault diagnosis system is designed for the plant. It is shown that the proposed approach significantly reduces the complexity of constructing and storing the diagnosis system.",2004,0,
2957,2958,Analysis of asymmetrical faults in power systems using dynamic phasors,"This paper presents application of the dynamic phasor modeling technique to unbalanced polyphase power systems. The proposed technique is a polyphase generalization of the dynamic phasor approach, and it is applicable to nonlinear power system models. In a steady-state, the dynamic phasors reduce to standard phasors from AC circuit theory. The technique produces results that are very close to those obtained from time-domain simulations. Simulations in terms of dynamic phasors typically allow larger integration steps than the standard time-domain formulation. We present simulations of unbalanced faults involving a three-phase synchronous generator connected to an infinite bus through a transmission line, and we demonstrate that models based on dynamic phasors provide very accurate descriptions of observed transients",2000,0,
2958,2959,Effectiveness of a new inductive fault current limiter model in MV networks,"A realistic model for a novel saturable core superconducting FCL (SCFCL) prototype is presented, and incorporated into time-domain power simulation software PSCADTM/EMTDCTM. The present work incorporates non-linear material properties data of the magnetic core with inductance to produce a limiting effect on the line current in real time. The novelty of this core design is the inclusion of a superconducting material as the magnetisation DC coil to saturate the core, instead of using the superconductor directly within the magnetic circuit. The FCL model's accuracy was validated against experimental test results, and its performance analysed by its placement in a UK generic network at MV level. Implementation simulations showed the device could achieve a 50% current clipping capacity, when placed in an MV network. Other standard FCL tests were performed on this model and their results are presented.",2010,0,
2959,2960,On the error modeling of dead reckoned data in a distributed virtual environment,"In this paper, the authors aim to analyze the error of dead reckoned data generated from received data in a discrete temporal axis in a distributed virtual environment (DVE). That is, compared with the data received in continuous time, data acquired in discrete time has a certain degradation or uncertainty of information. Our way of analysis is to introduce a mathematical model of this degradation with regard to the metrics of the temporal interval. We introduced polynomial models for dead reckoning method between frames using parametrics calculated from the data over the last several frames. By employing the method of error analysis of a numerical analysis to the above polynomial models, we formulate theoretical models which approximate the statistical error of dead reckoned data based on parameters such as the update interval and changes in the data. This study enables the discussion on the optimality of the update interval in a DVE using dead reckoning. Finally, we evaluate the adaptability of the theoretical model by conducting simulation experiments generated by pen motion of writing string of letters by human. As a result, we confirm that the proposed theoretical model closely approximates the average error in the simulation",2005,0,
2960,2961,Fault detection and visualization through micron-resolution X-ray imaging,"This paper describes a novel, non-intrusive method for the detection of faults within printed circuit boards (PCBs) and their components using digital imaging and image analysis techniques. High-resolution X-ray imaging systems provide a means to detect and analyze failures and degradations down to micron-levels both within the PCB itself and the components that populate the board. Further, software tools can aid in the analysis of circuit features to determine whether a failure has occurred, and to obtain positive visual confirmation that a failure has occurred. Many PCB and component failures previously undetectable through todaypsilas test methodologies are now detectable using this approach.",2008,0,
2961,2962,Adaptive fault recovery for networked reconfigurable systems,"The device-level size and complexity of reconfigurable architectures makes fault tolerance an important concern in system design. In this paper, we introduce a fully automated fault recovery system for networked systems, which contain FPGAs (field programmable gate arrays). If a fault is detected hat cannot be addressed locally, fault information is transferred to a reconfiguration server. Following design recompilation to avoid the fault, a new FPGA configuration is returned to the remote system and computation is reinitiated. To illustrate the benefit of this approach, we have implemented a complete fault recovery system, which requires no manual intervention. An important part of the system is a timing-driven incremental router for Xilinx Virtex devices. This router is directly interfaced to Xilinx JBits and uses no CAD tools from the standard Xilinx Alliance tool flow. Our completed system has been applied to three benchmark designs and exhibits complete fault recovery in up to 12x less time than the standard incremental Xilinx PAR flow.",2003,0,
2962,2963,Perspective Correction Method for Chinese Document Images,"Perspective distortion often appears in the image documents which were token by digital camera. This phenomenon will lead to recognition errors or failures. Therefore, a new correction algorithm is proposed in this paper for perspective distortion images of Chinese documents. The algorithm makes use of Chinese document image's horizontal characteristics of text line and Chinese characterpsilas features of vertical strokes, to find distortion information and then rectify the perspective image. This method does not require information of imagepsilas edge and paragraphpsilas format. It has a good effect against the incomplete perspective images and irregular paragraph's format. Experiment show that it takes on fast, accurate and high-robust features when using this method to correct perspective distortion in the document images.",2008,0,
2963,2964,Application of ANN to power system fault analysis,"This paper presents the computer architecture development using Artificial Neural Network (ANN) as an approach for predicting fault in a large interconnected transmission system. Transmission line faults can be classified using the bus voltage and line fault current. Monitoring the performance of these two factors are very useful for power system protection devices. The ANN is designed to be incorporated with a matrix based software tool MATLAB Version 6.0, which deals with fault diagnosis in power system. In MATLAB software modules, the balanced and unbalanced fault can be simulated. The data generated from this software are to be used as training and testing sets in the Neural Ware Simulator.",2002,0,
2964,2965,A binary Particle Swarm Optimization approach to fault diagnosis in parallel and distributed systems,"The efficient diagnosis of hardware and software faults in parallel and distributed systems remains a challenge in today's most prolific decentralized environments. System-level fault diagnosis is concerned with the identification of all faulty components among a set of hundreds (or even thousands) of interconnected units, usually by thoroughly examining a collection of test outcomes carried out by the nodes under a specific test model. This task has non-polynomial complexity and can be posed as a combinatorial optimization problem. Here, we apply a binary version of the Particle Swarm Optimization meta-heuristic approach to solve the system-level fault diagnosis problem (BPSO-FD) under the invalidation and comparison diagnosis models. Our method is computationally simpler than those already published in literature and, according to our empirical results, BPSO-FD quickly and reliably identifies the true ensemble of faulty units and scales well for large parallel and distributed systems.",2010,0,
2965,2966,Fault tolerant detection and tracking of multiple sources in WSNs using binary data,"This paper investigates the use of a Wireless Sensor Network for detecting and tracking the location of multiple event sources (targets) using only binary data. Due to the simple nature of the sensor nodes, sensing can be tampered (accidentally or maliciously), resulting in a significant number of sensor nodes reporting erroneous observations. Therefore, it is essential that any event tracking algorithm used in Wireless Sensor Networks (WSNs) exhibits fault tolerant behavior in order to tolerate misbehaving nodes. The main contribution of this paper is the development of a simple and decentralized algorithm that uses the binary observations of the sensors for tracking multiple targets in a fault-tolerant way. Furthermore, tracking is performed in real-time by the alarmed sensor nodes that are elected as leaders, utilizing only information from their neighbors.",2009,0,
2966,2967,Optimized design of a low-pass filter using defected ground structures,"A novel three-pole low-pass filter is designed using low impedance microstrip line and one DGS section in this paper. An equivalent circuit model of a defected ground structure (DGS) is applied to study the characteristics of DGS. Parameters of the model are extracted from the EM simulation results by matching it to a one-pole low-pass filter. The lumped element values of the low-pass filter are optimized in circuit simulator by applying the circuit model of DGS. It is demonstrated that the filter can provide a sharp rate of attenuation in the stop-band as predicted. To further verify this method, a filter using DGS is fabricated measured. The comparison between simulation and measurement confirms the effectiveness of the proposed method.",2005,0,
2967,2968,Applications of fuzzy-logic-wavelet-based techniques for transformers inrush currents identification and power systems faults classification,"The advent of wavelet transforms (WTs) and fuzzy-inference mechanisms (FIMs) with the ability of the first to focus on system transients using short data windows and of the second to map complex and nonlinear power system configurations provide an excellent tool for high speed digital relaying. This work presents a new approach to real-time fault classification in power transmission systems, and identification of power transformers magnetising inrush currents using fuzzy-logic-based multicriteria approach Omar A.S. Youssef [2004, 2003] with a wavelet-based preprocessor stage Omar A.S. Youssef [2003, 2001]. Three inputs, which are functions of the three line currents, are utilised to detect fault types such as LG, LL, LLG as well as magnetising inrush currents. The technique is based on utilising the low-frequency components generated during fault conditions on the power system and/or magnetising inrush currents. These components are extracted using an online wavelet-based preprocessor stage with data window of 16 samples (based on 1.0 kHz sampling rate and 50 Hz power frequency). Generated data from the simulation of an 330 /33Y kV, step-down transformer connected to a 330 kV model power system using EMTP software were used by the MATLAB program to test the performance of the technique as to its speed of response, computational burden and reliability. Results are shown and they indicate that this approach can be used as an effective tool for high-speed digital relaying, and that computational burden is much simpler than the recently postulated fault classification.",2004,0,
2968,2969,"A full-featured, error-resilient, scalable wavelet video codec based on the set partitioning in hierarchical trees (SPIHT) algorithm","Compressed video bitstreams require protection from channel errors in a wireless channel. The 3-D set partitioning in hierarchical trees (SPIHT) coder has proved its efficiency and its real-time capability in the compression of video. A forward-error-correcting (FEC) channel (RCPC) code combined with a single automatic-repeat request (ARQ) proved to be an effective means for protecting the bitstream. There were two problems with this scheme: (1) the noiseless reverse channel ARQ may not be feasible in practice and (2) in the absence of channel coding and ARQ, the decoded sequence was hopelessly corrupted even for relatively clean channels. We eliminate the need for ARQ by making the 3-D SPIHT bitstream more robust and resistant to channel errors. We first break the wavelet transform into a number of spatio-temporal tree blocks which can be encoded and decoded independently by the 3-D SPIHT algorithm. This procedure brings the added benefit of parallelization of the compression and decompression algorithms, and enables implementation of region-based coding. We demonstrate the packetization of the bitstream and the reorganization of these packets to achieve scalability in bit rate and/or resolution in addition to robustness. Then we encode each packet with a channel code. Not only does this protect the integrity of the packets in most cases, but it also allows detection of packet-decoding failures, so that only the cleanly recovered packets are reconstructed. In extensive comparative tests, the reconstructed video is shown to be superior to that of MPEG-2, with the margin of superiority growing substantially as the channel becomes noisier. Furthermore, the parallelization makes possible real-time implementation in hardware and software",2002,0,
2969,2970,Fault Tolerant Network Routing through Software Overlays for Intelligent Power Grids,"Control decisions of intelligent devices in critical infrastructure can have a significant impact on human life and the environment. Ensuring that the appropriate data is available is crucial for making informed decisions. Such considerations are becoming increasingly important in today's cyber-physical systems that combine computational decision making on the cyber side with physical control on the device side. The job of ensuring the timely arrival of data falls onto the network that connects these intelligent devices. This network needs to be fault tolerant. When nodes, devices or communication links fail along a default route of a message from A to B, the underlying hardware and software layers should ensure that this message will actually be delivered as long as alternative routes exist. Existence and discovery of multi-route pathways is essential in ensuring delivery of critical data. In this work, we present methods of developing network topologies of smart devices that will enable multi-route discovery in an intelligent power grid. This will be accomplished through the utilization of software overlays that (1) maintain a digital structure for the physical network and (2) identify new routes in the case of faults.",2010,0,
2970,2971,Single-event-upset-like fault injection: a comprehensive framework,An approach to reproduce radiation ground testing results for the study of microprocessors vulnerability to single event upset (SEU) is described in this paper. Resulting cross-sections fit very well with measured ones.,2005,0,
2971,2972,Implementation of a bug algorithm in the e-puck from a hybrid control viewpoint,"In this paper, the implementation in the e-puck robot of an algorithm to keep going in a trajectory while evading fixed obstacles is presented. A review of some existing algorithms for trajectory tracking with obstacles avoidance is done. Also the basic characteristics of the mobile robot e-puck and the programming environment used to implement the control algorithm and prove the performance of the robot are summarized. By modeling the kinematics of the robot and simulating the implementation of the algorithm, the good performance of the control in both the simulated environment and the real robot in different surroundings are shown. The effects of different environmental factors in the performance of the robot are analyzed. This leads to suggest some algorithm improvements as a matter of future works.",2010,0,
2972,2973,Flux signature analysis: An alternative method for the fault diagnosis of induction machines,"Intensive research efforts have been focused on the signature analysis (SA) to detect electrical and mechanical faults of induction machines. Different signals can be used: voltage, current, flux and power. In the case of current signals (CSA), the interpretation of one-phase current spectrum or the three-phase current space vector spectrum provides direct information on the presence of abnormal conditions. With proper operative condition, a similar interpretation can be obtained by using flux sensor (FSA). In this paper, it is proved that a simple external leakage flux sensor is more efficient than the classical motor current signature analysis (MCSA) to detect both stator and rotor faults in induction machines.",2005,0,
2973,2974,A machine-learning-based fault diagnosis approach for intelligent condition monitoring,"We propose a machine-learning-based fault diagnosis approach for condition monitoring on the constant-speed rotating machines via vibration signals. There are mainly five phases in our approach, i.e., vibration signal measurement, discrete-wavelet-transformation-based preprocessing, feature extraction, base-line encoding, and fuzzy neural network. The advantage of this approach can identify the condition and faults of machine without sufficient diagnosis knowledge. Experimental results have demonstrated this approach is a useful tool for condition monitoring application.",2010,0,
2974,2975,What Determines Appropriate Trust of and Reliance on an Automated Collaborative System? Effects of Error Type and Domain Knowledge,"In this investigation we evaluated the effect of two types of factors that affect human-automation interaction: those specific to the automation (Error Type: miss versus false alarm) and those specific to the human (domain experience, in this study automated farm equipment experience versus no experience). Participants performed a simulated harvesting task and used an obstacle avoidance automated decision aid. The type of unreliability of the automation had a major impact on behavioral reliance as a function of components of the avoidance decision task. The analysis of the effects of domain experience on automation use indicated that those with experience operating agricultural vehicles had different tendencies of reliance. Specifically, participants with experience operating agricultural vehicles were less likely to rely on automated alarms than those without experience. The results of this investigation have important implications for understanding how humans adjust their behavior according to the characteristics of an automated system",2006,0,
2975,2976,Diagnosing Failures in Wireless Networks Using Fault Signatures,"Detection and diagnosis of failures in wireless networks is of crucial importance. It is also a very challenging task, given the myriad of problems that plague present day wireless networks. A host of issues such as software bugs, hardware failures, and environmental factors, can cause performance degradations in wireless networks. As part of this study, we propose a new approach for diagnosing performance degradations in wireless networks, based on the concept of ``fault signatures''. Our goal is to construct signatures for known faults in wireless networks and utilize these to identify particular faults. Via preliminary experiments, we show how these signatures can be generated and how they can help us in diagnosing network faults and distinguishing them from legitimate network events. Unlike most previous approaches, our scheme allows us to identify the root cause of the fault by capturing the state of the network parameters during the occurrence of the fault.",2010,0,
2976,2977,A heuristic method to reduce fault candidates for a speedy fault diagnosis,"In this paper, we present a heuristic method to reduce fault candidates for an efficient fault diagnosis. This paper uses a matching algorithm for the exact fault diagnosis. But the time consumption of a fault diagnosis using the matching algorithm is huge. So, we present a new method to reduce the fault diagnosis time. The method to reduce the time consumption is separated into two different phases which are a pattern comparison and a back-tracing comparison in failing pattern. The proposed method reduces fault candidates by comparing failing patterns with good patterns during critical path tracing process and comparing back-tracing from non-erroneous POs with back-tracing erroneous POs. The proposed method increases the simulation speed than the conventional algorithms. And this method is also applicable to any other fault diagnosis algorithms. Experimental results on ISCAS'85 and ISCAS'89 benchmark circuits show that fault candidate lists are reduced than those of previous diagnosis methods.",2008,0,
2977,2978,Design and implementation of inference engine for fault prognosis in Power System,"According to the device fault and line fault that exist in power system, a componentized inference engine which combines forward inference and backward inference is designed based on the analysis of the categories and characteristics of the fault in this paper. Besides, a knowledge base which supports the inference engine is also designed. The engine and knowledge base are implemented based on the .Net framework and MySQL separately.",2010,0,
2978,2979,Bit-error rate of binary digital modulation schemes in generalized gamma fading channels,"We derive a closed-form expression for the bit-error rate of binary digital modulation schemes in a generalized fading channel that is modeled by the three-parameter generalized gamma distribution. This distribution is very versatile and generalizes or accurately approximates many of the commonly used channel models for multipath, shadow, and composite fading. The result is expressed in terms of Meijer's G-function, which can be easily evaluated numerically.",2005,0,
2979,2980,Characterization of printed solder paste excess and bridge related defects,"Surface mount technology (SMT) involves the printing of solder paste on to printed circuit board (PCB) interconnection pads prior to component placement and reflow soldering. This paper focuses on the solder paste deposition process. With an approximated cause ratio of 50 - 70% of post assembly defects, solder paste deposition represents the most significant cause initiator of the three sub-processes. Paradigmatic cause models, and associated design rules and effects data are extrapolated from academic and industrial literature and formulated into physical models that identify and integrate the process into three discrete solder paste deposition events - i.e. (i) stencil / PCB alignment, (ii) print stroke / aperture filling and (iii) stencil separation / paste transfer. The projectpsilas industrial partners are producers of safety-critical products and have recognised the in-service reliability benefits of electro-mechanical interface elimination when multiple smaller circuit designs are assimilated into one larger printed circuit assembly (PCA). However, increased solder paste deposition related defect rates have been reported with larger PCAs and therefore, print process physical models need to account for size related phenomena.",2008,0,
2980,2981,Fault detection and diagnosis for stand alone system based on induction generator,"This work presents an intelligent stand-alone system that is monitoring and analyzing results with the insertion of faults in this system. To fault detection it is used root mean square value combined with neural network. To ensure the fault signature, it was made different harmonic analysis simulations. The results obtained so far indicated that is possible detect the failure in the stand-alone system, so that, improving the repair of this system.",2009,0,
2981,2982,Orbit drift correction using correctors with ultra-high DAC resolution,At BESSY the planned continuous orbit drift correction could not go into routine operation as originally foreseen: the resolution of the 3 mrad correctors controlled by 16 bit DACs was insufficient and perturbed specific experiments unacceptably. Now a novel 216 bit coarse/fine type I/O board solved this problem while preserving the full dynamic range of the correctors. Permanent correction activity no more deteriorates experimental conditions. A typical orbit definition within +/- 5 m at more than 90% of the BPMs during a day is achieved. Even large perturbations caused by e.g. decaying superconducting wave length shifter currents or residual effects of undulator operations are adequately suppressed,2001,0,
2982,2983,An Improved Spatial Error Concealment Algorithm Based on H.264,"The losses of packets are inevitable when the video is transported over error-prone networks. Error concealment methods can reduce the quality degradation of the received video by masking the effects of such errors. This paper presents a novel spatial error concealment algorithm based on the directional entropy in the available neighboring Macro Blocks (MBs), which can adaptively switch between weighted pixel average (WPA) adopted in H.264 and an improved directional interpolation algorithm to recover the lost MBs. In this work, the proposed algorithm was evaluated on H.264 reference software JM8.6. The illustrative examples demonstrate that the proposed method can achieve better Peak to Signal-to-Noise Ratio (PSNR) performance and visual quality, compared with WPA and the conventional directional interpolation algorithm respectively.",2009,0,
2983,2984,Increasing user's privacy control through flexible Web bug detection,"People usually provide personal information when visiting Web sites, even though they are not aware of this fact. In some cases, the collected data is misused, resulting on user privacy violation. The existing tools which aim at guaranteeing user privacy usually restrict access to personalized services. In this work, we propose the Web bug detector. Upon detecting and informing users about browsing tracking mechanisms which invisibly collect their personal information when visiting sites, it represents an alternative that provides a better control over privacy while allowing personalization. Through experimental results, we demonstrate the applicability of our strategy by applying the detector to a real workload. We found that about 5.37% of user's requests were being tracked by third-party sites.",2005,0,
2984,2985,On application of precision servo mode and fault control strategies to actuator models for structural applications,"The complexity of modern high precision servomechanism systems, which involve not only the tracking function of the servomechanism but also the coordinated system-level control of numerous supporting mechanical, electrical and software subsystems, is placing discrete event controller design into the industrial spotlight. The control of such systems often walks a fine line: autonomy is desirable, because the system is often not conveniently accessible; however, high reliability is also desirable, and the complex software to realize autonomous response is often unacceptable because of the cost and time required for development and verification. Using the framework of an hysteretic actuator employed in the stabilization of a structure under seismic excitation, an approach that is becoming an industry standard for a class of high precision servomechanisms is described, wherein N-squared diagrams are used to model the discrete event portion of the system. The approach practically balances the competing requirements of autonomy and reliability, and has been successfully applied in a timely and cost-effective manner on several complex systems. Additionally, it relates in a straightforward manner to the class of time-varying discrete-time state space systems.",2002,0,
2985,2986,Model for fault tolerance and policy from RM-ODP expressed in UML/OCL,"Fault tolerance (FT) is a topic of major concern in achieving dependable systems, for both real time as well as non real time systems. The paper provides a model of achieving fault tolerance, based on the ISO/ITU Reference Model for Open Distributed Processing (RM-ODP). This reference model provides a system software engineering methodology for fault tolerance, an object based model of fault tolerance, system requirements for achieving fault tolerance in an open manner, modeling constructs and rules to enable a proper system specification of fault tolerance, and business rules in terms of policies to achieve a well formed system specification. All these aspects are discussed at some depth, but the author primarily focuses on how certain behavior can be specified and achieved in an object based system, the constructs of the Unified Modeling Language (UML) and the Object Constraint Language (OCL)",2000,0,
2986,2987,Study for Performance Benchmark of Bank Intermediary Business on High-Performance Fault-Tolerant Computers,"The dominant position of High-Performance Fault-Tolerant (HPFT) computers in security and economics has advanced the studies on the performance benchmarks on the HPFT computers in the specific field, such as bank finance and telecommunication etc. Although TPC (Transaction Processing Council) has proposed some benchmarks models for different OLTP (On-Line Transaction Processing) complex business, such as TPC-C and TPC-E, there is still a lack of the performance benchmark model dedicated to the bank intermediary business on HPFT computers. This paper proposes a Bank Intermediary Business performance benchmark (BIBbench), and gives a solution to test and evaluate this benchmark on HPFT computers for bank intermediary business. In this paper, we present the architecture of BIBbench, defining the structures and attributes of the business model, the database model and the transaction/frame model, and illuminating the workload generation mechanism of the intermediary business system as well. The BIBbench testing environment architecture is also discussed in the paper, as well as the testing solutions and tools. Currently, this BIBbench has been partly implemented on the Oracle 10g database system, and some performance testing experiences based on the BIBbench for HPFT computers have been made.",2010,0,
2987,2988,Localizing Program Errors via Slicing and Reasoning,"Model-based program debugging exploits discrepancies between the program behavior anticipated by a programmer and the program's actual behavior when executed on a set of inputs. From symptoms exhibited by a failing trace, potential culprits in the program canbe localized. However, since the cause of the error is nested deeper into the code than the error itself, localizing errors and correcting the errors are most time consuming hard work. The error trace produced by a model checker may contain more information than it appears. Thus, counter examples can be enough and are indicative for the cause of violation of the property. We present an assumption-based approach to localize the cause of a property violation using reasoning with constraints. In order to reduce the time consuming for error localizing, we first use dynamic program slicing to localize several statements to account for the violation of property. Assumption among these statements is then made to point out which statement(s) is (are) faulty. Some constraints will be introduced from the properties which are model checked for the program. A calculus of reasoning with these constraints is processed under the assumption along a counterexample. If the result may be consistent, the assumption is true (we can localize errors in those statements which the assumption suppose them to be faulty), otherwise, the assumption is wrong and another assumption should be made. Some examples support the applicability and effectiveness of our approach.",2008,0,
2988,2989,"Fault detection, isolation, and recovery using spline tools and differential flatness with application to a magnetic levitation system","This paper discusses fault detection and isolation for continuous-time systems using B-Splines and the notion of differential flatness. The idea is, from the system's flat outputs (which are obtained directly from measurement or from an observer), we algebraically produce every other measured signal, including the inputs. The corresponding signals are then compared. In nominal condition, a measured signal and its counterpart derived from the flat outputs are similar up to noise and the filter's bandwidth. In the occurrence of faults, they are different. We then use this information to signify a fault and to compensate for it. The techniques used to produce signals from the flat outputs, and filter out the noise, are based on a B-Splines parametrisation.",2010,0,
2989,2990,Structural Error Verification in Active Rule-Based Systems using Petri Nets,"A knowledge base needs to be verified so that it works corretly. Up to date, approaches on production rule base verification have been reported adequately. However, active rule base verification cannot be found. In this paper, we primitively define structural errors in active rule base. Then, a verification approach is proposed based on Conditional Colored Petri Nets.",2006,0,
2990,2991,An approach to ultrasonic fault machinery monitoring by using the wigner-ville and choi-williams distributions,The present work shows that the Wigner-Ville and Choi-Williams distributions are useful to determine the proper operation of rotating axes driven by motors and speed controllers. The bearing diagnosis obtained by analyzing real sound samples using phase-array ultrasonic technology will be discussed to highlight the importance of the proposed tools for industrial machinery monitoring.,2008,0,
2991,2992,High-speed error correcting code LSI with throughput of 5 to 48 Gbps,"We proved that the hardware implementation of the proposed code and the new packet synchronization system was effectively realized by using a unique circuit configuration. A three-dimensional size-five coder and decoding-synchronization system was implemented on FPGA. The developed FPGA was applied to a high-speed MPEG communication device, which can transmit a movie signal of 20 Mbps.",2003,0,
2992,2993,Fault Tolerance Virtual Router for Linux Virtual Server,"A growing variety of edge network access devices appear on the marketplace that perform various functions which are meant to complement generic routerspsila capabilities, such as firewalling, intrusion detection, virus scanning, network address translation, traffic shaping, route optimization. Because these edge network access devices are deployed on the critical path between a user site and its Internet service provider. Nowadays the availability of network services is very important for many businesses and it is extremely important that overload or failure of one network can not prevent the normal usage of all other services. This paper focuses on analysis of various protocols and an implementation of RGP (redundant gateway protocol), that can treat the above problem. It runs in user-space for Linux operating system.",2009,0,
2993,2994,FADI: a fault tolerant environment for open distributed computing,"FADI (fault tolerant distributed environment) is a complete programming environment for the reliable execution of distributed application programs. FADI encompasses all aspects of modern fault-tolerant distributed computing. The built-in user-transparent error detection mechanism covers processor node crashes and hardware transient failures. The mechanism also integrates user-assisted error checks into the system failure model. The nucleus non-blocking checkpointing mechanism combined with a novel selective message logging technique delivers an efficient, low-overhead backup and recovery mechanism for distributed processes. FADI also provides a means of remote automatic process allocation on distributed system nodes",2000,0,
2994,2995,Accurate DS-CDMA bit-error probability calculation in Rayleigh fading,A binary direct-sequence spread-spectrum multiple-access system with random sequences in flat Rayleigh fading is considered. A new explicit closed-form expression is obtained for the characteristic function of the multiple-access interference signals. It is shown that the overall error rate can be expressed by a single integral whose integrand is nonnegative and exponentially decaying. Bit-error rates (BERs) are obtained with this expression to any desired accuracy with minimal computational complexity. The dependence of the system BER on the number of transitions in the target user signature chip sequence is explicitly derived. The results are used to examine definitively the validity of three Gaussian approximations and to compare the performances of synchronous systems to asynchronous systems,2002,0,
2995,2996,Use of Faulted Phase Earthing using a custom built earth fault controller,"In order to reduce customer hours lost (CHL) and customer interruptions (CI), the use of Faulted Phase Earthing (FPE) is being considered on the Irish 20 kV distribution system. The operation of this particular FPE system is enabled by the use of a custom built Earth Fault Controller (EFC) that has the ability to detect high impedance faults of up to 12 k??. The EFC can also successfully identify single pole switching events, which have at times caused the mal-operation of existing protection. FPE involves the earthing of a faulted phase during a single line to ground fault. This ensures that the fault site is made safer and that no customers are interrupted during the fault.",2010,0,
2996,2997,A novel fault-dependent-time-settings algorithm for overcurrent relays,"A typical structure of electric power distribution networks is radial or a normally open-loop structure with a single supply path between the high-voltage supply substation and the end-consumers. The selectivity of a protection scheme for the radial network is generally achieved through time-coordination of different protection devices (relays, reclosers and fuses). However, future distribution networks may become more meshed (normally closed-loop) in order to integrate distributed generators (DGs) and more active demand-side response. This will, in turn, require novel protection schemes since the current time-coordinated protection which assumes radial network structure may no longer be effective. Furthermore, it is also recognized that today's practice of automatically disconnecting DGs when a fault occurs in their vicinity may cause un-necessary generation shortages. It is, instead, desired to adjust protection so that a DG unit remains connected to the system when nearby faults occur as long as there are no related safety problems. This paper introduces a novel fault-dependent time settings algorithm for overcurrent relays which is capable of overcoming these problems. The implementation of the proposed algorithm on the existing protection scheme is enabled by using communications among the relays. The proposed algorithm ensured a reduced relay tripping time in the primary protection zone by enhancing the selectivity of the existing protection scheme. Consequently, it becomes possible to operate distributed generators during times when nearby faults occur. While the use of the proposed algorithm is illustrated in the distribution network, the same algorithm is also applicable to the transmission networks.",2009,0,
2997,2998,A fault-tolerant protocol for energy-efficient permutation routing in wireless networks,"A wireless network (WN) is a distributed system where each node is a small hand-held commodity device called a station. Wireless sensor networks have received increasing interest in recent years due to their usage in monitoring and data collection in a wide variety of environments like remote geographic locations, industrial plants, toxic locations, or even office buildings. Two of the most important issues related to a WN are their energy constraints and their potential for developing faults. A station is usually powered by a battery which cannot be recharged while on a mission. Hence, any protocol run by a WN should be energy-efficient. Moreover, it is possible that all stations deployed as part of a WN may not work perfectly. Hence, any protocol designed for a WN should work well even when some of the stations are faulty. The permutation routing problem is an abstraction of many routing problems in a wireless network. In an instance of the permutation routing problem, each of the p-stations in the network is the sender and recipient of n/p packets. The task is to route the packets to their correct destinations. We consider the permutation routing problem in a single-hop wireless network, where each station is within the transmission range of all other stations. We design a protocol for permutation routing on a WN which is both energy efficient and fault tolerant. We present both theoretical estimates and extensive simulation results to show that our protocol is efficient in terms of energy expenditure at each node even when some of the nodes are faulty. Moreover, we show that our protocol is also efficient for the unbalanced permutation routing problem when each station is the sender and recipient of an unequal number of packets.",2005,0,
2998,2999,Multiple hypotheses and their credibility in on-line fault diagnosis,"In this study, a new method that handles multiple hypotheses is presented for fault diagnosis using sequence of event recorders (SERs). To quantify the certainty of hypotheses, a method to calculate their credibility is provided. The proposed techniques are integrated in a generalized alarm analysis module (GAAM) and have been tested with numerous scenarios from the Italian power system",2001,0,
2999,3000,Development of fault detection and reporting for non-central maintenance aircraft,"This paper describes how real-time faults can be automatically detected in Boeing 737 airplanes without significant hardware or software modifications, or potentially expensive system re-certification by employing a novel approach to Airplane Conditioning and Monitoring System (ACMS) usage. The ACMS is a function of the Digital Flight Data Acquisition Unit (DFDAU), which also collects aircraft parameters and transmits them to the Flight Data Recorder (FDR). The DFDAU receives digital and analog data from various airplane subsystems, which is also available to the ACMS. Exploiting customized ACMS software allows airline operators to specify collection and processing of various aircraft parameters for flight data monitoring, maintenance, and operational efficiency trending. Employing a rigorous systems engineering approach with detailed signal analysis, fault detection algorithms are created for software implementation within the ACMS to support ground-based reporting systems. To date, over 160 algorithms are in development based upon the existing Fault Reporting and Fault Isolation Manual (FRM/FIM) structure and availability of system signals for individual faults. Following successful field-testing and implementation, 737 airplane customers have access to a state of fault detection automation not previously available on aircraft without central maintenance monitoring.",2010,0,
3000,3001,Efficient Diagnosis of Scan Chains with Single Stuck-at Faults,"Locating the scan chain fault is a critical step for IC manufacturers to analyze failure for yield improvement. In this paper, we propose a diagnosis scheme to locate the single stuck-at fault in scan chains. Our diagnosis scheme is an improved design to a previously proposed scheme which can diagnose the output of each cell flip-flop in the scan chain. With our scheme, not only the output of each cell flip-flop can be diagnosed, but also the inverse output of each cell flip-flop and the serial input of the scan chain as well. Our proposed diagnosis scheme is efficient and takes (4n+6) clock cycles in the worst case for an n-bit scan chain.",2009,0,
3001,3002,Effects of time synchronization errors in GNSS-aided INS,"The effects of time synchronization errors in a GNSS-aided inertial navigation system (INS) are studied in terms of the increased error covariance of the state vector. Expressions for evaluating the error covariance of the navigation state vector-given the vehicle trajectory and the model of the INS error dynamics-are derived. Two different cases are studied in some detail. The first case considers a navigation system in which the timing error is not included in the integration filter. This leads to a system with an increased error covariance and a bias in the estimated forward acceleration. In the second case, a parameterization of the timing error is included as a part of the estimation problem in the data integration. Simulation results show that by including the timing error in the estimation problem, almost perfect time synchronization is obtained and the bias in the forward acceleration is removed.",2008,0,
3002,3003,A method for diagnosing resistive open faults with considering adjacent lines,"It is believed that resistive open faults can cause small delay defects at wires, contacts, and/or vias of a circuit. However, it remains to be elucidated whether any methods could diagnose resistive open faults. We propose a method for diagnosing resistive open faults by using a diagnostic delay fault simulation with the minimum detectable delay fault size. We also introduce a fault excitation function for the resistive open fault to improve the accuracy of the diagnostic result. The fault excitation function for the resistive open fault can determine a size of an additional delay at a faulty line with considering the effect of the adjacent lines. We demonstrated that the proposed method is capable of identifying fault locations for the resistive open fault with a small computation cost.",2010,0,
3003,3004,AVF Stressmark: Towards an Automated Methodology for Bounding the Worst-Case Vulnerability to Soft Errors,"Soft error reliability is increasingly becoming a first-order design concern for microprocessors, as a result of higher transistor counts, shrinking device geometries and lowering of operating voltages. It is important for designers to be able to validate whether the Soft Error Rate (SER) targets of their design have been met, and help end users select the processor best suited to their reliability goals. The knowledge of the observable worst-case SER allows designers to select their design point, and bound the worst-case vulnerability at that design point. We highlight the lack of a methodology for evaluation of the overall observable worst-case SER. Hence, there is a clear need for a so called stress mark that can demonstrably approach the observable worst-case SER. The worst-case thus obtained can be used to identify reliability bottlenecks, validate safety margins used for reliability design and identify inadequacies in benchmark suites used to evaluate SER. Starting from a comprehensive study about how micro architecture-dependent program characteristics affect soft errors, we derive the insights needed to develop an automated and flexible methodology for generating a stress mark that approaches the maximum SER of an out-of-order processor. We demonstrate how our methodology enables architects to quantify the impact of SER-mitigation mechanisms on the worst-case SER of the processor. The stress mark achieves 1.4X higher SER in the core, 2.5X higher SER in DL1 and DTLB, and 1.5X higher SER in L2 as compared to the highest SER induced by SPEC CPU2006 and MiBench programs.",2010,0,
3004,3005,Probabilistic Algebraic Analysis of Fault Trees With Priority Dynamic Gates and Repeated Events,"This paper focuses on a sub-class of Dynamic Fault Trees (DFTs), called Priority Dynamic Fault Trees (PDFTs), containing only static gates, and Priority Dynamic Gates (Priority-AND, and Functional Dependency) for which a priority relation among the input nodes completely determines the output behavior. We define events as temporal variables, and we show that, by adding to the usual Boolean operators new temporal operators denoted BEFORE and SIMULTANEOUS, it is possible to derive the structure function of the Top Event with any cascade of Priority Dynamic Gates, and repetition of basic events. A set of theorems are provided to express the structure function in a sum-of-product canonical form, where each product represents a set of cut sequences for the system. We finally show through some examples that the canonical form can be exploited to determine directly and algebraically the failure probability of the Top Event of the PDFT without resorting to the corresponding Markov model. The advantage of the approach is that it provides a complete qualitative description of the system, and that any failure distribution can be accommodated.",2010,0,
3005,3006,Probabilistic Approach to Fault Detection in Discrete Event Systems,"Fault diagnosis is performed by an external observer/diagnoser that functions as a finite state machine and which has access to the input sequence applied to the system but has only limited access to the system state or output. The observer/diagnoser is only able to obtain partial information regarding the state of the given system at intermittent time intervals that are determined by certain synchronizing conditions between the system and the observer/diagnoser. By adopting a probabilistic framework, mathematical analysis has been made to optimally choose the synchronizing conditions and develop adaptive strategies that achieve a low probability of aliasing, i.e., a low probability that the external observer/diagnoser incorrectly declares the system as fault-free",2007,0,
3006,3007,A study of the effects of transient fault injection into the VHDL model of a fault-tolerant microcomputer system,"This work presents a campaign of fault injection to validate the dependability of a fault tolerant microcomputer system. The system is duplex with cold stand-by sparing, parity detection and a watchdog timer. The faults have been injected on a chip-level VHDL model, using an injection tool designed for this purpose. We have carried out a set of injection experiments (with 3000 injections each), injecting transient faults of types stuck-at, bit-flip, indetermination and delay on both the signals and variables of the system, running two different workloads. We have analysed the pathology of the propagated errors, measured their latency, and calculated both detection and recovery coverage. For instance, system detection coverages (including non-effective errors) up to 98%, and system recovery coverage up to 94% have been obtained for short transient faults",2000,0,
3007,3008,A method for computing error vector magnitude in GSM EDGE systems-simulation results,"This paper describes the error vector magnitude (EVM) as it is specified for a GSM EDGE (8-PSK) system and presents a method of its derivation. Simulation results of the algorithm applied to a nonlinear power amplifier are shown, and compared to the data obtained with an alternative commercial implementation. The validity of the current EVM proposal as a system linearity figure of merit is also discussed.",2001,0,
3008,3009,"Fault-tolerant hard-real-time communication of dynamically reconfigurable, distributed embedded systems","Building up hard-real-time networks for distributed systems with dynamical structures poses severe design challenges. As the set of nodes varies, the use of protocols, which request a complete setup with every modification, becomes prohibitive. This paper presents a new concept (called TrailCable) that aims at increasing the flexibility in building up hard-real-time communication systems. The concept consists of an extendable network made up of point-to-point connections, with each node acting as a router with an integrated communication scheduler. The protocol offers the possibility to use unallocated bandwidth for periodic data transmissions. Moreover, functionalities that guarantee the communication channels to be fault-tolerant are also included. Through the collection of these properties, the protocol described in this paper is well adapted to highly dynamic, distributed real-time systems. As a case study, this new communication protocol is applied to an innovative railway system, the so-called RailCab.",2005,0,
3009,3010,Software release control using defect based quality estimation,"We describe two case studies to investigate the application of a state variable model to control the system test phase of software products. The model consists of two components: a feedback control portion and a model parameter estimation portion. The focus in this study is on the assessment of the goodness of the estimates and predictions of the model parameters and their utility in the management of the system test phase. Two large network management applications developed and tested at Sun Microsystems served as the subjects in these studies. Unlike the release of products based on marketing or deadline pressure, estimates of the number of residual defects are used to control the quality of the product being released. The estimates of the number of defects in the application when the test phase began and at the current checkpoint are obtained. In addition a prediction is made regarding the reduction in the number of remaining defects over the remaining period. The estimates and predictions assist the management in planning the test phase and allow inferring the level of customer support needed subsequent to product release. The results of both case studies are satisfactory and, when viewed in light of other studies conducted at Sun Microsystems, show the applicability of the state variable model to the management of the software test process.",2004,0,
3010,3011,Parallelization of the nearest-neighbour search and the cross-validation error evaluation for the kernel weighted k-nn algorithm applied to large data dets in matlab,The kernel weighted k-nearest neighbours (KWKNN) algorithm is an efficient kernel regression method that achieves competitive results with lower computational complexity than least-squares support vector machines and Gaussian processes. This paper presents the parallel implementation on a cluster platform of the sequential KWKNN implemented in Matlab. This implies both the parallelization of the k nearest-neighbour search and the evaluation of the cross-validation error on a large distributed data set. The results demonstrate the good performances of the implementation.,2009,0,
3011,3012,Fault-tolerant DSM on the SOME-Bus multiprocessor architecture with message combining,"Summary form only given. We present a broadcast-based architecture called the SOME-Bus interconnection network, which directly links processor nodes without contention, and can efficiently interconnect several hundred nodes. Each node has a dedicated output channel and an array of receivers, with one receiver dedicated to every other node's output channel. The SOME-Bus eliminates the need for global arbitration and provides bandwidth that scales directly with the number of nodes in the system. Under the distributed shared memory (DSM) paradigm, the SOME-bus allows strong integration of the transmitter, receiver and cache controller hardware to produce a highly integrated system-wide cache coherence mechanism. Backward error recovery fault-tolerance techniques can exploit DSM data replication and SOME-Bus broadcasts with little additional network traffic and corresponding performance degradation. Simulation results show that in the SOME-Bus architecture under the DSM paradigm, messages tend to wait at the node output network interface. Consequently, to minimize the effect of increased network traffic, messages can be combined at the node output queue to form a new message containing the payloads of all original messages. We use simulation to examine the effect of such message combining on the performance of SOME-Bus, in the presence of additional traffic due to fault tolerance, and we compare it to similar performance measures of a reduced SOME-Bus network where two nodes share one channel.",2004,0,
3012,3013,A Generic Fault Countermeasure Providing Data and Program Flow Integrity,"So far many software countermeasures against fault attacks have been proposed. However, most of them are tailored to a specific cryptographic algorithm or focus on securing the processed data only. In this work we present a generic and elegant approach by using a highly fault secure algebraic structure. This structure is compatible to finite fields and rings and preserves its error detection property throughout addition and multiplication. Additionally, we introduce a method to generate a fingerprint of the instruction sequence. Thus, it is possible to check the result for data corruption as well as for modifications in the program flow. This is even possible if the order of the instructions is randomized. Furthermore, the properties of the countermeasure allow the deployment of error detection as well as error diffusion. We point out that the overhead for the calculations and for the error checking within this structure is reasonable and that the transformations are efficient. In addition we discuss how our approach increases the security in various kinds of fault scenarios.",2008,0,
3013,3014,Automatic road feature detection and correlation for the correction of consumer satellite navigation system mapping,"This paper presents a novel approach for the use of on-vehicle video analysis aimed at the verification and correction of consumer satellite navigation system mapping information. The proposed system automatically detects road and environment features (e.g. flyover bridges, road junctions, traffic lights and road signs) for real-time comparison to information available from corresponding navigation mapping. This can be used both for secondary feature-based localization of vehicle position and the verification of roadway mapping information against the true environment.",2010,0,
3014,3015,Teaching the Art of Fault Diagnosis in Electronics by a Virtual Learning Environment,"A virtual learning environment (VLE) to improve understanding of simple faul tfinding was created from a series of Web pages, an online quiz with automated marking, and a local-area-network-based simulator. It was tested on 57 first-year students (in 2002) and 69 students in 2003, taking a module in engineering design in electrical engineering in which a battery charger was designed and constructed. The results indicate that there was better than 100% improvement in the number of working battery chargers in both tested years. In addition, the students who used the VLE produced more working chargers and were better able to identify circuit blocks than those that did not. The learning approach is described by the adaptive character of thought cited in the present paper.",2005,0,
3015,3016,Modeling and performance considerations for automated fault isolation in complex systems,"The purpose of this paper is to document the modeling considerations and performance metrics that were examined in the development of a large-scale Fault Detection, Isolation and Recovery (FDIR) system. The FDIR system is envisioned to perform health management functions for both a launch vehicle and the ground systems that support the vehicle during checkout and launch countdown by using a suite of complimentary software tools that alert operators to anomalies and failures in real-time. The FDIR team members developed a set of operational requirements for the models that would be used for fault isolation and worked closely with the vendor of the software tools selected for fault isolation to ensure that the software was able to meet the requirements. Once the requirements were established, example models of sufficient complexity were used to test the performance of the software. The results of the performance testing demonstrated the need for enhancements to the software in order to meet the demands of the full-scale ground and vehicle FDIR system. The paper highlights the importance of the development of operational requirements and preliminary performance testing as a strategy for identifying deficiencies in highly scalable systems and rectifying those deficiencies before they imperil the success of the project.",2010,0,
3016,3017,ReStore: Symptom-Based Soft Error Detection in Microprocessors,"Device scaling and large-scale integration have led to growing concerns about soft errors in microprocessors. To date, in all but the most demanding applications, implementing parity and ECC for caches and other large, regular SRAM structures have been sufficient to stem the growing soft error tide. This will not be the case for long and questions remain as to the best way to detect and recover from soft errors in the remainder of the processor - in particular, the less structured execution core. In this work, we propose the ReStore architecture, which leverages existing performance enhancing checkpointing hardware to recover from soft error events in a low cost fashion. Error detection in the ReStore architecture is novel: symptoms that hint at the presence of soft errors trigger restoration of a previous checkpoint. Example symptoms include exceptions, control flow misspeculations, and cache or translation look-aside buffer misses. Compared to conventional soft error detection via full replication, the ReStore framework incurs little overhead, but sacrifices some amount of error coverage. These attributes make it an ideal means to provide very cost effective error coverage for processor applications that can tolerate a nonzero, but small, soft error failure rate. Our evaluation of an example ReStore implementation exhibits a 2times increase in MTBF (mean time between failures) over a standard pipeline with minimal hardware and performance overheads. The MTBF increases by 20times if ReStore is coupled with protection for certain particularly vulnerable pipeline structures",2006,0,
3017,3018,Research on automatic detection for defect on bearing cylindrical surface,"At present, it is still by using manual method to detect the defects on micro bearing surface. The method is laborious and time consuming. Moreover, it has a low efficiency and high miss-detection rate. Due to the fact, an on-line automatic detection system is developed using linear CCD. The proposed system is composed of three subsystems: the detection environment setting subsystem, the automatic detection subsystem, and the data management subsystem. In order to lead the above subsystems to cooperate with each other, control software is developed with LabVIEW 8.5 as a platform. Experimental results indicate that the system realizes the predefined functions and caters to the requirements of stability, real-time performance, and accuracy. Thus it can be applied for actual production.",2010,0,
3018,3019,Reliability modeling incorporating error processes for Internet-distributed software,"The paper proposes several improvements to conventional software reliability growth models (SRGMs) to describe actual software development processes by eliminating an unrealistic assumption that detected errors are immediately corrected. A key part of the proposed models is the ""delay-effect factor"", which measures the expected time lag in correcting the detected faults during software development. To establish the proposed model, we first determine the delay-effect factor to be included In the actual correction process. For the conventional SRGMs, the delay-effect factor is basically non-decreasing. This means that the delayed effect becomes more significant as time moves forward. Since this phenomenon may not be reasonable for some applications, we adopt a bell-shaped curve to reflect the human learning process in our proposed model. Experiments on a real data set for Internet-distributed software has been performed, and the results show that the proposed new model gives better performance in estimating the number of initial faults than previous approaches",2001,0,
3019,3020,Fault Detection System Activated by Failure Information,"We propose a fault detection system activated by an application when the application recognizes the occurrence of a failure, in order to realize self managing systems that automatically find the source of a failure. In existing detection systems, there are three issues for constructing self managing applications: i) the detection results are not sent to the applications, ii) they can not identify the source failure from all of the detected failures, and iii) configuring the detection system for networked system is hard work. For overcoming these issues, the proposed system takes three approaches: i) the system receives failure information from an application and returns a result set to the application, ii) the system identifies the source failure using relationships among errors, and Hi) the system obtains information of the monitored system from a database. The relationship is expressed by a tree. This tree is called error relationship tree. The database provides information which are system entities such as hardware devices, software object, and network topology. When the proposed system starts looking for the source of a failure, causal relations from an error relation tree are referred to, and the correspondence of error definitions and actual objects is derived using the database. We show the design of the detection operation activated by the failure information and the architecture of the proposed system.",2007,0,
3020,3021,Pose measurement and tracking system for motion-correction of unrestrained small animal PET/SPECT imaging,"An optical landmark-based pose measurement and tracking system is under development to provide in-scan animal position data for a new SPECT imaging system for unrestrained laboratory animals. The animal position and orientation data provides motion correction during image reconstruction. This paper describes new developments and progress using landmark markers placed on the animal along with strobed infrared lighting with improvements in accuracy for the extraction of head feature positions during motion. A stereo infrared imaging approach acquires images of the markers through a transparent enclosure, segments the markers, corrects for distortion and rejects unwanted reflections. Software estimates intrinsic as well as extrinsic camera calibration parameters and provides a full six degree-of-freedom (DOF) camera-to-camera calibration. A robust stereo point correspondence and 3D measurement calculation based on the fundamental matrix provides the pose at camera frame rates. Experimental testing has been conducted on calibrated fixtures with six DOF measurement capabilities as well as on live laboratory mice. Results show significantly improved accuracy and repeatability of the measurements. The live mouse results have demonstrated that reliable, accurate tracking measurements can be consistently achieved for the full SPECT image acquisition.",2003,0,
3021,3022,Multiresolution sensor fusion approach to PCB fault detection and isolation,"This paper describes a novel approach to printed circuit board (PCB) testing that fuses the products of individual, non-traditional sensors to draw conclusions regarding overall PCB health and performance. This approach supplements existing parametric test capabilities with the inclusion of sensors for electromagnetic emissions, laser Doppler vibrometry, off-gassing and material parameters, and X-ray and Terahertz spectral images of the PCB. This approach lends itself to the detection and prediction of entire classes of anomalies, degraded performance, and failures that are not detectable using current automatic test equipment (ATE) or other test devices performing end-to-end diagnostic testing of individual signal parameters. This greater performance comes with a smaller price tag in terms of non-recurring development and recurring maintenance costs over currently existing test program sets. The complexities of interfacing diverse and unique sensor technologies with the PCB are discussed from both the hardware and software perspective. Issues pertaining to creating a whole-PCB interface, not just at the card-edge connectors, are addressed. In addition, we discuss methods of integrating and interpreting the unique software inputs obtained from the various sensors to determine the existence of anomalies that may be indicative of existing or pending failures within the PCB. Indications of how these new sensor technologies may comprise future test systems, as well as their retrofit into existing test systems, will also be provided.",2008,0,
3022,3023,Robust Detection of Faults in Frequency Control Loops,"This paper presents a robust approach to real-time detection of faults in the load-frequency control loop of interconnected power systems. The detection of faults takes place under different operating conditions, in the presence of modeling uncertainties, unknown changes in the load demand, and other external disturbances, such as plant and sensor noise. Although the approach is applicable to N-area systems, a two-area interconnected power system example is considered for simplicity",2007,0,
3023,3024,Wavelet neural network method for fault diagnosis of push-pull circuits,"A wavelet neural network method for fault diagnosis of push-pull circuits is presented. Firstly, output voltage signals under faulty conditions are obtained with simulation. Then wavelet coefficients of output voltage signals are gained by Daubechies wavelet decomposition, and faulty feature vectors are extracted from coefficients. After training the networks by faulty feature vectors, the wavelet neural networks model of the circuit fault diagnosis system is built. The simulation result shows the fault diagnosis method of the push-pull circuits with wavelet neural network is effective.",2005,0,
3024,3025,Finding Bugs in Web Applications Using Dynamic Test Generation and Explicit-State Model Checking,"Web script crashes and malformed dynamically generated webpages are common errors, and they seriously impact the usability of Web applications. Current tools for webpage validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic Web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a Web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 673 faults in six PHP Web applications.",2010,0,
3025,3026,The challenge of accurate software project status reporting: a two stage model incorporating status errors and reporting bias,"Software project managers perceive and report about the project's status. Recognizing that their status perceptions might be wrong and that they may not faithfully report what they believe leads to a natural question - how different is the true software project status from the reported status? In this paper, we construct a two-stage model which accounts for project manager errors in perception and bias that might be applied before reporting the project's status to executives. We call the combined effect of errors in perception and bias ""project statics distortion"". The probabilistic model has its roots in information theory and uses the discrete project status from traffic-light reporting. The true states of projects of varying risk were elicited from a panel of five experts, and these formed the model input. Key findings suggest that executives should be skeptical of favorable status reports, and that, for higher-risk projects, executives should concentrate on reducing bias if they are to improve the accuracy of project reporting.",2001,0,
3026,3027,Fan Fault Diagnosis System Based on Virtual Prototyping Technology,"One fault diagnosis system is proposed to monitor the fan system condition based on virtual prototyping technology. According to the real fan system structure and its foundation condition, the three-dimensional model is built. Under virtual environment, the components are assembled, and the constraints and driver are added. After validate the model, the typical mechanical faults of fans (unbalance, misalignment etc.) are simulated. By virtual sensors, the acquired fan data are used to establish the corresponding fan condition database. The real fan condition can be identified through comparing measured signal with virtual data. Pattern recognition and early prediction are feasible for dynamic operation of the equipment during the process of incipient fault. The contribution of this paper is to propose a new approach to equipment maintenance that makes the maintenance process ""low consumption and high efficiency"".",2008,0,
3027,3028,A Theoretical Framework for Probability Coefficients: A New Methodology for Fault Detection,"A new spectral method that eliminates the need of inner product evaluations in determination of signature of a combinational circuit realizing given Boolean function is described. The signature is obtained using probability coefficients of the function instead of conventional spectral signature. Theoretical relations for achievable computational advantage in terms of required additions in computing all 2n probability coefficients of ""n"" variable function have been developed. It is shown that for n ges 5, only 50% additions are needed to compute all probability coefficients as compared to spectral coefficients. The fault detection techniques based on spectral signature can be used with probability signature also to offer computational advantage.",2008,0,
3028,3029,Iterative (TURBO) IQ Imbalance Estimation and Correction in BICM-ID for Flat Fading Channels,"TURBO principle has been exploited gainfully to implement many receiver functions. RF front-end impairments are a serious issue in high spectral efficient applications. IQ imbalance is one of these impairments and in this work, we study the issue of IQ imbalance correction using baseband signal processing techniques. In particular, we propose an estimation technique based on EM algorithm. Such a technique is developed rather intuitively for the case of a Bit Interleaved Coded modulation - Iterative Detection (BICM-ID) receiver for burst mode communications. The resulting TURBO IQ Decorrelator is embedded in the BICM-ID loop, is blind in the sense that it does not require any training symbols or tones. Performance is simulated for 64QAM under flat fading channel conditions.",2007,0,
3029,3030,Fault-tolerance abilities implementation with spare cells in bio-inspired hardware systems,"Network communication algorithms development is presented in the paper, with the purpose to implement bio-inspired hardware systems which exhibit the abilities of living organisms, such as: evolution capabilities, self-healing and fault-tolerance. In the first steps of these research efforts an embryonic system with bi-dimensional FPGA-based artificial cell network is designed and tested through careful computer-aided simulations. Two specially developed algorithms were implemented in the network communication strategy, in order to avoid physical faults and errors in the laboratory experimented VLSI hardware architecture. The basic challenge of all these experiments is to develop embryonic systems with fault-tolerant and self-healing properties, as main hardware structures in a large scale high security process control and industrial applications.",2009,0,
3030,3031,Fabrication error in resonant frequency of microstrip antenna,"In order to fabricate microstrip antenna with high precision for micromechanical application, we calculated the error in the resonant frequency of a patch antenna. Accuracy of the obtained results is compared with the experimental data. An accurate analysis allows success on first fabrication",2001,0,
3031,3032,Coverage of a microarchitecture-level fault check regimen in a superscalar processor,"Conventional processor fault tolerance based on time/space redundancy is robust but prohibitively expensive for commodity processors. This paper explores an unconventional approach to designing a cost-effective fault-tolerant superscalar processor. The idea is to engage a regimen of microarchitecture-level fault checks. A few simple microarchitecture-level fault checks can detect many arbitrary faults in large units, by observing microarchitecture-level behavior and anomalies in this behavior. Previously, we separately proposed checks for the fetch and decode stages, rename stage, and issue stage of a contemporary superscalar processor. While each piece hinted at the possibility of a complete regimen - for an overall fault-tolerant superscalar processor - this totality was not explored. This paper provides the culmination by building a full regimen into a superscalar processor. We show for the first time that the regimen-based approach provides substantial coverage of an entire superscalar processor. Analysis reveals vulnerable areas which should be the focus for regimen additions.",2008,0,
3032,3033,Gazing estimation and correction from elliptical features of one iris,"The accuracy of eye gaze estimation by image information is affected by several objective factors, including the image resolution, anatomical structure of eye, posture change, etc. Especially, the irregular movements of head and eye are the main problem and key technology being researched. We describe an effective way of estimating the eye gazing from the elliptical features of one iris under the conditions without auxiliary source, head fixing equipment or multiple-camera. Firstly, we give the preliminary estimations of the gazing direction and then obtain the vectors describing translation and rotation of eyeball movement using central projection on the cross section passing through the line-of-sight, which avoids the complex computations involved in known methods. We also disambiguate the solution on the basis of the experimental findings. Secondly, the error correction is carried on the BP neural network trained by a sample collection of the translation and rotation vectors. In our simulations, we achieve an accuracy of 0.8 on the test images which are different from the training images. The result is found to be better than that of the existing non-intrusive method with single-camera. The performance of the algorithm proves that the proposed method has excellent generalized abilities.",2010,0,
3033,3034,Evolutionary generation of test data for multiple paths coverage with faults detection,"The aim of software testing is to find faults in the program under test. Generating test data which can reveal faults is the core issue. Although existing methods of path-oriented testing can generate test data which traverse target paths, they cannot guarantee that the data find the faults in the program. In this paper, we transform the problem into a multi-objective optimization problem with constrains and propose a method of evolutionary generation of test data for multiple paths coverage with faults detection. First, we establish the mathematical model of this problem and then a strategy based on multi-objective genetic algorithms is given. Finally we apply the proposed method in some programs under test and the experimental results validate that our method can find specified faults effectively. Compared with other methods of test data generation for multiple paths coverage, our method has greater advantage in faults detection and testing efficiency.",2010,0,
3034,3035,Control of the matrix converter based WECS for fault ride-through enhancement,"Due to steadily increased in wind power penetration, regulatory standards for grid interconnection have evolved to require that wind generation systems ride-through disturbances such as faults and support the grid during such events. Keeping the converter online during and after short-circuit faults, and guaranteeing the actual standards of the converter connected to the grid, is becoming a very critical issue. From these goals, in this paper, an optimal control of matrix converter (MC) based wind turbine have been developed, where adaptive fuzzy logic controls along with improved SVPWM switching have been used extensively to ensure that current levels remain within design limits, even at greatly reduced voltage levels, thus enhancing the fault ride-through capability.",2010,0,
3035,3036,The application of multi-function interface MVB NIC in distributed locomotive fault detecting and recording system,"Locomotive condition monitoring and fault diagnosis system is an important component of modern locomotive, it needs a reliable, high-speed communication network to ensure that the system's reliable operation in the complex locomotive environment. The Controller Area Network (CAN) used in the existing distributed locomotive fault detecting and recording system is not suitable for vehicles bus, so the paper brought forward the scheme using the Multifunction Vehicle Bus (MVB). Firstly, it described the alteration of system structure and operating principle key design concepts in detail, next designed the multi-function interface MVB NIC using SOPC (system on a programmable chip) technology, given the realization of hardware and software, ultimately proceeded the network test in the lab, and verified the correctness and feasibility of the design. The improved network has farther transmission distance, higher rates, better reliability and real-time.",2010,0,
3036,3037,A desktop environment for assessment of fault diagnosis based fault tolerant flight control laws,"We present a simulation based software environment conceived to allow an easy assessment of fault diagnosis based fault tolerant control techniques. The new tool is primary intended for the development of advanced flight control applications with fault accommodation abilities, where the requirements for increased autonomy and safety play a premier role.",2008,0,
3037,3038,Error Reporting Logic,"When a system fails to meet its specification, it can be difficult to find the source of the error and determine how to fix it. In this paper, we introduce error reporting logic (ERL), an algorithm and tool that produces succinct explanations for why a target system violates a specification expressed in first order predicate logic. ERL analyzes the specification to determine which parts contributed to the failure, and it displays an error message specific to those parts. Additionally, ERL uses a heuristic to determine which object in the target system is responsible for the error. Results from a small user study suggest that the combination of a more focused error message and a responsible object for the error helps users to find the failure in the system more effectively. The study also yielded insights into how the users find and fix errors that may guide future research.",2008,0,
3038,3039,Error-Related EEG Potentials Generated During Simulated BrainComputer Interaction,"Brain-computer interfaces (BCIs) are prone to errors in the recognition of subject's intent. An elegant approach to improve the accuracy of BCIs consists in a verification procedure directly based on the presence of error-related potentials (ErrP) in the electroencephalogram (EEG) recorded right after the occurrence of an error. Several studies show the presence of ErrP in typical choice reaction tasks. However, in the context of a BCI, the central question is: ldquoAre ErrP also elicited when the error is made by the interface during the recognition of the subject's intent?rdquo We have thus explored whether ErrP also follow a feedback indicating incorrect responses of the simulated BCI interface. Five healthy volunteer subjects participated in a new human-robot interaction experiment, which seem to confirm the previously reported presence of a new kind of ErrP. However, in order to exploit these ErrP, we need to detect them in each single trial using a short window following the feedback associated to the response of the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 83.5% and 79.2%, respectively, using a classifier built with data recorded up to three months earlier.",2008,0,
3039,3040,Effects of microstructural defects in multilayer LTCC stripline,"This paper proposes novel stripline models including embedded pores and sharpened conductor edges, which are commonly introduced during the multilayer low-temperature cofired-ceramic (LTCC) process. This model enables designers to obtain conductivity and tan of the stripline that are difficult to obtain using the experimental methods from arbitrary frequencies. This paper confirms that the proposed models are appropriate for LTCC striplines by comparing the simulated results with the experimental results. We found that embedded pores contributed to increase in unloaded quality factor (Qu) and characteristic impedance in the range of 5% to 6% while effective <sub>r</sub> decreased in the range of 11%. Sharpened edges contributed to maximum peak in Qu and decreased characteristic impedance. These models will contribute to precision design of the future LTCC striplines.",2006,0,
3040,3041,Fault-tolerant techniques for Ambient Intelligent distributed systems,"Ambient Intelligent Systems provide an unexplored hardware platform for executing distributed applications under strict energy constraints. These systems must respond quickly to changes in user behavior or environmental conditions and must provide high availability and fault-tolerance under given quality constraints. These systems will necessitate fault-tolerance to be built into applications. One way to provide such fault-tolerance is to employ the use of redundancy. Hundreds of computational devices will be available in deeply networked ambient intelligent systems, providing opportunities to exploit node redundancy to increase application lifetime or improve quality of results if it drops below a threshold. Pre-copying with remote execution is proposed as a novel, alternative technique of code migration to enhance system lifetime for ambient intelligent systems. Self-management of the system is considered in two different scenarios: applications that tolerate graceful quality degradation and applications with single-point failures. The proposed technique can be part of a design methodology for prolonging the lifetime of a wide range of applications under various types of faults, despite scarce energy resources.",2003,0,
3041,3042,Fault Tolerance Mechanism in Secure Mobile Agent Platform System,"The mobile agent paradigm has attracted many attentions recently but it is still not widely used. One of the barriers is the difficulty in protecting an agent from failure because an agent is able to migrate over the network autonomously. Design and implementation of mechanisms to relocate computations requires a careful consideration of fault tolerance, especially on open networks like the Internet. . In the context of mobile agents, fault-tolerance prevents a partial or complete loss of the agent, i.e., ensures that the agent arrives at its destination. In this paper, we propose fault tolerant mechanism based on replication and voting for mobile agent platform system. The proposed mechanism has been implemented and evaluated the effects of varying of degree, different replication methods and voting frequencies on Secure Mobile Agent Platform System (SMAPS). We also report on the result of reliability and performance issue involved in mobile agent for Internet application.",2009,0,
3042,3043,Hybrid Fault Detection Technique: A Case Study on Virtex-II Pro's PowerPC 405,"Hardening processor-based systems against transient faults requires new techniques able to combine high fault detection capabilities with the usual design requirements, e.g., reduced design-time, low area overhead, reduced (or null) accessibility to processor internal hardware. This paper proposes the adoption of a hybrid approach, which combines ideas from previous techniques based on software transformations with the introduction of an Infrastructure IP with reduced memory and performance overheads, to harden system based on the PowerPC 405 core available in Virtex-II Pro FPGAs. The proposed approach targets faults affecting the memory elements storing both the code and the data, independently of their location (inside or outside the processor). Extensive experimental results including comparisons with previous approaches are reported, which allow practically evaluating the characteristics of the method in terms of fault detection capabilities and area, memory and performance overheads",2006,0,
3043,3044,Markov Chain Analysis of Thermally Induced Soft Errors in Subthreshold Nanoscale CMOS Circuits,"The development of future nanoscale CMOS circuits, characterized by lower supply voltages and smaller dimensions, raises the question of logic stability of such devices with respect to electrical noise. This paper presents a theoretical framework that can be used to investigate the thermal noise probability distributions for equilibrium and nonequilibrium logic states of CMOS flip-flops operated at subthreshold voltages. Representing the investigated system as a 2-D queue, a symbolic solution is proposed for the moments of the probability density function for large queues where Monte Carlo and eigenvector methods cannot be used. The theoretical results are used to calculate the mean time to failure of flip-flops built in a current 45-nm silicon-on-insulator technology modeled in the subthreshold regime including parasitics. As a predictive tool, the framework is used to investigate the reliability of flip-flops built in a future technology described in the International Technology Roadmap for Semiconductors. Monte Carlo simulations and explicit symbolic calculations are used to validate the theoretical model and its predictions.",2009,0,
3044,3045,Nonlinear Discrete-time feedback error learning with PI Controller for AC servo motor,The theme of this paper is to show the efficiency of nonlinear discrete time error learning(NDTFEL) in real application and how to improve it. PI Controller is chosen to improve the stability and reliability of the system by increasing robustness of the system. The simulations show how NDTFEL work on error rejecting. The results also show how PI controller improve the stability of NDTFEL.,2008,0,
3045,3046,A trust-based distributed data fault detection algorithm for wireless sensor networks,"Fault detection is a difficult and complex task in WSN because of there are many factors that influence data and could cause faults. Large-scale sensor networks impose energy and communication constraints, thus it is difficult to collect data from each individual sensor node and process it at the sink to detect faulty sensors. The proposed approach saves energy and improves network lifetime by detecting data faults locally in cluster head and therefore reducing the number of transmissions required to convey relevant information to the sink. This paper presents a novel approach for detecting sensors which produce faulty data in a distributed way as well as identifying the type of data faults using trust concepts to gain a high degree of confidence. We validate our method with simulations results.",2008,0,
3046,3047,Flow Control Using a Combination of Robust and NeuroFuzzy Controllers in Feedback Error Learning Framework,"In this paper a novel hybrid strategy is employed in order to improve the controller performance. The main idea is combination of classical and intelligent controllers. Feedback error learning (FEL) as a two degrees of freedom (2DOF) control scheme, has been introduced based on this idea. This paper takes a step ahead of traditional FEL schemes which combine a PID controller with an intelligent inverse based controller. We introduce a robust FEL scheme and the robust controller replaces the conventional PID controller. The Robust controller is designed based on the Hinfin approach and the intelligent controller has ANFIS structure. This novel algorithm is implemented in a Flow plant to track the desired value of flow and reject unwanted disturbances in the practical system. The results are brought to prove the practical power of the novel method and are compared with other control schemes.",2006,0,
3047,3048,Matching of multi-scale digital raster map using precise geometric correction,"In the processing and applying of the digital map, it is one of the key technologies that aligning the maps from different projective coordinate systems to a standard. Using the principium of geometric correction, the paper compared and analyzed three correction models of the raster map, presented the idea of correcting the maps of different scale with corresponsive correction models, and citing a map of certain scale, the author compared the correction effects and errors of three models by programming and proved the accuracy and feasibility of this idea",2006,0,
3048,3049,Instruction Precomputation for Fault Detection,"Fault tolerance (FT) is becoming increasingly important in computing systems. This work proposes and evaluates the instruction precomputation technique to detect hardware faults. Applications are profiled off-line, and the most frequent instruction instances with their operands and results are loaded into the precomputation table when executing. The precomputation-based error detection technique is used in conjunction with another method that duplicates all instructions and compares the results. In the precomputation-enabled version, whenever possible, the instruction compares its result with a precomputed value, rather than executing twice. Another precomputation-based scheme does not execute the precomputed instructions at all, assuming that precomputation provides sufficient reliability. Precomputation improves the fault coverage (including permanent and some other faults) and performance of the duplication method. The proposed method is compared to an instruction memoization-based technique. The performance improvements of the precomputation- and memoization-based schemes are comparable, while precomputation has a better long-lasting fault coverage and is considerably cheaper.",2009,0,
3049,3050,"Comparison between random and pseudo-random generation for BIST of delay, stuck-at and bridging faults","The combination of higher quality requirements and sensitivity of high performance circuits to delay defects has led to an increasing emphasis on delay testing of VLSI circuits. As delay testing using external testers requires expensive ATE, built-in self test (BIST) is an alternative technique that can significantly reduce the test cost. The generation of test patterns in this case is usually pseudo-random (produced from an LFSR), and it has been proven that Single Input Change (SIC) test sequences are more effective than classical Multiple Input Change (MIC) test sequences when a high robust delay fault coverage is targeted. In this paper, we first question the use of a pseudo-random generation to produce effective delay test pairs. We demonstrate that using truly random test pairs (produced from a software generation) to test path delay faults in a given circuit produces higher delay fault coverage than that obtained with pseudo-random test pairs obtained from a classical primitive LFSR. Next, we show that the same conclusion can be drawn when stuck-at or bridging fault coverage is targeted rather delay fault coverage. A modified hardware TPG structure allowing the generation of truly random test patterns is introduced at the end of the paper",2000,0,
3050,3051,Object-oriented executives and components for fault tolerance,"We have created two kinds of reusable, object-oriented software components to facilitate building fault tolerant applications. Executive components orchestrate familiar software fault tolerance techniques in a data type independent manner. Building block components provide fault tolerance utilities and application-specific functions. We use a three-level class framework (or design pattern) to create data type and application-independent classes at the highest level, define data type-dependent base classes in the middle level, and organize application and data type-specific derived classes at the lowest level. This approach employs polymorphism, pointer conversions and Run-Time Type Information. These techniques have successfully handled applications with dissimilar data types. Reusing these components greatly speeds the development of applications that exploit software fault tolerance techniques",2001,0,
3051,3052,Structure in errors: a case study in fingerprint verification,"Measuring the accuracy of biometrics systems is important. Accuracy estimates depend very much on the quality of the test data that are used: including poor quality data will degrade the accuracy estimates. Factors that determine the good quality data and poor quality data can not be revealed by simple accuracy estimates. We propose a novel methodology to analyze how the overall accuracy estimate of a system relates to the specific quality of biometrics samples. Using a large collection of fingerprint samples, we present an analysis of system accuracy, which suggests that a significant part of the error is due to few fingers.",2002,0,
3052,3053,Design of an Experimental System for Digital Circuit Fault Diagnosis Based on Support Vector Machine,"In order to combine the theory and practice of support vector machine (SVM) in the field of fault diagnosis, a novel universal experimental system for digital circuitspsila fault diagnosis is designed for the application study of SVM. The circuit is simulated in FPGA and its input and output pins are assigned to the DO and DI channels automatically by matrix switch. The faults and test points can be set easily. The algorithms of SVM and test vector generation etc. can be carried out in different software modules. The overall design and experimentation are discussed in detail. A practical example is given also. The experimental system is very convenient for the study of SVM and the efficiency is improved observably.",2009,0,
3053,3054,MMAR:A deadlock recovery-based fault tolerant routing algorithm for mesh/torus networks,"In direct networks, such as mesh and torus, the switching capacity will increase as the number of components increase. But the fault probability of the network also increases with the increasing of components. This paper proposes a novel fault-tolerant algorithm, named as minimal misrouted adaptive routing (MMAR) which is based on true fully adaptive routing algorithm and deadlock recovery mechanism. Due to the high adaptability, MMAR can accommodate arbitrary shaped fault models using minimal number of virtual channels in each physical link. When encountering concave fault models, MMAR minimizes the length of the misrouted path by avoiding routing the message into the irrespective holes. Simulation results show that MMAR can work efficiently and achieve favorable performance.",2007,0,
3054,3055,Fault injection in mixed-signal environment using behavioral fault modeling in Verilog-A,"Fault injection methods have been used for analyzing dependability characteristics of systems for years. In this paper we propose a practical mixed-signal fault injection flow that is fast as well as accurate. We described three classes of most common faults: i) Single event transients, ii) Electro-Magnetic interference and iii) Power disturbance faults. Fault models are implemented directly into circuit's devices using behavioral fault description in Verilog-A language. As an example for dependability evaluation, some test circuits have been prepared and the results of fault injection on their designs have been reported.",2010,0,
3055,3056,Comparisons of multipath modeling strategies for the estimation of GPS positioning error,"In this article, two objectives were planned: the choice of an appropriate electromagnetic multipath model, and a suitable description of the environment.",2009,0,
3056,3057,The Study of Non-uniformity Correction Algorithm for IRFPA Based on Neural Network,"It is very important to study non-uniformity correction algorithm in infrared focal plane array (IRFPA). In order to improve the convergence speed and non-stability in traditional neural network non-uniformity correction algorithm, a new scene-based non-uniformity correction algorithm for IRFPA was designed in this paper. The algorithm firstly arrange a pixelpsilas gray value and its around eight pixelspsila gray value from small to big and compute the mid 5 valuespsila mean in this new sequence as the pixelpsilas new gray value. Then using a traditional neural network algorithm do a non-uniformity correction on the infrared image again. Besides, we try to use a new estimating algorithm to calculate precisely the scope of the convergence constant in iterative equations. Compared with the result of several algorithms, the new algorithm has better correction effect than other three algorithms, and gets faster convergence speed.",2008,0,
3057,3058,Reducing Corrective Maintenance Effort Considering Module's History,"A software package evolves in time through various maintenance release steps whose effectiveness depends mainly on the number of faults left in the modules. The testing phase is therefore critical to discover these faults. The purpose of this paper is to show a criterion to estimate an optimal repartition of available testing time among software modules in a maintenance release. In order to achieve this objective we have used fault prediction techniques based both on classical complexity metrics and an additional, innovative factor related to the modules age in terms of release. This method can actually diminish corrective maintenance effort, while assuring a high reliability for the delivered software.",2005,0,
3058,3059,Analysis of long-lived isotopes in the presence of short-lived isotopes using zero dead time correction,"High Purity Germanium (HPGe) detector systems are routinely used in counting laboratories in many types of nuclear facilities such as nuclear power plants and fuel production sites. These systems generally consist of a lead-shielded HPGe detector, Multi Channel Analyzer (MCA), and analytical software. These systems are used to analyze a wide variety of sample types for many different isotopes. Analysis of certain sample types, such as those from the reactor coolant or the off-gas extraction system, in nuclear power plant radiochemistry laboratories is complicated by the presence of short-lived isotopes. With these isotopes present, the sample count rate begins at a higher value than the ending count rate with a rapid change in count rate often observed. This decaying of the sample count rate causes the true count rate of the peaks to be unknown for those MCAs that use the traditional Live Time Clock extension methods. The current method of compensating for these short-lived isotopes is simply to delay starting the acquisition until these isotopes decay (typically 45-60 minutes). This has the effect of reducing the throughput capacity of the laboratory meaning fewer samples can be counted in any given period. The use of ""loss free counting"" methods in radiochemistry laboratories has been unacceptable because these methods do not provide the uncertainty in the measurement which must be reported with the activity calculation from the counting laboratory. An innovative MCA with a zero dead time (ZDTTM) correction method will be presented which (1) compensates for the decaying count rate caused by the short-lived isotopes, thus eliminating the need for delaying the start time of the acquisition; and (2) calculates the uncertainty in the activity determination, thus satisfying the reporting requirements of the counting laboratory. Data from the analysis, including the uncertainty, of long-lived isotopes in reactor coolant samples both in the presence and absence of short-lived isotopes will be presented.",2001,0,
3059,3060,Analysis of a Sort of Unusual Mal-Operation of Transformer Differential Protection Due to Removal of External Fault,"Several cases of mal-operation of transformer differential protection with second-harmonic blocking after clearance of external fault are reported. These mal-operations all occurred at the nonrestraint region of percentage restraint plane. The previous theory cannot be utilized directly to analyze this phenomenon. Therefore, a mathematical model for analyzing the transient course of external fault inception and removal, together with the CT model involving the magnetic hysteresis effect, is proposed in this paper. It is proved that the magnetic linkage of one CT core can be pushed into the region nearby the saturation point by the high fault current with aperiodic component. As soon as the external fault is removed, the magnetic linkage formed by the primary current with low amplitude is not high enough to pull the operating point of the magnetic linkage back to the linear region. This phenomenon is named as CT local transient saturation, which results in the big measuring angle error and relative smooth waveform. In this case, the transformer differential protection using second harmonic blocking inevitably mal-operates. This point of view is verified with the simulation tests.",2008,0,
3060,3061,Bug analysis and corresponding error models in real designs,"This paper presents the item-missing error model. It stems from the analysis of real bugs that are collected in two market-oriented projects: (1) the AMBA interface of a general-purpose microprocessor IP core; (2) a wireless sensor network oriented embedded processor. The bugs are analyzed via code structure comparison, and it is found that item-missing errors merit attention. The test generation method for item-missing error model is proposed. Structural information obtained from this error model is helpful to reach a greater probability of bug detection than that in random-generation verification with only functional constraints. Finally, the proposed test method is applied in verification of our designs, and experimental results demonstrate the effectiveness of this method.",2007,0,
3061,3062,A rapid system prototyping platform for error control coding in optical CDMA networks,"This paper presents a rapid system prototyping platform for error-control codes (e.g. turbo and turbo product), which are to be used for optical CDMA transmission. The platform is based on system generator from Xilinx, a visual design tool based on Matlab/Simulink environment and enables a ""push of a button"" transition from specification to implementation. Components of the platform (a library of communication modules, debugging and emulation tools), design methodology of the platform and evaluation of some example communication systems are presented.",2005,0,
3062,3063,Delay defect characteristics and testing strategies,"Several factors influence production delay testing and corresponding DFT techniques: defect sources, design styles. ability to monitor process characteristics, test generation time. available test time, and tester memory. We present an overview of delay defect characteristics and the impact of delay defects on IC quality. We also discuss practical delay-testing strategy in terms of test pattern generation, test application speed, DFT, and test cost.",2003,0,
3063,3064,Exploring FPGA structures for evolving fault tolerant hardware,"This work explores different types of FPGA (field programmable gate array) structures for evolving fault tolerant hardware. A three-tier model for providing fault tolerance to the digital circuits evolved on FPGAs is proposed. This model combines the process level redundancy provided by the GA (genetic algorithm) based evolution techniques and the structural level redundancy supported by the FPGA architectures. Simulation results using the ISCAS'89 benchmark circuits have been carried out to study the effect of granularity on the time taken for the evolution process, the dimensionality of the evolution and the number of solutions that need to be evolved for fault coverage. The effect of using a divide and conquer approach to reduce the time taken for evolution has been studied proving that this is a feasible approach even for complex circuits.",2003,0,
3064,3065,Software-based self-test methodology for crosstalk faults in processors,"Due to signal integrity problems inherent sensitivity to timing, power supply voltage and temperature, it is desirable to test AC failures such as crosstalk-induced errors at operational speed and in the circuit's natural operational environment. To overcome the daunting cost and increasing performance hindrance of high-speed external testers, Software-Based. Self-Test (SBST) is proposed as a high-quality. low-cost at-speed testing solution for AC failures in programmable processors and System-on-Chips (SoC). SBST utilizes low-cost testers, applies tests and captures test responses in the natural operational environment. Hence SBST avoids artificial testing environment and external tester induced inaccuracies. Different from testing for stuck-at faults, testing for crosstalk faults requires a sequence of test vectors delivered at the operational speed. SBST applies tests in functional mode using instructions. Different instructions impose different controllability and observability constraints on a module-under-test (MUT). The complexity of searching for an appropriate sequence of instructions and operands becomes prohibitively high. In this paper, we propose a novel methodology to conquer the complexity challenge by efficiently combining structural test generation technique with instruction-level constraints. MUT in several time frames is automatically flattened and augmented with Super Virtual Constraint Circuits (SuperVCCs), which guide an automatic test pattern generation (ATPG) tool to select. appropriate test instructions and operands. The proposed methodology enables automatic test-program generation and high-fidelity test solution:for AC failures. Experimental results are shown on a commercial embedded processor (Xtensa/sup /spl trade// from Tensilica Inc).",2003,0,
3065,3066,Impact of inverter faults in the overall performance of permanent magnet synchronous motor drives,"The aim of this paper is to present an overall analysis of a typical permanent magnet synchronous motor drive under different operating conditions. Single power switch open-circuit faults as well as single phase open-circuit faults are introduced in the inverter and their effects are investigated trough a drive global performance evaluation. This includes the study of phase currents and voltages harmonic distortion and power factor on the mains supply side, the DC bus current, power losses and efficiency of the rectifier as well as of the system comprising the inverter and the motor. In addition, total drive efficiency values are also reported. Experimental and simulation results under these faulty operating conditions are presented, as well as results under healthy operating conditions, with the objective to establish a reference for comparison.",2009,0,
3066,3067,A Data Mining Model to Predict Software Bug Complexity Using Bug Estimation and Clustering,"Software defect(bug) repositories are great source of knowledge. Data mining can be applied on these repositories to explore useful interesting patterns. Complexity of a bug helps the development team to plan future software build and releases. In this paper a prediction model is proposed to predict the bug's complexity. The proposed technique is a three step method. In the first step, fix duration for all the bugs stored in bug repository is calculated and complexity clusters are created based on the calculated bug fix duration. In second step, bug for which complexity is required its estimated fix time is calculated using bug estimation techniques. And in the third step based on the estimated fix time of bug it is mapped to a complexity cluster, which defines the complexity of the bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.",2010,0,
3067,3068,Towards immune inspired fault tolerance in embedded systems,"The immune system is a remarkable natural system that is proving to be of great inspiration to computer scientists and engineers alike. This paper discusses the role that the immune system can play in the development of fault tolerant embedded systems. Initial work in the area has highlighted the use of the immune process of negative selection, and more importantly the concept of self/non-self discrimination in the application of artificial immune systems to fault tolerance. This paper reviews those works, highlights issues relating to the way in which this area is approached, and raises important points that need to be considered before effective immune inspired fault tolerant systems can be constructed.",2002,0,
3068,3069,Soft error resilient VLSI architecture for signal processing,"This paper presents a reliability-configurable coarse-grained reconfigurable array for signal processing, which offers flexible reliability to soft error. A notion of cluster is introduced as a basic element of the proposed reconfigurable array, each of which can select one of four operation modes with different levels of spatial redundancy and area-efficiency. Evaluation of permanent error rates demonstrates that four different reliability levels can be achieved by a cluster of the reconfigurable array. A fault-tolerance evaluation of Viterbi decoder mapped on the proposed reconfigurable array demonstrates that there is a considerable trade-off between reliability and area overhead.",2009,0,
3069,3070,Case study of fault-tolerant architectures for 90nm CMOS crythographic cores,This paper presents a case study of different fault-tolerant architectures. The emphasis is on the silicon realization. A 128 bit AES cryptographic core has been designed and fabricated as a main topology on which the fault-tolerant architectures have been applied. One of the fault-tolerant architectures is a novel four-layer architecture exhibiting a large immunity to permanent as well as random failures. Characteristics of the averaging/ thresholding layer are emphasized. Measurement results show advantage of four-layer architecture over triple modular redundancy in terms of reliability.,2007,0,
3070,3071,User-behavior based software fault detection for device,"The paper focuses on the Pre-Detection for device fault to ensure the reliability of the devices. A User-Behavior based software fault detection method was proposed, which imports User-Behavior analysis to select the high priority service set for Pre-Detection of the device software fault. The method is user oriented and could improve the efficiency of detection. The architecture and flow for the User-Behavior based software fault detection were introduced. In addition, the User-Behavior Analysis model and the User-Behavior Set-Selection model to reflect the dependence degree of the user to each service were given. At last, the method was validated by simulation and proved to be valid by comparing with other Pre-Detection methods.",2010,0,
3071,3072,Novel AC line conditioner for power factor correction,A new ac line conditioner is presented for high input power factor and clean ac output voltages for isolating the linear or nonlinear loads. A three-phase two-leg switching mode rectifier with neutral-point-clamped topology is proposed to draw the sinusoidal line currents from the ac mains. The carrier-based current controller is used in the inner control loop to track the line current commands with unity power factor. The dc bus voltage controller is adopted in the outer control loop to regulate the dc-link voltage. A voltage compensator is used to balance the neutral point voltage on the dc tank. A three-phase two-leg inverter with neutral-point-clamped topology is adopted in the system to provide the clean ac output voltages to the critical or sensitive loads. The carrier-based current control scheme is adopted to improve the instantaneous output voltages. Experimental results show the validity and effectiveness of the proposed control strategy.,2004,0,
3072,3073,Generic faultloads based on software faults for dependability benchmarking,"The most critical component of a dependability benchmark is the faultload, as it should represent a repeatable, portable, representative, and generally accepted set of faults. These properties are essential to achieve the desired standardization level required by a dependability benchmark but, unfortunately, are very hard to achieve. This is particularly true for software faults, which surely accounts for the fact that this important class of faults has never been used in known dependability benchmark proposals. This paper proposes a new methodology for the definition of faultloads based on software faults for dependability benchmarking. Faultload properties such as repeatability, portability and scalability are also analyzed and validated through experimentation using a case study of dependability benchmarking of Web-servers. We concluded that software fault-based faultloads generated using our methodology are appropriate and useful for dependability benchmarking. As our methodology is not tied to any specific software vendor or platform, it can be used to generate faultloads for the evaluation of any software product such as OLTP systems.",2004,0,
3073,3074,Detecting Inconsistent Values Caused by Interaction Faults Using Automatically Located Implicit Redundancies,"This paper addresses the problem of detecting inconsistent values caused by interaction faults originated from an external system.This type of error occurs when a correctly formatted message that is not corrupted during transmission is generated with a field that contains incorrect data.When traditional schemes cannot be used, one alternative is resorting to receiver-based strategies that employ implicit redundancies - relations between events or data, often identified by a human expert.We propose an approach for detecting inconsistent values using implicit redundancies which are automatically located in examples of communications.We show that, even without adding any redundant information to the communication, the proposed approach can achieve a reasonable error detection coverage in fields where sequential relations exist.Other aspects, such as false alarms and latency, are also evaluated.",2008,0,
3074,3075,Confidentiality and Real Errors: A Contradiction?,"During industrial software development and deployment, a wealth of data is accumulated, which could be used for the evolvement and refinement of methods and tools for error analysis, statistical evaluation of errors, dynamic handling of errors, and the prediction of faults and failures. Unfortunately, this data is always classified as highly sensitive as it contains customer related information, quality and quality assurance related information and gives insights into internal development processes. There is a need for neutralization techniques to overcome these hurdles",2006,0,
3075,3076,A Human Factors fault tree analysis method for software engineering,"Human Factors Analysis has realistic and profound significance to improve the quality and reliability of software. However, there is little research on the methods applied in software engineering to analyze human error. This paper proposes a human factors analysis method, which applies the fault tree analysis method to seek the human factors causing software accidents. Fault tree analysis method brings great flexibility and it is a graph deduction, which makes it easier to find the critical links of human errors.",2008,0,
3076,3077,A Web-Based Fault Diagnosis of Analogue Circuits System,"With the coming of the network era and the continuous improvement of information requirement, the fault diagnosis of devices is advancing from traditional mono device and field mode to distributed and remote mode, which can largely improve the ability of device maintenance. Several NI technologies are used in this paper to realize virtual instrument remote application and computer support collaborative work environment for device remote fault diagnosis. A simulated experiment on Fault Diagnosis of Analogue Circuits system is performed and the feasibility of the scheme is proved.",2010,0,
3077,3078,Fault-tolerant Video on Demand in RSerPool Architecture,"With the advent of Internet, video over IP is gaining popularity. In such an environment, scalability and fault tolerance will be the key issues. Existing video on demand (VoD) service systems are usually neither scalable nor tolerant to server faults and hence fail to comply to multi-user, failure-prone networks such as the Internet. Current research areas concerning VoD often focus on increasing the throughput and reliability of single server, but rarely addresses the smooth provision of service during server as well as network failures. Reliable Server Pooling (RSerPool), being capable of providing high availability by using multiple redundant servers as single source point, can be a solution to overcome the above failures. During a possible server failure, the continuity of service is retained by another server. In order to achieve transparent failover, efficient state sharing is an important requirement. In this paper, we present an elegant, simple, efficient and scalable approach which has been developed to facilitate the transfer of state by the client itself, using extended cookie mechanism, which ensures that there is no noticeable change in disruption or the video quality.",2006,0,
3078,3079,Machine Learning and Bias Correction of MODIS Aerosol Optical Depth,"Machine-learning approaches (neural networks and support vector machines) are used to explore the reasons for a persistent bias between aerosol optical depth (AOD) retrieved from the MODerate resolution Imaging Spectroradiometer (MODIS) and the accurate ground-based Aerosol Robotic Network. While this bias falls within the expected uncertainty of the MODIS algorithms, there is room for algorithm improvement. The results of the machine-learning approaches suggest a link between the MODIS AOD biases and surface type. MODIS-derived AOD may be showing dependence on the surface type either because of the link between surface type and surface reflectance or because of the covariance between aerosol properties and surface type.",2009,0,
3079,3080,A model of asynchronous machines for stator fault detection and isolation,This paper presents a new model of asynchronous machines. This model allows one to take into account unbalanced stator situations which can be produced by stator faults like short circuits in windings. A mathematical transformation is defined and applied to the classical abc model equations. All parameters which affect this new model can be known online. This makes the model very useful for control algorithms and fault detection and isolation algorithms. The model is checked by comparing simulation data with actual data obtained from laboratory experiments.,2003,0,
3080,3081,A Compiler-Microarchitecture Hybrid Approach to Soft Error Reduction for Register Files,"For embedded systems, where neither energy nor reliability can be easily sacrificed, this paper presents an energy efficient soft error protection scheme for register files (RFs). Unlike previous approaches, the proposed method explicitly optimizes for energy efficiency and can exploit the fundamental tradeoff between reliability and energy. While even simple compiler-managed RF protection scheme can be more energy efficient than hardware schemes, this paper formulates and solves further compiler optimization problems to significantly enhance the energy efficiency of RF protection schemes by an additional 30% on average, as demonstrated in our experiments on a number of embedded application benchmarks.",2010,0,
3081,3082,Fault-tolerant average execution time optimization for general-purpose multi-processor system-on-chips,"Fault-tolerance is due to the semiconductor technology development important, not only for safety-critical systems but also for general-purpose (non-safety critical) systems. However, instead of guaranteeing that deadlines always are met, it is for general-purpose systems important to minimize the average execution time (AET) while ensuring fault-tolerance. For a given job and a soft (transient) error probability, we define mathematical formulas for AET that includes bus communication overhead for both voting (active replication) and rollback-recovery with checkpointing (RRC). And, for a given multi-processor system-on-chip (MPSoC), we define integer linear programming (ILP) models that minimize AET including bus communication overhead when: (1) selecting the number of checkpoints when using RRC, (2) finding the number of processors and job-to-processor assignment when using voting, and (3) defining fault-tolerance scheme (voting or RRC) per job and defining its usage for each job. Experiments demonstrate significant savings in AET.",2009,0,
3082,3083,Segmentation of contrast enhanced CT images for attenuation correction of PET/CT data,"The use of contrast media in PET/CT imaging has been suggested to cause PET artifacts during the CT-based attenuation correction process. In this paper, we evaluate three algorithms that segment intravenous (IV) contrast-enhanced tissue from chest CT images to minimize possible artifacts. The algorithms that were evaluated are template matching, 3D region growing, and snake-based technique. These methods were tested using 5 patient studies. The segmentation result of each method was compared to its corresponding manually segmented images on a voxel-wise basis, and a squared difference between the two segmentation results was calculated. The averaged squared differences of all 5 patients for the template matching, region growing, and snake-based method were 19.0%plusmn7.1%, 65.2%plusmn51.5%, and 13.5%plusmn6.5% respectively. We concluded that the snake model is most suitable for efficiently segmenting the contrast-enhanced CT images among the three methods",2004,0,
3083,3084,Compact ASIC implementation of the ICEBERG block cipher with concurrent error detection,"ICEBERG is a block cipher that has been recently proposed for security applications requiring efficient FPGA implementations. In this paper, we investigate a compact ASIC implementation of ICEBERG and consider the novel application of concurrent error detection to protect the implementation from fault-based attacks. The compact architecture of ICEBERG requires about 5800 gates with a throughput of 552 Mbps in an ASIC implementation based on 0.18 mum CMOS technology. The addition of an effective multiple parity concurrent error detection scheme to protect the hardware from fault attacks results in a 62% area overhead.",2008,0,
3084,3085,Parameter Estimations in Linear Regression Models with AR(2) Errors in Which the Parameters Have a Special Relationship,"The purpose of this paper is to study parameters estimations in linear regression model with AR(2) errors A<sub>t</sub> = A<sub>1</sub>A<sub>t-1</sub> + A<sub>2</sub>A<sub>t-2</sub> - A<sub>t</sub>, t = 1, 2,A, n in which the parameters have a special relationship A<sub>2</sub> = A<sub>1</sub> <sup>2</sup>. For the properties of variance-covariance matrix A , This kind of models are transformed into the standard linear regression models without autocorrelation errors and apply the method of cycle generalized least squares (CGLS) to estimate parameters. Simulation results show that efficiency of CGLS method is superior over the method of generalized least squares (GLS) under mean square error criterion.",2009,0,
3085,3086,Real time fault injection using a modified debugging infrastructure,"Dependability is a critical factor in computer systems, requiring high quality validation & verification procedures in the development stage. At the same time, digital devices are getting smaller and access to their internal signals and registers is increasingly complex, requiring innovative debugging methodologies. To address this issue, most recent microprocessors include an on-chip debug (OCD) infrastructure to facilitate common debugging operations. This paper proposes an enhanced OCD infrastructure with the objective of supporting the verification of fault-tolerant mechanisms through fault injection campaigns. This upgraded on-chip debug and fault injection (OCD-FI) infrastructure provides an efficient fault injection mechanism with improved capabilities and dynamic behavior. Preliminary results show that this solution provides flexibility in terms of fault triggering and allows high speed real-time fault injection in memory elements",2006,0,
3086,3087,Towards Optimal Fault Tolerant Scheduling in Computational Grid,Grid environment has significant challenges due to diverse failures encountered during job execution. Computational grids provide the main execution platform for long running jobs. Such jobs require long commitment of grid resources. Therefore fault tolerance in such an environment cannot be ignored. Most of the grid middleware have either ignored failure issues or have developed adhoc solutions. Most of the existing fault tolerance techniques are application dependant and causes cognitive problem. This paper examines existing fault detection and tolerance techniques in various middleware. We have proposed fault tolerant layered grid architecture with cross-layered design. In our approach Hybrid Particle Swarm Optimization (HPSO) algorithm and Anycast technique are used in conjunction with the Globus middleware. We have adopted a proactive and reactive fault management strategy for centralized and distributed environments. The proposed strategy is helpful in identifying root cause of failures and resolving cognitive problem. Our strategy minimizes computation and communication thus achieving higher reliability. Anycast limits the effect of Denial of Service/Distributed Denial of Service D(DoS) attacks nearest to the source of the attack thus achieving better security. Significant performance improvement is achieved through using Anycast before HPSO. The selection of more reliable nodes results in less overhead of checkpointing.,2007,0,
3087,3088,Incorporating error detection and online reconfiguration into a regular architecture for the advanced encryption standard,"Fault injection based attacks on cryptographic devices aim at recovering the secret keys by inducing an error in the computation process. They are now considered a real threat and countermeasures against them must be taken. In this paper, we describe an extension to an existing AES architecture proposed by Mangard et al. (2003), which provides error detection and fault tolerance by exploiting the high regularity of the architecture. The proposed design is capable of performing online error detection and reconfiguring internal data paths to protect against faults occurring in the computation process. We also describe how different redundancy levels provide protection against different numbers of errors. The presented design incorporating fault detection and tolerance has the same throughput as the base architecture but incurs a nonnegligible area overhead. This overhead is about 40% for the fault detection circuitry and 134% for the entire fault detection and tolerance (through reconfiguration). Although quite high, this overhead is still lower than for reference solutions such as duplication (providing detection) and triple modular redundancy (providing fault masking).",2005,0,
3088,3089,An approach to minimize build errors in direct metal laser sintering,"This paper discusses the effect of geometric shape on the accuracy of direct metal laser sintering (DMLS) prototypes. The percentage shrinkages due to different shapes are investigated and their empirical relationship is determined. A new speed-compensation (SC) method is proposed to reduce uneven shrinkage affected by the two-dimensional geometric shape at each layer. From case studies conducted, the optimized SC method is found to be efficient in improving the accuracy of prototypes fabricated. Note to Practitioners-This paper aims to address the problem of dimensional errors of parts built by the direct metal laser sintering (DMLS) process. Existing compensation approaches are normally based on a general relationship between the nominal dimensions and the errors after sintering. However, the effect arising from different geometric shapes is not considered. A new approach is proposed using different scan speed settings to compensate for the effect of geometric shapes to improve the dimensional accuracy of the entire part. During processing, the laser sinters along the trajectory are guided by the hatch vectors or dexel. An appropriate experimental method is used to establish the relationship for different scan speeds with the dexel length to the final accuracy. When building the part, the laser scan speed is adjusted dynamically according to the dexel length which varies with the geometric shape of the part. The case study demonstrates that the proposed method can generate correct speed settings to effectively increase the dimensional accuracy of the final part. Although this method has been developed based on the DMLS process, it is also applicable to other laser sintering processes. In future research, other process parameters, such as laser power, will be considered independently, or together with the scan speed, for possible further improvement on the dimensional accuracy.",2006,0,
3089,3090,Fault tolerant MPEG-4 Digital Video Recorder,"Digital video recorder (DVR), the security device that records video onto storage devices like hard disks or DVR systems, becomes more and more popular nowadays. Currently, many companies try to develop advanced DVR systems and add new state-of-the-art functions to them like motion detection, image enhancement, etc. One of the most important factors of the DVR systems is fault-tolerant, which is the ability to record, playback and store video data into a storage device without interruption or data loosing. In this paper, we propose a fault-tolerant DVR system that supports the MPEG-4 codec for high video compression ratio on the embedded PowerPC processor. Our fault- tolerant DVR system guarantees consumers against losing recorded video data by applying our new file system and overwriting policy. And a graphic user interface with various functionalities can well satisfy consumers' demands.",2008,0,
3090,3091,"Bottom-Up Construction of Minimum-Cost <emphasis emphasistype=""smcaps"">and</emphasis>/ <emphasis emphasistype=""smcaps"">or</emphasis> Trees for Sequential Fault Diagnosis","The problem of generating the sequence of tests required to reach a diagnostic conclusion with minimum average cost, which is also known as a test-sequencing problem, is considered. The traditional test-sequencing problem is generalized here to include asymmetrical tests. In general, the next test to execute depends on the results of previous tests. Hence, the test-sequencing problem can naturally be formulated as an optimal binary AND/OR decision tree construction problem, whose solution is known to be NP-hard. Our approach is based on integrating concepts from one-step look-ahead heuristic algorithms and basic ideas of Huffman coding to construct an AND/OR decision tree bottom-up as opposed to heuristics proposed in the literature that construct the AND/OR trees top-down. The performance of the algorithm is demonstrated on numerous test cases, with various properties.",2007,0,
3091,3092,"Using product, process, and execution metrics to predict fault-prone software modules with classification trees","Software-quality classification models can make predictions to guide improvement efforts to those modules that need it the most. Based on software metrics, a model can predict which modules will be considered fault-prone, or not. We consider a module fault-prone if any faults were discovered by customers. Useful predictions are contingent on the availability of candidate predictors that are actually related to faults discovered by customers. With a diverse set of candidate predictors in hand, classification-tree modeling is a robust technique for building such software quality models. This paper presents an empirical case study of four releases of a very large telecommunications system. The case study used the regression-tree algorithm in the S-Plus package and then applied our general decision rule to classify modules. Results showed that in addition to product metrics, process metrics and execution metrics were significant predictors of faults discovered by customers",2000,0,
3092,3093,Embedded model-based fault diagnosis for on-board diagnosis of engine control systems,"In this paper, a model-based fault diagnosis scheme for on-board diagnosis in spark ignition (SI) engine control systems is presented. The developed fault diagnosis system fully makes use of the available control structure and is embedded into the control loops. As a result, the implementation of the diagnosis system is realized with low demands on engineering costs, computational power and memory. The developed diagnosis scheme has been successfully applied to the air intake system of an SI-engine",2005,0,
3093,3094,Decentralized architecture for fault tolerant multi agent system,"Multi agent systems (MAS) are expected to be involved in futuristic technologies. Agents require some execution environment in which they can publish their service interfaces and can provide services to other agent. Such execution environment is called agent platform (AP). From a technical point of view any abnormal behavior of platform can distress agents residing on that platform. That's why it is necessary to provide a suitable architecture for the AP which should not only provide fault tolerance but also scalability features. There also exist some management components within the platform, which provide services to application agents. All the agents within MAS are managed by agent management system (AMS) which is the mandatory supervisory authority of any AP. To be more scalable, a single agent platform can be distributed over several machines which not only provides load balancing but also fault tolerance depending upon the distributed architecture of the AP. In existing systems, AMS is centralized i.e. it exists on one machine. With centralized AMS, this infrastructure lacks fault tolerance, which is a key feature of high assurance. Absence of fault tolerance is the main reason for the small number of deployments of MAS. Failure of AMS leads towards abnormal behavior in the distributed platform. This paper proposes virtual agent cluster (VAC) paradigm which strongly supports decentralized AMS to achieve fault tolerance in distributed AP. VAC provides fault tolerance by using separate communication layers among different machines. Experiments show that it improves performance, brings autonomy and supports fault recovery along with load balancing in distributed AP.",2005,0,
3094,3095,An Effective Error Concealment Framework For H.264 Decoder Based on Video Scene Change Detection,"In this paper, we propose an effective error concealment framework for H.264 decoder based on the scene change detection. The proposed framework quickly and accurately detects whether scene change occurs in the decoding frame, based on the detection result, both corrupted intra frames and damaged inter frames can be reconstructed by spatial or improved temporal EC (Error Concealment) algorithm. The experiment shows that, compared with the traditional error concealment method in the H.264/A VC non- normative decoder, the proposed framework has better robustness and can efficiently improve the visual quality and PSNR of the decoded video.",2007,0,
3095,3096,System-level modeling and validation increase design productivity and save errors,"As the complexity of system on chip (SoC) devices rises to include scores, in some cases hundreds, of distinct blocks, system validation becomes a critical concern. A variety of techniques have emerged to help designers verify that individual blocks of a device meet performance specification. But what about functional intent? Are performance goals achieved? In this paper, we make the case for high-level system validation before RTL implementation, and present a flow to approach this increasingly essential task.",2005,0,
3096,3097,The Effectiveness of Regression Testing Techniques in Reducing the Occurrence of Residual Defects,"Regression testing is a necessary maintenance activity that can ensure high quality of the modified software system, and a great deal of research on regression testing has been performed. Most of the studies performed to date, however, have evaluated regression testing techniques under the limited context, such as a short-term assessment, which do not fully account for system evolution or industrial circumstances. One important issue associated with a system lifetime view that we have overlooked in past years is the effects of residual defects - defects that persist undetected - across several releases of a system. Depending on an organization's business goals and the type of system being built, residual defects might affect the level of success of the software products. In this paper, we conducted an empirical study to investigate whether regression testing techniques are effective in reducing the occurrence and persistence of residual defects across a system's lifetime, in particular, considering test case prioritization techniques. Our results show that heuristics can be effective in reducing both the occurrence of residual defects and their age. Our results also indicate that residual defects and their age have a strong impact on the cost-benefits of test case prioritization techniques.",2010,0,
3097,3098,The TIRAN approach to reusing software implemented fault tolerance,"Available solutions for fault tolerance in embedded automation are often based on strong customisation, have impacts on the whole life-cycle, and require highly specialised design teams, thus making dependable embedded systems costly and difficult to develop and maintain. The TIRAN project develops a framework which provides fault tolerance capabilities to automation systems, with the goal of allowing portable, reusable and cost-effective solutions. Application developers are allowed to select, configure and integrate in their own environment a variety of software-based functions for error detection, confinement and recovery provided by the framework",2000,0,
3098,3099,Reconfigurable control system design for fault diagnosis and accommodation,"The online fault tolerant control problem for dynamic systems under unanticipated failures is investigated from a realistic point of view without any specific assumption on type of system dynamical structure or failure scenarios. The necessary and sufficient conditions for system online stability under catastrophic failures have been derived using the discrete-time Lyapunov stability theory. Based upon existing control theory and modern intelligent techniques, an online fault accommodation control strategy is proposed to deal with the desired trajectory-tracking problems for systems suffering from various unknown and unanticipated catastrophic component failures. Through the online estimator, effective control signals to accommodate the dynamic failures can be computed using only the partially available information of the faults. To investigate the feasibility of using the developed technique for unanticipated fault accommodation in real hardware under the real-time environment, an online fault tolerant control test bed has been constructed to validate the proposed technology. Both online simulations and the real-time experiment show encouraging results and promising futures of online real-time fault tolerant control based solely upon insufficient information of the system dynamics and the failure modes",2001,0,
3099,3100,PLR: A Software Approach to Transient Fault Tolerance for Multicore Architectures,"Transient faults are emerging as a critical concern in the reliability of general-purpose microprocessors. As architectural trends point toward multicore designs, there is substantial interest in adapting such parallel hardware resources for transient fault tolerance. This paper presents process-level redundancy (PLR), a software technique for transient fault tolerance, which leverages multiple cores for low overhead. PLR creates a set of redundant processes per application process and systematically compares the processes to guarantee correct execution. Redundancy at the process level allows the operating system to freely schedule the processes across all available hardware resources. PLR uses a software-centric approach to transient fault tolerance, which shifts the focus from ensuring correct hardware execution to ensuring correct software execution. As a result, many benign faults that do not propagate to affect program correctness can be safely ignored. A real prototype is presented that is designed to be transparent to the application and can run on general-purpose single-threaded programs without modifications to the program, operating system, or underlying hardware. The system is evaluated for fault coverage and performance on a four-way SMP machine and provides improved performance over existing software transient fault tolerance techniques with a 16.9 percent overhead for fault detection on a set of optimized SPEC2000 binaries.",2009,0,
3100,3101,The analysis of frequency deviation on synchrophasor calculation and correction methods,"This paper gave a brief introduction of Phasor Measurement Unit (PMU) in the Wide Area Measurement System (WAMS) of power systems, and studied the error characteristics of the discrete Fourier calculation of phasor in the condition of a sinusoidal signal with frequency deviation from nominal frequency. Then, a program calculated the phasor using discrete Fourier transform under the platform of MATLAB was implemented, and the simulating analysis of off-nominal frequency was studied in detail. The simulating results showed that the error characteristics of calculated phasor had taken on the error ellipse shape. Finally, several methods of error correction were introduced for calculating the accurate phasor in power systems.",2009,0,
3101,3102,FACTS: A Framework for Fault-Tolerant Composition of Transactional Web Services,"Along with the standardization of Web services composition language and the widespread acceptance of composition technologies, Web services composition is becoming an efficient and cost-effective way to develop modern business applications. As Web services are inherently unreliable, how to deliver reliable Web services composition over unreliable Web services is a significant and challenging problem. In this paper, we propose FACTS, a framework for fault-tolerant composition of transactional Web services. We identify a set of high-level exception handling strategies and a new taxonomy of transactional Web services to devise a fault-tolerant mechanism that combines exception handling and transaction techniques. We also devise a specification module and a verification module to assist service designers to construct fault-handling logic conveniently and correctly. Furthermore, we design an implementation module to automatically implement fault-handling logic in WS-BPEL. A case study demonstrates the viability of our framework and experimental results show that FACTS can improve fault tolerance of composite services with acceptable overheads.",2010,0,
3102,3103,Research on Precise Synchronization for TMR Fault-Tolerant Embedded Computer,"This article presents a precise synchronization algorithm based on status tracking and locking mechanism. Tracking execution state of triple computer and running state of time base counter through dual state machine not only can implement precise synchronization of TMR computer, making status synchronization precision and time-base synchronization precision below 30ns, but also save valuable interconnection resource, reduce implementation cost and other overhead of system resources.",2009,0,
3103,3104,Fault Tolerant Mechanism in Dynamic Multi-homed IPv6 Mobile Networks,"Dynamic mobile network is a kind of moving network which has multiple independent wireless personal area networks interconnecting with MANET networking. Since a mobile device like a cellular phone can work as a mobile router for a dynamic mobile network, there are some fault problems related with traffic overload, reliability and energy consumption. And the essential issue of highly dynamic mobile network is fault tolerant mechanism. In this paper, we propose fast path redirection mechanism and candidate GMR election mechanism in order to enhance fault tolerance. As a result, our approaches support more reliable and seamless connectivity to all the nodes in mobile networks. Finally simulation results show the performance and efficiency of the mechanisms in terms of energy consumption and packet loss.",2007,0,
3104,3105,Application of Multi-layer Feed-forward Neural Network in Fault Diagnosis Based on FBP Algorithm,"This paper aims at the BP neural network model, to against the problems of the weakness of capability of knowledge acquisition and low stability of learning and memory. The paper put forward a new fast error back propagation algorithm, and give an example to make a comparison between BP algorithm and FBP algorithm on fault diagnosis, The diagnosis results indicate the reliability of this method.",2008,0,
3105,3106,Error-Aware Design,"The universal underlying assumption made today is that systems on chip must maintain 100% correctness regardless of the application. This work advocates the concept that some applications - by construction - are inherently error tolerant and therefore do not require this strict bound of 100% correctness. In such cases, it is possible to exploit this tolerance by aggressively reducing the supply voltage, thereby reducing power consumption significantly. This approach is demonstrated on several case studies in imaging, video and wireless communication fields.",2007,0,
3106,3107,Reliability analysis of fault tolerant drive topologies,"This paper examines fault tolerant power converter topologies and develops a technique for producing data to compare the reliability of topologies. The various states of fault-tolerant systems are modelled, following which reliability curves and failure rate data is constructed.",2008,0,
3107,3108,Fault-tolerant logic gates using neuromorphic CMOS Circuits,"Fault-tolerant design methods for VLSI circuits, which have traditionally been addressed at system level, will not be adequate for future very-deep submicron CMOS devices where serious degradation of reliability is expected. Therefore, a new design approach has been considered at low level of abstraction in order to implement robustness and fault-tolerance into these devices. Moreover, fault tolerant properties of multi-layer feed-forward artificial neural networks have been demonstrated. Thus, we have implemented this concept at circuit-level, using spiking neurons. Using this approach, the NOT, NAND and NOR Boolean gates have been developed in the AMS 0.35 mum CMOS technology. A very straightforward mapping between the value of a neural weight and one physical parameter of the circuit has also been achieved. Furthermore, the logic gates have been simulated using SPICE corners analysis which emulates manufacturing variations which may cause circuit faults. Using this approach, it can be shown that fault-absorbing neural networks that operate as the desired function can be built.",2007,0,
3108,3109,Design and evaluation of hybrid fault-detection systems,"As chip densities and clock rates increase, processors are becoming more susceptible to transient faults that can affect program correctness. Up to now, system designers have primarily considered hardware-only and software-only fault-detection mechanisms to identify and mitigate the deleterious effects of transient faults. These two fault-detection systems, however, are extremes in the design space, representing sharp trade-offs between hardware cost, reliability, and performance. In this paper, we identify hybrid hardware/software fault-detection mechanisms as promising alternatives to hardware-only and software-only systems. These hybrid systems offer designers more options to fit their reliability needs within their hardware and performance budgets. We propose and evaluate CRAFT, a suite of three such hybrid techniques, to illustrate the potential of the hybrid approach. For fair, quantitative comparisons among hardware, software, and hybrid systems, we introduce a new metric, mean work to failure, which is able to compare systems for which machine instructions do not represent a constant unit of work. Additionally, we present a new simulation framework which rapidly assesses reliability and does not depend on manual identification of failure modes. Our evaluation illustrates that CRAFT, and hybrid techniques in general, offer attractive options in the fault-detection design space.",2005,0,
3109,3110,Simulation and fault detection of three-phase induction motors,"Computer simulation of electric motor operation is particularly useful for gaining an insight into their dynamic behaviour and electro-mechanical interaction. A suitable model enables motor faults to be simulated and the change in corresponding parameters to be predicted without physical experimentation. This paper presents both a theoretical and experimental analysis of asymmetric stator and rotor faults in induction machines. A three-phase induction motor was simulated and operated under normal healthy operation, with one broken rotor bar and with voltage imbalances on one phase of supply. The results illustrate good agreement between both simulated and experimental results.",2002,0,
3110,3111,Web-Interface on Grid System for Atmospheric Corrections of Modis Data in Coastal Area,"Coastal areas ecosystem, representing one of the most delicate and complex relationship between natural environment and human activities, is one of the main subject of governments programs for environmental monitoring and eco-sustainable environmental management. This paper describes a quantitatively evaluation of MODIS water leaving reflectance products on coastal areas, where the occurrence of land contaminated pixels (mixed sea-land) and a different atmospheric absorption, make often unreliable or ambiguous the values measured from satellite and the quality flag related to values of these areas, often marks them as unreliable. The evaluation is performed by comparison to in situ measurements acquired in the framework of IMCA project (Integrated Monitoring of Coastal Areas) from June 2006 to May 2008. The user can perform his own atmospheric corrections setting the best models and parameters for the area of interest.In order to allow an easy access and management of different data set, pre-processing and processing parameters selection, a prototype of Web Gis for environmental monitoring has been realized.",2009,0,
3111,3112,Correction of in-band self-interference due to imperfect frequency synthesizer,"We consider a synthesizer proposed for multiband OFDM. This paper presents a simple way to correct the in-band default caused by the non-ideal frequency synthesizer, in the case of one user. This compensation is a first step toward the possibility of a conception of a less constrained synthesizer.",2009,0,
3112,3113,Research on architecture and design principles of COTS components based generic fault-tolerant computer,"A novel fault-tolerant architecture based on COTS components is put forward and implemented in this paper. In order to make observable the internal states of COTS components, and in order to concurrently perform fault-tolerance function and normal function and control the behavior of each COTS component, the authors have devised an intelligent hardware module dedicated to fault-tolerance processing, which can significantly offload application processors. This architecture digs every inherent fault-detection mechanism and adopts layered fault protection mechanism to raise fault-tolerance coverage. This architecture is efficient, flexible, scalable and transparent with respect to fault-tolerance. It is Byzantine fault safe and also supports online repair. The authors also raise some design tradeoffs when designing COTS components based fault-tolerant computer.",2005,0,
3113,3114,Concurrent monitoring of PCI bus transactions for timely detection of errors initiated by FPGA-based applications,"The integration of FPGAs as co-processing engines in various dependable workstations makes the real time detection of errors an important issue. Thus, concurrent error detection components should enforce the operation of such systems, in order to prevent the propagation of errors within the system. The functionality of such monitoring modules should not interfere with the performance of the workstation and must ensure high system availability. An embedded monitoring component with concurrent error detection features was implemented to illustrate the benefits of this approach. The monitor checks for PCI protocol and application errors based on the forensic analysis of a workstation with embedded FPGA-based co-processing support.",2007,0,
3114,3115,Video error correction using steganography,"The transmission of any data is always subject to corruption due to errors, but video transmission, because of its real time nature must deal with these errors without retransmission of the corrupted data. The error can be handled by using forward error correction in the encoder or error concealment techniques in the decoder. The MPEG-2 compliant coder described here uses steganography to transmit data for error correction in conjunction with several error concealment techniques in the decoder. The decoder resynchronizes more quickly with fewer errors than traditional resynchronization techniques. This provides for a much higher quality picture in an error-prone environment while creating an almost imperceptible degradation of the picture in an error free environment",2001,0,
3115,3116,Burst-Error Analysis of Dual-Hop Fading Channels Based on the Second-Order Channel Statistics,"The burst-error (BE) rate of dual-hop fading channels under a fixed fade threshold is estimated based on the level crossing rate (LCR) and average fade duration (AFD). The LCR and AFD of the equivalent signal-to-noise ratio (SNR) are first derived for dual-hop Nakagami-m and Weibull fading channels with a fixed-gain amplify-and-forward (AF) relay, where closed-form lower and upper bounds are derived for the LCR and AFD of the Nakagami-m fading channels. Numerical results from theoretical evaluations and Monte Carlo simulations are illustrated to validate the analysis and to compare the performance of the two fading channels.",2010,0,
3116,3117,Integrated error management for media-on-demand services,"Data servers for multimedia applications like news-on-demand represent a severe bottleneck, because a potentially (very) high number of users concurrently retrieve data with high data rates. In the Intermediate Storage Node Concept (INSTANCE) project, we develop a new architecture for media-on-demand servers that maximizes the number of concurrent clients a single server can support. Traditional bottlenecks, like copy operations, multiple copies of the same data element in main memory, and checksum calculation in communication protocols are avoided by applying three orthogonal techniques: zero-copy-one-copy memory architecture, network level framing, and integrated error management. We describe the design, implementation, and evaluation of our integrated error management mechanism. Our results show that the reuse of parity information from a RAID system as forward error correction information in the transport protocol reduces the server workload and enables smooth playout at the client",2001,0,
3117,3118,Eliminating Concurrency Bugs with Control Engineering,"In the multicore era, concurrency bugs threaten to reduce programmer productivity, impair software safety, and erode end-user value. Control engineering can eliminate concurrency bugs by constraining software behavior, preventing runtime failures, and offloading onerous burdens from human programmers onto automatically synthesized control logic.",2009,0,
3118,3119,Measuring experimental error in microprocessor simulation,"We measure the experimental error that arises from the use of non-validated simulators in computer architecture research, with the goal of increasing the rigor of simulation-based studies. We describe the methodology that we used to validate a microprocessor simulator against a Compaq DS-10L workstation, which contains an Alpha 21264 processor. Our evaluation suite consists of a set of 21 microbenchmarks that stress different aspects of the 21264 microarchitecture. Using the microbenchmark suite as the set of workloads, we describe how we reduced our simulator error to an arithmetic mean of 2%, and include details about the specific aspects of the pipeline that required extra care to reduce the error. We show how these low-level optimizations reduce average error from 40% to less than 20% on macrobenchmarks drawn from the SPEC2000 suite. Finally, we examine the degree to which performance optimizations are stable across different simulators, showing that researchers would draw different conclusions, in some cases, if using validated simulators",2001,0,
3119,3120,Color error-diffusion halftoning,"Grayscale halftoning converts a continuous-tone image (e.g., 8 bits per pixel) to a lower resolution (e.g., 1 bit per pixel) for printing or display. Grayscale halftoning by error diffusion uses feedback to shape the quantization noise into high frequencies where the human visual system (HVS) is least sensitive. In color halftoning, the application of grayscale error-diffusion methods to the individual colorant planes fails to exploit the HVS response to color noise. Ideally the quantization error must be diffused to frequencies and colors, to which the HVS is least sensitive. Further it is desirable for the color quantization to take place in a perceptual space so that the colorant vector selected as the output color is perceptually closest to the color vector being quantized. This article discusses the design principles of color error diffusion that differentiate it from grayscale error diffusion, focusing on color error diffusion halftoning systems using the red, green, and blue (RGB) space for convenience.",2003,0,
3120,3121,Bridging the Gap between Fault Trees and UML State Machine Diagrams for Safety Analysis,"Poorly designed software systems are one of main causes of accidents in safety-critical systems, and thus, the importance of safety analysis for software has greatly increased over the recent years. Software safety can be improved by analyzing both its desired and undesired behaviors, and this in turn requires expressive power such that both can be modeled. However, there is a considerable gap between modeling methods for desired and undesired behaviors. Therefore, we propose a method to bridge the gap between fault trees (for undesired behavior) and UML state machine diagrams (for desired behavior). More specifically, we present rules and algorithms that facilitate the transformation of a hazard (in the context of fault trees) to a UML state machine diagram. We illustrate our proposed approach via an example on a microwave-oven system. Our proposed transformation can help engineers identify how the hazards may occur, thereby allowing them to prevent the hazard from occurring.",2010,0,
3121,3122,A Study on Fault Diagnosis and Performance Evaluation of Propulsion System,"Recently, as the feasibility study shows that trans-Korea railway and trans-continental railway are advantageous, interest in high-speed railway system is increasing. Because railway vehicle is environment-friendly and safe than compared with airplane and ship, its market-sharing increases gradually. KHST(Korean high speed train) has been developed by KRRI (Korea railroad research institute). An electric railway system is composed of high-tech subsystems, among which main electric equipment such as transformers and converter are critical components determining the performance of rolling stock. We developed a measurement system for online test and evaluation of performances of KHST. The measurement system is composed of software part and hardware part. Perfect interface between multi-users is possible. A new method to measure temperature was applied to the measurement system. By using the system, fault diagnosis and performance evaluation of electric equipment in Korean high speed train was conducted during test running",2006,0,
3122,3123,Influence of team size and defect detection technique on inspection effectiveness,"Inspection team size and the set of defect detection techniques used by the team are major characteristics of the inspection design, which influences inspection effectiveness, benefit and cost. The authors focus on the inspection performance of a nominal, that is non-communicating team, similar to the situation of an inspection team after independent individual preparation. We propose a statistical model based on empirical data to calculate the expected values for the inspection effectiveness and effort of synthetic nominal teams. Further, we introduce an economic model to compute the inspection benefits, net gain, and return on investment. With these models we determine (a) the best mix of reading techniques (RTs) to maximize the average inspection performance for a given team size, (b) the optimal team size and RT mix for a given inspection time budget, and (c) the benefit of an additional inspector for a given team size. Main results of the investigation with data from a controlled experiment are: (a) benefits of an additional inspector for a given RT diminished quickly with growing team size, thus, above a given team size a mix of different RTs is more effective and has a higher net gain than using only one RT; (b) the cost-benefit model limits team size, since the diminishing gain of an additional inspector at some point is more than offset by his additional cost",2001,0,
3123,3124,Research on the Fault Diagnosis for Boiler System Based on Fuzzy Neural Network,"The fuzzy logic was applied into the neural network and the application of fault diagnosis for boiler system with the integrated fuzzy neural network is investigated on the basis of the introductions of the basic principle of artificial neural network (ANN) and the principle of fault diagnosis for boiler system based on neural network. A example of training process and testing results about the sample of boiler was given. At last, it is proved that this integrated method can acquire a better result on fault diagnosis for boiler through error analysis compared with the traditional standard BP network.",2009,0,
3124,3125,Behavioral fault simulation : implementation and experimental results,"We present an original approach for performing Behavioral Fault Simulation (BFS). This approach involves three main steps : (i) the definition of an internal modeling of behavioral descriptions, and the determination of a Fault Model; (ii) the definition of a fault simulation technique; (iii) the implementation of this technique. We give in this paper a description of the BFS software implementation. We point out how object oriented programming has been used for defining an evolutive and efficient tool. Finally, this paper deals with experiments conducted on ITC'99 benchmarks in order to validate a VHDL behavioral fault simulator (BFS). The effectiveness of the BFS software is clearly demonstrated through the obtained results",2002,0,
3125,3126,Implementation and evaluation of transparent fault-tolerant Web service with kernel-level support,"Most of the techniques used for increasing the availability of Web services do not provide fault tolerance for requests being processed at the time of server failure. Other schemes require deterministic servers or changes to the Web client. These limitations are unacceptable for many current and future applications of the Web. We have developed an efficient implementation of a client-transparent mechanism for providing fault-tolerant Web service that does not have the limitations mentioned above. The scheme is based on a hot standby backup server that maintains logs of requests and replies. The implementation includes modifications to the Linux kernel and to the Apache Web server, using their respective module mechanisms. We describe the implementation and present an evaluation of the impact of the backup scheme in terms of throughput, latency, and CPU processing cycles overhead.",2002,0,
3126,3127,A new intelligent fast Petri-net model for fault section estimation of distribution systems,"This paper proposes a new Petri nets (PNs) knowledge representation scheme to quickly estimate the fault section of distribution systems when a fault occurs. Based on the practical guidelines and the heuristic rules obtained by interacting with the dispatchers of distribution systems, a PNs model is first built to represent the related knowledge about the task of substation fault diagnosis. The PNs model built is then transformed into matrix forms, which are relied on to infer the result of fault diagnosis through simple matrix operations. Due to its graphic representation of the heuristic rules and parallel rule-firing manner via matrix operations, the human expertise on fault diagnosis can be advantageously expressed and exploited by means of the PNs knowledge-representing approach. The system is demonstrated on a practical system of Chung-Hsiao substation at Tainan City, Taiwan. Flexibility and effectiveness of the PNs model have been validated for the fault section estimation",2000,0,
3127,3128,Perceptually optimized error resilient transcoding using attention-based intra refresh,"While deployment of wireless channels has become widespread and fast-growing for mobile applications, transmitting data over these existing error-prone networks can be very unreliable and challenging due to time-varying interference and channel errors. Many error-resilient algorithms have been proposed to provide adequate resilient features in order to protect video data from channel errors. However, these algorithms often aim to achieve the optimal decoded video quality in terms of mean square error without any consideration for the visual quality. In this paper, we present a perceptually error-resilient method for video transcoding based on the attention-based intra refresh technique and the characteristics of the human visual system to enhance the perceptual performance of the transcoded video. Specifically, the foveated just noticeable distortion and visual attention models are employed to estimate the perceptual loss impact due to error propagation for allocating intra-refreshed macroblocks in the transcoded video. Experimental results show that the proposed method can achieve a much better performance than the existing methods in terms of both the visual quality and perceptual quality measure.",2010,0,
3128,3129,The effect of noise estimation error in the LDPC decoding performance,"The number of quantization bits of the input signals r<sub>n</sub> needs to be optimally determined through the trade-off between the H/W complexity and the BER performance in LDPC codes applications. Also, an effective means to incorporate a channel reliability L<sub>c</sub> in the log-MAP based LDPC decoding is highly required, because it has a major effect on both the complexity and performance. In this paper so as to effectively incorporate L<sub>c</sub> in LDPC decoding. The optimal number of quantization bits of r<sub>n</sub> is investigated through Monte-Carlo simulations assuming that bit-shifting approach is adopted. In addition, the effects of an incorrect estimation of noise variance on the performance of LDPC codes are investigated. There is a confined range in which the effects of an in correct estimation can be ignored",2004,0,
3129,3130,Scheduling in Grid: Rescheduling MPI applications using a fault-tolerant MPI implementation,"Due to advancement in grid technologies, resources spread across the globe can be accessed using standard general-purpose protocols. Simulations and scientific experiments were earlier restricted due to limited availability of the resources. These are now carried out vigorously in the grid. Grid environments are dynamic in nature. The resources in a grid are heterogeneous in nature and are not under a central control. So scheduling in grid is complex. The initial schedule obtained for an application may not be good as it involves the selection of resources at a future time. The resource characteristics like CPU availability, memory availability, network bandwidth etc keep changing. Rescheduling becomes necessary under these conditions. The research experiment uses the fault-tolerant functionalities of MPICH-V2 to migrate MPI processes. Load-balancing modules, which make a decision of when and where to migrate a process are added into the MPICH-V2 system. Simulations are done to show that process migration is viable rescheduling technique for computationally intensive applications. The research experiment also gives brief descriptions of some existing fault-tolerant MPI implementations.",2007,0,
3130,3131,Exact computation of maximally dominating faults and its application to n-detection tests for full-scan circuits,"The size of an n-detection test set increases approximately linearly with n. This increase in size may be too fast when an upper bound on test set size must be satisfied. A test generation method is proposed for obtaining a more gradual increase in the sizes of n-detection test sets, while still ensuring that every additional test would be useful in improving the test set quality. The method is based on the use of fault-dominance relations to identify a small subset of faults (called maximally dominating faults) whose numbers of detections are likely to have a high impact on the defect coverage of the test set. Structural analysis obtains a superset of the maximally dominating fault set. A method is proposed for determining exact sets of maximally dominating faults. New types of n-detection test sets are based on the approximate and exact sets of maximally dominating faults. The test sets are called (n,n<sub>2</sub>)-detection test sets and (n,n<sub>2</sub>,n<sub>3</sub>)-detection test sets. Experimental results demonstrate the usefulness of these test sets in producing high-quality n-detection test sets for the combinational logic of ISCAS-89 benchmark circuits.",2004,0,
3131,3132,The computation model of code error distortion based on the rate-distortion theory,"This paper starts from the end to end error distortion, constructs the statistical model of the information sources encoding distortion, channel error distortion, intra frame encoding macro block error diffusion, and the inter frame encoding macro block error diffusion, and brings forwards a rate distortion robust encoding control algorithm model under a low computation quantity. This method not only can acquire the optimal control point in the rate-distortion curve from the global perspective under the code error environment, but also can self adaptive adjust the bit allocation and the quantization parameter under given network bandwidth. Furthermore, it can also implement intra frame macro block refresh at the same time and play important role in counteracting the channel code error to minimize the total distortion under certain bit-rated according to current network package loss probability, by which to establish the rate distortion model of associated signal source channel and apply the rate-distortion optimized solution method to optimal allocate the bit-rate between the signal source coding and the channel coding. This method can provide valuable reference for the video robust coding transmission and resource allocation under the wireless environment.",2005,0,
3132,3133,Increasing System Availability with Local Recovery Based on Fault Localization,"Due to the fact that software systems cannot be tested exhaustively, software systems must cope with residual defects at run-time. Local recovery is an approach for recovering from errors, in which only the defective parts of the system are recovered while the other parts are kept operational. To be efficient, local recovery must be aware of which component is at fault. In this paper, we combine a fault localization technique (spectrum-based fault localization, SFL) with local recovery techniques to achieve fully autonomous fault detection, isolation, and recovery. A framework is used for decomposing the system into separate units that can be recovered in isolation, while SFL is used for monitoring the activities of these units and diagnose the faulty one whenever an error is detected. We have applied our approach to MPlayer, a large open-source software. We have observed that SFL can increase the system availability by 23.4% on average.",2010,0,
3133,3134,Error Detection Using Model Checking vs. Simulation,"Design simulation and model checking are two alternative and complementary techniques for verifying hardware designs. This paper presents a comparison between the two techniques based on detection of design errors, performance, and memory use. We perform error detection experiments using model checking and simulation to detect errors injected into a verification benchmark suite. The results allow a quantitative comparison of simulation and model checking which can be used to understand weaknesses of both approaches",2006,0,
3134,3135,Analysis of the effect of Java software faults on security vulnerabilities and their detection by commercial web vulnerability scanner tool,"Most software systems developed nowadays are highly complex and subject to strict time constraints, and are often deployed with critical software faults. In many cases, software faults are responsible for security vulnerabilities which are exploited by hackers. Automatic web vulnerability scanners can help to locate these vulnerabilities. Trustworthiness of the results that these tools provide is important; hence, relevance of the results must be assessed. We analyze the effect on security vulnerabilities of Java software faults injected on source code of Web applications. We assess how these faults affect the behavior of the scanner vulnerability tool, to validate the results of its application. Software fault injection techniques and attack trees models were used to support the experiments. The injected software faults influenced the application behavior and, consequently, the behavior of the scanner tool. High percentage of uncovered vulnerabilities as well as false positives points out the limitations of the tool.",2010,0,
3135,3136,Modeling of fault-tolerant mobile agents execution in distributed systems,"The reliable execution of a mobile agent is a very important design issue to build a mobile agent system and many fault-tolerant schemes have been proposed. Hence, in this paper, we present FATOMAS, a java based fault-tolerant mobile agent system based on an algorithm presented in an earlier paper. In contrary to the standard ""place-dependent"" architectural approach, FATOMAS uses the novel agent-dependent approach introduced in the paper. In this approach, the protocol that provides fault tolerance travels with the agent. This has the important advantage to allow fault-tolerant mobile agent execution with out the need to modify the underlying mobile agent platform. We derive the FATOMAS (Fault-Tolerant Mobile Agent System) design which offers a user transparent fault tolerance that can be activated on request, according to the needs of the task, also discuss how transactional agent with types of commitment constraints can commit. Furthermore this paper proposes a solution for effective agent deployment using dynamic agent domains.",2005,0,
3136,3137,Space Mapping With Adaptive Response Correction for Microwave Design Optimization,"Output space mapping is a technique introduced to enhance the robustness of the space-mapping optimization process in case the space-mapped coarse model cannot provide sufficient matching with the fine model. The technique often works very well; however, in some cases it fails. Especially in the microwave area where the typical model response (e.g., |<i>S</i> <sub>21</sub>|) is a highly nonlinear function of the free parameter (e.g., frequency), the output space-mapping correction term may actually increase the mismatch between the surrogate and fine models for points other than the one at which the term was calculated, as in the surrogate model optimization process. In this paper, an adaptive response correction scheme is presented to work in conjunction with space-mapping optimization algorithms. This technique is designed to alleviate the difficulties of the standard output space mapping by adaptive adjustment of the response correction term according to the changes of the space-mapped coarse model response. Examples indicate the robustness of our approach.",2009,0,
3137,3138,Context-Aware Adaptive Applications: Fault Patterns and Their Automated Identification,"Applications running on mobile devices are intensely context-aware and adaptive. Streams of context values continuously drive these applications, making them very powerful but, at the same time, susceptible to undesired configurations. Such configurations are not easily exposed by existing validation techniques, thereby leading to new analysis and testing challenges. In this paper, we address some of these challenges by defining and applying a new model of adaptive behavior called an Adaptation Finite-State Machine (A-FSM) to enable the detection of faults caused by both erroneous adaptation logic and asynchronous updating of context information, with the latter leading to inconsistencies between the external physical context and its internal representation within an application. We identify a number of adaptation fault patterns, each describing a class of faulty behaviors. Finally, we describe three classes of algorithms to detect such faults automatically via analysis of the A-FSM. We evaluate our approach and the trade-offs between the classes of algorithms on a set of synthetically generated Context-Aware Adaptive Applications (CAAAs) and on a simple but realistic application in which a cell phone's configuration profile changes automatically as a result of changes to the user's location, speed, and surrounding environment. Our evaluation describes the faults our algorithms are able to detect and compares the algorithms in terms of their performance and storage requirements.",2010,0,
3138,3139,Stability guaranteed active fault tolerant control of networked control systems,The stability guaranteed active fault tolerant control against actuators failures in networked control systems (NCS) is addressed. A detailed design procedure is formulated as a convex optimization problem which can be efficiently solved by existing software. An illustrative example is given to show the efficiency of the proposed method for NCS.,2007,0,
3139,3140,Process system fault source tracing based on Bayesian networks,"Process systems are different from discrete manufacturing systems in that they are composed of many interlocking subsystems that consist of various tightly coupled units. Hence, whenever a small unit of a subsystem functions badly, it could influence or cause the whole system to function abnormally. Under this circumstance, how to locate abnormal or failed units will be a very formidable task. By introducing Bayesian networks into the tracking of fault sources in process systems, a new method of fault source tracing is proposed, and a model is set up accordingly. To guarantee the accuracy of the modeling process, a series of rules which must be abided by are defined. And, a mapping between the Bayesian network and the units of a process system is developed. Furthermore, to make full use of this new Bayesian network model, its probability characteristics and the problem-solving method exploiting it are expanded and investigated in depth. Additionally, a complete reasoning for the fault source tracing based on the Bayesian network is described. Finally, an example is provided to demonstrate the modeling and reasoning processes, and thereby verifies the practicality and validity of this model in tracing abnormalities in process systems.",2009,0,
3140,3141,Getting errors to catch themselves - self-testing of VLSI circuits with built-in hardware,"As the electronics industry continues to grow, technology feature sizes continue to decrease, and complex systems and levels of integration continue to increase, the need for better and more effective methods of testing to ensure reliable operations of chips, the mainstay of today's all digital systems, is being increasingly felt. One obvious way to significantly improve the testability of digital VLSI circuits and save testing time is to use built-in self-testing (BIST), where the basic idea is to have the chip test itself. BIST is a design methodology that combines the concepts of built-in test (BIT) and self-test (ST) in one, termed BIST. This technique generates test patterns and evaluates test responses inside the chip system, and has been widely used in many commercial VLSI products with appreciable success. The subject paper endeavors to present a comprehensive overview of the general methodology of BIST from its various perspectives, and in the sequel attempts to relate its significance in the particular context of modern embedded cores-based system-on-chip (SOC) technology.",2005,0,
3141,3142,Improved Algorithm for Detection of Self-clearing Transient Cable Faults,This paper presents a transient cable fault detection technique for typical medium voltage cable. The subject of transient cable failures is firstly introduced together with an illustration of the typical fault characteristic. A power system test model was developed to replicate the physical phenomena and this is illustrated to show a good correlation with physical recordings. This model is shown to be very useful for validating the detection technique as it allows easy variation of the fault duration. Studies show that the technique is able to offer a number of significant advantages for distribution systems especially as an early warning system.,2008,0,
3142,3143,Compensation for Heeling Error of MEMS Accelerator Sensor in Vehicle ABS,"Precision of wheel speed and vehicle speed determines accuracy and reliability of ABS. When measuring vehicle speed, MEMS inertia accelerator sensor is usually adopted to carry out indirect measurement. For MEMS inertia accelerator sensor, output signal of accelerator sensor not only has relationship with vehicle accelerator, but also with heeling angle of MEMS inertia accelerator sensor. Transient heeling angle can be obtained by separating acceleration and heeling angle from output signal of MEMS inertia accelerator sensor. When compensating for MEMS inertia accelerator signal in braking process by separated heeling angle, feint accelerator error produced by single-axis MEMS inertia accelerator sensor can be basically eliminated. Then, accuracy increase of actual accelerator signal will improve brake control performance of ABS controller",2006,0,
3143,3144,Fault-tolerance of functional programs based on the parallel graph reduction,"Recently, parallel computing has been applied to many systems. Functional programming is suitable for parallel programming because of referential transparency and is applied to symbol processing systems and parallel database systems. Programs with some functional programming can be regarded as graphs and are processed in terms of reduction of the corresponding graph. The paper proposes fault tolerance of functional programming based on graph reduction. The proposed method stores the received graph as a message log and an erroneous task is recovered by using the checkpoint and the stored graph. Computer simulations reveal that the time overhead of the proposed method is small. If the checkpoint interval is 30 seconds and the number of tasks is 3, for example, the time overhead is less than 10%",2001,0,
3144,3145,Automatic fault detection and diagnosis in complex software systems by information-theoretic monitoring,"Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In this paper we use normalized mutual information as a similarity measure to identify clusters of correlated metrics, without knowing the specific form. We show how we can apply the Wilcoxon rank-sum test to identify anomalous behaviour. We present two diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, and SigScore, which incorporates knowledge of component dependencies. We evaluate our mechanisms in the context of a complex enterprise application. Through fault injection experiments, we show that we can detect 17 out of 22 faults without any false positives. We diagnose the faulty component in the top five anomaly scores 7 times out of 17 using SigScore, which is 40% better than when system structure is ignored.",2009,0,
3145,3146,Fault Detection System Using Directional Classified Rule-Based in AHU,"Monitoring systems used at present to operate air handling unit (AHU) optimally do not have a function that enables to detect faults properly when there are faults of such as operating plants or performance falling, so they are unable to manage faults rapidly and operate optimally. In this paper, we have developed a fault detection system, directional classified rule-based, which can be used in AHU system. In order to experiment this algorithm, it was applied to AHU system which is installed inside environment chamber(EC), verified its own practical effect, and confirmed its own applicability to the related field in the future.",2007,0,
3146,3147,Directional ground-fault indicator for high-resistance grounded systems,"Locating ground faults is a difficult and challenging problem for low-voltage power systems that are ungrounded or have high-impedance grounding. Recent work in pilot signals has renewed efforts in developing fault location methodologies. This paper presents a method for directional ground-fault indication that utilizes the fundamental frequency voltages and currents. Although the ground-fault current is small and usually less than the load currents, the fault has zero-sequence components that distinguish it from the load. Signal processing techniques are used to identify and compare the fault signals to determine the fault direction. The process takes advantage of the currents flowing from the distributed grounding capacitance. An experimental microprocessor-based directional indicator unit is tested in an industrial power distribution system. Directional indication of ground faults is applied near tap-off branch circuit connections. Promising results from field test conducted in a harmonic-noisy setting are presented. Directional indicator units simplify the search process on large networks, thus reducing the time and effort necessary to locate and remove the fault, and thereby significantly reduces the probability of a second ground fault with its destructive currents.",2003,0,
3147,3148,Testing content-addressable memories using functional fault models and march-like algorithms,"Functional tests for content-addressable memories (CAM's) are presented in this paper. In addition to several traditional functional fault models for RAM's, we also consider the fault models based on physical defects, such as shorts between two circuit nodes and transistor stuck-on and stuck open faults. Accordingly, several functional fault models are proposed. In order to make our approach suited to various application-specific CAM's, we propose tests which require only three fundamental types of operation (i.e., write, erase, and compare), and the test results can be observed entirely from the single-bit Hit output. A complete, compact test is also proposed, which has low complexity and is suitable for modern high-density and large-capacity CAMs-it requires only 2N+3w+2 compare operations and 8N write operations to cover the functional fault models discussed, where N is the number of words and w is the word length",2000,0,
3148,3149,Ephemeris type a fault analysis and mitigation for LAAS,"The Local Area Augmentation System (LAAS) has been developed by the FAA to enable precision approach and landing operations using the Global Positioning System (GPS). Each LAAS installation provides services through a LAAS Ground Facility (LGF) which is located at the airport it serves. By monitoring the GPS signals, measurements, and navigation messages, the LGF is able to exclude unhealthy satellites and broadcast real-time range-correction messages for healthy satellites to users via a VHF data link. Airborne users apply these corrections to remove errors that are common between the LGF and the aircraft. The LGF is also responsible for warning the aircraft of any potential integrity threats that cannot easily be resolved by excluding unhealthy satellites. One source of potential errors is the satellite broadcast ephemeris message, which users decode and use to compute GPS satellite positions. In LAAS, potential GPS ephemeris faults are categorized into two types, A and B, based upon whether or not the fault is associated with a satellite maneuver. This work focuses on aviation navigation threats caused by Type A faults. To detect and mitigate these threats, we investigate two LGF monitors based on comparing expected ranges and range rates (based on broadcast ephemeris) with those measured by the LGF. The effectiveness of these monitors is analyzed and verified in this paper.",2010,0,
3149,3150,Fault-based side-channel cryptanalysis tolerant Rijndael symmetric block cipher architecture,"Fault-based side channel cryptanalysis is very effective against symmetric and asymmetric encryption algorithms. Although straightforward hardware and time redundancy based Concurrent Error Detection (CED) architectures can be used to thwart such attacks, they entail significant overhead (either area or performance). In this paper we investigate systematic approaches to low-cost, low-latency CED for Rijndael symmetric encryption algorithm. These approaches exploit the inverse relationship that exists between Rijndael encryption and decryption at various levels and develop CED architectures that explore the trade-off between area overhead, performance penalty and error detection latency. The proposed techniques have been validated on FPGA implementations",2001,0,
3150,3151,Enhanced fault ride-through scheme and coordinated reactive power control for DFIG,"A novel scheme for enhancing the fault ride-through (FRT) capability of Doubly Fed Induction Generator (DFIG) is proposed in this paper. In addition, a coordinated reactive power control strategy for grid side and rotor side converters of DFIG is also proposed. This coordinated control aids rapid recovery of terminal voltage at the clearance of severe grid fault. Extensive simulation study is carried out employing PSCAD/EMTDC software and the results demonstrate the effectiveness of the proposed FRT scheme and reactive power control strategy.",2010,0,
3151,3152,Analysis and methodology for multiple-fault diagnosis,"In this paper, we propose a multiple-fault-diagnosis methodology based on the analysis of failing patterns and the structure of diagnosed circuits. We do not consider the multiple-fault behavior explicitly, but rather partition the failing outputs and use an incremental simulation-based technique to diagnose failures one at a time. Our methodology can be further improved by selecting appropriate diagnostic test patterns. The n-detection tests allow us to apply a simple single-fault-based diagnostic algorithm, and yet achieve good diagnosability for multiple faults. Experimental results demonstrate that our technique is highly efficient and effective. It has an approximately linear time complexity with respect to the fault multiplicity and achieves a high diagnostic resolution for multiple faults. Real manufactured industrial chips affected by multiple faults can be diagnosed in minutes of central processing unit (CPU) time.",2006,0,
3152,3153,AOA Assisted NLOS Error Mitigation for TOA-Based Indoor Positioning Systems,"One of the major challenges for TOA-based accurate indoor positioning systems is the blockage of the LOS path, or equivalently direct path (DP), due to obstructions. Since accurate ranging of these systems depend on the detection of the DP between the transmitter and receiver, significant errors will be introduced into the ranging measurements once the DP cannot be detected. This condition is hence called the undetected direct path (UDP) condition. Owing to the fact that indoor wireless channels exhibit rich multipath propagation, the multipath components other than the DP may be utilized in mitigating errors occurring in UDP areas. In this paper, we introduce a method based on both TOA and AOA information from other multipath components to substantially mitigate the ranging error. We present our results as a comparison with commonly used methods for TOA-based ranging and we show that our proposed technique outperforms these traditional methods.",2007,0,
3153,3154,Fabric defects detecting and rank scoring based on Fisher criterion discrimination,"Automatic texture defect detection is highly important for many fields of visual inspection. This paper studies the application of advanced computer image processing techniques for solving the problem of automated defect detection for textile fabrics. The approach is used for the quality inspection of local defects embedded in homogeneous textured surfaces. Above all, the size of the basic texture units of the fabric image is acquired by calculating auto correlation function in weft direction and in wrap direction. Then the sizes of the basic texture units are taken as criterion to segment the fabric image. During scanning the fabric texture image, the basic units are segmented. And the Fisher criterion discriminator is used to assign each unit to a class at the same time. Afterwards, the fabric detects are measured according to the relationship of the suffix of the image pixel and the scale of the image and ranked scale by comparing with America Four Points System. Experiments with real fabric image data show that it is effective.",2009,0,
3154,3155,IEEE 1451 standard and wireless sensor networks: An over view of fault tolerant algorithms,"The IEEE 1451 standards family provides a set of common interfaces for connecting transducers to existing instrumentation and control networks. Currently IEEE 1451.5 is proposed to define the interface for wireless sensors to a system without a network connection. IEEE 1451.6 is also being defined for a network using a controller area network (CAN). As the IEEE 1451.5 and 1451.6 support both wired and wireless sensor networks, sensor networks shall evolve as heterogeneous networks. We present review of the fault tolerance algorithms in use for wireless sensor networks and issues related to the NCAP for IEEE 1451 standard to address the wired and wireless communication. In this work we attempt to survey some heuristics and algorithms for fault-tolerance and fusion in multi-sensor environments and discuss the Health check protocol and fault diagnosis process [Reya, 2005]. We also identified the possible sensor failures with wide range of sensors under different work conditions",2006,0,
3155,3156,A Fault Diagnosis and Security Framework for Water Systems,"Water resources management is a key challenge that will become even more crucial in the years ahead. From a system-theoretic viewpoint, there is a need to develop rigorous design and analysis tools for control, fault diagnosis and security of water distribution networks. This work develops a mathematical framework suitable for fault diagnosis and security in water systems; in addition it investigates the problem of determining a suitable set of locations for sensor placement in large-scale drinking water distribution networks such that contaminant detection is optimized. This work contributes to the research by presenting a problem formulation were the state-space representation of the propagation and reaction dynamics is coupled with the impact dynamics describing the damage caused by a contamination of the water distribution network. We propose a solution methodology for the sensor-placement problem by considering several risk-objectives, and by utilizing various optimization and evolutionary computation techniques. To illustrate the methodology, we present results of a simplified and a real water distribution network.",2010,0,
3156,3157,Displacement sensor axis misalignment error,"Because of valve displacement great importance in valve systems in many marine power aggregates and other servomechanisms and equipment, controlling measurement of this parameter would be taken. Hysteresis measurement must also be taken after every mounting and periodically revision activities. Controlling measurement complies of displacement sensor and computer system applied with adequate hardware and software solutions. Displacement sensor axis misalignment error cannot be avoided in any situation where sensor axis line of symmetry is not parallel with axis line of valve movement. Therefore, elevation error must be considered. Other errors and imperfections in measuring process because of clearly thesis description in this paper are negligible (measuring method, sensor class, nonlinearity, software signal transition). Paper goal is pointing error values regarding sensor axis elevation and its presentation in meaningful ways, with comprehensive further influences",2006,0,
3157,3158,Low-complexity frame importance modelling and resource allocation scheme for error-resilience H.264 video streaming,"In this paper, we addressed the problem of redundancy allocation for protecting packet loss for better quality of service (QoS) in real-time H.264 video streaming. A novel error-resilient approach is proposed for the transmission of pre-encoded H.264 video stream under bandwidth constrained networks. A novel frame importance model is derived for estimating relative importance index for different H.264 video frames. Combining with the characteristics of the network, the optimal resource allocation strategy for different video frames can be determined for achieving improved error resilience. The model uses frame error propagation index (FEPI) to characterize video quality degradation caused by error propagation in different frames in a GOP when suffer from packet loss. This model can be calculated in DCT domain with the parameters extracted directly from the bitstream. Therefore, the complexity of the proposed scheme is very low and much better for real-time video transmission. Simulation results show that the proposed scheme can improve the receiver side reconstructed video quality remarkably under different channel loss patterns.",2008,0,
3158,3159,Fault Detection and Localization in Smart Grid: A Probabilistic Dependence Graph Approach,"Fault localization in the nation's power grid networks is known to be challenging, due to the massive scale and inherent complexity. In this study, we model the phasor angles across the buses as a Gaussian Markov random field (GMRF), where the partial correlation coefficients of GMRF are quantified in terms of the physical parameters of power systems. We then take the GMRF-based approach for fault diagnosis, through change detection and localization in the partial correlation matrix of GMRF. Specifically, we take advantage of the topological hierarchy of power systems, and devise a multi-resolution inference algorithm for fault localization, in a distributed manner. Simulation results are used to demonstrate the effectiveness of the proposed approach.",2010,0,
3159,3160,An Unsupervised Diagnosis for Process Tool Fault Detection: The Flexible Golden Pattern,The flexible golden pattern (FGP) algorithm uses a patented technology of empirical scoring to detect abnormal behavior for semiconductor processing equipment or a specific processing chamber during wafer production. This algorithm does not entirely rely on manual extraction of features from data acquired on each tool. It is able to automatically select good pattern indicators from raw (temporal) signal traces. It is able to diagnose unusual behavior disregarding specificity proper to a recipe or even a chamber or even a tool if the algorithm is calibrated for such a purpose. The algorithm does not need any complicated parameter settings; the diagnosis is established by comparison of the normal process behavior to the abnormal one.,2007,0,
3160,3161,A Case Study of Software Security Test Based on Defects Threat Tree Modeling,"Due to the increasing complexity of software applications, traditional function security testing ways, which only test and validate software security mechanisms, are becoming ineffective to detect latent software security defects (SSD). However, the most of vulnerabilities result from some typical SSD. According to CERT/CC, ten defects known are responsible for 75% of security breaches in today software applications. On the base of threat tree modeling, we use it in the integrated software security test model. For introducing the usefulness of the method, we use the test model in M<sup>3</sup>TR software security test.",2010,0,
3161,3162,Neural network approach to diagnose phase shifter faults of antenna arrays,"The diagnosis of faulty phase shifter in a uniform linear phased array antenna using a new method is presented. For parallel feeding of antenna elements each element has a separate phase shifter. The phase shifter for any particular element may fail due to failure in drive electronics. The failures of phase shifter are called phase shifter faults. In this work, an artificial neural network approach is adopted to diagnose phase shifter faults. A linear array of 21 elements with uniform spacing and uniform excitation with a progressive phase shift of A is considered. A feed forward back propagation algorithm is used to train a neural network with a deviation radiation pattern which is the difference between the measured radiation pattern of array with normal phase shifters and degraded radiation pattern of array with a faulty phase shifter. The network thus trained predicted the number of the antenna element with faulty phase shifter with a high success rate. This is illustrated in a confusion matrix.",2006,0,
3162,3163,Performance Analysis of Digital Flight Control Systems With Rollback Error Recovery Subject to Simulated Neutron-Induced Upsets,"This paper introduces a class of stochastic hybrid models for the analysis of closed-loop control systems implemented with NASA's Recoverable Computer System (RCS). Such systems have been proposed to ensure reliable control performance in harsh environments. The stochastic hybrid model consists of a stochastic finite-state automaton driven by a Markov input process, which in turn drives a switched linear discrete-time dynamical system. Stability and output tracking performance are analyzed using an extension of the existing theory for Markov jump-linear systems. The theory is then applied to predict the tracking error performance of a Boeing 737 at cruising altitude and in closed-loop with an RCS subject to neutron-induced single-event upsets. The results are validated using experimental data obtained from a simulated neutron environment in NASA's SAFETI Laboratory.",2008,0,
3163,3164,Differential power analysis and differential fault attack resistant AES algorithm and its VLSI implementation,"This paper proposes an AES algorithm against both differential power analysis and differential fault analysis and its hardware implementation. This new algorithm emphasizes the feature of defending hardware against two kinds of side-channel attack simultaneously. Since the modified AES algorithm is much more complex than the original one, this paper exploits low hardware cost architecture to realize it. Furthermore, a pipelined structure is adopted to achieve high throughput. Simulations show that this architecture can protect hardware against both differential power analysis and differential fault attack. Synthesis result demonstrates that this design achieves adequately high data throughput with low hardware cost.",2008,0,
3164,3165,Supervised Neural Network Modeling: An Empirical Investigation Into Learning From Imbalanced Data With Labeling Errors,"Neural network algorithms such as multilayer perceptrons (MLPs) and radial basis function networks (RBFNets) have been used to construct learners which exhibit strong predictive performance. Two data related issues that can have a detrimental impact on supervised learning initiatives are class imbalance and labeling errors (or class noise). Imbalanced data can make it more difficult for the neural network learning algorithms to distinguish between examples of the various classes, and class noise can lead to the formulation of incorrect hypotheses. Both class imbalance and labeling errors are pervasive problems encountered in a wide variety of application domains. Many studies have been performed to investigate these problems in isolation, but few have focused on their combined effects. This study presents a comprehensive empirical investigation using neural network algorithms to learn from imbalanced data with labeling errors. In particular, the first component of our study investigates the impact of class noise and class imbalance on two common neural network learning algorithms, while the second component considers the ability of data sampling (which is commonly used to address the issue of class imbalance) to improve their performances. Our results, for which over two million models were trained and evaluated, show that conclusions drawn using the more commonly studied C4.5 classifier may not apply when using neural networks.",2010,0,
3165,3166,Workshop on fault diagnosis and tolerance in cryptography,"Cryptographic devices are becoming increasingly ubiquitous and complex, making reliability an important design objective. Moreover, the diffusion of mobile, low-price consumer electronic equipment containing cryptographic components makes them more vulnerable to attack procedures, in particular to those based on injection of faults. This workshop aims at providing researchers in both the dependability and cryptography communities an opportunity to start bridging the gap between fault diagnosis and tolerance techniques, and cryptography.",2004,0,
3166,3167,Fault detection for OSPF based E-NNI routing with probabilistic testing algorithm,"In this paper, a probabilistic testing algorithm is proposed to increase the fault coverage for OSPF based E-NNI routing protocol testing. It automatically constructs random network topologies and checks database information consistency with real optical network topology and resource for each generated topology. Theoretical analysis indicates that our algorithm can efficiently increase the fault coverage. This algorithm has been implemented in a software test tool called E-NNI Routing Testing System (ERTS). Experiment results based on ERTS are also reported.",2008,0,
3167,3168,Service Fault Localization Using Probing Technology,"We describe the proactive probing and probing on demand algorithms that are used to monitor the health of a serviceable component system proactively and to localize faulty components intelligently. This application-level performance monitoring mechanism can greatly reduce the overhead of satisfying service-level objectives. When abnormal events occur, we are able to follow relevant dependent paths to localize possible faulty serviceable components based on the outcomes of incremental on demand probing. In this paper, we continue analyzing the impact of the algorithms in the process of probing and diagnosis",2006,0,
3168,3169,Error performance and throughput evaluation of a multi-Gbps millimeter-wave WPAN system in the presence of adjacent and co-channel interference,"This paper investigates the impact of adjacent channel interference (ACI) and co-channel interference (CCI) on error performance and throughput of a multi-Gbps millimeterwave wireless personal area network (WPAN) system in a realistic residential line-of-sight (LOS) and non-line-of-sight (NLOS) multipath environment. The main contribution of this paper is providing a multi-Gbps WPAN system design in the challenging multipath environment in the presence of ACI/CCI. Based on the investigation results, we have provided ACI/CCI rejection as a reference for victim receiver protection design. In the NLOS environment, the ACI rejection (i.e. ACI that causes 0.5 dB degradation in the required signal-to-noise ratio (SNR) to achieve bit error rate (BER) of 10<sup>-6</sup>) for pi/2-BPSK, QPSK, 8 PSK and 16 QAM are 13, 7, 0 and -6dB respectively. And the CCI rejection for similar modulation schemes are -18, -20, -26 and -29 respectively. Secondly, we have clarified the LOS-NLOS relationship of the ACI/CCI impact to system performance. ACI in multipath NLOS environment causes an additional 5 dB degradation to error performance as compared to ACI in the LOS environment. CCI on the other hand, has similar impact on error performance in both LOS and NLOS environment. Thirdly, we have clarified the relationship between modulation spectral efficiency and robustness against ACI/CCI. In an environment with no or low ACI/CCI, the maximum achievable throughput for pi/2-BPSK, QPSK, 8 PSK and 16 QAM in LOS environment are 1.2, 2.5, 3.8 and 5 Gbps respectively. In NLOS environment, the achievable throughput decreases to 1, 1.9, 2.8 and 3.8 Gbps respectively. As ACI/CCI increases, the throughput of higherorder modulation schemes such as 16 QAM decreases the most rapidly, followed by 8 PSK and QPSK. The throughput for pi/2-BPSK has the highest tolerance against increasing ACI/CCI, at the expense of lower maximum achievable throughput.",2009,0,
3169,3170,Fault tolerant IPMS motor drive based on adaptive backstepping observer with unknown stator resistance,"This work considers the problem of designing a fault tolerant system for IPMS motor drive subject to current sensor fault. To achieve this goal, two control strategies are considered. The first is based on field oriented control and a developed adaptive backstepping observer which simultaneously are used in the case of fault-free. The second approach proposed is concerned with fault tolerant strategy based on observer for faulty conditions. Stator resistance as possible source of system uncertainty is taken into account under different operating conditions. Current sensors failures are detected and observer based on adaptive backstepping approach is used to estimate currents and stator resistance. The nonlinear observer stability study based on the Lyapunov theory guarantees the stability and convergence of the estimated quantities, if the appropriate adaptation laws are designed and persistency of excitation condition is satisfied. In our control approach, references of d-q axis currents are generated on the basis of maximum power factor per ampere control scheme related to IPMSM drive. The complete proposed scheme is simulated using MATLAB/Simulink software. Simulation is made to illustrate the proposed strategy.",2008,0,
3170,3171,Analyzing fault tolerance on parallel genetic programming by means of dynamic-size populations,"This paper presents an experimental research on the size of individuals when dynamic size populations are employed with Genetic Programming (GP). By analyzing the individual's size evolution, some ideas are presented for reducing the length of the best individual while also improving the quality. This research has been performed studying both individual's size and quality of solutions, considering the fixed-size populations and also dynamic size by means of the plague operator. We propose an improvement to the Plague operator, that we have called Random Plague, that positively affects the quality of solutions and also influences the individuals' size. The results are then considered from a quite different point of view, the presence of processors failures when parallel execution over distributed computing environments are employed. We show that results strongly encourage the use of Parallel GP on non fault-tolerant computing resources: experiments shows the fault tolerant nature of Parallel GP.",2007,0,
3171,3172,Faults Location in High Voltage Transmission System using ICA,"Various methods for fault location in transmission lines have been proposed in the literature. This work presents an alternative method based upon the analysis of independent components (ICA) to localize the distance at which single-phase, two-phase, two-phase grounding and three-phase faults occur in a 500 kV transmission system starting from different angles of fault incidence at different distances along the line together with fault signals or travelling waves subject to other perturbations unrelated with the desired fault signal at the fault distance placement. Simulation results of a 500kV high voltage transmission system got with the software Alternative transients program (ATP) show that the proposed methodology implemented with Matlab is a rather efficient tod for various fault types location.",2007,0,
3172,3173,Using rational filters for digital correction of a spectrometric microtransducer,"Raw spectrometric data are subject to systematic errors of an instrumental type that may be reduced, provided a mathematical model of the spectrometer, or its pseudoinverse, i.e., an operator of reconstruction, is identified. The idea of identifying this operator, directly during calibration of the spectrometer, is developed in this paper. The applicability of an operator of reconstruction, having the form of a rational filter, is studied when it is used for correction of the instrumental errors introduced by a low-resolution spectrometric microtransducer (SMT) that is intended for designing a microspectrometer. Several algorithms of correction are developed and systematically studied using real-world spectra and a nonlinear mathematical model of the microtransducer, proposed by the authors in a previous publication",2000,0,
3173,3174,Distribution system grounding impacts on fault responses,"One of the main concerns of utilities, nowadays, is grounding the distribution systems. Distribution systems are usually three phase systems with a returning current neutral wire. Grounding the neutral wire will affect the power quality and characteristics of distribution systems during unbalanced conditions, specially phase to ground faults. In this paper several case studies have been done to investigate the impacts of different types of grounding the distribution systems on fault responses. The fault responses are fault current, voltage swell, substation ground potential rise and neutral wire voltages and currents. Some sensitivity studies are also performed to see the effects of grounding parameters on fault responses. The results are presented in graphical and tabular forms.",2008,0,
3174,3175,A portable and fault-tolerant microprocessor based on the SPARC v8 architecture,"The architecture and implementation of the LEON-FT processor is presented. LEON-FT is a fault-tolerant 32 bit processor based on the SPARC V8 instruction set. The processors tolerates transient SEU errors by using techniques such as TMR registers, on-chip EDAC, parity, pipeline restart, and forced cache miss. The first prototypes were manufactured on the Atmel ATC35 0.35 m CMOS process, and subjected to heavy-ion fault-injection at the Louvain Cyclotron. The heavy-ion tests showed that all of the injected errors (>100,000) were successfully corrected without timing or software impact. The device SEU threshold was measured to be below 6 MeV while ion energy-levels of up to 110 MeV were used for error injection.",2002,0,
3175,3176,Analysis of reliability in nanoscale circuits and systems based on a-priori statistical fault-modeling methodology,"This paper presents a new approach for monitoring and estimating device reliability of nanometer-scale devices prior to fabrication. A four-layer architecture exhibiting a large immunity to permanent as well as random failures is used. A complete Monte Carlo based tool for a-priori functional fault tolerance analysis was developed, that induces different failure models, and does subsequent evaluation of system reliability under realistic constraints. A structured fault modeling architecture is also proposed, which is together with the tool a part of the new reliability design method representing a compatible improvement of existing IC design methodologies",2005,0,
3176,3177,Dynamic simulation of renewable energy sources and requirements on fault ride through behavior,"Currently, power generation from renewable energy sources is of global significance and will continue to grow during the coming years. The grid integration of intermitting renewable energy sources is not only an issue of distribution networks, but effects the transmission grids as well. The largest amount of new installations are connected to the transmission grid, especially into 110-kV. Further, large wind farms with some 100 MW have already been connected to the 400-kV-system. The renewable energy sources are connected to the power network via power electronic converters, and this type of connection schema can change the short circuit conditions in the network. Today, usually a tripping command is generated within the first period, but in the future contributions to the short circuit current by renewable sources are necessary to fulfil the selective clearing of faults. To investigate the new requirements some novel simulation models are necessary, and described in the paper, for: converter connected DC sources (fuel cell, PV plants), doubly fed induction generator, DFIG, converter connected synchronous generator. Using these models the simulation allows for investigating the behavior for various control design. Requested is the delivery of rated current up to voltage dips of 100% for the whole duration of the fault. The results of some simulations and discussion of them have led to new rules for grid access. These rules are implemented into the grid code",2006,0,
3177,3178,Cluster fault-tolerance: An experimental evaluation of checkpointing and MapReduce through simulation,"Traditionally, cluster computing has employed checkpointing to address fault tolerance. Recently, new models for parallel applications have grown in popularity namely MapReduce and Dryad, with runtime systems providing their own re-execute based fault tolerance mechanisms, but with no analysis of their failure characteristics. Another development is the availability of failure data spanning years for systems of significant size at Los Alamos National Labs (LANL), but the time between failure (TBF) for these systems is a poor fit to the exponential distribution assumed by optimization work in checkpointing, bringing these results into question. The work in this paper describes a discrete event simulation driven by the LANL data and by models of parallel checkpointing and MapReduce tasks. The simulation allows us to then evaluate and assess the fault tolerance characteristics of these tasks with the goal of minimizing the expected running time of a parallel program in a cluster in the presence of faults for both fault tolerance models.",2009,0,
3178,3179,Biomechanical evaluation of unilateral maxillary defect restoration based on modularized finite element model of normal human skull,"A standardized and modularized finite element model of a normal human skull is established to simulate the unilateral defective pattern of the maxillary and the stress distribution of craniofacial skeleton with repair subject to the bite force. Based on the model, the stress evaluation of autologous bone grafted and zygomatic implant is to optimize the repairing method, and to rebuild the occlusal function",2005,0,
3179,3180,Fault detection and classification in transmission lines based on wavelet transform and ANN,"This paper proposes a novel method for transmission-line fault detection and classification using oscillographic data. The fault detection and its clearing time are determined based on a set of rules obtained from the current waveform analysis in time and wavelet domains. The method is able to single out faults from other power-quality disturbances, such as voltage sags and oscillatory transients, which are common in power systems operation. An artificial neural network classifies the fault from the voltage and current waveforms pattern recognition in the time domain. The method has been used for fault detection and classification from real oscillographic data of a Brazilian utility company with excellent results",2006,0,
3180,3181,Virtual instrumentation and its application in diagnosis of faults in power transformers,"This paper presents a new approach to detect, localize and investigate the feasibility of identifying winding insulation failures. The diagnosis is based on the time-frequency analysis of signals recorded during lightning impulse tests. The virtual instrument is implemented with an acquisition board inserted into a PC and with software developed with lab view tools which sample the voltage and current signal and furnish the extent of insulation failure. The acquired signal is decomposed using multiresolution signal decomposition techniques to detect and localize the time instant of occurrence of fault",2000,0,
3181,3182,"Delay faults in dual-rail, self-reset wave-pipelined circuits","This paper presents a method to detect delay faults in wave-pipeline high speed arithmetic circuits that are constructed of dual-rail self-reset logic gates with input-disable. For this category of circuits we develop a fault model and show that standard test pattern generation algorithm can be used after using a 9-valued logic set. Also, we demonstrate that as soon as a delay fault occurs at any stage of the pipeline, the fault is eventually manifested at the output of the circuit as if a stuck-at fault existed in the circuit for that wave.",2007,0,
3182,3183,Error Protection and Interleaving for Wireless Transmission of JPEG 2000 Images and Video,"The transmission of JPEG 2000 images or video over wireless channels has to cope with the high probability and burstyness of errors introduced by Gaussian noise, linear distortions, and fading. At the receiver side, there is distortion due to the compression performed at the sender side, and to the errors introduced in the data stream by the channel. Progressive source coding can also be successfully exploited to protect different portions of the data stream with different channel code rates, based upon the relative importance that each portion has on the reconstructed image. Unequal error protection (UEP) schemes are generally adopted, which offer a close to the optimal solution. In this paper, we present a dichotomic technique for searching the optimal UEP strategy, which lends ideas from existing algorithms, for the transmission of JPEG 2000 images and video over a wireless channel. Moreover, we also adopt a method of virtual interleaving to be used for the transmission of high bit rate streams over packet loss channels, guaranteeing a large PSNR advantage over a plain transmission scheme. These two protection strategies can also be combined to maximize the error correction capabilities.",2009,0,
3183,3184,A novel economical single stage battery charger with power factor correction,"A single stage AC-DC topology with power factor correction is proposed for battery charger applications. Desired features for battery charger such as low cost, fast charging, charge profile programmability, high efficiency and high reliability are fully achieved by means of proposed solution. Additionally, its multiphase operation configuration provides easy power scaling. The proposed approach is superior to conventional ferro-resonant regulation widely used for EV (electrical vehicle) charger applications. It is especially suitable to low cost and high power applications. The feasibility and practical value of the proposed approach are verified by the experimental results from a 1 kW product prototype.",2003,0,
3184,3185,Undetected disk errors in RAID arrays,"Though remarkably reliable, disk drives do fail occasionally. Most failures can be detected immediately; moreover, such failures can be modeled and addressed using technologies such as RAID (Redundant Arrays of Independent Disks). Unfortunately, disk drives can experience errors that are undetected by the drivewhich we refer to as undetected disk errors (UDEs). These errors can cause silent data corruption that may go completely undetected (until a system or application malfunction) or may be detected by software in the storage I/O stack. Continual increases in disk densities or in storage array sizes and more significantly the introduction of desktop-class drives in enterprise storage systems are increasing the likelihood of UDEs in a given system. Therefore, the incorporation of UDE detection (and correction) into storage systems is necessary to prevent increasing numbers of data corruption and data loss events. In this paper, we discuss the causes of UDEs and their effects on data integrity. We describe some of the basic techniques that have been applied to address this problem at various software layers in the I/O stack and describe a family of solutions that can be integrated into the RAID subsystem.",2008,0,
3185,3186,Faults diagnosis for power transformer based on support vector machine,"The power transformer is very important equipment in a power system, and it is necessary to carry through faults diagnosis for it. Support vector machine is a machine learning algorithm based on statistical learning theory, which can get good classification effects with a few learning samples. A new power transformer fault diagnosis method based on support vector machine is presented in this paper. The method has many advantages for transformer faults diagnosis, such as simple algorithm, good classification and high efficiency. This faults diagnosis method finally has been proved by many practical faults data of power transformer. Compared experiment results with the traditional three-ratio method, this method has higher diagnosis right ratio. So it shows that such method is very feasible and is very suitable for power transformer faults diagnosis.",2010,0,
3186,3187,An Antecedence Graph Approach for Fault Tolerance in a Multi-Agent,"In this paper, we propose a strategy to implement fault-tolerance in a multi-agent system. We have based our strategy on the concept of antecedence graphs, used in causal logging and as used by the manetho protocol for distributed systems. Each agent in the multi-agent system keeps an antecedence graph of all the collaborating agents in the system. If one or more agents fail due to any reason, the other agents can reconstruct the same agent state in a partial or comprehensive manner by using their own antecedence graphs. The recovering agents then regenerate their antecedence graphs and message logs and replay the messages to achieve a global consistent state, after which normal operation continues. We believe that introducing fault tolerance in a multi-agent system through antecedence graphs is novel and provides a low overhead and effective solution for fault-tolerance in a multi-agent system.",2006,0,
3187,3188,Investigating the impact of reading techniques on the accuracy of different defect content estimation techniques,"Software inspections have established an impressive track record for early defect detection and correction. To increase their benefits, recent research efforts have focused on two different areas: systematic reading techniques and defect content estimation techniques. While reading techniques are to provide guidance for inspection participants on how to scrutinize a software artifact in a systematic manner, defect content estimation techniques aim at controlling and evaluating the inspection process by providing an estimate of the total number of defects in an inspected document. Although several empirical studies have been conducted to evaluate the accuracy of defect content estimation techniques, only few consider the reading approach as an influential factor. The authors examine the impact of two specific reading techniques: a scenario based reading technique and checklist based reading, on the accuracy of different defect content estimation techniques. The examination is based on data that were collected in a large experiment with students of the Vienna University of Technology. The results suggest that the choice of the reading technique has little impact on the accuracy of defect content estimation techniques. Although more empirical work is necessary to corroborate this finding, it implies that practitioners can use defect content estimation techniques without any consideration of their current reading technique",2001,0,
3188,3189,Maintenance data mining and visualization for fault trend analysis,"This paper describes research efforts currently underway to acquire and analyze test data to determine whether trends and other tendencies may exist that may be indicative of future circuit board failures and potential reduced weapon system readiness. We begin by citing that in today's test environment using test program sets (TPSs) hosted on automatic test equipment (ATE), no provisions are made for capturing or analyzing Unit Under Test (UUT) data, on a large scale. The distributed resources used to perform UUT testing further complicate the situation,since no methodology currently exists that can demonstrate whether trends or events exist in the data that may be indicative of supportability, maintainability, or readiness problems. Our approach. is based upon fulfilling the need to recognize changes in the tolerance of equipment performance. This can be accomplished through the large-scale recording and analysis of test data that can aid in the performance of remote testing and recognition of tolerance changes and other issues that effect diagnostic ability. This would also facilitate taking appropriate corrective action to predict and/or compensate for such behavior before significant mission impact or failure occurs",2001,0,
3189,3190,Designing quantum adder circuits and evaluating their error performance,"With the advent of efficient quantum algorithms and technological advances, design of quantum circuits has gained importance. Minimization of the gate count and the number of gate levels are the two major objectives in quantum circuit design. The peculiar nature of quantum decoherence that leads to quantum errors mandates completion of all the quantum gate operations within a time bound, hence reduction in the gate count and the number of circuit levels leads to lowering the errors and the overall cost in quantum circuits. In this paper, we propose the design of adder circuits using CNOT and C<sup>k</sup>NOT gates, with significant reduction in gate count and number of gate levels over their existing counterparts in the literature. We then present a software model for evaluating errors in quantum computing circuits and employ it for evaluating the error performance of our proposed quantum adder circuits.",2008,0,
3190,3191,Fuzzy logic-based fault-type identification in unbalanced radial power distribution system,"In this paper, a fuzzy logic-based algorithm to identify the type of faults in radial, unbalanced distribution system has been developed. The proposed technique is able to accurately identify the phase(s) involved in all ten types of shunt faults that may occur in an electric power distribution system under different fault types, fault resistance, fault inception angle, system topology and loading levels. The proposed method needs only three line current measurements available at the substation and can perform the fault classification task in about half-cycle period. All the test results show that the proposed fault identifier is well suited for identifying fault types in radial, unbalanced distribution system.",2006,0,
3191,3192,Comparison of TCP to XCP performance on channels with correlated errors employing error correction and interleaving,"This paper compares the performance of the TCP and XCP transport protocols over a lossy, large bandwidth-delay link. The link employs block-based error correction codes ECC, along with interleaving which seems to affect both TCP and XCP bandwidth utilization performance. The results are obtained by numerical simulation of TCP and XCP connections over the channel. Single connection results show that the delay introduced by interleaving significantly affects TCP performance, while XCP performance is somewhat better. However, when large interleavers are employed, both protocols suffer. This is likely due to decorrelation of packet errors introduced by interleaving.",2005,0,
3192,3193,Error resilience tools in the MPEG-4 and H.264 video coding standards,"In this paper we introduce error resilience tools which are used by MPEG-4 and H. 264 video coding standards. MPEG-4 as well as H.264 offers error resilience tools which were implemented by older video coding standards, but they also introduce new tools and improved efficiency of previously used tools. This paper offers only brief review of them, but it focuses on their most important characteristics.",2008,0,
3193,3194,An Autofocus Approach for Residual Motion Errors With Application to Airborne Repeat-Pass SAR Interferometry,"Airborne repeat-pass SAR systems are very sensible to subwavelength deviations from the reference track. To enable repeat-pass interferometry, a high-precision navigation system is needed. Due to the limit of accuracy of such systems, deviations in the order of centimeters remain between the real track and the processed one, causing mainly undesirable phase undulations and misregistration in the interferograms, referred to as residual motion errors. Up to now, only interferometric approaches, as multisquint, are used to compensate for such residual errors. In this paper, we present for the first time the use of the autofocus technique for residual motion errors in the repeat-pass interferometric context. A very robust autofocus technique has to be used to cope with the demands of the repeat-pass applications. We propose a new robust autofocus algorithm based on the weighted least squares phase estimation and the phase curvature autofocus (PCA) extended to the range-dependent case. We call this new algorithm weighted PCA. Different from multisquint, the autofocus approach has the advantage of being able to estimate motion deviations independently, leading to better focused data and correct impulse-response positioning. As a consequence, better coherence and interferometric-phase accuracy are achieved. Repeat-pass interferometry based only on image processing gains in robustness and reliability, since its performance does not deteriorate with time decorrelation and no assumptions need to be made on the interferometric phase. Repeat-pass data of the E-SAR system of the German Aerospace Center (DLR) are used to demonstrate the performance of the proposed approach.",2008,0,
3194,3195,Using Field-Repairable Control Logic to Correct Design Errors in Microprocessors,"Functional correctness is a vital attribute of any hardware design. Unfortunately, due to extremely complex architectures, widespread components, such as microprocessors, are often released with latent bugs. The inability of modern verification tools to handle the fast growth of design complexity exacerbates the problem even further. In this paper, we propose a novel hardware-patching mechanism, called the field-repairable control logic (FRCL), that is designed for in-the-field correction of errors in the design's control logic-the most common type of defects, as our analysis demonstrates. Our solution introduces an additional component in the processor's hardware, a state matcher, that can be programmed to identify erroneous configurations using signals in the critical control state of the processor. Once a flawed configuration is ldquomatched,rdquo the processor switches into a degraded mode, a mode of operation which excludes most features of the system and is simple enough to be formally verified, yet still capable to execute the full instruction-set architecture at one instruction at a time. Once the program segment exposing the design flaw has been executed in a degraded mode, we can switch the processor back to its full-performance mode. In this paper, we analyze a range of approaches to selecting signals comprising the processor's critical control state and evaluate their effectiveness in representing a variety of design errors. We also introduce a new metric (average specificity per signal) that encodes the bug-detection capability and amount of control state of a particular critical signal set. We demonstrate that the FRCL can support the detection and correction of multiple design errors with a performance impact of less than 5% as long as the incidence of the flawed configurations is below 1% of dynamic instructions. In addition, the area impact of our solution is less than 2% for the two microprocessor designs that we investigated in our experiments.",2008,0,
3195,3196,A type system for statically detecting spreadsheet errors,"We describe a methodology for detecting user errors in spreadsheets, using the notion of units as our basic elements of checking. We define the concept of a header and discuss two types of relationships between headers, namely is-a and has-a relationships. With these, we develop a set of rules to assign units to cells in the spreadsheet. We check for errors by ensuring that every cell has a well-formed unit. We describe an implementation of the system that allows the user to check Microsoft Excel spreadsheets. We have run our system on practical examples, and even found errors in published spreadsheets.",2003,0,
3196,3197,A unified approach for fault tolerance and dynamic power management in fixed-priority real-time embedded systems,"This paper investigates an integrated approach for achieving fault tolerance and energy savings in real-time embedded systems. Fault tolerance is achieved via checkpointing, and energy is saved using dynamic voltage scaling (DVS). The authors present a feasibility analysis for checkpointing schemes for a constant processor speed as well as for variable processor speeds. DVS is then carried out on the basis of the feasibility analysis. The authors incorporate important practical issues such as faults during checkpointing, rollback recovery time, memory access time, and energy needed for checkpointing, as well as DVS and context switching overhead. Numerical results based on real-life checkpointing data and processor data sheets show that compared to fault-oblivious methods, the proposed approach significantly reduces power consumption and guarantees timely task completion in the presence of faults.",2006,0,
3197,3198,An approach to system-wide fault tolerance for FPGAs,"This paper deals with the construction of an entire FPGA based and fault-tolerant computer system spanning all layers of modern computer architecture. This starts with the protection of the fundamental FPGA configuration matrix, continues to the HDL design of multiple hardware components, essentially required to run regular applications on FPGAs, including processor, memory and interfaces and ends up in the implementation of an operating system running radiation hardened software. Joining all these separate layers with their individual approaches to fault tolerance increases the overall radiation susceptibility to a maximum value and enables the use in high-energy physics particle accelerators. The current design phase is shown exemplary for a fault-tolerant soft core CPU including validation results.",2009,0,
3198,3199,Disjoint-Paths and Fault-Tolerant Routing on Recursive Dual-Net,"The recursive dual-net is a newly proposed interconnection network for of massive parallel computers. The recursive dual-net is based on a recursive dual-construction of a base network. A k-level dual-construction for k > 0 creates a network containing (2n<sub>0</sub>)<sup>2</sup> <sup>k</sup> nodes with node-degree d<sub>0</sub> + k, where no and do are the number of nodes and the node-degree of the base network, respectively. The recursive dual-net is node and edge symmetric and can contain huge number of nodes with small node-degree and short diameter. Disjoint-paths routing and fault-tolerant routing are fundamental and critical issues for the performance of an interconnection network. In this paper, we propose efficient algorithms for disjoint-paths and fault-tolerant routings on the recursive dual-net.",2009,0,
3199,3200,Analytic representation of eddy current sensor data for fault diagnostics,A complex representation of eddy-current sensor data is proposed and used for the detection of blade fault conditions in turbine engines. The representation is applied to the problem of detecting synchronous vibrations using a single sensor,2005,0,
3200,3201,Invariant checkers: An efficient low cost technique for run-time transient errors detection,"Semiconductor technology evolution brings along higher soft error rates and long duration transients, which require new low cost system level approaches for error detection and mitigation. Known software based error detection techniques imply a high overhead in terms of memory usage and execution times. In this work, the use of software invariants as a means to detect transient errors affecting a system at run-time is proposed. The technique is based on the use of a publicly available tool to automate the invariant detection process, and the decomposition of complex algorithms into simpler ones, which are checked through the verification of their invariants during the execution of the program. A sample program is used as a case study, and fault injection campaigns are performed to verify the error detection capability of the proposed technique. The experimental results show that the proposed technique provides high error detection capability, with low execution time overhead.",2009,0,
3201,3202,Effects of Internal Hole-Defects Location on the Propagation Parameters of Acoustic Wave in the Maple Wood,"To detect the effects of internal hole-defects location on the propagation parameters of acoustic wave in the wood, forty maple wood samples used as the study objects are tested by using PLG (Portable Lumber Grader) instrument in this paper. The propagation velocity and vibration frequency of acoustic wave in intact wood and defective wood were compared, and then the correlation between the propagation velocity or vibration frequency and the elastic modulus were discussed respectively. The analysis results showed that: (1) there were significant positive correlations between the propagation velocity or vibration frequency of acoustic wave and the elastic modulus of the intact and defective maple wood samples; (2) the propagation velocity and vibration frequency of acoustic wave in defective wood samples were lower than those of intact wood samples; and (3) the changes of acoustic wave propagation parameters were different when the location of internal hole-defects of wood samples were different.",2010,0,
